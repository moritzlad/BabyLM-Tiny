confinement effect tends to blueshift the emission spectra as the quantum dot decreases in size. There are 4 major categories of quantum dot heterostructures: type I, inverse type I, type II, and inverse type II. Type I quantum dots are composed of a semiconductor core encapsulated in a second semiconductor material with a larger bandgap, which can passivate non-radiative recombination sites at the surface of the quantum dots and improve quantum yield. Inverse type I quantum dots have a semiconductor layer with a smaller bandgap which leads to delocalized charge carriers in the shell. For type II and inverse type II dots, either the conduction or valence band of the core is located within the bandgap of the shell, which can lead to spatial separation of charge carriers in the core and shell. For all of these core/shell systems, the deposition of the outer layer can lead to potential lattice mismatch, which can limit the ability to grow a thick shell without reducing photoluminescent performance. One such reason for the decrease in performance can be attributed to the physical strain being put on the lattice. In a case where ZnSe/ZnS (type I) and ZnSe/CdS (type II) quantum dots were being compared, the diameter of the uncoated ZnSe core (obtained using TEM) was compared to the capped core diameter (calculated via effective mass approximation model) [lattice strain source] to better understand the effect of core-shell strain. Type I heterostructures were found to induce compressive strain and “squeeze” the core, while the type II heterostructures had the effect of stretching the core under tensile strain. Because the fluorescent properties of quantum dots are dictated by nanocrystal size, induced changes in core dimensions can lead to shifting of emission wavelength, further proving why an intermediate semiconductor layer is necessary to rectify lattice mismatch and improve quantum yield. One such core/double-shell system is the CdSe/ZnSe/ZnS nanocrystal. In a study comparing CdSe/ZnS and CdSe/ZnSe nanocrystals, the former was found to have PL yield 84% of the latter’s, due to a lattice mismatch. To study the double-shell system, after synthesis of the core CdSe nanocrystals, a layer of ZnSe was coated prior to the ZnS outer shell, leading to an improvement in fluorescent efficiency by 70%. Furthermore, the two additional layers were found to improve resistance of the nanocrystals against photo-oxidation, which can contribute to degradation of the emission spectra. It is also standard for surface passivation techniques to be applied to these core/double-shell systems, as well. As mentioned above, oleic acid is one such organic capping ligand that is used to promote colloidal stability and control nanocrystal growth, and can even be used to initiate a second round of ligand exchange and surface functionalization. However, because of the detrimental effect organic ligands have on PL efficiency, further studies have been conducted to obtain all-inorganic quantum dots. In one such study, intensely luminescent all-inorganic nanocrystals (ILANs) were synthesized via a ligand exchange process which substituted metal salts for the oleic acid ligands, and were found to have comparable photoluminescent quantum yields to that of existing red- and green-emitting quantum dots. Production There are several ways to fabricate quantum dots. Possible methods include colloidal synthesis, self-assembly, and electrical gating. Colloidal synthesis Colloidal semiconductor nanocrystals are synthesized from solutions, much like traditional chemical processes. The main difference is the product neither precipitates as a bulk solid nor remains dissolved. Heating the solution at high temperature, the precursors decompose forming monomers which then nucleate and generate nanocrystals. Temperature is a critical factor in determining optimal conditions for the nanocrystal growth. It must be high enough to allow for rearrangement and annealing of atoms during the synthesis process while being low enough to promote crystal growth. The concentration of monomers is another critical factor that has to be stringently controlled during nanocrystal growth. The growth process of nanocrystals can occur in two different regimes: "focusing" and "defocusing". At high monomer concentrations, the critical size (the size where nanocrystals neither grow nor shrink) is relatively small, resulting in growth of nearly all particles. In this regime, smaller particles grow faster than large ones (since larger crystals need more atoms to grow than small crystals) resulting in the size distribution focusing, yielding an improbable distribution of nearly monodispersed particles. The size focusing is optimal when the monomer concentration is kept such that the average nanocrystal size present is always slightly larger than the critical size. Over time, the monomer concentration diminishes, the critical size becomes larger than the average size present, and the distribution defocuses. There are colloidal methods to produce many different semiconductors. Typical dots are made of binary compounds such as lead sulfide, lead selenide, cadmium selenide, cadmium sulfide, cadmium telluride, indium arsenide, and indium phosphide. Dots may also be made from ternary compounds such as cadmium selenide sulfide. Further, recent advances have been made which allow for synthesis of colloidal perovskite quantum dots. These quantum dots can contain as few as 100 to 100,000 atoms within the quantum dot volume, with a diameter of approximately 10 to 50 atom diameters. This corresponds to about 2 to 10 nanometers, and at 10 nm in diameter, nearly 3 million quantum dots could be lined up end to end and fit within the width of a human thumb. Large batches of quantum dots may be synthesized via colloidal synthesis. Due to this scalability and the convenience of benchtop conditions, colloidal synthetic methods are promising for commercial applications. Plasma synthesis Plasma synthesis has evolved to be one of the most popular gas-phase approaches for the production of quantum dots, especially those with covalent bonds. For example, silicon and germanium quantum dots have been synthesized by using nonthermal plasma. The size, shape, surface and composition of quantum dots can all be controlled in nonthermal plasma. Doping that seems quite challenging for quantum dots has also been realized in plasma synthesis. Quantum dots synthesized by plasma are usually in the form of powder, for which surface modification may be carried out. This can lead to excellent dispersion of quantum dots in either organic solvents or water (i. e., colloidal quantum dots). Fabrication The electrostatic potential needed to create a quantum dot can be realized with several methods. These include external electrodes, doping, strain, or impurities. Self-assembled quantum dots are typically between 5 and 50 nm in size. Quantum dots defined by lithographically patterned gate electrodes, or by etching on two-dimensional electron gases in semiconductor heterostructures can have lateral dimensions between 20 and 100 nm. The formation of quantum dots can be spontaneous when a semiconductor material is deposited on a substrate and a difference in lattice space exists between them. By means of advanced nanofabrication technologies it is possible to manipulate properties of the quantum dots, such as their interactions, shape, size and transparency. For example, when negative voltage is applied to a metal gate around a QD, as response, its diameter starts to be gradually squeezed, as a consequence, the number of electrons on the dot starts to decrease one by one, this could be made until there are no more left. The previous property allows to record the current flow as the number of electrons on the dot, this implies that the energy variates. Some quantum dots are small regions of one material buried in another with a larger band gap. These can be so-called core–shell structures, for example, with CdSe in the core and ZnS in the shell, or from special forms of silica called ormosil. Sub-monolayer shells can also be effective ways of passivating the quantum dots, such as PbS cores with sub-monolayer CdS shells. Quantum dots sometimes occur spontaneously in quantum well structures due to monolayer fluctuations in the well's thickness. Self-assembled quantum dots nucleate spontaneously under certain conditions during molecular beam epitaxy (MBE) and metalorganic vapour-phase epitaxy (MOVPE), when a material is grown on a substrate to which it is not lattice matched. The resulting strain leads to the formation of islands on top of a two-dimensional wetting layer. This growth mode is known as Stranski–Krastanov growth. The islands can be subsequently buried to form the quantum dot. A widely used type of quantum dots grown with this method are indium gallium arsenide (InGaAs) quantum dots in gallium arsenide (GaAs). Such quantum dots have the potential for applications in quantum cryptography (that is, single-photon sources) and quantum computation. The main limitations of this method are the cost of fabrication and the lack of control over positioning of individual dots. Individual quantum dots can be created from two-dimensional electron or hole gases present in remotely doped quantum wells or semiconductor heterostructures called lateral quantum dots. The sample surface is coated with a thin layer of resist and a lateral pattern is then defined in the resist by electron beam lithography. This pattern can then be transferred to the electron or hole gas by etching, or by depositing metal electrodes (lift-off process) that allow the application of external voltages between the electron gas and the electrodes. Such quantum dots are mainly of interest for experiments and applications involving electron or hole transport and they are also used as spin qubits. A strength of this type of quantum dots is that their energy spectrum can be engineered by controlling the geometrical size, shape, and the strength of the confinement potential with gate electrodes. These quantum dots can be easily connected by tunnel barriers to conducting leads, which allows the application of the techniques of tunneling spectroscopy for their investigation. Complementary metal–oxide–semiconductor (CMOS) technology can be employed to fabricate silicon quantum dots. Ultra small (20 nm × 20 nm) CMOS transistors behave as single electron quantum dots when operated at cryogenic temperature over a range of −269 °C (4 K) to about −258 °C (15 K). The transistor displays Coulomb blockade due to progressive charging of electrons (holes) one by one. The number of electrons (holes) confined in the channel is driven by the gate voltage, starting from an occupation of zero electrons (holes), and it can be set to one or many. Viral assembly Genetically engineered M13 bacteriophage viruses allow preparation of quantum dot biocomposite structures. It had previously been shown that genetically engineered viruses can recognize specific semiconductor surfaces through the method of selection by combinatorial phage display. Additionally, it is known that liquid crystalline structures of wild-type viruses (Fd, M13, and TMV) are adjustable by controlling the solution concentrations, solution ionic strength, and the external magnetic field applied to the solutions. Consequently, the specific recognition properties of the virus can be used to organize inorganic nanocrystals, forming ordered arrays over the length scale defined by liquid crystal formation. Using this information, Lee et al. (2000) were able to create self-assembled, highly oriented, self-supporting films from a phage and ZnS precursor solution. This system allowed them to vary both the length of bacteriophage and the type of inorganic material through genetic modification and selection. Electrochemical assembly Highly ordered arrays of quantum dots may also be self-assembled by electrochemical techniques. A template is created by causing an ionic reaction at an electrolyte–metal interface which results in the spontaneous assembly of nanostructures, including quantum dots, onto the metal which is then used as a mask for mesa-etching these nanostructures on a chosen substrate. Bulk manufacture Quantum dot manufacturing relies on a process called high temperature dual injection which has been scaled by multiple companies for commercial applications that require large quantities (hundreds of kilograms to tons) of quantum dots. This reproducible production method can be applied to a wide range of quantum dot sizes and compositions. The bonding in certain cadmium-free quantum dots, such as III–V-based quantum dots, is more covalent than that in II–VI materials, therefore it is more difficult to separate nanoparticle nucleation and growth via a high temperature dual injection synthesis. An alternative method of quantum dot synthesis, the molecular seeding process, provides a reproducible route to the production of high-quality quantum dots in large volumes. The process utilises identical molecules of a molecular cluster compound as the nucleation sites for nanoparticle growth, thus avoiding the need for a high temperature injection step. Particle growth is maintained by the periodic addition of precursors at moderate temperatures until the desired particle size is reached. The molecular seeding process is not limited to the production of cadmium-free quantum dots; for example, the process can be used to synthesise kilogram batches of high-quality II–VI quantum dots in just a few hours. Another approach for the mass production of colloidal quantum dots can be seen in the transfer of the well-known hot-injection methodology for the synthesis to a technical continuous flow system. The batch-to-batch variations arising from the needs during the mentioned methodology can be overcome by utilizing technical components for mixing and growth as well as transport and temperature adjustments. For the production of CdSe based semiconductor nanoparticles this method has been investigated and tuned to production amounts of kilograms per month. Since the use of technical components allows for easy interchange in regards of maximum throughput and size, it can be further enhanced to tens or even hundreds of kilograms. In 2011 a consortium of U.S. and Dutch companies reported a milestone in high-volume quantum dot manufacturing by applying the traditional high temperature dual injection method to a flow system. On 23 January 2013 Dow entered into an exclusive licensing agreement with UK-based Nanoco for the use of their low-temperature molecular seeding method for bulk manufacture of cadmium-free quantum dots for electronic displays, and on 24 September 2014 Dow commenced work on the production facility in South Korea capable of producing sufficient quantum dots for "millions of cadmium-free televisions and other devices, such as tablets". Mass production was due to commence in mid-2015. On 24 March 2015, Dow announced a partnership deal with LG Electronics to develop the use of cadmium free quantum dots in displays. Health and safety Some quantum dots pose risks to human health and the environment under certain conditions. Notably, the studies on quantum dot toxicity have focused on particles containing cadmium and have yet to be demonstrated in animal models after physiologically relevant dosing. In vitro studies, based on cell cultures, on quantum dots (QD) toxicity suggest that their toxicity may derive from multiple factors including their physicochemical characteristics (size, shape, composition, surface functional groups, and surface charges) and their environment. Assessing their potential toxicity is complex as these factors include properties such as QD size, charge, concentration, chemical composition, capping ligands, and also on their oxidative, mechanical, and photolytic stability. Many studies have focused on the mechanism of QD cytotoxicity using model cell cultures. It has been demonstrated that after exposure to ultraviolet radiation or oxidation by air, CdSe QDs release free cadmium ions causing cell death. Group II–VI QDs also have been reported to induce the formation of reactive oxygen species after exposure to light, which in turn can damage cellular components such as proteins, lipids, and DNA. Some studies have also demonstrated that addition of a ZnS shell inhibits the process of reactive oxygen species in CdSe QDs. Another aspect of QD toxicity is that there are, in vivo, size-dependent intracellular pathways that concentrate these particles in cellular organelles that are inaccessible by metal ions, which may result in unique patterns of cytotoxicity compared to their constituent metal ions. The reports of QD localization in the cell nucleus present additional modes of toxicity because they may induce DNA mutation, which in turn will propagate through future generation of cells, causing diseases. Although concentration of QDs in certain organelles have been reported in in vivo studies using animal models, no alterations in animal behavior, weight, hematological markers, or organ damage has been found through either histological or biochemical analysis. These findings have led scientists to believe that intracellular dose is the most important determining factor for QD toxicity. Therefore, factors determining the QD endocytosis that determine the effective intracellular concentration, such as QD size, shape, and surface chemistry determine their toxicity. Excretion of QDs through urine in animal models also have demonstrated via injecting radio-labeled ZnS-capped CdSe QDs where the ligand shell was labeled with 99mTc. Though multiple other studies have concluded retention of QDs in cellular levels, exocytosis of QDs is still poorly studied in the literature. While significant research efforts have broadened the understanding of toxicity of QDs, there are large discrepancies in the literature, and questions still remain to be answered. Diversity of this class of material as compared to normal chemical substances makes the assessment of their toxicity very challenging. As their toxicity may also be dynamic depending on the environmental factors such as pH level, light exposure, and cell type, traditional methods of assessing toxicity of chemicals such as LD50 are not applicable for QDs. Therefore, researchers are focusing on introducing novel approaches and adapting existing methods to include this unique class of materials. Furthermore, novel strategies to engineer safer QDs are still under exploration by the scientific community. A recent novelty in the field is the discovery of carbon quantum dots, a new generation of optically active nanoparticles potentially capable of replacing semiconductor QDs, but with the advantage of much lower toxicity. Optical properties Quantum dots have been gaining interest from the scientific community because of their interesting optical properties, the main being band gap tunability. When an electron is excited to the conduction band, it leaves behind a vacancy in the valence band called hole. These two opposite charges are bound by Coulombic interactions in what is called an exciton and their spatitial separation is defined by the exciton Bohr radius. In a nanostructure of comparable size to the exciton Bohr radius, the exciton is physically confined within the semiconductor resulting in an increase of the band gap of the material. This dependence can be predicted using the Brus model. As the confinement energy depends on the quantum dot's size, both absorption onset and fluorescence emission can be tuned by changing the size of the quantum dot during its synthesis. The larger the dot, the redder (lower-energy) its absorption onset and fluorescence spectrum. Conversely, smaller dots absorb and emit bluer (higher-energy) light. Recent articles suggest that the shape of the quantum dot may be a factor in the coloration as well, but as yet not enough information is available . Furthermore, it was shown that the lifetime of fluorescence is determined by the size of the quantum dot. Larger dots have more closely spaced energy levels in which the electron–hole pair can be trapped. Therefore, electron–hole pairs in larger dots live longer causing larger dots to show a longer lifetime. To improve fluorescence quantum yield, quantum dots can be made with shells of a larger bandgap semiconductor material around them. The improvement is suggested to be due to the reduced access of electron and hole to non-radiative surface recombination pathways in some cases, but also due to reduced Auger recombination in others. Applications Quantum dots are particularly promising for optical applications due to their high extinction coefficient and ultrafast optical nonlinearities with potential applications for developing all-optical systems. They operate like a single-electron transistor and show the Coulomb blockade effect. Quantum dots have also been suggested as implementations of qubits for quantum information processing, and as active elements for thermoelectrics. Tuning the size of quantum dots is attractive for many potential applications. For instance, larger quantum dots have a greater spectrum shift toward red compared to smaller dots and exhibit less pronounced quantum properties. Conversely, the smaller particles allow one to take advantage of more subtle quantum effects. Being zero-dimensional, quantum dots have a sharper density of states than higher-dimensional structures. As a result, they have superior transport and optical properties. They have potential uses in diode lasers, amplifiers, and biological sensors. Quantum dots may be excited within a locally enhanced electromagnetic field produced by gold nanoparticles, which then can be observed from the surface plasmon resonance in the photoluminescent excitation spectrum of (CdSe)ZnS nanocrystals. High-quality quantum dots are well suited for optical encoding and multiplexing applications due to their broad excitation profiles and narrow/symmetric emission spectra. The new generations of quantum dots have far-reaching potential for the study of intracellular processes at the single-molecule level, high-resolution cellular imaging, long-term in vivo observation of cell trafficking, tumor targeting, and diagnostics. CdSe nanocrystals are efficient triplet photosensitizers. Laser excitation of small CdSe nanoparticles enables the extraction of the excited state energy from the quantum dots into bulk solution, thus opening the door to a wide range of potential applications such as photodynamic therapy, photovoltaic devices, molecular electronics, and catalysis. Biology In modern biological analysis, various kinds of organic dyes are used. However, as technology advances, greater flexibility in these dyes is sought. To this end, quantum dots have quickly filled in the role, being found to be superior to traditional organic dyes on several counts, one of the most immediately obvious being brightness (owing to the high extinction coefficient combined with a comparable quantum yield to fluorescent dyes) as well as their stability (allowing much less photobleaching). It has been estimated that quantum dots are 20 times brighter and 100 times more stable than traditional fluorescent reporters. For single-particle tracking, the irregular blinking of quantum dots is a minor drawback. However, there have been groups which have developed quantum dots which are essentially nonblinking and demonstrated their utility in single-molecule tracking experiments. The use of quantum dots for highly sensitive cellular imaging has seen major advances. The improved photostability of quantum dots, for example, allows the acquisition of many consecutive focal-plane images that can be reconstructed into a high-resolution three-dimensional image. Another application that takes advantage of the extraordinary photostability of quantum dot probes is the real-time tracking of molecules and cells over extended periods of time. Antibodies, streptavidin, peptides, DNA, nucleic acid aptamers, or small-molecule ligands can be used to target quantum dots to specific proteins on cells. Researchers were able to observe quantum dots in lymph nodes of mice for more than 4 months. Quantum dots can have antibacterial properties similar to nanoparticles and can kill bacteria in a dose-dependent manner. One mechanism by which quantum dots can kill bacteria is through impairing the functions of antioxidative system in the cells and down regulating the antioxidative genes. In addition, quantum dots can directly damage the cell wall. Quantum dots have been shown to be effective against both gram- positive and gram-negative bacteria. Semiconductor quantum dots have also been employed for in vitro imaging of pre-labeled cells. The ability to image single-cell migration in real time is expected to be important to several research areas such as embryogenesis, cancer metastasis, stem cell therapeutics, and lymphocyte immunology. One application of quantum dots in biology is as donor fluorophores in Förster resonance energy transfer, where the large extinction coefficient and spectral purity of these fluorophores make them superior to molecular fluorophores It is also worth noting that the broad absorbance of QDs allows selective excitation of the QD donor and a minimum excitation of a dye acceptor in FRET-based studies. The applicability of the FRET model, which assumes that the Quantum Dot can be approximated as a point dipole, has recently been demonstrated The use of quantum dots for tumor targeting under in vivo conditions employ two targeting schemes: active targeting and passive targeting. In the case of active targeting, quantum dots are functionalized with tumor-specific binding sites to selectively bind to tumor cells. Passive targeting uses the enhanced permeation and retention of tumor cells for the delivery of quantum dot probes. Fast-growing tumor cells typically have more permeable membranes than healthy cells, allowing the leakage of small nanoparticles into the cell body. Moreover, tumor cells lack an effective lymphatic drainage system, which leads to subsequent nanoparticle accumulation. Quantum dot probes exhibit in vivo toxicity. For example, CdSe nanocrystals are highly toxic to cultured cells under UV illumination, because the particles dissolve, in a process known as photolysis, to release toxic cadmium ions into the culture medium. In the absence of UV irradiation, however, quantum dots with a stable polymer coating have been found to be essentially nontoxic. Hydrogel encapsulation of quantum dots allows for quantum dots to be introduced into a stable aqueous solution, reducing the possibility of cadmium leakage. Then again, only little is known about the excretion process of quantum dots from living organisms. In another potential application, quantum dots are being investigated as the inorganic fluorophore for intra-operative detection of tumors using fluorescence spectroscopy. Delivery of undamaged quantum dots to the cell cytoplasm has been a challenge with existing techniques. Vector-based methods have resulted in aggregation and endosomal sequestration of quantum dots while electroporation can damage the semi-conducting particles and aggregate delivered dots in the cytosol. Via cell squeezing, quantum dots can be efficiently delivered without inducing aggregation, trapping material in endosomes, or significant loss of cell viability. Moreover, it has shown that individual quantum dots delivered by this approach are detectable in the cell cytosol, thus illustrating the potential of this technique for single-molecule tracking studies. Photovoltaic devices The tunable absorption spectrum and high extinction coefficients of quantum dots make them attractive for light harvesting technologies such as photovoltaics. Quantum dots may be able to increase the efficiency and reduce the cost of today's typical silicon photovoltaic cells. According to an experimental report from 2004, quantum dots of lead selenide (PbSe) can produce more than one exciton from one high-energy photon via the process of carrier multiplication or multiple exciton generation (MEG). This compares favorably to today's photovoltaic cells which can only manage one exciton per high-energy photon, with high kinetic energy carriers losing their energy as heat. On the other hand, the quantum-confined ground-states of colloidal quantum dots (such as lead sulfide, PbS) incorporated in wider-bandgap host semiconductors (such as perovskite) can allow the generation of photocurrent from photons with energy below the host bandgap, via a two-photon absorption process, offering another approach (termed intermediate band, IB) to exploit a broader range of the solar spectrum and thereby achieve higher photovoltaic efficiency. Colloidal quantum dot photovoltaics would theoretically be cheaper to manufacture, as they can be made using simple chemical reactions. Quantum dot only solar cells Aromatic self-assembled monolayers (SAMs) (such as 4-nitrobenzoic acid) can be used to improve the band alignment at electrodes for better efficiencies. This technique has provided a record power conversion efficiency (PCE) of 10.7%. The SAM is positioned between ZnO–PbS colloidal quantum dot (CQD) film junction to modify band alignment via the dipole moment of the constituent SAM molecule, and the band tuning may be modified via the density, dipole and the orientation of the SAM molecule. Quantum dot in hybrid solar cells Colloidal quantum dots are also used in inorganic–organic hybrid solar cells. These solar cells are attractive because of the potential for low-cost fabrication and relatively high efficiency. Incorporation of metal oxides, such as ZnO, TiO2, and Nb2O5 nanomaterials into organic photovoltaics have been commercialized using full roll-to-roll processing. A 13.2% power conversion efficiency is claimed in Si nanowire/PEDOT:PSS hybrid solar cells. Quantum dot with nanowire in solar cells Another potential use involves capped single-crystal ZnO nanowires with CdSe quantum dots, immersed in mercaptopropionic acid as hole transport medium in order to obtain a QD-sensitized solar cell. The morphology of the nanowires allowed the electrons to have a direct pathway to the photoanode. This form of solar cell exhibits 50–60% internal quantum efficiencies. Nanowires with quantum dot coatings on silicon nanowires (SiNW) and carbon quantum dots. The use of SiNWs instead of planar silicon enhances the antiflection properties of Si. The SiNW exhibits a light-trapping effect due to light trapping in the SiNW. This use of SiNWs in conjunction with carbon quantum dots resulted in a solar cell that reached 9.10% PCE. Graphene quantum dots have also been blended with organic electronic materials to improve efficiency and lower cost in photovoltaic devices and organic light emitting diodes (OLEDs) compared to graphene sheets. These graphene quantum dots were functionalized with organic ligands that experience photoluminescence from UV–visible absorption. Quantum dot anodes for batteries Improvement in batteries Improvements can be seen in the electrical conductivity and charge retention of batteries when QDs are added to anodes. In a comparison made between Pure MnO and MnO doped with quantum dot for the capacity of charge and discharge in (mAh/g) against the number of cycles, it can be seen that battery capacity, or the amount of energy that a battery can hold, is higher in MnO quantum dot-doped batteries than in batteries without, and remains higher after many charging/discharging cycles, taking in consideration a current density of Ag^-1. There exists a constant average difference of around 250 mAh/g in favor of the doped compound for both charge and discharge comparisons, comparing from 0 to 60 cycles, going from 1000 mAh/g to 450 mAh/g in the first 60 cycles for the doped compound, and from 750 mAh/g to 200 mAh/g for the pure MnO. A comparison using Graphene Quantum Dots for a NP-SiAl compound not only shows higher discharge capacities but also an improved electrochemical impedance spectroscopy plot, indicating that the battery has better electrical conductivity. For the case of the NP-SiAl/GQDs, the value of -Z´´/ohm reaches a peak of 300, for 250 Z´/ohm, while for the pure NP-SiAl, the peak of 300 -Z´´/ohm is reached at 650 Z´/ohm. Energy in QDs In terms of energy, each individual quantum dot presents an energy level which is compared to that of an atom.Extending this property, an artificial lattice (made out of QDs) would have an energy band structure similar to the one of a crystalline semiconductor. The energy level of a dot is dependent on the amount of charge in it and its capacitance.The energy present in electrons is proportional to the square of the wavelength, which makes the energy levels to rise quickly Common QDs for batteries Carbon quantum dots and Graphite quantum dots are the main types of quantum dots used in batteries. The graphene quantum dots are made out of graphene sheets which are attached among them, forming a morphology similar to a 2D-disk. The carbon quantum dots have an isotropic spherical structure and are made out of crystalline and amorphous carbon sheets. From these common quantum dots, the graphene ones, are usually more crystalline than the carbon ones, this is because they have the crystallinity of a mono-layered and few-layered graphene. Light-emitting diodes Several methods are proposed for using quantum dots to improve existing light-emitting diode (LED) design, including quantum dot light-emitting diode (QD-LED or QLED) displays, and quantum dot white-light-emitting diode (QD-WLED) displays. Because quantum dots naturally produce monochromatic light, they can be more efficient than light sources which must be color filtered. QD-LEDs can be fabricated on a silicon substrate, which allows them to be integrated onto standard silicon-based integrated circuits or microelectromechanical systems. Quantum dot displays Quantum dots are valued for displays because they emit light in very specific Gaussian distributions. This can result in a display with visibly more accurate colors. A conventional color liquid crystal display (LCD) is usually backlit by fluorescent lamps (CCFLs) or conventional white LEDs that are color filtered to produce red, green, and blue pixels. Quantum dot displays use blue-emitting LEDs rather than white LEDs as the light sources. The converting part of the emitted light is converted into pure green and red light by the corresponding color quantum dots placed in front of the blue LED or using a quantum dot infused diffuser sheet in the backlight optical stack. Blank pixels are also used to allow the blue LED light to still generate blue hues. This type of white light as the backlight of an LCD panel allows for the best color gamut at lower cost than an RGB LED combination using three LEDs. Another method by which quantum dot displays can be achieved is the electroluminescent (EL) or electro-emissive method. This involves embedding quantum dots in each individual pixel. These are then activated and controlled via an electric current application. Since this is often light emitting itself, the achievable colors may be limited in this method. Electro-emissive QD-LED TVs exist in laboratories only. The ability of QDs to precisely convert and tune a spectrum makes them attractive for LCD displays. Previous LCD displays can waste energy converting red-green poor, blue-yellow rich white light into a more balanced lighting. By using QDs, only the necessary colors for ideal images are contained in the screen. The result is a screen that is brighter, clearer, and more energy-efficient. The first commercial application of quantum dots was the Sony XBR X900A series of flat panel televisions released in 2013. In June 2006, QD Vision announced technical success in making a proof-of-concept quantum dot display and show a bright emission in the visible and near infrared region of the spectrum. A QD-LED integrated at a scanning microscopy tip was used to demonstrate fluorescence near-field scanning optical microscopy (NSOM) imaging. Photodetector devices Quantum dot photodetectors (QDPs) can be fabricated either via solution-processing, or from conventional single-crystalline semiconductors. Conventional single-crystalline semiconductor QDPs are precluded from integration with flexible organic electronics due to the incompatibility of their growth conditions with the process windows required by organic semiconductors. On the other hand, solution-processed QDPs can be readily integrated with an almost infinite variety of substrates, and also postprocessed atop other integrated circuits. Such colloidal QDPs have potential applications in visible- and infrared-light cameras, machine vision, industrial inspection, spectroscopy, and fluorescent biomedical imaging. Photocatalysts Quantum dots also function as photocatalysts for the light driven chemical conversion of water into hydrogen as a pathway to solar fuel. In photocatalysis, electron hole pairs formed in the dot under band gap excitation drive redox reactions in the surrounding liquid. Generally, the photocatalytic activity of the dots is related to the particle size and its degree of quantum confinement. This is because the band gap determines the chemical energy that is stored in the dot in the excited state. An obstacle for the use of quantum dots in photocatalysis is the presence of surfactants on the surface of the dots. These surfactants (or ligands) interfere with the chemical reactivity of the dots by slowing down mass transfer and electron transfer processes. Also, quantum dots made of metal chalcogenides are chemically unstable under oxidizing conditions and undergo photo corrosion reactions. Fundamental Materials Science Quantum dots can also be used to study fundamental effects in materials science. By coupling two or more such quantum dots, an artificial molecule can be made, exhibiting hybridization even at room temperature. Precise assembly of quantum dots can form superlattices that act as artificial solid-state materials that exhibit unique optical and electronic properties. Theory Quantum dots are theoretically described as a point-like, or zero dimensional (0D) entity. Most of their properties depend on the dimensions, shape, and materials of which QDs are made. Generally, QDs present different thermodynamic properties from their bulk materials. One of these effects is melting-point depression. Optical properties of spherical metallic QDs are well described by the Mie scattering theory. Quantum confinement in semiconductors The energy levels of a single particle in a quantum dot can be predicted using the particle in a box model in which the energies of states depend on the length of the box. For an exciton inside a quantum dot, there is also the Coulomb interaction between the negatively charged electron and the positively charged hole. By comparing the quantum dot's size to the exciton Bohr radius, three regimes can be defined. In the 'strong confinement regime', the quantum dot's radius is much smaller than the exciton Bohr radius, respectively the confinement energy dominates over the Coulomb interaction. In the 'weak confinement' regime, the quantum dot is larger than the exciton Bohr radius, respectively the confinement energy is smaller than the Coulomb interactions between electron and hole. The regime where the exciton Bohr radius and confinement potential are comparable is called the 'intermediate confinement regime'. Band gap energy The band gap can become smaller in the strong confinement regime as the energy levels split up. The exciton Bohr radius can be expressed as: a B ∗ = ε r ( m μ ) a B {\displaystyle a_{\rm {B}}^{*}=\varepsilon _{\rm {r}}\left({\frac {m}{\mu }}\right)a_{\rm {B}}} where aB = 0.053 nm is the Bohr radius, m is the mass, μ is the reduced mass, and εr is the size-dependent dielectric constant (relative permittivity). This results in the increase in the total emission energy (the sum of the energy levels in the smaller band gaps in the strong confinement regime is larger than the energy levels in the band gaps of the original levels in the weak confinement regime) and the emission at various wavelengths. If the size distribution of QDs is not enough peaked, the convolution of multiple emission wavelengths is observed as a continuous spectra. Confinement energy The exciton entity can be modeled using the particle in the box. The electron and the hole can be seen as hydrogen in the Bohr model with the hydrogen nucleus replaced by the hole of positive charge and negative electron mass. Then the energy levels of the exciton can be represented as the solution to the particle in a box at the ground level (n = 1) with the mass replaced by the reduced mass. Thus by varying the size of the quantum dot, the confinement energy of the exciton can be controlled. Bound exciton energy There is Coulomb attraction between the negatively charged electron and the positively charged hole. The negative energy involved in the attraction is proportional to Rydberg's energy and inversely proportional to square of the size-dependent dielectric constant of the semiconductor. When the size of the semiconductor crystal is smaller than the exciton Bohr radius, the Coulomb interaction must be modified to fit the situation. Therefore, the sum of these energies can be represented by Brus equation: E confinement = ℏ 2 π 2 2 a 2 ( 1 m e + 1 m h ) = ℏ 2 π 2 2 μ a 2 E exciton = − 1 ε r 2 μ m e R y = − R y ∗ E = E bandgap + E confinement + E exciton = E bandgap + ℏ 2 π 2 2 μ a 2 − R y ∗ {\displaystyle {\begin{aligned}E_{\textrm {confinement}}&={\frac {\hbar ^{2}\pi ^{2}}{2a^{2}}}\left({\frac {1}{m_{\rm {e}}}}+{\frac {1}{m_{\rm {h}}}}\right)={\frac {\hbar ^{2}\pi ^{2}}{2\mu a^{2}}}\\[6px]E_{\textrm {exciton}}&=-{\frac {1}{\varepsilon _{\rm {r}}^{2}}}{\frac {\mu }{m_{\rm {e}}}}R_{y}=-R_{y}^{*}\\[6px]E&=E_{\textrm {bandgap}}+E_{\textrm {confinement}}+E_{\textrm {exciton}}\\&=E_{\textrm {bandgap}}+{\frac {\hbar ^{2}\pi ^{2}}{2\mu a^{2}}}-R_{y}^{*}\end{aligned}}} where μ is the reduced mass, a is the radius of the quantum dot, me is the free electron mass, mh is the hole mass, and εr is the size-dependent dielectric constant. Although the above equations were derived using simplifying assumptions, they imply that the electronic transitions of the quantum dots will depend on their size. These quantum confinement effects are apparent only below the critical size. Larger particles do not exhibit this effect. This effect of quantum confinement on the quantum dots has been repeatedly verified experimentally and is a key feature of many emerging electronic structures. The Coulomb interaction between confined carriers can also be studied by numerical means when results unconstrained by asymptotic approximations are pursued. Besides confinement in all three dimensions (that is, a quantum dot), other quantum confined semiconductors include: Quantum wires, which confine electrons or holes in two spatial dimensions and allow free propagation in the third. Quantum wells, which confine electrons or holes in one dimension and allow free propagation in two dimensions. Models A variety of theoretical frameworks exist to model optical, electronic, and structural properties of quantum dots. These may be broadly divided into quantum mechanical, semiclassical, and classical. Quantum mechanics Quantum mechanical models and simulations of quantum dots often involve the interaction of electrons with a pseudopotential or random matrix. Semiclassical Semiclassical models of quantum dots frequently incorporate a chemical potential. For example, the thermodynamic chemical potential of an N-particle system is given by μ ( N ) = E ( N ) − E ( N − 1 ) {\displaystyle \mu (N)=E(N)-E(N-1)} whose energy terms may be obtained as solutions of the Schrödinger equation. The definition of capacitance, 1 C ≡ Δ V Δ Q {\displaystyle {\frac {1}{C}}\equiv {\frac {\Delta V}{\Delta Q}}} with the potential difference Δ V = Δ μ e = μ ( N + Δ N ) − μ ( N ) e {\displaystyle \Delta V={\frac {\Delta \mu }{e}}={\frac {\mu (N+\Delta N)-\mu (N)}{e}}} may be applied to a quantum dot with the addition or removal of individual electrons, Δ N = 1 and Δ Q = e {\displaystyle \Delta N=1\quad {\text{and}}\quad \Delta Q=e} Then C ( N ) = e 2 μ ( N + 1 ) − μ ( N ) = e 2 I ( N ) − A ( N ) {\displaystyle C(N)={\frac {e^{2}}{\mu (N+1)-\mu (N)}}={\frac {e^{2}}{I(N)-A(N)}}} is the quantum capacitance of a quantum dot, where we denoted by I(N) the ionization potential and by A(N) the electron affinity of the N-particle system. Classical mechanics Classical models of electrostatic properties of electrons in quantum dots are similar in nature to the Thomson problem of optimally distributing electrons on a unit sphere. The classical electrostatic treatment of electrons confined to spherical quantum dots is similar to their treatment in the Thomson, or plum pudding model, of the atom. The classical treatment of both two-dimensional and three-dimensional quantum dots exhibit electron shell-filling behavior. A "periodic table of classical artificial atoms" has been described for two-dimensional quantum dots. As well, several connections have been reported between the three-dimensional Thomson problem and electron shell-filling patterns found in naturally occurring atoms found throughout the periodic table. This latter work originated in classical electrostatic modeling of electrons in a spherical quantum dot represented by an ideal dielectric sphere. History For thousands of years, glassmakers were able to make colored glass by adding different dusts and powdered elements such as silver, gold and cadmium and then used different temperatures to produce shades of glass. In the 19th century, scientists started to understand how glass color depended on elements and heating-cooling techniques. It was also found that for the same element and preparation, the color depended on the dust particles' size. Herbert Fröhlich in the 1930s first explored the idea that material properties can depend on the macroscopic dimensions of a small particle due to quantum size effects. The first quantum dots were synthesized in a glass matrix by Alexei A. Onushchenko and Alexey Ekimov in 1981 at the Vavilov State Optical Institute and independently in colloidal suspension by Louis E. Brus team at Bell Labs in 1983. They were first theorized by Alexander Efros in 1982. It was quickly identified that the optical changes that appeared for very small particles were due to quantum mechanical effects. The term quantum dot first appeared in a paper first authored by Mark Reed in 1986. According to Brus, the term "quantum dot" was coined by Daniel S. Chemla while they were working at Bell Labs. In 1993, David J. Norris, Christopher B. Murray and Moungi Bawendi at the Massachusetts Institute of Technology reported on a hot-injection synthesis method for producing reproducible quantum dots with well-defined size and with high optical quality. The method opened the door to the development of large-scale technological applications of quantum dots in a wide range of areas. The Nobel Prize in Chemistry 2023 was awarded to Moungi Bawendi, Louis E. Brus and Alexey Ekimov "for the discovery and synthesis of quantum dots." See also References Further reading External links Quantum Dots: Technical Status and Market Prospects Quantum dots that produce white light could be the light bulb's successor Single quantum dots optical properties Quantum dot on arxiv.org Quantum Dots Research and Technical Data Simulation and interactive visualization of Quantum Dots wave function The Kapitza–Dirac effect is a quantum mechanical effect consisting of the diffraction of matter by a standing wave of light, in complete analogy to the diffraction of light by a periodic grating, but with the role of matter and light reversed. The effect was first predicted as the diffraction of electrons from a standing wave of light by Paul Dirac and Pyotr Kapitsa (or Peter Kapitza) in 1933. The effect relies on the wave–particle duality of matter as stated by the de Broglie hypothesis in 1924. The matter-wave diffraction by a standing wave of light was first observed using a beam of neutral atoms. Later, the Kapitza-Dirac effect as originally proposed was observed in 2001. Overview In 1924, French physicist Louis de Broglie postulated that matter exhibits a wave-like nature given by: λ = h p , {\displaystyle \lambda ={\frac {h}{p}},} where h is the Planck constant, and p is the particle's momentum, and λ is the wavelength of the matter wave. From this, it follows that interference effects between particles of matter will occur. This forms the basis of the Kapitza–Dirac effec: the diffraction of matter wave due to a standing wave of light. A coherent beam of light will diffract into several peaks once it passes through a periodic diffraction grating. Due to matter-wave duality, the matter can be diffracted by a periodic diffraction grating as well. Such a diffraction grating can be made out of physical matter, but can also be created by a standing wave of light formed by a pair of counterpropagating light beams, due light-matter interaction. Here, the standing wave of light forms the spatially periodic grating that will diffract the matter wave, as we will now explain. The original idea proposes that a beam of electron can be diffracted by a standing wave formed by a superposition of two counterpropagating beams of light. The diffraction is caused by light-matter interaction. In this case, each electron absorbs a photon from one of the beams, and re-emits a photon into the other beam traveling to the opposite direction. This describes a stimulated Compton scattering of photons by the electrons, since the re-emission here is stimulated by the presence of a second beam of light. Due to the nature of the stimulated Compton scattering, the re-emitted photon must carry the same frequency and opposite direction of the absorbed one. Consequently, the momentum transferred to the electron must have a magnitude of 2 ℏ k {\displaystyle 2\hbar k} where k {\displaystyle k} is the wavevector of the light forming the standing wave pattern. Although the original proposal focused on electrons, the above analysis can be generalized to other types of matter waves that interacts with the light. Cold neutral atoms, for example, can also experience the Kapitza-Dirac effect. Indeed, one of the first observations of Kapitza-Dirac effect was using a beams of cold sodium atoms. Today, the Kapitza-Dirac effect is a standard tool to calibrate the depth of optical lattices which are formed by standing waves of light. Different regimes of diffraction Diffraction from a periodic grating, regardless of electromagnetic or matter wave, can be roughly divided into two regimes: the Bragg regime and Raman-Nath regime. In the Bragg regime, essentially only one diffraction peak is produced. In the Raman-Nath regime, multiple diffraction peaks can be observed. It is helpful to go back to the familiar example of light diffraction from a matter grating. In this case, The Bragg regime is reached with a thick grating, whereas the Raman-Nath regime is obtained with a thin grating. The same language can be applied to Kapitza-Dirac effect. Here, the concept of "thickness" of the grating can be transferred to the amount of time the matter wave spent in the light field. Here we give an example in the Raman-Nath regime, where the matter spends an amount of time in the standing wave that is short compared to the so-called recoil frequency of the particle. This approximation holds if the interaction time is less than the inverse of the recoil frequency of the particle, τ ≪ 1 / ω rec {\displaystyle \tau \ll 1/\omega _{\text{rec}}} where ω rec = ℏ k 2 2 m {\displaystyle \omega _{\text{rec}}={\frac {\hbar k^{2}}{2m}}} . A coherent beam of particles incident on a standing wave of electromagnetic radiation (typically light) will be diffracted according to the equation: n λ = 2 d sin ⁡ Θ , {\displaystyle n\lambda =2d\sin \Theta ,} where n is an integer, λ is the de Broglie wavelength of the incident particles, d is the spacing of the grating and θ is the angle of incidence. Diffraction pattern in the Raman-Nath regime Here we present an analysis of the diffraction pattern of the Kapitza-Dirac effect in the Raman-Nath regime For a matter wave interacting in a standing wave of light, the effect of the light-matter interaction can be parametrized by the potential energy U ( z , t ) = U 0 f ( t ) sin 2 ⁡ ( k z ) , {\displaystyle U(z,t)=U_{0}f(t)\sin ^{2}(kz),} where U 0 {\displaystyle U_{0}} is the strength of the potential energy, and f ( t ) {\displaystyle f(t)} describes the pulse shape of applied standing wave. For example, for ultracold atoms trapped in an optical lattice, U 0 = ℏ ω R 2 δ {\displaystyle U_{0}={\frac {\hbar \omega _{\text{R}}^{2}}{\delta }}} due to the AC Stark shift. As described previously, the Raman-Nath regime is reached when the duration is short. In this case, the kinetic energy can be ignored and the resulting Schrodinger equation is greatly simplified. For a given initial state | ψ 0 ⟩ {\displaystyle \left|\psi _{0}\right\rangle } , the time-evolution within the Raman-Nath regime is then given by | ψ ⟩ = | ψ 0 ⟩ e − i ℏ ∫ d t ′ U ( z , t ′ ) = | ψ 0 ⟩ e − i 2 δ ω R 2 τ e i 2 δ ω R 2 τ cos ⁡ ( 2 k z ) , {\displaystyle \left|\psi \right\rangle =\left|\psi _{0}\right\rangle e^{-{\frac {i}{\hbar }}\int dt'U(z,t')}=\left|\psi _{0}\right\rangle e^{-{\frac {i}{2\delta }}\omega _{\text{R}}^{2}\tau }e^{{\frac {i}{2\delta }}\omega _{\text{R}}^{2}\tau \cos(2kz)},} where τ = ∫ d t ′ f 2 ( t ′ ) {\textstyle \tau =\int dt'f^{2}(t')} and the integral is over the duration of the interaction. Using the Jacobi–Anger expansion for Bessel functions of the first kind, e i α cos ⁡ ( β ) = ∑ n = − ∞ ∞ i n J n ( α ) e i n β {\textstyle e^{i\alpha \cos(\beta )}=\sum _{n=-\infty }^{\infty }i^{n}J_{n}(\alpha )e^{in\beta }} , the above wavefunction becomes | ψ ⟩ = | ψ 0 ⟩ e − i 2 δ ω R 2 τ ∑ n = − ∞ ∞ i n J n ( ω R 2 2 δ τ ) e i 2 n k z = e − i 2 δ ω R 2 τ ∑ n = − ∞ ∞ i n J n ( ω R 2 2 δ τ ) | g , 2 n ℏ k ⟩ {\displaystyle {\begin{aligned}\left|\psi \right\rangle &=\left|\psi _{0}\right\rangle e^{-{\frac {i}{2\delta }}\omega _{\text{R}}^{2}\tau }\sum _{n=-\infty }^{\infty }i^{n}J_{n}\left({\frac {\omega _{\text{R}}^{2}}{2\delta }}\tau \right)e^{i2nkz}\\&=e^{-{\frac {i}{2\delta }}\omega _{\text{R}}^{2}\tau }\sum _{n=-\infty }^{\infty }i^{n}J_{n}\left({\frac {\omega _{\text{R}}^{2}}{2\delta }}\tau \right)\left|g,2n\hbar k\right\rangle \end{aligned}}} where in the second line | ψ 0 ⟩ {\displaystyle |\psi _{0}\rangle } has been taken to be | g , 0 ⟩ {\displaystyle |g,0\rangle } . It can now be seen that 2 n ℏ k {\displaystyle 2n\hbar k} momentum states are populated with a probability of P n = J n 2 ( θ ) {\displaystyle P_{n}=J_{n}^{2}(\theta )} where n = 0 , ± 1 , ± 2 , … {\displaystyle n=0,\pm 1,\pm 2,\ldots } and the pulse area (duration and amplitude of the interaction) θ = ω R 2 2 δ τ = ω R ( 2 ) τ {\textstyle \theta ={\frac {\omega _{\text{R}}^{2}}{2\delta }}\tau =\omega _{\text{R}}^{(2)}\tau } . The transverse RMS momentum of the diffracted particles is therefore linearly proportional to the pulse area: p rms = ∑ n = − ∞ ∞ ( n ℏ k ) 2 P n = 2 θ ℏ k . {\displaystyle p_{\text{rms}}=\sum _{n=-\infty }^{\infty }(n\hbar k)^{2}P_{n}={\sqrt {2}}\theta \hbar k.} Realisation The invention of the laser in 1960 allowed the production of coherent light and therefore the ability to construct the standing waves of light that are required to observe the effect experimentally. Kapitsa–Dirac scattering of sodium atoms by a near resonant standing wave laser field was experimentally demonstrated in 1985 by the group of D. E. Pritchard at the Massachusetts Institute of Technology. A supersonic atomic beam with sub-recoil transverse momentum was passed through a near resonant standing wave and diffraction up to 10ħk was observed. The scattering of electrons by an intense optical standing wave was experimentally realised by the group of M. Bashkansky at AT&T Bell Laboratories, New Jersey, in 1988. The Kapitza-Dirac effect is routinely used in calibration of the depth of the optical lattices. == References == In quantum mechanics, the Hilbert space is the space of complex-valued functions belonging to L 2 ( R 3 , d 3 x ) {\displaystyle L^{2}(\mathbb {R} ^{3},d^{3}x)} , where the simple R 3 {\displaystyle \mathbb {R} ^{3}} is the classical configuration space of free particle which has finite degrees of freedom, and d 3 x {\displaystyle d^{3}x} is the Lebesgue measure on R 3 {\displaystyle \mathbb {R} ^{3}} . In the quantum mechanics the domain space of the wave functions ψ {\displaystyle \psi } is the classical configuration space R 3 {\displaystyle \mathbb {R} ^{3}} . In classical field theory, the configuration space of the field is an infinite-dimensional space. The single point denoted A {\displaystyle A} in this space is represented by the set of functions A I ( x → ) ∈ R 3 {\displaystyle A_{I}({\vec {x}})\in \mathbb {R} ^{3}} where x → ∈ R 3 {\displaystyle {\vec {x}}\in \mathbb {R} ^{3}} and I {\displaystyle I} represents an index set. In quantum field theory, it is expected that the Hilbert space is also the L 2 {\displaystyle L^{2}} space on the configuration space of the field, which is infinite dimensional, with respect to some Borel measure naturally defined. However, it is often hard to define a concrete Borel measure on the classical configuration space, since the integral theory on infinite dimensional space is involved. Thus the intuitive expectation should be modified, and the concept of quantum configuration space should be introduced as a suitable enlargement of the classical configuration space so that an infinite dimensional measure, often a cylindrical measure, can be well defined on it. In quantum field theory, the quantum configuration space, the domain of the wave functions Ψ {\displaystyle \Psi } , is larger than the classical configuration space. While in the classical theory we can restrict ourselves to suitably smooth fields, in quantum field theory we are forced to allow distributional field configurations. In fact, in quantum field theory physically interesting measures are concentrated on distributional configurations. That physically interesting measures are concentrated on distributional fields is the reason why in quantum theory fields arise as operator-valued distributions. The example of a scalar field can be found in the references == References == Compton scattering (or the Compton effect) is the quantum theory of scattering of a high-frequency photon through an interaction with a charged particle, usually an electron. Specifically, when the photon interacts with a loosely bound electron, it releases the electron from an outer valence shell of an atom or molecule. The effect was discovered in 1923 by Arthur Holly Compton while researching the scattering of X-rays by light elements, which earned him the Nobel Prize in Physics in 1927. The Compton effect significantly deviated from dominating classical theories, using both special relativity and quantum mechanics to explain the interaction between high frequency photons and charged particles. Photons can interact with matter at the atomic level (e.g. photoelectric effect and Rayleigh scattering), at the nucleus, or with only an electron. Pair production and the Compton effect occur at the level of the electron. When a high-frequency photon scatters due to an interaction with a charged particle, the photon's energy is reduced, and thus its wavelength is increased. This trade-off between wavelength and energy in response to the collision is the Compton effect. Because of conservation of energy, the energy that is lost by the photon is transferred to the recoiling particle (such an electron would be called a "Compton recoil electron"). This implies that if the recoiling particle initially carried more energy than the photon has, the reverse would occur. This is known as inverse Compton scattering, in which the scattered photon increases in energy. Introduction In Compton's original experiment (see Fig. 1), the energy of the X-ray photon (≈ 17 keV) was significantly larger than the binding energy of the atomic electron, so the electrons could be treated as being free after scattering. The amount by which the light's wavelength changes is called the Compton shift. Although Compton scattering from a nucleus exists, Compton scattering usually refers to the interaction involving only the electrons of an atom. The Compton effect was observed by Arthur Holly Compton in 1923 at Washington University in St. Louis and further verified by his graduate student Y. H. Woo in the years following. Compton was awarded the 1927 Nobel Prize in Physics for the discovery. The effect is significant because it demonstrates that light cannot be explained purely as a wave phenomenon. Thomson scattering, the classical theory of an electromagnetic wave scattered by charged particles, cannot explain shifts in wavelength at low intensity: classically, light of sufficient intensity for the electric field to accelerate a charged particle to a relativistic speed will cause radiation-pressure recoil and an associated Doppler shift of the scattered light, but the effect would become arbitrarily small at sufficiently low light intensities regardless of wavelength. Thus, if we are to explain low-intensity Compton scattering, light must behave as if it consists of particles. Or the assumption that the electron can be treated as free is invalid resulting in the effectively infinite electron mass equal to the nuclear mass (see e.g. the comment below on elastic scattering of X-rays being from that effect). Compton's experiment convinced physicists that light can be treated as a stream of particle-like objects (quanta called photons), whose energy is proportional to the light wave's frequency. As shown in Fig. 2, the interaction between an electron and a photon results in the electron being given part of the energy (making it recoil), and a photon of the remaining energy being emitted in a different direction from the original, so that the overall momentum of the system is also conserved. If the scattered photon still has enough energy, the process may be repeated. In this scenario, the electron is treated as free or loosely bound. Experimental verification of momentum conservation in individual Compton scattering processes by Bothe and Geiger as well as by Compton and Simon has been important in disproving the BKS theory. Compton scattering is commonly described as inelastic scattering. This is because, unlike the more common Thomson scattering that happens at the low-energy limit, the energy in the scattered photon in Compton scattering is less than the energy of the incident photon. As the electron is typically weakly bound to the atom, the scattering can be viewed from either the perspective of an electron in a potential well, or as an atom with a small ionization energy. In the former perspective, energy of the incident photon is transferred to the recoil particle, but only as kinetic energy. The electron gains no internal energy, respective masses remain the same, the mark of an elastic collision. From this perspective, Compton scattering could be considered elastic because the internal state of the electron does not change during the scattering process. In the latter perspective, the atom's state is changed, constituting an inelastic collision. Whether Compton scattering is considered elastic or inelastic depends on which perspective is being used, as well as the context. Compton scattering is one of four competing processes when photons interact with matter. At energies of a few eV to a few keV, corresponding to visible light through soft X-rays, a photon can be completely absorbed and its energy can eject an electron from its host atom, a process known as the photoelectric effect. High-energy photons of 1.022 MeV and above may bombard the nucleus and cause an electron and a positron to be formed, a process called pair production; even-higher-energy photons (beyond a threshold energy of at least 1.670 MeV, depending on the nuclei involved), can eject a nucleon or alpha particle from the nucleus in a process called photodisintegration. Compton scattering is the most important interaction in the intervening energy region, at photon energies greater than those typical of the photoelectric effect but less than the pair-production threshold. Description of the phenomenon By the early 20th century, research into the interaction of X-rays with matter was well under way. It was observed that when X-rays of a known wavelength interact with atoms, the X-rays are scattered through an angle θ {\displaystyle \theta } and emerge at a different wavelength related to θ {\displaystyle \theta } . Although classical electromagnetism predicted that the wavelength of scattered rays should be equal to the initial wavelength, multiple experiments had found that the wavelength of the scattered rays was longer (corresponding to lower energy) than the initial wavelength. In 1923, Compton published a paper that explained the X-ray shift by attributing particle-like momentum to light quanta (Albert Einstein had proposed light quanta in 1905 in explaining the photo-electric effect, but Compton did not build on Einstein's work). The energy of light quanta depends only on the frequency of the light. In his paper, Compton derived the mathematical relationship between the shift in wavelength and the scattering angle of the X-rays by assuming that each scattered X-ray photon interacted with only one electron. His paper concludes by reporting on experiments which verified his derived relation: λ ′ − λ = h m e c ( 1 − cos ⁡ θ ) , {\displaystyle \lambda '-\lambda ={\frac {h}{m_{\text{e}}c}}(1-\cos {\theta }),} where λ {\displaystyle \lambda } is the initial wavelength, λ ′ {\displaystyle \lambda '} is the wavelength after scattering, h {\displaystyle h} is the Planck constant, m e {\displaystyle m_{\text{e}}} is the electron rest mass, c {\displaystyle c} is the speed of light, and θ {\displaystyle \theta } is the scattering angle. The quantity ⁠h/mec⁠ is known as the Compton wavelength of the electron; it is equal to 2.43×10−12 m. The wavelength shift λ′ − λ is at least zero (for θ = 0°) and at most twice the Compton wavelength of the electron (for θ = 180°). Compton found that some X-rays experienced no wavelength shift despite being scattered through large angles; in each of these cases the photon failed to eject an electron. Thus the magnitude of the shift is related not to the Compton wavelength of the electron, but to the Compton wavelength of the entire atom, which can be upwards of 10000 times smaller. This is known as "coherent" scattering off the entire atom since the atom remains intact, gaining no internal excitation. In Compton's original experiments the wavelength shift given above was the directly measurable observable. In modern experiments it is conventional to measure the energies, not the wavelengths, of the scattered photons. For a given incident energy E γ = h c / λ {\displaystyle E_{\gamma }=hc/\lambda } , the outgoing final-state photon energy, E γ ′ {\displaystyle E_{\gamma ^{\prime }}} , is given by E γ ′ = E γ 1 + ( E γ / m e c 2 ) ( 1 − cos ⁡ θ ) . {\displaystyle E_{\gamma ^{\prime }}={\frac {E_{\gamma }}{1+(E_{\gamma }/m_{\text{e}}c^{2})(1-\cos \theta )}}.} Derivation of the scattering formula A photon γ with wavelength λ collides with an electron e in an atom, which is treated as being at rest. The collision causes the electron to recoil, and a new photon γ′ with wavelength λ′ emerges at angle θ from the photon's incoming path. Let e′ denote the electron after the collision. Compton allowed for the possibility that the interaction would sometimes accelerate the electron to speeds sufficiently close to the velocity of light as to require the application of Einstein's special relativity theory to properly describe its energy and momentum. At the conclusion of Compton's 1923 paper, he reported results of experiments confirming the predictions of his scattering formula, thus supporting the assumption that photons carry momentum as well as quantized energy. At the start of his derivation, he had postulated an expression for the momentum of a photon from equating Einstein's already established mass-energy relationship of E = mc2 to the quantized photon energies of hf, which Einstein had separately postulated. If mc2 = hf, the equivalent photon mass must be hf/c2. The photon's momentum is then simply this effective mass times the photon's frame-invariant velocity c. For a photon, its momentum p = h f / c {\displaystyle p=hf/c} , and thus hf can be substituted for pc for all photon momentum terms which arise in course of the derivation below. The derivation which appears in Compton's paper is more terse, but follows the same logic in the same sequence as the following derivation. The conservation of energy E merely equates the sum of energies before and after scattering. E γ + E e = E γ ′ + E e ′ . {\displaystyle E_{\gamma }+E_{\text{e}}=E_{\gamma '}+E_{{\text{e}}'}.} Compton postulated that photons carry momentum; thus from the conservation of momentum, the momenta of the particles should be similarly related by p γ = p γ ′ + p e ′ , {\displaystyle \mathbf {p} _{\gamma }=\mathbf {p} _{\gamma '}+\mathbf {p} _{{\text{e}}'},} in which pe is omitted as being negligible. The photon energies are related to the frequencies by E γ = h f {\displaystyle E_{\gamma }=hf} E γ ′ = h f ′ {\displaystyle E_{\gamma '}=hf'} where h is the Planck constant. Before the scattering event, the electron is treated as sufficiently close to being at rest that its total energy consists entirely of the mass–energy equivalence of its rest mass me, E e = m e c 2 . {\displaystyle E_{\text{e}}=m_{\text{e}}c^{2}.} After scattering, the possibility that the electron might be accelerated to a significant fraction of the speed of light, requires that its total energy be represented using the relativistic energy–momentum relation E e ′ = ( p e ′ c ) 2 + ( m e c 2 ) 2 . {\displaystyle E_{{\text{e}}'}={\sqrt {(p_{{\text{e}}'}c)^{2}+(m_{\text{e}}c^{2})^{2}}}~.} Substituting these quantities into the expression for the conservation of energy gives h f + m e c 2 = h f ′ + ( p e ′ c ) 2 + ( m e c 2 ) 2 . {\displaystyle hf+m_{\text{e}}c^{2}=hf'+{\sqrt {(p_{{\text{e}}'}c)^{2}+(m_{\text{e}}c^{2})^{2}}}.} This expression can be used to find the magnitude of the momentum of the scattered electron, Note that this magnitude of the momentum gained by the electron (formerly zero) exceeds the energy/c lost by the photon, 1 c ( h f − h f ′ + m e c 2 ) 2 − m e 2 c 4 > h f − h f ′ c . {\displaystyle {\frac {1}{c}}{\sqrt {(hf-hf'+m_{\text{e}}c^{2})^{2}-m_{\text{e}}^{2}c^{4}}}>{\frac {hf-hf'}{c}}~.} Equation (1) relates the various energies associated with the collision. The electron's momentum change involves a relativistic change in the energy of the electron, so it is not simply related to the change in energy occurring in classical physics. The change of the magnitude of the momentum of the photon is not just related to the change of its energy; it also involves a change in direction. Solving the conservation of momentum expression for the scattered electron's momentum gives p e ′ = p γ − p γ ′ . {\displaystyle \mathbf {p} _{{\text{e}}'}=\mathbf {p} _{\gamma }-\mathbf {p} _{\gamma '}.} Making use of the scalar product yields the square of its magnitude, p e ′ 2 = p e ′ ⋅ p e ′ = ( p γ − p γ ′ ) ⋅ ( p γ − p γ ′ ) = p γ 2 + p γ ′ 2 − 2 p γ p γ ′ cos ⁡ θ . {\displaystyle {\begin{aligned}p_{{\text{e}}'}^{\,2}&=\mathbf {p} _{{\text{e}}'}\cdot \mathbf {p} _{{\text{e}}'}=(\mathbf {p} _{\gamma }-\mathbf {p} _{\gamma '})\cdot (\mathbf {p} _{\gamma }-\mathbf {p} _{\gamma '})\\&=p_{\gamma }^{\,2}+p_{\gamma '}^{\,2}-2p_{\gamma }\,p_{\gamma '}\cos \theta .\end{aligned}}} In anticipation of p γ c {\displaystyle p_{\gamma }c} being replaced with hf, multiply both sides by c2, p e ′ 2 c 2 = p γ 2 c 2 + p γ ′ 2 c 2 − 2 c 2 p γ p γ ′ cos ⁡ θ . {\displaystyle p_{{\text{e}}'}^{\,2}c^{2}=p_{\gamma }^{\,2}c^{2}+p_{\gamma '}^{\,2}c^{2}-2c^{2}p_{\gamma }\,p_{\gamma '}\cos \theta .} After replacing the photon momentum terms with hf/c, we get a second expression for the magnitude of the momentum of the scattered electron, Equating the alternate expressions for this momentum gives ( h f − h f ′ + m e c 2 ) 2 − m e 2 c 4 = ( h f ) 2 + ( h f ′ ) 2 − 2 h 2 f f ′ cos ⁡ θ , {\displaystyle (hf-hf'+m_{\text{e}}c^{2})^{2}-m_{\text{e}}^{\,2}c^{4}=\left(hf\right)^{2}+\left(hf'\right)^{2}-2h^{2}ff'\cos {\theta },} which, after evaluating the square and canceling and rearranging terms, further yields 2 h f m e c 2 − 2 h f ′ m e c 2 = 2 h 2 f f ′ ( 1 − cos ⁡ θ ) . {\displaystyle 2hfm_{\text{e}}c^{2}-2hf'm_{\text{e}}c^{2}=2h^{2}ff'\left(1-\cos \theta \right).} Dividing both sides by 2hff′mec yields c f ′ − c f = h m e c ( 1 − cos ⁡ θ ) . {\displaystyle {\frac {c}{f'}}-{\frac {c}{f}}={\frac {h}{m_{\text{e}}c}}\left(1-\cos \theta \right).} Finally, since fλ = f′λ′ = c, It can further be seen that the angle φ of the outgoing electron with the direction of the incoming photon is specified by Applications Compton scattering Compton scattering is of prime importance to radiobiology, as it is the most probable interaction of gamma rays and high energy X-rays with atoms in living beings and is applied in radiation therapy. Compton scattering is an important effect in gamma spectroscopy which gives rise to the Compton edge, as it is possible for the gamma rays to scatter out of the detectors used. Compton suppression is used to detect stray scatter gamma rays to counteract this effect. Magnetic Compton scattering Magnetic Compton scattering is an extension of the previously mentioned technique which involves the magnetisation of a crystal sample hit with high energy, circularly polarised photons. By measuring the scattered photons' energy and reversing the magnetisation of the sample, two different Compton profiles are generated (one for spin up momenta and one for spin down momenta). Taking the difference between these two profiles gives the magnetic Compton profile (MCP), given by J mag ( p z ) {\displaystyle J_{\text{mag}}(\mathbf {p} _{z})} – a one-dimensional projection of the electron spin density. J mag ( p z ) = 1 μ ∬ − ∞ ∞ ( n ↑ ( p ) − n ↓ ( p ) ) d p x d p y {\displaystyle J_{\text{mag}}(\mathbf {p} _{z})={\frac {1}{\mu }}\iint _{-\infty }^{\infty }(n_{\uparrow }(\mathbf {p} )-n_{\downarrow }(\mathbf {p} ))d\mathbf {p} _{x}d\mathbf {p} _{y}} where μ {\displaystyle \mu } is the number of spin-unpaired electrons in the system, n ↑ ( p ) {\displaystyle n_{\uparrow }(\mathbf {p} )} and n ↓ ( p ) {\displaystyle n_{\downarrow }(\mathbf {p} )} are the three-dimensional electron momentum distributions for the majority spin and minority spin electrons respectively. Since this scattering process is incoherent (there is no phase relationship between the scattered photons), the MCP is representative of the bulk properties of the sample and is a probe of the ground state. This means that the MCP is ideal for comparison with theoretical techniques such as density functional theory. The area under the MCP is directly proportional to the spin moment of the system and so, when combined with total moment measurements methods (such as SQUID magnetometry), can be used to isolate both the spin and orbital contributions to the total moment of a system. The shape of the MCP also yields insight into the origin of the magnetism in the system. Inverse Compton scattering Inverse Compton scattering is important in astrophysics. In X-ray astronomy, the accretion disk surrounding a black hole is presumed to produce a thermal spectrum. The lower energy photons produced from this spectrum are scattered to higher energies by relativistic electrons in the surrounding corona. This is surmised to cause the power law component in the X-ray spectra (0.2–10 keV) of accreting black holes. The effect is also observed when photons from the cosmic microwave background (CMB) move through the hot gas surrounding a galaxy cluster. The CMB photons are scattered to higher energies by the electrons in this gas, resulting in the Sunyaev–Zel'dovich effect. Observations of the Sunyaev–Zel'dovich effect provide a nearly redshift-independent means of detecting galaxy clusters. Some synchrotron radiation facilities scatter laser light off the stored electron beam. This Compton backscattering produces high energy photons in the MeV to GeV range subsequently used for nuclear physics experiments. Non-linear inverse Compton scattering Non-linear inverse Compton scattering (NICS) is the scattering of multiple low-energy photons, given by an intense electromagnetic field, in a high-energy photon (X-ray or gamma ray) during the interaction with a charged particle, such as an electron. It is also called non-linear Compton scattering and multiphoton Compton scattering. It is the non-linear version of inverse Compton scattering in which the conditions for multiphoton absorption by the charged particle are reached due to a very intense electromagnetic field, for example the one produced by a laser. Non-linear inverse Compton scattering is an interesting phenomenon for all applications requiring high-energy photons since NICS is capable of producing photons with energy comparable to the charged particle rest energy and higher. As a consequence NICS photons can be used to trigger other phenomena such as pair production, Compton scattering, nuclear reactions, and can be used to probe non-linear quantum effects and non-linear QED. See also References Further reading S. Chen; H. Avakian; V. Burkert; L. Vandenaweele; P. Eugenio; the CLAS collaboration; Ambrozewicz; Anghinolfi; Asryan; Bagdasaryan; Baillie; Ball; Baltzell; Barrow; Batourine; Battaglieri; Beard; Bedlinskiy; Bektasoglu; Bellis; Benmouna; Berman; Biselli; Bonner; Bouchigny; Boiarinov; Bosted; Bradford; Branford; et al. (2006). "Measurement of Deeply Virtual Compton Scattering with a Polarized Proton Target". Physical Review Letters. 97 (7): 072002. arXiv:hep-ex/0605012. Bibcode:2006PhRvL..97g2002C. doi:10.1103/PhysRevLett.97.072002. PMID 17026221. S2CID 15326395. Compton, Arthur H. (May 1923). "A Quantum Theory of the Scattering of X-Rays by Light Elements". Physical Review. 21 (5): 483–502. Bibcode:1923PhRv...21..483C. doi:10.1103/PhysRev.21.483. (the original 1923 paper on the APS website) Stuewer, Roger H. (1975), The Compton Effect: Turning Point in Physics (New York: Science History Publications) External links Compton Scattering – Georgia State University Compton Scattering Data – Georgia State University Derivation of Compton shift equation In particle physics and quantum mechanics, mixing angles are the angles between two sets of (complex-valued) orthogonal basis vectors, or states, usually the eigenbases of two quantum mechanical operators. The choice of angles (parameterization) is not unique but based on convention. Mathematics The relation between two eigenbases is described completely by a unitary matrix, the analogue of a rotation matrix in a complex vector space. The number of degrees of freedom in this matrix is usually reduced by removing any excess complex phase from the transformation, since in most cases that is not a measurable quantity. For two-dimensional vector space this reduces the matrix to a rotation matrix, which can be described completely by one mixing angle. In a three dimensional space there are three mixing angles and one additional complex phase parameter. Different conventions exist for how the three angles are defined, such as Euler angles. Notable mixing angles Some notable mixing angles in particle physics are: Neutrino mixing angles (PMNS matrix), describing the mixing between the mass and flavour eigenstates of neutrinos, which explains neutrino oscillations Quark mixing angles including the Cabbibo angle (CKM matrix), describing the mixing between the mass and flavour eigenstates of quarks The Weinberg angle or weak mixing angle, describing the mixing between the electromagnetic and weak forces Higgs mixing angle A quantum point contact (QPC) is a narrow constriction between two wide electrically conducting regions, of a width comparable to the electronic wavelength (nano- to micrometer). The importance of QPC lies in the fact that they prove quantisation of ballistic conductance in mesoscopic systems. The conductance of a QPC is quantized in units of 2 e 2 / h {\displaystyle 2e^{2}/h} , the so-called conductance quantum. Quantum point contacts were first reported in 1988 by a Dutch team from Delft University of Technology and Philips Research and, independently, by a British team from the Cavendish Laboratory. They are based on earlier work by the British group which showed how split gates could be used to convert a two-dimensional electron gas into one-dimension, first in silicon and then in gallium arsenide. This quantisation is reminiscent of the quantisation of the Hall conductance, but is measured in the absence of a magnetic field. The zero-field conductance quantisation and the smooth transition to the quantum Hall effect on applying a magnetic field are essentially consequences of the equipartition of current among an integer number of propagating modes in the constriction. Fabrication There are several different ways of fabricating a quantum point contact. It can be realized in a break-junction by pulling apart a piece of conductor until it breaks. The breaking point forms the point contact. In a more controlled way, quantum point contacts are formed in a two-dimensional electron gas (2DEG), e.g. in GaAs/AlGaAs heterostructures. By applying a voltage to suitably shaped gate electrodes, the electron gas can be locally depleted and many different types of conducting regions can be created in the plane of the 2DEG, among them quantum dots and quantum point contacts. Another means of creating a QPC is by positioning the tip of a scanning tunneling microscope close to the surface of a conductor. Properties Geometrically, a quantum point contact is a constriction in the transverse direction which presents a resistance to the motion of electrons. Applying a voltage V {\displaystyle V} across the point contact induces a current to flow, the magnitude of this current is given by I = G V {\displaystyle I=GV} , where G {\displaystyle G} is the conductance of the contact. This formula resembles Ohm's law for macroscopic resistors. However, there is a fundamental difference here resulting from the small system size which requires a quantum mechanical analysis. It is most common to study QPC in two dimensional electron gases. This way the geometric constriction of the point contact turns the conductance through the opening to a one dimensional system. Moreover, it requires a quantum mechanical description of the system that results in the quantisation of conductance. Quantum mechanically, the current through the point contact is equipartitioned among the 1D subands, or transverse modes, in the constriction. It is important to state that the previous discussion does not take into account possible transitions among modes. The Landauer formula can actually be generalized to express this possible transitions G = 2 e 2 h ∑ n , m | T n , m | 2 {\displaystyle G={2e^{2} \over h}\sum _{n,m}|T_{n,m}|^{2}} , where T n , m {\displaystyle T_{n,m}} is the transition matrix which incorporates non-zero probabilities of transmission from mode n to m. At low temperatures and voltages, unscattered and untrapped electrons contributing to the current have a certain energy/momentum/wavelength called Fermi energy/momentum/wavelength. Much like in a waveguide, the transverse confinement in the quantum point contact results in a "quantization" of the transverse motion—the transverse motion cannot vary continuously, but has to be one of a series of discrete modes. The waveguide analogy is applicable as long as coherence is not lost through scattering, e.g., by a defect or trapping site. The electron wave can only pass through the constriction if it interferes constructively, which for a given width of constriction, only happens for a certain number of modes N {\displaystyle N} . The current carried by such a quantum state is the product of the velocity times the electron density. These two quantities by themselves differ from one mode to the other, but their product is mode independent. As a consequence, each state contributes the same amount of e 2 / h {\displaystyle e^{2}/h} per spin direction to the total conductance G = N G 0 {\displaystyle G=NG_{0}} . This is a fundamental result; the conductance does not take on arbitrary values but is quantized in multiples of the conductance quantum G 0 = 2 e 2 / h {\displaystyle G_{0}=2e^{2}/h} , which is expressed through the electron charge e {\displaystyle e} and the Planck constant h {\displaystyle h} . The integer number N {\displaystyle N} is determined by the width of the point contact and roughly equals the width divided by half the electron wavelength. As a function of the width of the point contact (or gate voltage in the case of GaAs/AlGaAs heterostructure devices), the conductance shows a staircase behavior as more and more modes (or channels) contribute to the electron transport. The step-height is given by G Q {\displaystyle G_{Q}} . On increasing the temperature, one finds experimentally that the plateaux acquire a finite slope until they are no longer resolved. This is a consequence of the thermal smearing of the Fermi-Dirac distribution. The conductance steps should disappear for T ≲ Δ E / 4 k B ≈ 4 K {\displaystyle T\lesssim \Delta E/4k_{\rm {B}}\approx 4\;\mathrm {K} } (here ∆E is the subband splitting at the Fermi level). This is confirmed both by experiment and by numerical calculations. An external magnetic field applied to the quantum point contact lifts the spin degeneracy and leads to half-integer steps in the conductance. In addition, the number N {\displaystyle N} of modes that contribute becomes smaller. For large magnetic fields, N {\displaystyle N} is independent of the width of the constriction, given by the theory of the quantum Hall effect. The 0.7 anomaly Anomalous features on the quantized conductance steps are often observed in transport measurements of quantum point contacts. A notable example is the plateau at 0.7 G Q {\displaystyle 0.7G_{Q}} , the so-called 0.7-structure, arising due to enhanced electron-electron interactions arising from a smeared van Hove singularity in the local 1D density of states in the vicinity of the charge constriction. Unlike the conductance steps, the 0.7-structure becomes more pronounced at higher temperature. 0.7-structure analogues are sometimes observed on higher conductance steps. Quasi-bound states arising from impurities, charge traps, and reflections within the constriction may also result in conductance structure close to the 1D limit. Applications Apart from studying fundamentals of charge transport in mesoscopic conductors, quantum point contacts can be used as extremely sensitive charge detectors. Since the conductance through the contact strongly depends on the size of the constriction, any potential fluctuation (for instance, created by other electrons) in the vicinity will influence the current through the QPC. It is possible to detect single electrons with such a scheme. In view of quantum computation in solid-state systems, QPCs can be used as readout devices for the state of a quantum bit (qubit). In device physics, the configuration of QPCs is used for demonstrating a fully ballistic field-effect transistor. Another application of the device is its use as a switch. A nickel wire is brought close enough to a gold surface and then, by the use of a piezoelectric actuator, the distance between the wire and the surface can be changed and thus, the transport characteristics of the device change between electron tunneling and ballistic. References Further reading C.W.J.Beenakker and H. van Houten (1991). "Quantum Transport in Semiconductor Nanostructures". Solid State Physics. 44: 1–228. arXiv:cond-mat/0412664. Bibcode:2004cond.mat.12664B. doi:10.1016/s0081-1947(08)60091-0. ISBN 9780126077445. S2CID 119082619. K. J. Thomas; et al. (1996). "Possible spin polarization in a one-dimensional electron gas". Physical Review Letters. 77 (1): 135–138. arXiv:cond-mat/9606004. Bibcode:1996PhRvL..77..135T. doi:10.1103/PhysRevLett.77.135. PMID 10061790. S2CID 8903637. Nicolás Agraït; Alfredo Levy Yeyati; Jan M. van Ruitenbeek (2003). "Quantum properties of atomic-sized conductors". Physics Reports. 377 (2–3): 81. arXiv:cond-mat/0208239. Bibcode:2003PhR...377...81A. doi:10.1016/S0370-1573(02)00633-6. S2CID 119409385. Timp, G. (1992). "Chapter 3: When Does a Wire Become an Electron Waveguide". Semiconductors and Semimetals Volume 35. Vol. 35. pp. 113–190. doi:10.1016/S0080-8784(08)62393-5. ISBN 9780127521350. In physics, the Leggett inequalities, named for Anthony James Leggett, who derived them, are a related pair of mathematical expressions concerning the correlations of properties of entangled particles. (As published by Leggett, the inequalities were exemplified in terms of relative angles of elliptical and linear polarizations.) Inequalities They are fulfilled by a large class of physical theories based on particular non-local and realistic assumptions, that may be considered to be plausible or intuitive according to common physical reasoning. The Leggett inequalities are violated by quantum mechanical theory. The results of experimental tests in 2007 and 2010 have shown agreement with quantum mechanics rather than the Leggett inequalities. Given that experimental tests of Bell's inequalities have ruled out local realism in quantum mechanics, the violation of Leggett's inequalities is considered to have falsified realism in quantum mechanics. In quantum mechanics "realism" means "notion that physical systems possess complete sets of definite values for various parameters prior to, and independent of, measurement". See also CHSH inequality Leggett–Garg inequality References External links "The Reality Tests", Joshua Roebke, SEED, June 2008. "A quantum renaissance", Markus Aspelmeyer and Anton Zeilinger, Physics World, July 2008. Archived 2016-03-04 at the Wayback Machine "Quantum theory survives latest challenge", Kate McAlpine, Physics World, December 2010. The characteristic rotational temperature (θR or θrot) is commonly used in statistical thermodynamics to simplify the expression of the rotational partition function and the rotational contribution to molecular thermodynamic properties. It has units of temperature and is defined as θ R = h c B ¯ k B = ℏ 2 2 k B I , {\displaystyle \theta _{\mathrm {R} }={\frac {hc{\overline {B}}}{k_{\mathrm {B} }}}={\frac {\hbar ^{2}}{2k_{\mathrm {B} }I}},} where B ¯ = B / h c {\displaystyle {\overline {B}}=B/hc} is the rotational constant, I is a molecular moment of inertia, h is the Planck constant, c is the speed of light, ħ = h/2π is the reduced Planck constant and kB is the Boltzmann constant. The physical meaning of θR is as an estimate of the temperature at which thermal energy (of the order of kBT) is comparable to the spacing between rotational energy levels (of the order of hcB). At about this temperature the population of excited rotational levels becomes important. Some typical values are given in the table. In each case the value refers to the most common isotopic species. References See also Rotational spectroscopy Vibrational temperature Vibrational spectroscopy Infrared spectroscopy Spectroscopy The Bauer 5GBioShield, usually shortened to 5GBioShield, is a fraudulent device which was claimed to protect against radiation from 5G mobile networks. The device was invented by clinical pharmacist Jacques Bauer and former scientist Ilija Lakicevic and marketed by alternative medicine activist Sacha Stone. The product, which was sold for approximately £330 through an affiliate marketing scheme, was found to be composed of a normal USB thumb drive and a sticker. As of April 26, 2022, The official website is no longer online. British Trading Standards officials determined that the device was a scam. Description The manufacturers claim that: "Through a process of quantum oscillation the 5G BioShield USB Key balances and reharmonizes the disturbing frequencies arising from the electric fog induced by devices, such as laptops, cordless phones, wifi, tablets, etc." The device is simply a common USB thumb drive containing marketing documents and usage instructions. The USB device is housed in a clear perspex block imprinted with a stylized version of St George slaying a dragon, as based on a medal originally made by William Wyon for Albert, Prince Consort. Lakicevic, the co-inventor of the product, describes the device as containing a "new energy" embedded in a sticker, and that the USB stick is merely a carrier and need not be powered on to work. Lakicevic's claims regarding this product were published in an issue of the International Journal of Science and Research (ITNJ), a pay-to-publish science journal with no peer review processes in place. Reception The device was recommended in a report published by Glastonbury Town Council. Town councillor Toby R. Hall stated that the device could be "helpful" and "provide protection" due to a "wearable holographic nano-layer catalyser". An analysis by Pen Test Partners, however, concluded that this device was nothing more than a 128 megabyte capacity generic USB thumb drive. The security firm concluded that the device "should [not] be promoted by publicly-funded bodies". Following this report, the device was investigated by Trading Standards and found to be a scam and the matter had been referred to City of London Police Fraud Squad. == References == The Schrödinger–Newton equation, sometimes referred to as the Newton–Schrödinger or Schrödinger–Poisson equation, is a nonlinear modification of the Schrödinger equation with a Newtonian gravitational potential, where the gravitational potential emerges from the treatment of the wave function as a mass density, including a term that represents interaction of a particle with its own gravitational field. The inclusion of a self-interaction term represents a fundamental alteration of quantum mechanics. It can be written either as a single integro-differential equation or as a coupled system of a Schrödinger and a Poisson equation. In the latter case it is also referred to in the plural form. The Schrödinger–Newton equation was first considered by Ruffini and Bonazzola in connection with self-gravitating boson stars. In this context of classical general relativity it appears as the non-relativistic limit of either the Klein–Gordon equation or the Dirac equation in a curved space-time together with the Einstein field equations. The equation also describes fuzzy dark matter and approximates classical cold dark matter described by the Vlasov–Poisson equation in the limit that the particle mass is large. Later on it was proposed as a model to explain the quantum wave function collapse by Lajos Diósi and Roger Penrose, from whom the name "Schrödinger–Newton equation" originates. In this context, matter has quantum properties, while gravity remains classical even at the fundamental level. The Schrödinger–Newton equation was therefore also suggested as a way to test the necessity of quantum gravity. In a third context, the Schrödinger–Newton equation appears as a Hartree approximation for the mutual gravitational interaction in a system of a large number of particles. In this context, a corresponding equation for the electromagnetic Coulomb interaction was suggested by Philippe Choquard at the 1976 Symposium on Coulomb Systems in Lausanne to describe one-component plasmas. Elliott H. Lieb provided the proof for the existence and uniqueness of a stationary ground state and referred to the equation as the Choquard equation. Overview As a coupled system, the Schrödinger–Newton equations are the usual Schrödinger equation with a self-interaction gravitational potential i ℏ ∂ Ψ ∂ t = − ℏ 2 2 m ∇ 2 Ψ + V Ψ + m Φ Ψ , {\displaystyle i\hbar \ {\frac {\partial \Psi }{\ \partial t\ }}=-{\frac {\ \hbar ^{2}}{\ 2\ m\ }}\ \nabla ^{2}\Psi \;+\;V\ \Psi \;+\;m\ \Phi \ \Psi \ ,} where V is an ordinary potential, and the gravitational potential Φ , {\displaystyle \ \Phi \ ,} representing the interaction of the particle with its own gravitational field, satisfies the Poisson equation ∇ 2 Φ = 4 π G m | Ψ | 2 . {\displaystyle \ \nabla ^{2}\Phi =4\pi \ G\ m\ |\Psi |^{2}~.} Because of the back coupling of the wave-function into the potential, it is a nonlinear system. Replacing Φ {\displaystyle \ \Phi \ } with the solution to the Poisson equation produces the integro-differential form of the Schrödinger–Newton equation: i ℏ ∂ Ψ ∂ t = [ − ℏ 2 2 m ∇ 2 + V − G m 2 ∫ | Ψ ( t , y ) | 2 | x − y | d 3 y ] Ψ . {\displaystyle i\hbar \ {\frac {\ \partial \Psi \ }{\partial t}}=\left[\ -{\frac {\ \hbar ^{2}}{\ 2\ m\ }}\ \nabla ^{2}\;+\;V\;-\;G\ m^{2}\int {\frac {\ |\Psi (t,\mathbf {y} )|^{2}}{\ |\mathbf {x} -\mathbf {y} |\ }}\;\mathrm {d} ^{3}\mathbf {y} \ \right]\Psi ~.} It is obtained from the above system of equations by integration of the Poisson equation under the assumption that the potential must vanish at infinity. Mathematically, the Schrödinger–Newton equation is a special case of the Hartree equation for n = 2. The equation retains most of the properties of the linear Schrödinger equation. In particular, it is invariant under constant phase shifts, leading to conservation of probability and exhibits full Galilei invariance. In addition to these symmetries, a simultaneous transformation m → μ m , t → μ − 5 t , x → μ − 3 x , ψ ( t , x ) → μ 9 / 2 ψ ( μ 5 t , μ 3 x ) {\displaystyle m\to \mu \ m\ ,\qquad t\to \mu ^{-5}t\ ,\qquad \mathbf {x} \to \mu ^{-3}\mathbf {x} \ ,\qquad \psi (t,\mathbf {x} )\to \mu ^{9/2}\psi (\mu ^{5}t,\mu ^{3}\mathbf {x} )} maps solutions of the Schrödinger–Newton equation to solutions. The stationary equation, which can be obtained in the usual manner via a separation of variables, possesses an infinite family of normalisable solutions of which only the stationary ground state is stable. Relation to semi-classical and quantum gravity The Schrödinger–Newton equation can be derived under the assumption that gravity remains classical, even at the fundamental level, and that the right way to couple quantum matter to gravity is by means of the semiclassical Einstein equations. In this case, a Newtonian gravitational potential term is added to the Schrödinger equation, where the source of this gravitational potential is the expectation value of the mass density operator or mass flux-current. In this regard, if gravity is fundamentally classical, the Schrödinger–Newton equation is a fundamental one-particle equation, which can be generalised to the case of many particles (see below). If, on the other hand, the gravitational field is quantised, the fundamental Schrödinger equation remains linear. The Schrödinger–Newton equation is then only valid as an approximation for the gravitational interaction in systems of a large number of particles and has no effect on the centre of mass. Many-body equation and centre-of-mass motion If the Schrödinger–Newton equation is considered as a fundamental equation, there is a corresponding N-body equation that was already given by Diósi and can be derived from semiclassical gravity in the same way as the one-particle equation: i ℏ ∂ ∂ t Ψ ( t , x 1 , … , x N ) = ( − ∑ j = 1 N ℏ 2 2 m j ∇ j 2 + ∑ j ≠ k V j k ( | x j − x k | ) − G ∑ j , k = 1 N m j m k ∫ d 3 y 1 ⋯ d 3 y N | Ψ ( t , y 1 , … , y N ) | 2 | x j − y k | ) Ψ ( t , x 1 , … , x N ) . {\displaystyle {\begin{aligned}i\hbar {\frac {\partial }{\partial t}}\Psi (t,\mathbf {x} _{1},\dots ,\mathbf {x} _{N})={\bigg (}&-\sum _{j=1}^{N}{\frac {\hbar ^{2}}{2m_{j}}}\nabla _{j}^{2}+\sum _{j\neq k}V_{jk}{\big (}|\mathbf {x} _{j}-\mathbf {x} _{k}|{\big )}\\&-G\sum _{j,k=1}^{N}m_{j}m_{k}\int \mathrm {d} ^{3}\mathbf {y} _{1}\cdots \mathrm {d} ^{3}\mathbf {y} _{N}\,{\frac {|\Psi (t,\mathbf {y} _{1},\dots ,\mathbf {y} _{N})|^{2}}{|\mathbf {x} _{j}-\mathbf {y} _{k}|}}{\bigg )}\Psi (t,\mathbf {x} _{1},\dots ,\mathbf {x} _{N}).\end{aligned}}} The potential V j k {\displaystyle V_{jk}} contains all the mutual linear interactions, e.g. electrodynamical Coulomb interactions, while the gravitational-potential term is based on the assumption that all particles perceive the same gravitational potential generated by all the marginal distributions for all the particles together. In a Born–Oppenheimer-like approximation, this N-particle equation can be separated into two equations, one describing the relative motion, the other providing the dynamics of the centre-of-mass wave-function. For the relative motion, the gravitational interaction does not play a role, since it is usually weak compared to the other interactions represented by V j k {\displaystyle V_{jk}} . But it has a significant influence on the centre-of-mass motion. While V j k {\displaystyle V_{jk}} only depends on relative coordinates and therefore does not contribute to the centre-of-mass dynamics at all, the nonlinear Schrödinger–Newton interaction does contribute. In the aforementioned approximation, the centre-of-mass wave-function satisfies the following nonlinear Schrödinger equation: i ℏ ∂ ψ c ( t , R ) ∂ t = ( ℏ 2 2 M ∇ 2 − G ∫ d 3 R ′ ∫ d 3 y ∫ d 3 z | ψ c ( t , R ′ ) | 2 ρ c ( y ) ρ c ( z ) | R − R ′ − y + z | ) ψ c ( t , R ) , {\displaystyle i\hbar {\frac {\partial \psi _{c}(t,\mathbf {R} )}{\partial t}}=\left({\frac {\hbar ^{2}}{2M}}\nabla ^{2}-G\int \mathrm {d} ^{3}\mathbf {R'} \,\int \mathrm {d} ^{3}\mathbf {y} \,\int \mathrm {d} ^{3}\mathbf {z} \,{\frac {|\psi _{c}(t,\mathbf {R'} )|^{2}\,\rho _{c}(\mathbf {y} )\rho _{c}(\mathbf {z} )}{\left|\mathbf {R} -\mathbf {R'} -\mathbf {y} +\mathbf {z} \right|}}\right)\psi _{c}(t,\mathbf {R} ),} where M is the total mass, R is the relative coordinate, ψ c {\displaystyle \psi _{c}} the centre-of-mass wave-function, and ρ c {\displaystyle \rho _{c}} is the mass density of the many-body system (e.g. a molecule or a rock) relative to its centre of mass. In the limiting case of a wide wave-function, i.e. where the width of the centre-of-mass distribution is large compared to the size of the considered object, the centre-of-mass motion is approximated well by the Schrödinger–Newton equation for a single particle. The opposite case of a narrow wave-function can be approximated by a harmonic-oscillator potential, where the Schrödinger–Newton dynamics leads to a rotation in phase space. In the context where the Schrödinger–Newton equation appears as a Hartree approximation, the situation is different. In this case the full N-particle wave-function is considered a product state of N single-particle wave-functions, where each of those factors obeys the Schrödinger–Newton equation. The dynamics of the centre-of-mass, however, remain strictly linear in this picture. This is true in general: nonlinear Hartree equations never have an influence on the centre of mass. Significance of effects A rough order-of-magnitude estimate of the regime where effects of the Schrödinger–Newton equation become relevant can be obtained by a rather simple reasoning. For a spherically symmetric Gaussian, Ψ ( t = 0 , r ) = ( π σ 2 ) − 3 / 4 exp ⁡ ( − r 2 2 σ 2 ) , {\displaystyle \Psi (t=0,r)=(\pi \sigma ^{2})^{-3/4}\exp \left(-{\frac {r^{2}}{2\sigma ^{2}}}\right),} the free linear Schrödinger equation has the solution Ψ ( t , r ) = ( π σ 2 ) − 3 / 4 ( 1 + i ℏ t m σ 2 ) − 3 / 2 exp ⁡ ( − r 2 2 σ 2 ( 1 + i ℏ t m σ 2 ) ) . {\displaystyle \Psi (t,r)=(\pi \sigma ^{2})^{-3/4}\left(1+{\frac {i\hbar t}{m\sigma ^{2}}}\right)^{-3/2}\exp \left(-{\frac {r^{2}}{2\sigma ^{2}\left(1+{\frac {i\hbar t}{m\sigma ^{2}}}\right)}}\right).} The peak of the radial probability density 4 π r 2 | Ψ | 2 {\displaystyle 4\pi r^{2}|\Psi |^{2}} can be found at r p = σ 1 + ℏ 2 t 2 m 2 σ 4 . {\displaystyle r_{p}=\sigma {\sqrt {1+{\frac {\hbar ^{2}t^{2}}{m^{2}\sigma ^{4}}}}}.} Now we set the acceleration r ¨ p = ℏ 2 m 2 r p 3 {\displaystyle {\ddot {r}}_{p}={\frac {\hbar ^{2}}{m^{2}r_{p}^{3}}}} of this peak probability equal to the acceleration due to Newtonian gravity: r ¨ = − G m r 2 , {\displaystyle {\ddot {r}}=-{\frac {Gm}{r^{2}}},} using that r p = σ {\displaystyle r_{p}=\sigma } at time t = 0 {\displaystyle t=0} . This yields the relation m 3 σ = ℏ 2 G ≈ 1.7 × 10 − 58 m kg 3 , {\displaystyle m^{3}\sigma ={\frac {\hbar ^{2}}{G}}\approx 1.7\times 10^{-58}~{\text{m}}\,{\text{kg}}^{3},} which allows us to determine a critical width for a given mass value and conversely. We also recognise the scaling law mentioned above. Numerical simulations show that this equation gives a rather good estimate of the mass regime above which effects of the Schrödinger–Newton equation become significant. For an atom the critical width is around 1022 metres, while it is already down to 10−31 metres for a mass of one microgram. The regime where the mass is around 1010 Da while the width is of the order of micrometres is expected to allow an experimental test of the Schrödinger–Newton equation in the future. A possible candidate are interferometry experiments with heavy molecules, which currently reach masses up to 10000 Da. Quantum wave function collapse The idea that gravity causes (or somehow influences) the wavefunction collapse dates back to the 1960s and was originally proposed by Károlyházy. The Schrödinger–Newton equation was proposed in this context by Diósi. There the equation provides an estimation for the "line of demarcation" between microscopic (quantum) and macroscopic (classical) objects. The stationary ground state has a width of a 0 ≈ ℏ 2 G m 3 . {\displaystyle a_{0}\approx {\frac {\hbar ^{2}}{Gm^{3}}}.} For a well-localised homogeneous sphere, i.e. a sphere with a centre-of-mass wave-function that is narrow compared to the radius of the sphere, Diósi finds as an estimate for the width of the ground-state centre-of-mass wave-function a 0 ( R ) ≈ a 0 1 / 4 R 3 / 4 . {\displaystyle a_{0}^{(R)}\approx a_{0}^{1/4}R^{3/4}.} Assuming a usual density around 1000 kg/m3, a critical radius can be calculated for which a 0 ( R ) ≈ R {\displaystyle a_{0}^{(R)}\approx R} . This critical radius is around a tenth of a micrometre. Roger Penrose proposed that the Schrödinger–Newton equation mathematically describes the basis states involved in a gravitationally induced wavefunction collapse scheme. Penrose suggests that a superposition of two or more quantum states having a significant amount of mass displacement ought to be unstable and reduce to one of the states within a finite time. He hypothesises that there exists a "preferred" set of states that could collapse no further, specifically, the stationary states of the Schrödinger–Newton equation. A macroscopic system can therefore never be in a spatial superposition, since the nonlinear gravitational self-interaction immediately leads to a collapse to a stationary state of the Schrödinger–Newton equation. According to Penrose's idea, when a quantum particle is measured, there is an interplay of this nonlinear collapse and environmental decoherence. The gravitational interaction leads to the reduction of the environment to one distinct state, and decoherence leads to the localisation of the particle, e.g. as a dot on a screen. Problems and open matters Three major problems occur with this interpretation of the Schrödinger–Newton equation as the cause of the wave-function collapse: Excessive residual probability far from the collapse point Lack of any apparent reason for the Born rule Promotion of the previously strictly hypothetical wave function to an observable (real) quantity. First, numerical studies agreeingly find that when a wave packet "collapses" to a stationary solution, a small portion of it seems to run away to infinity. This would mean that even a completely collapsed quantum system still can be found at a distant location. Since the solutions of the linear Schrödinger equation tend towards infinity even faster, this only indicates that the Schrödinger–Newton equation alone is not sufficient to explain the wave-function collapse. If the environment is taken into account, this effect might disappear and therefore not be present in the scenario described by Penrose. A second problem, also arising in Penrose's proposal, is the origin of the Born rule: To solve the measurement problem, a mere explanation of why a wave-function collapses – e.g., to a dot on a screen – is not enough. A good model for the collapse process also has to explain why the dot appears on different positions of the screen, with probabilities that are determined by the squared absolute-value of the wave-function. It might be possible that a model based on Penrose's idea could provide such an explanation, but there is as yet no known reason that Born's rule would naturally arise from it. Thirdly, since the gravitational potential is linked to the wave-function in the picture of the Schrödinger–Newton equation, the wave-function must be interpreted as a real object. Therefore, at least in principle, it becomes a measurable quantity. Making use of the nonlocal nature of entangled quantum systems, this could be used to send signals faster than light, which is generally thought to be in contradiction with causality. It is, however, not clear whether this problem can be resolved by applying the right collapse prescription, yet to be found, consistently to the full quantum system. Also, since gravity is such a weak interaction, it is not clear that such an experiment can be actually performed within the parameters given in our universe (see the referenced discussion about a similar thought experiment proposed by Eppley & Hannah). See also Nonlinear Schrödinger equation Semiclassical gravity Penrose interpretation Poisson's equation == References == In physics, canonical quantization is a procedure for quantizing a classical theory, while attempting to preserve the formal structure, such as symmetries, of the classical theory to the greatest extent possible. Historically, this was not quite Werner Heisenberg's route to obtaining quantum mechanics, but Paul Dirac introduced it in his 1926 doctoral thesis, the "method of classical analogy" for quantization, and detailed it in his classic text Principles of Quantum Mechanics. The word canonical arises from the Hamiltonian approach to classical mechanics, in which a system's dynamics is generated via canonical Poisson brackets, a structure which is only partially preserved in canonical quantization. This method was further used by Paul Dirac in the context of quantum field theory, in his construction of quantum electrodynamics. In the field theory context, it is also called the second quantization of fields, in contrast to the semi-classical first quantization of single particles. History When it was first developed, quantum physics dealt only with the quantization of the motion of particles, leaving the electromagnetic field classical, hence the name quantum mechanics. Later the electromagnetic field was also quantized, and even the particles themselves became represented through quantized fields, resulting in the development of quantum electrodynamics (QED) and quantum field theory in general. Thus, by convention, the original form of particle quantum mechanics is denoted first quantization, while quantum field theory is formulated in the language of second quantization. First quantization Single particle systems The following exposition is based on Dirac's treatise on quantum mechanics. In the classical mechanics of a particle, there are dynamic variables which are called coordinates (x) and momenta (p). These specify the state of a classical system. The canonical structure (also known as the symplectic structure) of classical mechanics consists of Poisson brackets enclosing these variables, such as {x, p} = 1. All transformations of variables which preserve these brackets are allowed as canonical transformations in classical mechanics. Motion itself is such a canonical transformation. By contrast, in quantum mechanics, all significant features of a particle are contained in a state | ψ ⟩ {\displaystyle |\psi \rangle } , called a quantum state. Observables are represented by operators acting on a Hilbert space of such quantum states. The eigenvalue of an operator acting on one of its eigenstates represents the value of a measurement on the particle thus represented. For example, the energy is read off by the Hamiltonian operator H ^ {\displaystyle {\hat {H}}} acting on a state | ψ n ⟩ {\displaystyle |\psi _{n}\rangle } , yielding H ^ | ψ n ⟩ = E n | ψ n ⟩ , {\displaystyle {\hat {H}}|\psi _{n}\rangle =E_{n}|\psi _{n}\rangle ,} where En is the characteristic energy associated to this | ψ n ⟩ {\displaystyle |\psi _{n}\rangle } eigenstate. Any state could be represented as a linear combination of eigenstates of energy; for example, | ψ ⟩ = ∑ n = 0 ∞ a n | ψ n ⟩ , {\displaystyle |\psi \rangle =\sum _{n=0}^{\infty }a_{n}|\psi _{n}\rangle ,} where an are constant coefficients. As in classical mechanics, all dynamical operators can be represented by functions of the position and momentum ones, X ^ {\displaystyle {\hat {X}}} and P ^ {\displaystyle {\hat {P}}} , respectively. The connection between this representation and the more usual wavefunction representation is given by the eigenstate of the position operator X ^ {\displaystyle {\hat {X}}} representing a particle at position x {\displaystyle x} , which is denoted by an element | x ⟩ {\displaystyle |x\rangle } in the Hilbert space, and which satisfies X ^ | x ⟩ = x | x ⟩ {\displaystyle {\hat {X}}|x\rangle =x|x\rangle } . Then, ψ ( x ) = ⟨ x | ψ ⟩ {\displaystyle \psi (x)=\langle x|\psi \rangle } . Likewise, the eigenstates | p ⟩ {\displaystyle |p\rangle } of the momentum operator P ^ {\displaystyle {\hat {P}}} specify the momentum representation: ψ ( p ) = ⟨ p | ψ ⟩ {\displaystyle \psi (p)=\langle p|\psi \rangle } . The central relation between these operators is a quantum analog of the above Poisson bracket of classical mechanics, the canonical commutation relation, [ X ^ , P ^ ] = X ^ P ^ − P ^ X ^ = i ℏ . {\displaystyle [{\hat {X}},{\hat {P}}]={\hat {X}}{\hat {P}}-{\hat {P}}{\hat {X}}=i\hbar .} This relation encodes (and formally leads to) the uncertainty principle, in the form Δx Δp ≥ ħ/2. This algebraic structure may be thus considered as the quantum analog of the canonical structure of classical mechanics. Many-particle systems When turning to N-particle systems, i.e., systems containing N identical particles (particles characterized by the same quantum numbers such as mass, charge and spin), it is necessary to extend the single-particle state function ψ ( r ) {\displaystyle \psi (\mathbf {r} )} to the N-particle state function ψ ( r 1 , r 2 , … , r N ) {\displaystyle \psi (\mathbf {r} _{1},\mathbf {r} _{2},\dots ,\mathbf {r} _{N})} . A fundamental difference between classical and quantum mechanics concerns the concept of indistinguishability of identical particles. Only two species of particles are thus possible in quantum physics, the so-called bosons and fermions which obey the following rules for each kind of particle: for bosons: ψ ( r 1 , … , r j , … , r k , … , r N ) = + ψ ( r 1 , … , r k , … , r j , … , r N ) , {\displaystyle \psi (\mathbf {r} _{1},\dots ,\mathbf {r} _{j},\dots ,\mathbf {r} _{k},\dots ,\mathbf {r} _{N})=+\psi (\mathbf {r} _{1},\dots ,\mathbf {r} _{k},\dots ,\mathbf {r} _{j},\dots ,\mathbf {r} _{N}),} for fermions: ψ ( r 1 , … , r j , … , r k , … , r N ) = − ψ ( r 1 , … , r k , … , r j , … , r N ) , {\displaystyle \psi (\mathbf {r} _{1},\dots ,\mathbf {r} _{j},\dots ,\mathbf {r} _{k},\dots ,\mathbf {r} _{N})=-\psi (\mathbf {r} _{1},\dots ,\mathbf {r} _{k},\dots ,\mathbf {r} _{j},\dots ,\mathbf {r} _{N}),} where we have interchanged two coordinates ( r j , r k ) {\displaystyle (\mathbf {r} _{j},\mathbf {r} _{k})} of the state function. The usual wave function is obtained using the Slater determinant and the identical particles theory. Using this basis, it is possible to solve various many-particle problems. Issues and limitations Classical and quantum brackets Dirac's book details his popular rule of supplanting Poisson brackets by commutators: One might interpret this proposal as saying that we should seek a "quantization map" Q {\displaystyle Q} mapping a function f {\displaystyle f} on the classical phase space to an operator Q f {\displaystyle Q_{f}} on the quantum Hilbert space such that Q { f , g } = 1 i ℏ [ Q f , Q g ] {\displaystyle Q_{\{f,g\}}={\frac {1}{i\hbar }}[Q_{f},Q_{g}]} It is now known that there is no reasonable such quantization map satisfying the above identity exactly for all functions f {\displaystyle f} and g {\displaystyle g} . Groenewold's theorem One concrete version of the above impossibility claim is Groenewold's theorem (after Dutch theoretical physicist Hilbrand J. Groenewold), which we describe for a system with one degree of freedom for simplicity. Let us accept the following "ground rules" for the map Q {\displaystyle Q} . First, Q {\displaystyle Q} should send the constant function 1 to the identity operator. Second, Q {\displaystyle Q} should take x {\displaystyle x} and p {\displaystyle p} to the usual position and momentum operators X {\displaystyle X} and P {\displaystyle P} . Third, Q {\displaystyle Q} should take a polynomial in x {\displaystyle x} and p {\displaystyle p} to a "polynomial" in X {\displaystyle X} and P {\displaystyle P} , that is, a finite linear combinations of products of X {\displaystyle X} and P {\displaystyle P} , which may be taken in any desired order. In its simplest form, Groenewold's theorem says that there is no map satisfying the above ground rules and also the bracket condition Q { f , g } = 1 i ℏ [ Q f , Q g ] {\displaystyle Q_{\{f,g\}}={\frac {1}{i\hbar }}[Q_{f},Q_{g}]} for all polynomials f {\displaystyle f} and g {\displaystyle g} . Actually, the nonexistence of such a map occurs already by the time we reach polynomials of degree four. Note that the Poisson bracket of two polynomials of degree four has degree six, so it does not exactly make sense to require a map on polynomials of degree four to respect the bracket condition. We can, however, require that the bracket condition holds when f {\displaystyle f} and g {\displaystyle g} have degree three. Groenewold's theorem can be stated as follows: The proof can be outlined as follows. Suppose we first try to find a quantization map on polynomials of degree less than or equal to three satisfying the bracket condition whenever f {\displaystyle f} has degree less than or equal to two and g {\displaystyle g} has degree less than or equal to two. Then there is precisely one such map, and it is the Weyl quantization. The impossibility result now is obtained by writing the same polynomial of degree four as a Poisson bracket of polynomials of degree three in two different ways. Specifically, we have x 2 p 2 = 1 9 { x 3 , p 3 } = 1 3 { x 2 p , x p 2 } {\displaystyle x^{2}p^{2}={\frac {1}{9}}\{x^{3},p^{3}\}={\frac {1}{3}}\{x^{2}p,xp^{2}\}} On the other hand, we have already seen that if there is going to be a quantization map on polynomials of degree three, it must be the Weyl quantization; that is, we have already determined the only possible quantization of all the cubic polynomials above. The argument is finished by computing by brute force that 1 9 [ Q ( x 3 ) , Q ( p 3 ) ] {\displaystyle {\frac {1}{9}}[Q(x^{3}),Q(p^{3})]} does not coincide with 1 3 [ Q ( x 2 p ) , Q ( x p 2 ) ] . {\displaystyle {\frac {1}{3}}[Q(x^{2}p),Q(xp^{2})].} Thus, we have two incompatible requirements for the value of Q ( x 2 p 2 ) {\displaystyle Q(x^{2}p^{2})} . Axioms for quantization If Q represents the quantization map that acts on functions f in classical phase space, then the following properties are usually considered desirable: Q x ψ = x ψ {\displaystyle Q_{x}\psi =x\psi } and Q p ψ = − i ℏ ∂ x ψ {\displaystyle Q_{p}\psi =-i\hbar \partial _{x}\psi ~~} (elementary position/momentum operators) f ⟼ Q f {\displaystyle f\longmapsto Q_{f}~~} is a linear map [ Q f , Q g ] = i ℏ Q { f , g } {\displaystyle [Q_{f},Q_{g}]=i\hbar Q_{\{f,g\}}~~} (Poisson bracket) Q g ∘ f = g ( Q f ) {\displaystyle Q_{g\circ f}=g(Q_{f})~~} (von Neumann rule). However, not only are these four properties mutually inconsistent, any three of them are also inconsistent! As it turns out, the only pairs of these properties that lead to self-consistent, nontrivial solutions are 2 & 3, and possibly 1 & 3 or 1 & 4. Accepting properties 1 & 2, along with a weaker condition that 3 be true only asymptotically in the limit ħ→0 (see Moyal bracket), leads to deformation quantization, and some extraneous information must be provided, as in the standard theories utilized in most of physics. Accepting properties 1 & 2 & 3 but restricting the space of quantizable observables to exclude terms such as the cubic ones in the above example amounts to geometric quantization. Second quantization: field theory Quantum mechanics was successful at describing non-relativistic systems with fixed numbers of particles, but a new framework was needed to describe systems in which particles can be created or destroyed, for example, the electromagnetic field, considered as a collection of photons. It was eventually realized that special relativity was inconsistent with single-particle quantum mechanics, so that all particles are now described relativistically by quantum fields. When the canonical quantization procedure is applied to a field, such as the electromagnetic field, the classical field variables become quantum operators. Thus, the normal modes comprising the amplitude of the field are simple oscillators, each of which is quantized in standard first quantization, above, without ambiguity. The resulting quanta are identified with individual particles or excitations. For example, the quanta of the electromagnetic field are identified with photons. Unlike first quantization, conventional second quantization is completely unambiguous, in effect a functor, since the constituent set of its oscillators are quantized unambiguously. Historically, quantizing the classical theory of a single particle gave rise to a wavefunction. The classical equations of motion of a field are typically identical in form to the (quantum) equations for the wave-function of one of its quanta. For example, the Klein–Gordon equation is the classical equation of motion for a free scalar field, but also the quantum equation for a scalar particle wave-function. This meant that quantizing a field appeared to be similar to quantizing a theory that was already quantized, leading to the fanciful term second quantization in the early literature, which is still used to describe field quantization, even though the modern interpretation detailed is different. One drawback to canonical quantization for a relativistic field is that by relying on the Hamiltonian to determine time dependence, relativistic invariance is no longer manifest. Thus it is necessary to check that relativistic invariance is not lost. Alternatively, the Feynman integral approach is available for quantizing relativistic fields, and is manifestly invariant. For non-relativistic field theories, such as those used in condensed matter physics, Lorentz invariance is not an issue. Field operators Quantum mechanically, the variables of a field (such as the field's amplitude at a given point) are represented by operators on a Hilbert space. In general, all observables are constructed as operators on the Hilbert space, and the time-evolution of the operators is governed by the Hamiltonian, which must be a positive operator. A state | 0 ⟩ {\displaystyle |0\rangle } annihilated by the Hamiltonian must be identified as the vacuum state, which is the basis for building all other states. In a non-interacting (free) field theory, the vacuum is normally identified as a state containing zero particles. In a theory with interacting particles, identifying the vacuum is more subtle, due to vacuum polarization, which implies that the physical vacuum in quantum field theory is never really empty. For further elaboration, see the articles on the quantum mechanical vacuum and the vacuum of quantum chromodynamics. The details of the canonical quantization depend on the field being quantized, and whether it is free or interacting. Real scalar field A scalar field theory provides a good example of the canonical quantization procedure. Classically, a scalar field is a collection of an infinity of oscillator normal modes. It suffices to consider a 1+1-dimensional space-time R × S 1 , {\displaystyle \mathbb {R} \times S_{1},} in which the spatial direction is compactified to a circle of circumference 2π, rendering the momenta discrete. The classical Lagrangian density describes an infinity of coupled harmonic oscillators, labelled by x which is now a label (and not the displacement dynamical variable to be quantized), denoted by the classical field φ, L ( ϕ ) = 1 2 ( ∂ t ϕ ) 2 − 1 2 ( ∂ x ϕ ) 2 − 1 2 m 2 ϕ 2 − V ( ϕ ) , {\displaystyle {\mathcal {L}}(\phi )={\tfrac {1}{2}}(\partial _{t}\phi )^{2}-{\tfrac {1}{2}}(\partial _{x}\phi )^{2}-{\tfrac {1}{2}}m^{2}\phi ^{2}-V(\phi ),} where V(φ) is a potential term, often taken to be a polynomial or monomial of degree 3 or higher. The action functional is S ( ϕ ) = ∫ L ( ϕ ) d x d t = ∫ L ( ϕ , ∂ t ϕ ) d t . {\displaystyle S(\phi )=\int {\mathcal {L}}(\phi )dxdt=\int L(\phi ,\partial _{t}\phi )dt\,.} The canonical momentum obtained via the Legendre transformation using the action L is π = ∂ t ϕ {\displaystyle \pi =\partial _{t}\phi } , and the classical Hamiltonian is found to be H ( ϕ , π ) = ∫ d x [ 1 2 π 2 + 1 2 ( ∂ x ϕ ) 2 + 1 2 m 2 ϕ 2 + V ( ϕ ) ] . {\displaystyle H(\phi ,\pi )=\int dx\left[{\tfrac {1}{2}}\pi ^{2}+{\tfrac {1}{2}}(\partial _{x}\phi )^{2}+{\tfrac {1}{2}}m^{2}\phi ^{2}+V(\phi )\right].} Canonical quantization treats the variables φ and π as operators with canonical commutation relations at time t= 0, given by [ ϕ ( x ) , ϕ ( y ) ] = 0 , [ π ( x ) , π ( y ) ] = 0 , [ ϕ ( x ) , π ( y ) ] = i ℏ δ ( x − y ) . {\displaystyle [\phi (x),\phi (y)]=0,\ \ [\pi (x),\pi (y)]=0,\ \ [\phi (x),\pi (y)]=i\hbar \delta (x-y).} Operators constructed from φ and π can then formally be defined at other times via the time-evolution generated by the Hamiltonian, O ( t ) = e i t H O e − i t H . {\displaystyle {\mathcal {O}}(t)=e^{itH}{\mathcal {O}}e^{-itH}.} However, since φ and π no longer commute, this expression is ambiguous at the quantum level. The problem is to construct a representation of the relevant operators O {\displaystyle {\mathcal {O}}} on a Hilbert space H {\displaystyle {\mathcal {H}}} and to construct a positive operator H as a quantum operator on this Hilbert space in such a way that it gives this evolution for the operators O {\displaystyle {\mathcal {O}}} as given by the preceding equation, and to show that H {\displaystyle {\mathcal {H}}} contains a vacuum state | 0 ⟩ {\displaystyle |0\rangle } on which H has zero eigenvalue. In practice, this construction is a difficult problem for interacting field theories, and has been solved completely only in a few simple cases via the methods of constructive quantum field theory. Many of these issues can be sidestepped using the Feynman integral as described for a particular V(φ) in the article on scalar field theory. In the case of a free field, with V(φ) = 0, the quantization procedure is relatively straightforward. It is convenient to Fourier transform the fields, so that ϕ k = ∫ ϕ ( x ) e − i k x d x , π k = ∫ π ( x ) e − i k x d x . {\displaystyle \phi _{k}=\int \phi (x)e^{-ikx}dx,\ \ \pi _{k}=\int \pi (x)e^{-ikx}dx.} The reality of the fields implies that ϕ − k = ϕ k † , π − k = π k † . {\displaystyle \phi _{-k}=\phi _{k}^{\dagger },~~~\pi _{-k}=\pi _{k}^{\dagger }.} The classical Hamiltonian may be expanded in Fourier modes as H = 1 2 ∑ k = − ∞ ∞ [ π k π k † + ω k 2 ϕ k ϕ k † ] , {\displaystyle H={\frac {1}{2}}\sum _{k=-\infty }^{\infty }\left[\pi _{k}\pi _{k}^{\dagger }+\omega _{k}^{2}\phi _{k}\phi _{k}^{\dagger }\right],} where ω k = k 2 + m 2 {\displaystyle \omega _{k}={\sqrt {k^{2}+m^{2}}}} . This Hamiltonian is thus recognizable as an infinite sum of classical normal mode oscillator excitations φk, each one of which is quantized in the standard manner, so the free quantum Hamiltonian looks identical. It is the φks that have become operators obeying the standard commutation relations, [φk, πk†] = [φk†, πk] = iħ, with all others vanishing. The collective Hilbert space of all these oscillators is thus constructed using creation and annihilation operators constructed from these modes, a k = 1 2 ℏ ω k ( ω k ϕ k + i π k ) , a k † = 1 2 ℏ ω k ( ω k ϕ k † − i π k † ) , {\displaystyle a_{k}={\frac {1}{\sqrt {2\hbar \omega _{k}}}}\left(\omega _{k}\phi _{k}+i\pi _{k}\right),\ \ a_{k}^{\dagger }={\frac {1}{\sqrt {2\hbar \omega _{k}}}}\left(\omega _{k}\phi _{k}^{\dagger }-i\pi _{k}^{\dagger }\right),} for which [ak, ak†] = 1 for all k, with all other commutators vanishing. The vacuum | 0 ⟩ {\displaystyle |0\rangle } is taken to be annihilated by all of the ak, and H {\displaystyle {\mathcal {H}}} is the Hilbert space constructed by applying any combination of the infinite collection of creation operators ak† to | 0 ⟩ {\displaystyle |0\rangle } . This Hilbert space is called Fock space. For each k, this construction is identical to a quantum harmonic oscillator. The quantum field is an infinite array of quantum oscillators. The quantum Hamiltonian then amounts to H = ∑ k = − ∞ ∞ ℏ ω k a k † a k = ∑ k = − ∞ ∞ ℏ ω k N k , {\displaystyle H=\sum _{k=-\infty }^{\infty }\hbar \omega _{k}a_{k}^{\dagger }a_{k}=\sum _{k=-\infty }^{\infty }\hbar \omega _{k}N_{k},} where Nk may be interpreted as the number operator giving the number of particles in a state with momentum k. This Hamiltonian differs from the previous expression by the subtraction of the zero-point energy ħωk/2 of each harmonic oscillator. This satisfies the condition that H must annihilate the vacuum, without affecting the time-evolution of operators via the above exponentiation operation. This subtraction of the zero-point energy may be considered to be a resolution of the quantum operator ordering ambiguity, since it is equivalent to requiring that all creation operators appear to the left of annihilation operators in the expansion of the Hamiltonian. This procedure is known as Wick ordering or normal ordering. Other fields All other fields can be quantized by a generalization of this procedure. Vector or tensor fields simply have more components, and independent creation and destruction operators must be introduced for each independent component. If a field has any internal symmetry, then creation and destruction operators must be introduced for each component of the field related to this symmetry as well. If there is a gauge symmetry, then the number of independent components of the field must be carefully analyzed to avoid over-counting equivalent configurations, and gauge-fixing may be applied if needed. It turns out that commutation relations are useful only for quantizing bosons, for which the occupancy number of any state is unlimited. To quantize fermions, which satisfy the Pauli exclusion principle, anti-commutators are needed. These are defined by {A, B} = AB + BA. When quantizing fermions, the fields are expanded in creation and annihilation operators, θk†, θk, which satisfy { θ k , θ l † } = δ k l , { θ k , θ l } = 0 , { θ k † , θ l † } = 0. {\displaystyle \{\theta _{k},\theta _{l}^{\dagger }\}=\delta _{kl},\ \ \{\theta _{k},\theta _{l}\}=0,\ \ \{\theta _{k}^{\dagger },\theta _{l}^{\dagger }\}=0.} The states are constructed on a vacuum | 0 ⟩ {\displaystyle |0\rangle } annihilated by the θk, and the Fock space is built by applying all products of creation operators θk† to |0⟩. Pauli's exclusion principle is satisfied, because ( θ k † ) 2 | 0 ⟩ = 0 {\displaystyle (\theta _{k}^{\dagger })^{2}|0\rangle =0} , by virtue of the anti-commutation relations. Condensates The construction of the scalar field states above assumed that the potential was minimized at φ = 0, so that the vacuum minimizing the Hamiltonian satisfies ⟨φ⟩ = 0, indicating that the vacuum expectation value (VEV) of the field is zero. In cases involving spontaneous symmetry breaking, it is possible to have a non-zero VEV, because the potential is minimized for a value φ = v . This occurs for example, if V(φ) = gφ4 − 2m2φ2 with g > 0 and m2 > 0, for which the minimum energy is found at v = ±m/√g. The value of v in one of these vacua may be considered as condensate of the field φ. Canonical quantization then can be carried out for the shifted field φ(x,t) − v, and particle states with respect to the shifted vacuum are defined by quantizing the shifted field. This construction is utilized in the Higgs mechanism in the standard model of particle physics. Mathematical quantization Deformation quantization The classical theory is described using a spacelike foliation of spacetime with the state at each slice being described by an element of a symplectic manifold with the time evolution given by the symplectomorphism generated by a Hamiltonian function over the symplectic manifold. The quantum algebra of "operators" is an ħ-deformation of the algebra of smooth functions over the symplectic space such that the leading term in the Taylor expansion over ħ of the commutator [A, B] expressed in the phase space formulation is iħ{A, B} . (Here, the curly braces denote the Poisson bracket. The subleading terms are all encoded in the Moyal bracket, the suitable quantum deformation of the Poisson bracket.) In general, for the quantities (observables) involved, and providing the arguments of such brackets, ħ-deformations are highly nonunique—quantization is an "art", and is specified by the physical context. (Two different quantum systems may represent two different, inequivalent, deformations of the same classical limit, ħ → 0.) Now, one looks for unitary representations of this quantum algebra. With respect to such a unitary representation, a symplectomorphism in the classical theory would now deform to a (metaplectic) unitary transformation. In particular, the time evolution symplectomorphism generated by the classical Hamiltonian deforms to a unitary transformation generated by the corresponding quantum Hamiltonian. A further generalization is to consider a Poisson manifold instead of a symplectic space for the classical theory and perform an ħ-deformation of the corresponding Poisson algebra or even Poisson supermanifolds. Geometric quantization In contrast to the theory of deformation quantization described above, geometric quantization seeks to construct an actual Hilbert space and operators on it. Starting with a symplectic manifold M {\displaystyle M} , one first constructs a prequantum Hilbert space consisting of the space of square-integrable sections of an appropriate line bundle over M {\displaystyle M} . On this space, one can map all classical observables to operators on the prequantum Hilbert space, with the commutator corresponding exactly to the Poisson bracket. The prequantum Hilbert space, however, is clearly too big to describe the quantization of M {\displaystyle M} . One then proceeds by choosing a polarization, that is (roughly), a choice of n {\displaystyle n} variables on the 2 n {\displaystyle 2n} -dimensional phase space. The quantum Hilbert space is then the space of sections that depend only on the n {\displaystyle n} chosen variables, in the sense that they are covariantly constant in the other n {\displaystyle n} directions. If the chosen variables are real, we get something like the traditional Schrödinger Hilbert space. If the chosen variables are complex, we get something like the Segal–Bargmann space. See also Correspondence principle Creation and annihilation operators Dirac bracket Moyal bracket Phase space formulation (of quantum mechanics) Geometric quantization References Historical References Silvan S. Schweber: QED and the men who made it, Princeton Univ. Press, 1994, ISBN 0-691-03327-7 General Technical References Alexander Altland, Ben Simons: Condensed matter field theory, Cambridge Univ. Press, 2009, ISBN 978-0-521-84508-3 James D. Bjorken, Sidney D. Drell: Relativistic quantum mechanics, New York, McGraw-Hill, 1964 Hall, Brian C. (2013), Quantum Theory for Mathematicians, Graduate Texts in Mathematics, vol. 267, Springer, Bibcode:2013qtm..book.....H, ISBN 978-1461471158. An introduction to quantum field theory, by M.E. Peskin and H.D. Schroeder, ISBN 0-201-50397-2 Franz Schwabl: Advanced Quantum Mechanics, Berlin and elsewhere, Springer, 2009 ISBN 978-3-540-85061-8 External links Pedagogic Aides to Quantum Field Theory Click on the links for Chaps. 1 and 2 at this site to find an extensive, simplified introduction to second quantization. See Sect. 1.5.2 in Chap. 1. See Sect. 2.7 and the chapter summary in Chap. 2. In quantum field theory, a false vacuum is a hypothetical vacuum state that is locally stable but does not occupy the most stable possible ground state. In this condition it is called metastable. It may last for a very long time in this state, but could eventually decay to the more stable one, an event known as false vacuum decay. The most common suggestion of how such a decay might happen in our universe is called bubble nucleation – if a small region of the universe by chance reached a more stable vacuum, this "bubble" (also called "bounce") would spread. A false vacuum exists at a local minimum of energy and is therefore not completely stable, in contrast to a true vacuum, which exists at a global minimum and is stable. Definition of true vs. false vacuum A vacuum is defined as a space with as little energy in it as possible. Despite the name, the vacuum still has quantum fields. A true vacuum is stable because it is at a global minimum of energy, and is commonly assumed to coincide with the physical vacuum state we live in. It is possible that a physical vacuum state is a configuration of quantum fields representing a local minimum but not global minimum of energy. This type of vacuum state is called a "false vacuum". Implications Existential threat If our universe is in a false vacuum state rather than a true vacuum state, then the decay from the less stable false vacuum to the more stable true vacuum (called false vacuum decay) could have dramatic consequences. The effects could range from complete cessation of existing fundamental forces, elementary particles and structures comprising them, to subtle change in some cosmological parameters, mostly depending on the potential difference between true and false vacuum. Some false vacuum decay scenarios are compatible with the survival of structures like galaxies, stars, and even biological life, while others involve the full destruction of baryonic matter or even immediate gravitational collapse of the universe. In this more extreme case, the likelihood of a "bubble" forming is very low (i.e. false vacuum decay may be impossible). A paper by Coleman and De Luccia that attempted to include simple gravitational assumptions into these theories noted that if this was an accurate representation of nature, then the resulting universe "inside the bubble" in such a case would appear to be extremely unstable and would almost immediately collapse: In general, gravitation makes the probability of vacuum decay smaller; in the extreme case of minimal energy-density difference, it can even stabilize the false vacuum, preventing vacuum decay altogether. We believe we understand this. For the vacuum to decay, building a bubble of total energy zero must be possible. In the absence of gravitation, this is no problem, no matter how small the energy-density difference; all one has to do is make the bubble big enough, and the volume/surface ratio will do the job. In the presence of gravitation, though, the negative energy density of the true vacuum distorts geometry within the bubble with the result that, for a small enough energy density, there is no bubble with a big enough volume/surface ratio. Within the bubble, the effects of gravitation are more dramatic. The geometry of space-time within the bubble is that of anti-de Sitter space, a space much like conventional de Sitter space except that its group of symmetries is O(3, 2) rather than O(4, 1). Although this space-time is free of singularities, it is unstable under small perturbations, and inevitably suffers gravitational collapse of the same sort as the end state of a contracting Friedmann universe. The time required for the collapse of the interior universe is on the order of ... microseconds or less. The possibility that we are living in a false vacuum has never been a cheering one to contemplate. Vacuum decay is the ultimate ecological catastrophe; in the new vacuum there are new constants of nature; after vacuum decay, not only is life as we know it impossible, so is chemistry as we know it. Nonetheless, one could always draw stoic comfort from the possibility that perhaps over time the new vacuum would sustain if not life as we know it, at least some structures capable of knowing joy. This possibility has now been eliminated. The second special case is decay into a space of vanishing cosmological constant, the case that applies if we are now living in the debris of a false vacuum that decayed at some early cosmic epoch. This case presents us with less interesting physics and with fewer occasions for rhetorical excess than the preceding one. It is now the interior of the bubble that is ordinary Minkowski space ... In a 2005 paper published in Nature, as part of their investigation into global catastrophic risks, MIT physicist Max Tegmark and Oxford philosopher Nick Bostrom calculate the natural risks of the destruction of the Earth at less than 1/109 per year from all natural (i.e. non-anthropogenic) events, including a transition to a lower vacuum state. They argue that due to observer selection effects, we might underestimate the chances of being destroyed by vacuum decay because any information about this event would reach us only at the instant when we too were destroyed. This is in contrast to events like risks from impacts, gamma-ray bursts, supernovae and hypernovae, the frequencies of which we have adequate direct measures. Inflation A number of theories suggest that cosmic inflation may be an effect of a false vacuum decaying into the true vacuum. The inflation itself may be the consequence of the Higgs field trapped in a false vacuum state with Higgs self-coupling λ and its βλ function very close to zero at the planck scale.: 218 A future electron-positron collider would be able to provide the precise measurements of the top quark needed for such calculations. Chaotic inflation theory suggests that the universe may be in either a false vacuum or a true vacuum state. Alan Guth, in his original proposal for cosmic inflation, proposed that inflation could end through quantum mechanical bubble nucleation of the sort described above. See history of Chaotic inflation theory. It was soon understood that a homogeneous and isotropic universe could not be preserved through the violent tunneling process. This led Andrei Linde and, independently, Andreas Albrecht and Paul Steinhardt, to propose "new inflation" or "slow roll inflation" in which no tunnelling occurs, and the inflationary scalar field instead graphs as a gentle slope. In 2014, researchers at the Chinese Academy of Sciences' Wuhan Institute of Physics and Mathematics gave an actual mathematical demonstration of the already existing idea that the universe could have been spontaneously created from nothing (no space, time, nor matter) by quantum fluctuations of a metastable false vacuum causing an expanding bubble of true vacuum. Vacuum decay varieties Electroweak vacuum decay The stability criteria for the electroweak interaction was first formulated in 1979 as a function of the masses of the theoretical Higgs boson and the heaviest fermion. Discovery of the top quark in 1995 and the Higgs boson in 2012 have allowed physicists to validate the criteria against experiment, therefore since 2012 the electroweak interaction is considered as the most promising candidate for a metastable fundamental force. The corresponding false vacuum hypothesis is called either "Electroweak vacuum instability" or "Higgs vacuum instability". The present false vacuum state is called d S {\displaystyle dS} (de Sitter space), while tentative true vacuum is called A d S {\displaystyle AdS} (Anti-de Sitter space). The diagrams show the uncertainty ranges of Higgs boson and top quark masses as oval-shaped lines. Underlying colors indicate if the electroweak vacuum state is likely to be stable, merely long-lived or completely unstable for given combination of masses. The "electroweak vacuum decay" hypothesis was sometimes misreported as the Higgs boson "ending" the universe. A 125.18±0.16 GeV/c2 Higgs boson mass is likely to be on the metastable side of stable-metastable boundary (estimated in 2012 as 123.8–135.0 GeV.) A definitive answer requires much more precise measurements of the top quark's pole mass, however, although improved measurement precision of Higgs boson and top quark masses further reinforced the claim of physical electroweak vacuum being in the metastable state as of 2018. Nonetheless, new physics beyond the Standard Model of Particle Physics could drastically change the stability landscape division lines, rendering previous stability and metastability criteria incorrect. Reanalysis of 2016 LHC run data in 2022 has yielded a slightly lower top quark mass of 171.77±0.38 GeV, close to vacuum stability line but still in the metastable zone. If measurements of the Higgs boson and top quark suggest that our universe lies within a false vacuum of this kind, this would imply that the bubble's effects will propagate across the universe at nearly the speed of light from its origin in space-time. A direct calculation within the Standard Model of the lifetime of our vacuum state finds that it is greater than 10 65 {\displaystyle 10^{65}} years with 95% confidence. Other decay modes Decay to smaller vacuum expectation value, resulting in decrease of Casimir effect and destabilization of protons. Decay to vacuum with larger neutrino mass (may have happened as late as few billion years ago). Decay to vacuum with no dark energy. Decay of the false vacuum at finite temperature was first observed in ferromagnetic superfluids of ultracold atoms. Bubble nucleation When the false vacuum decays, the lower-energy true vacuum forms through a process known as bubble nucleation. In this process, instanton effects cause a bubble containing the true vacuum to appear. The walls of the bubble (or domain walls) have a positive surface tension, as energy is expended as the fields roll over the potential barrier to the true vacuum. The former tends as the cube of the bubble's radius while the latter is proportional to the square of its radius, so there is a critical size R c {\displaystyle R_{c}} at which the total energy of the bubble is zero; smaller bubbles tend to shrink, while larger bubbles tend to grow. To be able to nucleate, the bubble must overcome an energy barrier of height where Δ Φ {\displaystyle \Delta \Phi } is the difference in energy between the true and false vacuums, γ {\displaystyle \gamma } is the unknown (possibly extremely large) surface tension of the domain wall, and R {\displaystyle R} is the radius of the bubble. Rewriting Eq. 1 gives the critical radius as A bubble smaller than the critical size can overcome the potential barrier via quantum tunnelling of instantons to lower energy states. For a large potential barrier, the tunneling rate per unit volume of space is given by where ℏ {\displaystyle \hbar } is the reduced Planck constant. As soon as a bubble of lower-energy vacuum grows beyond the critical radius defined by Eq. 2, the bubble's wall will begin to accelerate outward. Due to the typically large difference in energy between the false and true vacuums, the speed of the wall approaches the speed of light extremely quickly. The bubble does not produce any gravitational effects because the negative energy density of the bubble interior is cancelled out by the positive kinetic energy of the wall. Small bubbles of true vacuum can be inflated to critical size by providing energy, although required energy densities are several orders of magnitude larger than what is attained in any natural or artificial process. It is also thought that certain environments can catalyze bubble formation by lowering the potential barrier. Bubble wall has a finite thickness, depending on ratio between energy barrier and energy gain obtained by creating true vacuum. In the case when potential barrier height between true and false vacua is much smaller than energy difference between vacua, shell thickness become comparable with critical radius. Nucleation seeds In general, gravity is believed to stabilize a false vacuum state, at least for transition from d S {\displaystyle dS} (de Sitter space) to A d S {\displaystyle AdS} (Anti-de Sitter space), while topological defects including cosmic strings and magnetic monopoles may enhance decay probability. Black holes as nucleation seeds In a study in 2015, it was pointed out that the vacuum decay rate could be vastly increased in the vicinity of black holes, which would serve as a nucleation seed. According to this study, a potentially catastrophic vacuum decay could be triggered at any time by primordial black holes, should they exist. However, the authors note that if primordial black holes cause a false vacuum collapse, then it should have happened long before humans evolved on Earth. A subsequent study in 2017 indicated that the bubble would collapse into a primordial black hole rather than originate from it, either by ordinary collapse or by bending space in such a way that it breaks off into a new universe. In 2019, it was found that although small non-spinning black holes may increase true vacuum nucleation rate, rapidly spinning black holes will stabilize false vacuums to decay rates lower than expected for flat space-time. If particle collisions produce mini black holes, then energetic collisions such as the ones produced in the Large Hadron Collider (LHC) could trigger such a vacuum decay event, a scenario that has attracted the attention of the news media. It is likely to be unrealistic, because if such mini black holes can be created in collisions, they would also be created in the much more energetic collisions of cosmic radiation particles with planetary surfaces or during the early life of the universe as tentative primordial black holes. Hut and Rees note that, because cosmic ray collisions have been observed at much higher energies than those produced in terrestrial particle accelerators, these experiments should not, at least for the foreseeable future, pose a threat to our current vacuum. Particle accelerators have reached energies of only approximately eight tera electron volts (8×1012 eV). Cosmic ray collisions have been observed at and beyond energies of 5×1019 eV, six million times more powerful – the so-called Greisen–Zatsepin–Kuzmin limit – and cosmic rays in vicinity of origin may be more powerful yet. John Leslie has argued that if present trends continue, particle accelerators will exceed the energy given off in naturally occurring cosmic ray collisions by the year 2150. Fears of this kind were raised by critics of both the Relativistic Heavy Ion Collider and the Large Hadron Collider at the time of their respective proposal, and determined to be unfounded by scientific inquiry. In a 2021 paper by Rostislav Konoplich and others, it was postulated that the area between a pair of large black holes on the verge of colliding could provide the conditions to create bubbles of "true vacuum". Intersecting surfaces between these bubbles could then become infinitely dense and form micro-black holes. These would in turn evaporate by emitting Hawking radiation in the 10 milliseconds or so before the larger black holes collided and devoured any bubbles or micro-black holes in their way. The theory could be tested by looking for the Hawking radiation emitted just before the black holes merge. Bubble propagation A bubble wall, propagating outward at nearly the speed of light, has a finite thickness, depending on the ratio between the energy barrier and the energy gain obtained by creating true vacuum. In the case when the potential barrier height between true and false vacua is much smaller than the energy difference between vacua, the bubble wall thickness becomes comparable to the critical radius. Elementary particles entering the wall will likely decay to other particles or black holes. If all decay paths lead to very massive particles, the energy barrier of such a decay may result in a stable bubble of false vacuum (also known as a Fermi ball) enclosing the false-vacuum particle instead of immediate decay. Multi-particle objects can be stabilized as Q-balls, although these objects will eventually collide and decay either into black holes or true-vacuum particles. False vacuum decay in fiction False vacuum decay event is occasionally used as a plot device in works picturing a doomsday event. 1980 by Jack L. Chalker in his science-fiction novel The Return of Nathan Brazil, the fourth book in the Well of Souls series (although not named as such in the novel). 1988 by Geoffrey A. Landis in his science-fiction short story Vacuum States 2000 by Stephen Baxter in his science fiction novel Time 2002 by Greg Egan in his science fiction novel Schild's Ladder 2002 by Liu Cixin in his science fiction novel Zhao Wen Dao 2008 by Koji Suzuki in his science fiction novel Edge 2015 by Alastair Reynolds in his science fiction novel Poseidon's Wake 2018 by System Erasure in their video game ZeroRanger See also Eternal inflation – Hypothetical inflationary universe model Supercooling – Lowering the temperature of a liquid below its freezing point without it becoming a solid Superheating – Heating a liquid to a temperature above its boiling point without boiling Void – Vast empty spaces between filaments with few or no galaxies Quantum cosmology – Attempts to develop a quantum mechanical theory of cosmology Why is there anything at all? § Something may exist necessarily – Metaphysical question References Further reading Johann Rafelski and Berndt Muller (1985). The Structured Vacuum – thinking about nothing. Harri Deutsch. ISBN 978-3-87144-889-8. Sidney Coleman (1988). Aspects of Symmetry: Selected Erice Lectures. Cambridge University Press. ISBN 978-0-521-31827-3. External links SimpleBounce on GitHub calculates the Euclidean action for the bounce solution that contributes to the false vacuum decay. Rafelski, Johann; Müller, Berndt (1985). The Structured Vacuum – thinking about nothing (PDF). H. Deutsch. ISBN 3-87144-889-3. Guth, Alan. "An eternity of bubbles?". PBS. Archived from the original on 2012-08-25. Odenwald, Sten (1983). "The Decay of the False Vacuum". Simulation of False Vacuum Decay by Bubble Nucleation on YouTube – Joel Thorarinson. Quantum reflection is a uniquely quantum phenomenon in which an object, such as a neutron or a small molecule, reflects smoothly and in a wavelike fashion from a much larger surface, such as a pool of mercury. A classically behaving neutron or molecule will strike the same surface much like a thrown ball, hitting only at one atomic-scale location where it is either absorbed or scattered. Quantum reflection provides a powerful experimental demonstration of particle-wave duality, since it is the extended quantum wave packet of the particle, rather than the particle itself, that reflects from the larger surface. It is similar to reflection high-energy electron diffraction, where electrons reflect and diffraction from surfaces, and grazing incidence atom scattering, where the fact that atoms (and ions) can also be waves is used to diffract from surfaces. Definition In a workshop about quantum reflection, the following definition of quantum reflection was suggested: Quantum reflection is a classically counterintuitive phenomenon whereby the motion of particles is reverted "against the force" acting on them. This effect manifests the wave nature of particles and influences collisions of ultracold atoms and interaction of atoms with solid surfaces. Observation of quantum reflection has become possible thanks to recent advances in trapping and cooling atoms. Reflection of slow atoms Although the principles of quantum mechanics apply to any particles, usually the term "quantum reflection" means reflection of atoms from a surface of condensed matter (liquid or solid). The full potential experienced by the incident atom does become repulsive at a very small distance from the surface (of order of size of atoms). This is when the atom becomes aware of the discrete character of material. This repulsion is responsible for the classical scattering one would expect for particles incident on a surface. Such scattering can be diffuse rather than specular, so this component of the reflection is easy to distinguish. To reduce this part of the physical process, a grazing angle of incidence is used; this enhances the quantum reflection. This requirement of small incident velocities for the particles means that a non-relativistic approximation to quantum mechanics is appropriate. Single-dimensional approximation So far, one usually considers the single-dimensional case of this phenomenon, that is when the potential has translational symmetry in two directions ( y {\displaystyle y} and z {\displaystyle z} ), such that only a single coordinate ( x {\displaystyle x} ) is important. In this case one can examine the specular reflection of a slow neutral atom from a solid state surface . Where one has an atom in a region of free space close to a material capable of being polarized, a combination of the pure van der Waals interaction, and the related Casimir-Polder interaction attracts the atom to the surface of the material. The latter force dominates when the atom is comparatively far from the surface, and the former when the atom comes closer to the surface. The intermediate region is controversial as it is dependent upon the specific nature and quantum state of the incident atom. The condition for a reflection to occur as the atom experiences the attractive potential can be given by the presence of regions of space where the WKB approximation to the atomic wave-function breaks down. In accordance with this approximation the wavelength of the gross motion of the atom system toward the surface as a quantity local to every region along the x {\displaystyle x} axis is, λ ( x ) = h 2 m ( E − V ( x ) ) {\displaystyle \lambda \left(x\right)={\frac {h}{\sqrt {2m\left(E-V\left(x\right)\right)}}}} where m {\displaystyle m} is the atomic mass, E {\displaystyle ~E~} is its energy, and V ( x ) {\displaystyle ~V(x)~} is the potential it experiences, then it is clear that we cannot give meaning to this quantity where, | d λ ( x ) d x | ∼ 1 {\displaystyle \left|{\frac {d\lambda \left(x\right)}{dx}}\right|\sim 1} That is, in regions of space where the variation of the atomic wavelength is significant over its own length (i.e. the gradient of V ( x ) {\displaystyle V(x)} is steep), there is no meaning in the approximation of a local wavelength. This breakdown occurs irrespective of the sign of the potential, V ( x ) {\displaystyle ~V(x)~} . In such regions part of the incident atom wave-function may become reflected. Such a reflection may occur for slow atoms experiencing the comparatively rapid variation of the Van der Waals potential near the material surface. This is just the same kind of phenomenon as occurs when light passes from a material of one refractive index to another of a significantly different index over a small region of space. Irrespective of the sign of the difference in index, there will be a reflected component of the light from the interface. Indeed, quantum reflection from the surface of solid-state wafer allows one to make the quantum optical analogue of a mirror - the atomic mirror - to a high precision. Experiments with grazing incidence Practically, in many experiments with quantum reflection from Si, the grazing incidence angle is used (figure A). The set-up is mounted in a vacuum chamber to provide a several-meter path free of atoms; a good vacuum (at the level of 10−7 Torr or 130 μPa) is required. The magneto-optical trap (MOT) is used to collect cold atoms, usually excited He or Ne, approaching the point-like source of atoms. The excitation of atoms is not essential for the quantum reflection but it allows the efficient trapping and cooling using optical frequencies. In addition, the excitation of atoms allows the registration at the micro-channel plate (MCP) detector (bottom of the figure). Movable edges are used to stop atoms which do not go toward the sample (for example a Si plate), providing the collimated atomic beam. The He-Ne laser was used to control the orientation of the sample and measure the grazing angle θ {\displaystyle ~\theta ~} . At the MCP, there was observed relatively intensive strip of atoms which come straightly (without reflection) from the MOT, by-passing the sample, strong shadow of the sample (the thickness of this shadow could be used for rough control of the grazing angle), and the relatively weak strip produced by the reflected atoms. The ratio r {\displaystyle ~r~} of density of atoms registered at the center of this strip to the density of atoms at the directly illuminated region was considered as efficiency of quantum reflection, i.e., reflectivity. This reflectivity strongly depends on the grazing angle and speed of atoms. In the experiments with Ne atoms, usually just fall down, when the MOT is suddenly switched off. Then, the speed of atoms is determined as v = 2 g h {\displaystyle ~v={\sqrt {2gh}}~} , where g {\displaystyle ~g~} is acceleration of free fall, and h {\displaystyle ~h~} is distance from the MOT to the sample. In experiments described, this distance was of order of 0.5 meters (2 ft), providing the speed of order of 3 m/s (6.7 mph; 11 km/h). Then, the transversal wavenumber can be calculated as k = sin ⁡ ( θ ) m v ℏ {\displaystyle ~k=\sin(\theta ){\frac {mv}{\hbar }}~} , where m {\displaystyle ~m~} is mass of the atom, and ℏ {\displaystyle \hbar } is the Planck constant. In the case with He, the additional resonant laser could be used to release the atoms and provide them an additional velocity; the delay since the release of the atoms till the registration allowed to estimate this additional velocity; roughly, v = 1 t h {\displaystyle ~v={\frac {1}{t\!~h}}~} , where t {\displaystyle ~t~} is time delay since the release of atoms till the click at the detector. Practically, v {\displaystyle v} could vary from 20 to 130 m/s (45 to 291 mph; 72 to 468 km/h). Although the scheme at the figure looks simple, the extend facility is necessary to slow atoms, trap them and cool to millikelvin temperature, providing a micrometre size source of cold atoms. Practically, the mounting and maintaining of this facility (not shown in the figure) is the heaviest job in the experiments with quantum reflection of cold atoms. The possibility of an experiment with the quantum reflection with just a pinhole instead of MOT are discussed in the literature. Casimir and van der Waals attraction Despite this, there is some doubt as to the physical origin of quantum reflection from solid surfaces. As was briefly mentioned above, the potential in the intermediate region between the regions dominated by the Casimir-Polder and Van der Waals interactions requires an explicit Quantum Electrodynamical calculation for the particular state and type of atom incident on the surface. Such a calculation is very difficult. Indeed, there is no reason to suppose that this potential is solely attractive within the intermediate region. Thus the reflection could simply be explained by a repulsive force, which would make the phenomenon not quite so surprising. Furthermore, a similar dependence for reflectivity on the incident velocity is observed in the case of the absorption of particles in vicinity of a surface. In the simplest case, such absorption could be described with a non-Hermitian potential (i.e. one where probability is not conserved). Until 2006, the published papers interpreted the reflection in terms of a Hermitian potential; this assumption allows to build a quantitative theory. Efficient quantum reflection A qualitative estimate for the efficiency of quantum reflection can be made using dimensional analysis. Letting m {\displaystyle m} be mass of the atom and k = 2 π / λ {\displaystyle k=2\pi /\lambda } the normal component of its wave-vector, then the energy of the normal motion of the particle, E = ( ℏ k ) 2 2 m {\displaystyle E={\frac {(\hbar k)^{2}}{2m}}} should be compared to the potential, V ( x ) {\displaystyle V(x)} of interaction. The distance, | x t | {\displaystyle |x_{t}|} at which E = V ( x ) {\displaystyle E=V(x)} can be considered as the distance at which the atom will come across a troublesome discontinuity in the potential. This is the point at which the WKB method truly becomes nonsense. The condition for efficient quantum reflection can be written as k | x t | < 1 {\displaystyle k|x_{t}|<1} . In other words, the wavelength is small compared to the distance at which the atom may become reflected from the surface. If this condition holds, the aforementioned effect of the discrete character of the surface may be neglected. This argument produces a simple estimate for the reflectivity, r {\displaystyle r} , r = 1 ( 1 + k | x t | ) 4 {\displaystyle r={\frac {1}{(1+k|x_{t}|)^{4}}}} which shows good agreement with experimental data for excited neon and helium atoms, reflected from a flat silicon surface (fig.1), see and references therein. Such a fit is also in good agreement with a single-dimensional analysis of the scattering of atoms from an attractive potential,. Such agreement indicates, that, at least in the case of noble gases and Si surface, the quantum reflection can be described with single-dimensional hermitian potential, as the result of attraction of atoms to the surface. Ridged mirror The effect of quantum reflection can be enhanced using ridged mirrors . If one produces a surface consisting of a set of narrow ridges then the resulting non-uniformity of the material allows the reduction of the effective van der Waals constant; this extends the working ranges of the grazing angle. For this reduction to be valid, we must have small distances, L {\displaystyle L} between the ridges. Where L {\displaystyle L} becomes large, the non-uniformity is such that the ridged mirror must be interpreted in terms of multiple Fresnel diffraction or the Zeno effect; these interpretations give similar estimates for the reflectivity . See ridged mirror for the details. Similar enhancement of quantum reflection takes place where one has particles incident on an array of pillars . This was observed with very slow atoms (Bose–Einstein condensate) at almost normal incidence. Application of quantum reflection Quantum reflection makes the idea of solid-state atomic mirrors and atomic-beam imaging systems (atomic nanoscope) possible. The use of quantum reflection in the production of atomic traps has also been suggested. References See also Atom optics Ridged mirror Casimir force van der Waals potential In mathematical physics, the Dirac–von Neumann axioms give a mathematical formulation of quantum mechanics in terms of operators on a Hilbert space. They were introduced by Paul Dirac in 1930 and John von Neumann in 1932. Hilbert space formulation The space H {\displaystyle \mathbb {H} } is a fixed complex Hilbert space of countably infinite dimension (as a hilbert-basis). The observables of a quantum system are defined to be the (possibly unbounded) self-adjoint operators A {\displaystyle A} on H {\displaystyle \mathbb {H} } . A state ψ {\displaystyle \psi } of the quantum system is a unit vector of H {\displaystyle \mathbb {H} } , up to scalar multiples; or equivalently, a ray of the Hilbert space H {\displaystyle \mathbb {H} } . The expectation value of an observable A for a system in a state ψ {\displaystyle \psi } is given by the inner product ⟨ ψ , A ψ ⟩ {\displaystyle \langle \psi ,A\psi \rangle } . Operator algebra formulation The Dirac–von Neumann axioms can be formulated in terms of a C*-algebra as follows. The bounded observables of the quantum mechanical system are defined to be the self-adjoint elements of the C*-algebra. The states of the quantum mechanical system are defined to be the states of the C*-algebra (in other words the normalized positive linear functionals ω {\displaystyle \omega } ). The value ω ( A ) {\displaystyle \omega (A)} of a state ω {\displaystyle \omega } on an element A {\displaystyle A} is the expectation value of the observable A {\displaystyle A} if the quantum system is in the state ω {\displaystyle \omega } . Example If the C*-algebra is the algebra of all bounded operators on a Hilbert space H {\displaystyle \mathbb {H} } , then the bounded observables are just the bounded self-adjoint operators on H {\displaystyle \mathbb {H} } . If v {\displaystyle v} is a unit vector of H {\displaystyle \mathbb {H} } then ω ( A ) = ⟨ v , A v ⟩ {\displaystyle \omega (A)=\langle v,Av\rangle } is a state on the C*-algebra, meaning the unit vectors (up to scalar multiplication) give the states of the system. This is similar to Dirac's formulation of quantum mechanics, though Dirac also allowed unbounded operators, and did not distinguish clearly between self-adjoint and Hermitian operators. See also Quantum mechanics Schrödinger equation John von Neumann References Dirac, Paul (1930), The Principles of Quantum Mechanics Strocchi, F. (2008), An Introduction to the Mathematical Structure of Quantum Mechanics, Advanced Series in Mathematical Physics, vol. 28 (2nd ed.), World Scientific Publishing Co., Bibcode:2008ASMP...28.....S, doi:10.1142/7038, ISBN 9789812835222, MR 2484367 Takhtajan, Leon A. (2008), Quantum mechanics for mathematicians, Graduate Studies in Mathematics, vol. 95, Providence, RI: American Mathematical Society, doi:10.1090/gsm/095, ISBN 978-0-8218-4630-8, MR 2433906 von Neumann, John (1932), Mathematical Foundations of Quantum Mechanics, Berlin: Springer, MR 0066944 The time-dependent Ginzburg-Landau (TDGL) equations give the evolution in time of the steady-state equations of the Ginzburg-Landau theory (GL). Although phenomenological, these equations can be very useful in making qualitative predictions about the time evolution of superconductors, particularly in the mixed state where Abrikosov vortices or Pearl vortices may appear. Because of the phenomenological nature of GL theory, there are a number of different ways to expand its time dependence including different corrections and approximations. For example, in their seminal paper using TDGL to describe the time scale of fluctuations in one-dimensional superconducting wires, McCumber and Halperin adopt the following form (note units are CGS): τ ( T ) ( ∂ ∂ t + i 2 e V ℏ ) ψ = ( 1 − | ψ | 2 ) ψ + ξ ( T ) ( ∂ ∂ x − i 2 e ℏ c A x ) 2 ψ {\displaystyle \tau (T)({\frac {\partial }{\partial t}}+i{\frac {2eV}{\hbar }})\psi =(1-|\psi |^{2})\psi +\xi (T)({\frac {\partial }{\partial x}}-i{\frac {2e}{\hbar c}}A_{x})^{2}\psi } With ψ {\displaystyle \psi } the order parameter describing the degree of superconducting order; τ {\displaystyle \tau } the temperature-dependent GL relaxation time of the order parameter; V {\displaystyle V} the electrochemical potential; A x {\displaystyle A_{x}} the magnetic vector potential; and ξ {\displaystyle \xi } the superconducting coherence length. However, other forms exist. Sometimes the electrochemical potential is dropped for convenience, even though it increases the quantitative accuracy of the TDGL equations, and sometimes other correction terms are added. == References == A Bell test, also known as Bell inequality test or Bell experiment, is a real-world physics experiment designed to test the theory of quantum mechanics in relation to Albert Einstein's concept of local realism. Named for John Stewart Bell, the experiments test whether or not the real world satisfies local realism, which requires the presence of some additional local variables (called "hidden" because they are not a feature of quantum theory) to explain the behavior of particles like photons and electrons. The test empirically evaluates the implications of Bell's theorem. As of 2015, all Bell tests have found that the hypothesis of local hidden variables is inconsistent with the way that physical systems behave. Many types of Bell tests have been performed in physics laboratories, often with the goal of ameliorating problems of experimental design or set-up that could in principle affect the validity of the findings of earlier Bell tests. This is known as "closing loopholes in Bell tests". Bell inequality violations are also used in some quantum cryptography protocols, whereby a spy's presence is detected when Bell's inequalities cease to be violated. Overview The Bell test has its origins in the debate between Einstein and other pioneers of quantum physics, principally Niels Bohr. One feature of the theory of quantum mechanics under debate was the meaning of Heisenberg's uncertainty principle. This principle states that if some information is known about a given particle, there is some other information about it that is impossible to know. An example of this is found in observations of the position and the momentum of a given particle. According to the uncertainty principle, a particle's momentum and its position cannot simultaneously be determined with arbitrarily high precision. In 1935, Einstein, Boris Podolsky, and Nathan Rosen published a claim that quantum mechanics predicts that more information about a pair of entangled particles could be observed than Heisenberg's principle allowed, which would only be possible if information were travelling instantly between the two particles. This produces a paradox which came to be known as the "EPR paradox" after the three authors. It arises if any effect felt in one location is not the result of a cause that occurred in its past light cone, relative to its location. This action at a distance seems to violate causality, by allowing information between the two locations to travel faster than the speed of light. However, it is a common misconception to think that any information can be shared between two observers faster than the speed of light using entangled particles; the hypothetical information transfer here is between the particles. See no-communication theorem for further explanation. Based on this, the authors concluded that the quantum wave function does not provide a complete description of reality. They suggested that there must be some local hidden variables at work in order to account for the behavior of entangled particles. In a theory of hidden variables, as Einstein envisaged it, the randomness and indeterminacy seen in the behavior of quantum particles would only be apparent. For example, if one knew the details of all the hidden variables associated with a particle, then one could predict both its position and momentum. The uncertainty that had been quantified by Heisenberg's principle would simply be an artifact of not having complete information about the hidden variables. Furthermore, Einstein argued that the hidden variables should obey the condition of locality: Whatever the hidden variables actually are, the behavior of the hidden variables for one particle should not be able to instantly affect the behavior of those for another particle far away. This idea, called the principle of locality, is rooted in intuition from classical physics that physical interactions do not propagate instantly across space. These ideas were the subject of ongoing debate between their proponents. In particular, Einstein himself did not approve of the way Podolsky had stated the problem in the famous EPR paper. In 1964, John Stewart Bell proposed his famous theorem, which states that no physical theory of hidden local variables can ever reproduce all the predictions of quantum mechanics. Implicit in the theorem is the proposition that the determinism of classical physics is fundamentally incapable of describing quantum mechanics. Bell expanded on the theorem to provide what would become the conceptual foundation of the Bell test experiments. A typical experiment involves the observation of particles, often photons, in an apparatus designed to produce entangled pairs and allow for the measurement of some characteristic of each, such as their spin. The results of the experiment could then be compared to what was predicted by local realism and those predicted by quantum mechanics. In theory, the results could be "coincidentally" consistent with both. To address this problem, Bell proposed a mathematical description of local realism that placed a statistical limit on the likelihood of that eventuality. If the results of an experiment violate Bell's inequality, local hidden variables can be ruled out as their cause. Later researchers built on Bell's work by proposing new inequalities that serve the same purpose and refine the basic idea in one way or another. Consequently, the term "Bell inequality" can mean any one of a number of inequalities satisfied by local hidden-variables theories; in practice, many present-day experiments employ the CHSH inequality. All these inequalities, like the original devised by Bell, express the idea that assuming local realism places restrictions on the statistical results of experiments on sets of particles that have taken part in an interaction and then separated. To date, all Bell tests have supported the theory of quantum physics, and not the hypothesis of local hidden variables. These efforts to experimentally validate violations of the Bell inequalities resulted in John Clauser, Alain Aspect, and Anton Zeilinger being awarded the 2022 Nobel Prize in Physics. Conduct of optical Bell test experiments In practice most actual experiments have used light, assumed to be emitted in the form of particle-like photons (produced by atomic cascade or spontaneous parametric down conversion), rather than the atoms that Bell originally had in mind. The property of interest is, in the best known experiments, the polarisation direction, though other properties can be used. Such experiments fall into two classes, depending on whether the analysers used have one or two output channels. A typical CHSH (two-channel) experiment The diagram shows a typical optical experiment of the two-channel kind for which Alain Aspect set a precedent in 1982. Coincidences (simultaneous detections) are recorded, the results being categorised as '++', '+−', '−+' or '−−' and corresponding counts accumulated. Four separate subexperiments are conducted, corresponding to the four terms E(a, b) in the test statistic S (equation (2) shown below). The settings a, a′, b and b′ are generally in practice chosen to be 0, 45°, 22.5° and 67.5° respectively — the "Bell test angles" — these being the ones for which the quantum mechanical formula gives the greatest violation of the inequality. For each selected value of a and b, the numbers of coincidences in each category (N++, N−−, N+− and N−+) are recorded. The experimental estimate for E(a, b) is then calculated as: Once all four E’s have been estimated, an experimental estimate of the test statistic can be found. If S is numerically greater than 2 it has infringed the CHSH inequality. The experiment is declared to have supported the QM prediction and ruled out all local hidden-variable theories. A strong assumption has had to be made, however, to justify use of expression (2), namely, that the sample of detected pairs is representative of the pairs emitted by the source. Denial of this assumption is called the fair sampling loophole. A typical CH74 (single-channel) experiment Prior to 1982 all actual Bell tests used "single-channel" polarisers and variations on an inequality designed for this setup. The latter is described in Clauser, Horne, Shimony and Holt's much-cited 1969 article as being the one suitable for practical use. As with the CHSH test, there are four subexperiments in which each polariser takes one of two possible settings, but in addition there are other subexperiments in which one or other polariser or both are absent. Counts are taken as before and used to estimate the test statistic. where the symbol ∞ indicates absence of a polariser. If S exceeds 0 then the experiment is declared to have infringed the CH inequality and hence to have refuted local hidden-variables. This inequality is known as CH inequality instead of CHSH as it was also derived in a 1974 article by Clauser and Horne more rigorously and under weaker assumptions. Experimental assumptions In addition to the theoretical assumptions, there are practical ones. There may, for example, be a number of "accidental coincidences" in addition to those of interest. It is assumed that no bias is introduced by subtracting their estimated number before calculating S, but that this is true is not considered by some to be obvious. There may be synchronisation problems — ambiguity in recognising pairs because in practice they will not be detected at exactly the same time. Nevertheless, despite all the deficiencies of the actual experiments, one striking fact emerges: the results are, to a very good approximation, what quantum mechanics predicts. If imperfect experiments give us such excellent overlap with quantum predictions, most working quantum physicists would agree with John Bell in expecting that, when a perfect Bell test is done, the Bell inequalities will still be violated. This attitude has led to the emergence of a new sub-field of physics known as quantum information theory. One of the main achievements of this new branch of physics is showing that violation of Bell's inequalities leads to the possibility of a secure information transfer, which utilizes the so-called quantum cryptography (involving entangled states of pairs of particles). Notable experiments Over the past half century, a great number of Bell test experiments have been conducted. The experiments are commonly interpreted to rule out local hidden-variable theories, and in 2015 an experiment was performed that is not subject to either the locality loophole or the detection loophole (Hensen et al.). An experiment free of the locality loophole is one where for each separate measurement and in each wing of the experiment, a new setting is chosen and the measurement completed before signals could communicate the settings from one wing of the experiment to the other. An experiment free of the detection loophole is one where close to 100% of the successful measurement outcomes in one wing of the experiment are paired with a successful measurement in the other wing. This percentage is called the efficiency of the experiment. Advancements in technology have led to a great variety of methods to test Bell-type inequalities. Some of the best known and recent experiments include: Kasday, Ullman and Wu (1970) Leonard Ralph Kasday, Jack R. Ullman and Chien-Shiung Wu carried out the first experimental Bell test, using photon pairs produced by positronium decay and analyzed by Compton scattering. The experiment observed photon polarization correlations consistent with quantum predictions and inconsistent with local realistic models that obey the known polarization dependence of Compton scattering. Due to the low polarization selectivity of Compton scattering, the results did not violate a Bell inequality. Freedman and Clauser (1972) Stuart J. Freedman and John Clauser carried out the first Bell test that observed a Bell inequality violation, using Freedman's inequality, a variant on the CH74 inequality. Aspect et al. (1982) Alain Aspect and his team at Orsay, Paris, conducted three Bell tests using calcium cascade sources. The first and last used the CH74 inequality. The second was the first application of the CHSH inequality. The third (and most famous) was arranged such that the choice between the two settings on each side was made during the flight of the photons (as originally suggested by John Bell). Tittel et al. (1998) The Geneva 1998 Bell test experiments showed that distance did not destroy the "entanglement". Light was sent in fibre optic cables over distances of several kilometers before it was analysed. As with almost all Bell tests since about 1985, a "parametric down-conversion" (PDC) source was used. Weihs et al. (1998): experiment under "strict Einstein locality" conditions In 1998 Gregor Weihs and a team at Innsbruck, led by Anton Zeilinger, conducted an experiment that closed the "locality" loophole, improving on Aspect's of 1982. The choice of detector was made using a quantum process to ensure that it was random. This test violated the CHSH inequality by over 30 standard deviations, the coincidence curves agreeing with those predicted by quantum theory. Pan et al. (2000) experiment on the GHZ state This is the first of new Bell-type experiments on more than two particles; this one uses the so-called GHZ state of three particles. Rowe et al. (2001): the first to close the detection loophole The detection loophole was first closed in an experiment with two entangled trapped ions, carried out in the ion storage group of David Wineland at the National Institute of Standards and Technology in Boulder. The experiment had detection efficiencies well over 90%. Go et al. (Belle collaboration): Observation of Bell inequality violation in B mesons Using semileptonic B0 decays of Υ(4S) at Belle experiment, a clear violation of Bell Inequality in particle-antiparticle correlation is observed. Gröblacher et al. (2007) test of Leggett-type non-local realist theories A specific class of non-local theories suggested by Anthony Leggett is ruled out. Based on this, the authors conclude that any possible non-local hidden-variable theory consistent with quantum mechanics must be highly counterintuitive. Salart et al. (2008): separation in a Bell Test This experiment filled a loophole by providing an 18 km separation between detectors, which is sufficient to allow the completion of the quantum state measurements before any information could have traveled between the two detectors. Ansmann et al. (2009): overcoming the detection loophole in solid state This was the first experiment testing Bell inequalities with solid-state qubits (superconducting Josephson phase qubits were used). This experiment surmounted the detection loophole using a pair of superconducting qubits in an entangled state. However, the experiment still suffered from the locality loophole because the qubits were only separated by a few millimeters. Giustina et al. (2013), Larsson et al (2014): overcoming the detection loophole for photons The detection loophole for photons has been closed for the first time by Marissa Giustina, using highly efficient detectors. This makes photons the first system for which all of the main loopholes have been closed, albeit in different experiments. Christensen et al. (2013): overcoming the detection loophole for photons The Christensen et al. (2013) experiment is similar to that of Giustina et al. Giustina et al. did just four long runs with constant measurement settings (one for each of the four pairs of settings). The experiment was not pulsed so that formation of "pairs" from the two records of measurement results (Alice and Bob) had to be done after the experiment which in fact exposes the experiment to the coincidence loophole. This led to a reanalysis of the experimental data in a way which removed the coincidence loophole, and fortunately the new analysis still showed a violation of the appropriate CHSH or CH inequality. On the other hand, the Christensen et al. experiment was pulsed and measurement settings were frequently reset in a random way, though only once every 1000 particle pairs, not every time. Hensen et al., Giustina et al., Shalm et al. (2015): "loophole-free" Bell tests In 2015 the first three significant-loophole-free Bell-tests were published within three months by independent groups in Delft, Vienna and Boulder. All three tests simultaneously addressed the detection loophole, the locality loophole, and the memory loophole. This makes them “loophole-free” in the sense that all remaining conceivable loopholes like superdeterminism require truly exotic hypotheses that might never get closed experimentally. The first published experiment by Hensen et al. used a photonic link to entangle the electron spins of two nitrogen-vacancy defect centres in diamonds 1.3 kilometers apart and measured a violation of the CHSH inequality (S = 2.42 ± 0.20). Thereby the local-realist hypothesis could be rejected with a p-value of 0.039. Both simultaneously published experiments by Giustina et al. and Shalm et al. used entangled photons to obtain a Bell inequality violation with high statistical significance (p-value ≪10−6). Notably, the experiment by Shalm et al. also combined three types of (quasi-)random number generators to determine the measurement basis choices. One of these methods, detailed in an ancillary file, is the “'Cultural' pseudorandom source” which involved using bit strings from popular media such as the Back to the Future films, Star Trek: Beyond the Final Frontier, Monty Python and the Holy Grail, and the television shows Saved by the Bell and Dr. Who. Schmied et al. (2016): Detection of Bell correlations in a many-body system Using a witness for Bell correlations derived from a multi-partite Bell inequality, physicists at the University of Basel were able to conclude for the first time Bell correlation in a many-body system composed by about 480 atoms in a Bose–Einstein condensate. Even though loopholes were not closed, this experiment shows the possibility of observing Bell correlations in the macroscopic regime. Handsteiner et al. (2017): "Cosmic Bell Test" - Measurement Settings from Milky Way Stars Physicists led by David Kaiser of the Massachusetts Institute of Technology and Anton Zeilinger of the Institute for Quantum Optics and Quantum Information and University of Vienna performed an experiment that "produced results consistent with nonlocality" by measuring starlight that had taken 600 years to travel to Earth. The experiment “represents the first experiment to dramatically limit the space-time region in which hidden variables could be relevant.” Rosenfeld et al. (2017): "Event-Ready" Bell test with entangled atoms and closed detection and locality loopholes Physicists at the Ludwig Maximilian University of Munich and the Max Planck Institute of Quantum Optics published results from an experiment in which they observed a Bell inequality violation using entangled spin states of two atoms with a separation distance of 398 meters in which the detection loophole, the locality loophole, and the memory loophole were closed. The violation of S = 2.221 ± 0.033 rejected local realism with a significance value of P = 1.02×10−16 when taking into account 7 months of data and 55000 events or an upper bound of P = 2.57×10−9 from a single run with 10000 events. The BIG Bell Test Collaboration (2018): “Challenging local realism with human choices” An international collaborative scientific effort used arbitrary human choice to define measurement settings instead of using random number generators. Assuming that human free will exists, this would close the “freedom-of-choice loophole”. Around 100,000 participants were recruited in order to provide sufficient input for the experiment to be statistically significant. Rauch et al (2018): measurement settings from distant quasars In 2018, an international team used light from two quasars (one whose light was generated approximately eight billion years ago and the other approximately twelve billion years ago) as the basis for their measurement settings. This experiment pushed the timeframe for when the settings could have been mutually determined to at least 7.8 billion years in the past, a substantial fraction of the superdeterministic limit (that being the creation of the universe 13.8 billion years ago). The 2019 PBS Nova episode Einstein's Quantum Riddle documents this "cosmic Bell test" measurement, with footage of the scientific team on-site at the high-altitude Teide Observatory located in the Canary Islands. Storz et al (2023): Loophole-free Bell inequality violation with superconducting circuits In 2023, an international team led by the group of Andreas Wallraff at ETH Zurich demonstrated a loophole-free violation of the CHSH inequality with superconducting circuits deterministically entangled via a cryogenic link spanning a distance of 30 meters. Loopholes Though the series of increasingly sophisticated Bell test experiments has convinced the physics community that local hidden-variable theories are indefensible; they can never be excluded entirely. For example, the hypothesis of superdeterminism in which all experiments and outcomes (and everything else) are predetermined can never be excluded (because it is unfalsifiable). Up to 2015, the outcome of all experiments that violate a Bell inequality could still theoretically be explained by exploiting the detection loophole and/or the locality loophole. The locality (or communication) loophole means that since in actual practice the two detections are separated by a time-like interval, the first detection may influence the second by some kind of signal. To avoid this loophole, the experimenter has to ensure that particles travel far apart before being measured, and that the measurement process is rapid. More serious is the detection (or unfair sampling) loophole, because particles are not always detected in both wings of the experiment. It can be imagined that the complete set of particles would behave randomly, but instruments only detect a subsample showing quantum correlations, by letting detection be dependent on a combination of local hidden variables and detector setting. Experimenters had repeatedly voiced that loophole-free tests could be expected in the near future. In 2015, a loophole-free Bell violation was reported using entangled diamond spins over a distance of 1.3 kilometres (1,300 m) and corroborated by two experiments using entangled photon pairs. The remaining possible theories that obey local realism can be further restricted by testing different spatial configurations, methods to determine the measurement settings, and recording devices. It has been suggested that using humans to generate the measurement settings and observe the outcomes provides a further test. David Kaiser of MIT told the New York Times in 2015 that a potential weakness of the "loophole-free" experiments is that the systems used to add randomness to the measurement may be predetermined in a method that was not detected in experiments. Detection loophole A common problem in optical Bell tests is that only a small fraction of the emitted photons are detected. It is then possible that the correlations of the detected photons are unrepresentative: although they show a violation of a Bell inequality, if all photons were detected the Bell inequality would actually be respected. This was first noted by Philip M. Pearle in 1970, who devised a local hidden variable model that faked a Bell violation by letting the photon be detected only if the measurement setting was favourable. The assumption that this does not happen, i.e., that the small sample is actually representative of the whole is called the fair sampling assumption. To do away with this assumption it is necessary to detect a sufficiently large fraction of the photons. This is usually characterized in terms of the detection efficiency η {\displaystyle \eta } , defined as the probability that a photodetector detects a photon that arrives at it. Anupam Garg and N. David Mermin showed that when using a maximally entangled state and the CHSH inequality an efficiency of η > 2 2 − 2 ≈ 0.83 {\displaystyle \eta >2{\sqrt {2}}-2\approx 0.83} is required for a loophole-free violation. Later Philippe H. Eberhard showed that when using a partially entangled state a loophole-free violation is possible for η > 2 / 3 ≈ 0.67 {\displaystyle \eta >2/3\approx 0.67} , which is the optimal bound for the CHSH inequality. Other Bell inequalities allow for even lower bounds. For example, there exists a four-setting inequality which is violated for η > ( 5 − 1 ) / 2 ≈ 0.62 {\displaystyle \eta >({\sqrt {5}}-1)/2\approx 0.62} . Historically, only experiments with non-optical systems have been able to reach high enough efficiencies to close this loophole, such as trapped ions, superconducting qubits, and nitrogen-vacancy centers. These experiments were not able to close the locality loophole, which is easy to do with photons. More recently, however, optical setups have managed to reach sufficiently high detection efficiencies by using superconducting photodetectors, and hybrid setups have managed to combine the high detection efficiency typical of matter systems with the ease of distributing entanglement at a distance typical of photonic systems. Locality loophole One of the assumptions of Bell's theorem is the one of locality, namely that the choice of setting at a measurement site does not influence the result of the other. The motivation for this assumption is the theory of relativity, that prohibits communication faster than light. For this motivation to apply to an experiment, it needs to have space-like separation between its measurements events. That is, the time that passes between the choice of measurement setting and the production of an outcome must be shorter than the time it takes for a light signal to travel between the measurement sites. The first experiment that strived to respect this condition was Aspect's 1982 experiment. In it the settings were changed fast enough, but deterministically. The first experiment to change the settings randomly, with the choices made by a quantum random number generator, was Weihs et al.'s 1998 experiment. Scheidl et al. improved on this further in 2010 by conducting an experiment between locations separated by a distance of 144 km (89 mi). Coincidence loophole In many experiments, especially those based on photon polarization, pairs of events in the two wings of the experiment are only identified as belonging to a single pair after the experiment is performed, by judging whether or not their detection times are close enough to one another. This generates a new possibility for a local hidden variables theory to "fake" quantum correlations: delay the detection time of each of the two particles by a larger or smaller amount depending on some relationship between hidden variables carried by the particles and the detector settings encountered at the measurement station. The coincidence loophole can be ruled out entirely simply by working with a pre-fixed lattice of detection windows which are short enough that most pairs of events occurring in the same window do originate with the same emission and long enough that a true pair is not separated by a window boundary. Memory loophole In most experiments, measurements are repeatedly made at the same two locations. A local hidden variable theory could exploit the memory of past measurement settings and outcomes in order to increase the violation of a Bell inequality. Moreover, physical parameters might be varying in time. It has been shown that, provided each new pair of measurements is done with a new random pair of measurement settings, that neither memory nor time inhomogeneity have a serious effect on the experiment. Superdeterminism A necessary assumption to derive Bell's theorem is that the hidden variables are not correlated with the measurement settings. This assumption has been justified on the grounds that the experimenter has "free will" to choose the settings, and that such is necessary to do science in the first place. A (hypothetical) theory where the choice of measurement is determined by the system being measured is known as superdeterministic. Many-worlds loophole The many-worlds interpretation, also known as the Hugh Everett interpretation, is deterministic and has local dynamics, consisting of the unitary part of quantum mechanics without collapse. Bell's theorem does not apply because of an implicit assumption that measurements have a single outcome. See also Determinism – Quantum and classical mechanics Einstein's thought experiments Principle of locality Quantum indeterminacy References Further reading J. Barrett; D. Collins; L. Hardy; A. Kent; S. Popescu (2002). "Quantum Nonlocality, Bell Inequalities and the Memory Loophole". Phys. Rev. A. 66 (4): 042111. arXiv:quant-ph/0205016. Bibcode:2002PhRvA..66d2111B. doi:10.1103/PhysRevA.66.042111. S2CID 6524446. J. S. Bell (1987). Speakable and Unspeakable in Quantum Mechanics. Cambridge University Press. ISBN 978-0-521-33495-2. D. Kielpinski; A. Ben-Kish; J. Britton; V. Meyer; M.A. Rowe; C.A. Sackett; W.M. Itano; C. Monroe; D.J. Wineland (2001). "Recent Results in Trapped-Ion Quantum Computing". arXiv:quant-ph/0102086. P.G. Kwiat; E. Waks; A.G. White; I. Appelbaum; P.H. Eberhard (1999). "Ultrabright source of polarization-entangled photons". Physical Review A. 60 (2): R773–6. arXiv:quant-ph/9810003. Bibcode:1999PhRvA..60..773K. doi:10.1103/PhysRevA.60.R773. S2CID 16417960. Imre Fényes (Hungarian: [ˈimrɛ ˈfeːɲɛʃ]; 29 July 1917 – 13 November 1977) was a Hungarian physicist who was the first to propose a stochastic interpretation of quantum mechanics. Selected publications References External links Imre Fényes biography Peter Zoller (born 16 September 1952) is a theoretical physicist from Austria. He is professor at the University of Innsbruck and works on quantum optics and quantum information and is best known for his pioneering research on quantum computing and quantum communication and for bridging quantum optics and solid state physics. Biography Peter Zoller studied physics at the University of Innsbruck, obtained his doctorate there in February 1977, and became a lecturer at their Institute of Theoretical Physics. For 1978/79, he was granted a Max Kade stipend to research with Peter Lambropoulos at the University of Southern California. In 1980, he stayed at the University of Waikato in Hamilton, New Zealand, as a researcher with the group around Dan Walls. In 1981, Peter Zoller handed in his book "Über die lichtstatistische Abhängigkeit resonanter Multiphoton-Prozesse" at the University of Innsbruck to qualify as a professor by receiving the "venia docendi". He spent 1981/82 and 1988 as visiting fellow at the Joint Institute for Laboratory Astrophysics (JILA) of the University of Colorado, Boulder, and 1986 as guest professor at the Université de Paris-Sud 11, Orsay. In 1991, Peter Zoller was appointed Professor of Physics and JILA Fellow at JILA and at the Physics Department of the University of Colorado, Boulder. At the end of 1994, he accepted a chair at the University of Innsbruck, where he has worked ever since. From 1995 to 1999, he headed the Institute of Theoretical Physics, from 2001 to 2004, he was vice-dean of studies. Peter Zoller continues to keep in close touch with JILA as Adjoint Fellow. Numerous guest professorships have taken him to all major centers of physics throughout the world. He was Loeb lecturer in Harvard, Boston, MA (2004) and Yan Jici chair professor at the University of Science and Technology of China, Hefei, chair professor at Tsinghua University, Beijing (2004), Lorentz professor at the University of Leiden in the Netherlands (2005), Distinguished Lecturer at the Technion in Haifa (2007), Moore Distinguished Scholar at Caltech (2008/2010) and Arnold Sommerfeld Lecturer at LMU München (2010). In 2012/13 he was "Distinguished Fellow" at the Max Planck Institute of Quantum Optics in Garching, Munich. In 2014 he has been elected as an "External Scientific Member" at the Max Planck Institute of Quantum Optics. In 2015 he held the International Jacques Solvay Chair in Physics at the University of Brussels . Since 2003, Peter Zoller has also held the position of Scientific Director at the Institute for Quantum Optics and Quantum Information (IQOQI) of the Austrian Academy of Sciences. In 2018, Peter Zoller co-founded Alpine Quantum Technologies, a quantum computing hardware company. Research As a theoretician, Peter Zoller has written major works on the interaction of laser light and atoms. In addition to fundamental developments in quantum optics he has succeeded in bridging quantum information and solid state physics. The model of a quantum computer, suggested by him and Ignacio Cirac in 1995, is based on the interaction of lasers with cold ions confined in an electromagnetic trap. The principles of this idea have been implemented in experiments over recent years and it is considered one of the most promising concepts for the development of a scalable quantum computer. Zoller and his researcher colleagues have also managed to link quantum physics with solid state physics. One of his suggestions has been to build a quantum simulator with cold atoms and use it to research hitherto unexplained phenomena in high temperature superconductors. Zoller's ideas and concepts attract widespread interest within the scientific community and his works are highly cited. Books Peter Zoller and Crispin Gardiner have jointly written the books C W Gardiner and Peter Zoller: Quantum Noise; Springer, Berlin Heidelberg, 2nd ed. 1999, 3rd ed. 2004 ISBN 3540223010 Crispin Gardiner and Peter Zoller: The Quantum World of Ultra-Cold Atoms and Light Book I: Foundations of Quantum Optics, Imperial College Press, London and Singapore 2014. ISBN 9781783264605 Crispin Gardiner and Peter Zoller: The Quantum World of Ultra-Cold Atoms and Light Book II: Physics of Quantum Optical Devices, Imperial College Press, London and Singapore 2015. ISBN 9781783266166 Crispin Gardiner and Peter Zoller: The Quantum World of Ultra-Cold Atoms and Light Book III: Ultra-Cold Atoms, World Scientific, London and Singapore 2014. ISBN 9781786344175 Awards Peter Zoller has received numerous awards for his achievements in the field of quantum optics and quantum information and especially for his pioneering work on quantum computers and quantum communication. These include: the John Stewart Bell Prize (2019) the Norman F. Ramsey Prize (2018) of the American Physical Society the Micius Quantum Prize (2018) the Willis E. Lamb Award for Laser Science and Quantum Optics (2018) the Herbert Walther Award from the OSA (2016) the Wolf Prize in Physics (with Juan Ignacio Cirac) (2013) the Hamburg Prize for Theoretical Physics (2011) the Blaise Pascal Medal in Physics of the European Academy of Sciences (2011) the Benjamin Franklin Medal in Physics (2010) of the Franklin Institute (with Juan Ignacio Cirac and David Wineland) the BBVA Foundation Frontiers of Knowledge Award (2008), in the Basic Sciences category (ex aequo with Ignacio Cirac) the Dirac Medal of the ICTP (2006) the 6th International Quantum Communication Award (2006) the UNESCO Niels Bohr Medal (2005) the Max Planck Medal (2005) of the Deutsche Physikalische Gesellschaft the Humboldt Research Award (2000) the Schrödinger Prize (1998) of the Austrian Academy of Sciences the Max Born Award (1998) of the Optical Society of America the Wittgenstein Award (1998), Austria's highest scientific accolade the Ludwig Boltzmann Prize (1983) of the Austrian Physical Society. In 2001, Peter Zoller became full member of the Austrian Academy of Sciences. In 2008 he was elected to the United States National Academy of Sciences and the Royal Netherlands Academy of Arts and Sciences, in 2009 to the Spanish Royal Academy of Sciences, in 2010 to the German Academy of Sciences Leopoldina, in 2012 to the European Academy of Sciences, in 2013 to the Academia Europaea, and in 2023 in the Accademia Nazionale dei Lincei. He received honorary doctorates of the University of Concepción (2024), the University of Colorado Boulder (2019) and the University of Amsterdam (2012). See also Open quantum system Quantum jump method References External links Media related to Peter Zoller at Wikimedia Commons Biography Peter Zoller Archived 2013-12-19 at the Wayback Machine Peter Zoller at the Institute of Quantum Optics and Quantum Information (IQOQI) Quantum Optics Theory Group, University of Innsbruck Peter Zoller’s Thomson Reuters RESEARCHERID Quantum Computing: Peter Zoller and Ignacio Cirac on the Quantum Revolution on YouTube Quantum weirdness encompasses the aspects of quantum mechanics that challenge and defy human physical intuition. Human physical intuition is based on macroscopic physical phenomena as are experienced in everyday life, which can mostly be adequately described by the Newtonian mechanics of classical physics. Early 20th-century models of atomic physics, such as the Rutherford–Bohr model, represented subatomic particles as little balls occupying well-defined spatial positions, but it was soon found that the physics needed at a subatomic scale, which became known as "quantum mechanics", implies many aspects for which the models of classical physics are inadequate. These aspects include: quantum entanglement; quantum nonlocality, referred to by Einstein as "spooky action at a distance"; see also EPR paradox; quantum superposition, presented in dramatic form in the thought experiment known as Schrödinger's cat; the uncertainty principle; wave–particle duality; the probabilistic nature of wave function collapse, decried by Einstein, saying, "God does not play dice". See also Bell's theorem Interpretations of quantum mechanics Quantum tunneling Renninger negative-result experiment Wheeler's delayed-choice experiment References Further reading Book reviews Siegfried, Tom (6 January 2019). "'Beyond Weird' and 'What Is Real?' try to make sense of quantum weirdness". Science News. Boyle, Alan (January 9, 2023). "Sci-fi author and scientist team up to write a novel about consciousness and quantum weirdness". GeekWire. Articles Gardner, Martin (October 1982). "Quantum Weirdness". Discover: 69–75. Cho, Adrian (13 September 2005). "Outracing Quantum Weirdness". Science. Boyd, R. W.; Chan, Kam Wai Clifford; O'Sullivan, Malcolm N. (28 September 2007). "Physics. Quantum weirdness in the lab". Science. 317 (5846): 1874–5. doi:10.1126/science.1148947. PMID 17901320. d'Espagnat, Bernard (20 March 2009). "Quantum weirdness: What We Call 'Reality' is Just a State of Mind". The Guardian. Musser, George (19 January 2016). "Quantum Weirdness Now a Matter of Time". Quanta Magazine. Ananthaswamy, Anil (19 February 2016). "Quantum weirdness may hide an orderly reality after all". New Scientist. Wolchover, Natalie (7 February 2017). "Experiment Reaffirms Quantum Weirdness". Quanta Magazine. Wolchover, Natalie (11 October 2018). "Famous Experiment Dooms Alternative to Quantum Weirdness". Quanta Magazine. Schnabel, Roman (29 January 2020). "'Quantum Weirdness' in Exploitation by the International Gravitational-Wave Observatory Network". Annalen der Physik. 532 (3): 1900508. arXiv:1909.13723. doi:10.1002/andp.201900508. Tomaž Prosen (born 1970) is a Slovenian theoretical and mathematical physicist. His research has spanned non-equilibrium dynamics, statistical mechanics, quantum transport, and chaos theory. Early career Prosen earned his Diploma in Physics in 1991, and a Doctorate of Science in 1995, both from the University of Ljubljana. He finished both at a significantly younger age than usual. ISI named him a ‘Citation Superstar’ as one of the most cited young scientists in Slovenia in 2000. He was made a Full Professor at the University of Ljubljana by outstanding early election in 2008. Research Tomaž Prosen is primarily known for providing the first exact solutions for models of open quantum many-body systems and for the discovery of novel kinds of quantum conservation laws that settled long-standing questions about the nature of transport in fundamental models of low-dimensional quantum materials, such as the Heisenbeg spin chains and the one-dimensional Hubbard model. The latter work also provided a full description of canonical ensembles of quantum integrable systems paving the way for extensions of thermodynamics to integrable systems. He is also known for pioneering a novel approach for establishing quantum chaos in spin-1/2 systems, for which previously known semi-classical methods fail. This approach challenged conventional beliefs in theoretical physics by providing an exact solution to the dynamics of a chaotic model. Memberships He is a member of the European Academy of Sciences and Arts and the Slovenian Academy of Sciences and Arts. Notes External links Group Homepage of T Prosen at the University of Ljubljana T Prosen Google Scholar Profile In physics, lattice field theory is the study of lattice models of quantum field theory. This involves studying field theory on a space or spacetime that has been discretised onto a lattice. Details Although most lattice field theories are not exactly solvable, they are immensely appealing due to their feasibility for computer simulation, often using Markov chain Monte Carlo methods. One hopes that, by performing simulations on larger and larger lattices, while making the lattice spacing smaller and smaller, one will be able to recover the behavior of the continuum theory as the continuum limit is approached. Just as in all lattice models, numerical simulation provides access to field configurations that are not accessible to perturbation theory, such as solitons. Similarly, non-trivial vacuum states can be identified and examined. The method is particularly appealing for the quantization of a gauge theory using the Wilson action. Most quantization approaches maintain Poincaré invariance manifest but sacrifice manifest gauge symmetry by requiring gauge fixing. It's only after renormalization that gauge invariance can be recovered. Lattice field theory differs from these in that it keeps manifest gauge invariance, but sacrifices manifest Poincaré invariance—recovering it only after renormalization. The articles on lattice gauge theory and lattice QCD explore these issues in greater detail. See also Fermion doubling Further reading Creutz, M., Quarks, gluons and lattices, Cambridge University Press, Cambridge, (1985). ISBN 978-0521315357 (renewed version: (2023) ISBN 978-1009290395) DeGrand, T., DeTar, C., Lattice Methods for Quantum Chromodynamics, World Scientific, Singapore, (2006). ISBN 978-9812567277 Gattringer, C., Lang, C. B., Quantum Chromodynamics on the Lattice, Springer, (2010). ISBN 978-3642018497 Knechtli, F., Günther, M., Peardon, M., Lattice Quantum Chromodynamics: Practical Essentials, Springer, (2016). ISBN 978-9402409970 Lin, H., Meyer, H.B., Lattice QCD for Nuclear Physics, Springer, (2014). ISBN 978-3319080215 Makeenko, Y., Methods of contemporary gauge theory, Cambridge University Press, Cambridge, (2002). ISBN 0-521-80911-8. Montvay, I., Münster, G., Quantum Fields on a Lattice, Cambridge University Press, Cambridge, (1997). ISBN 978-0521599177 Rothe, H., Lattice Gauge Theories, An Introduction, World Scientific, Singapore, (2005). ISBN 978-9814365857 Smit, J., Introduction to Quantum Fields on a Lattice, Cambridge University Press, Cambridge, (2002). ISBN 978-0521890519 In mathematical physics, noncommutative quantum field theory (or quantum field theory on noncommutative spacetime) is an application of noncommutative mathematics to the spacetime of quantum field theory that is an outgrowth of noncommutative geometry and index theory in which the coordinate functions are noncommutative. One commonly studied version of such theories has the "canonical" commutation relation: [ x μ , x ν ] = i θ μ ν {\displaystyle [x^{\mu },x^{\nu }]=i\theta ^{\mu \nu }\,\!} where x μ {\displaystyle x^{\mu }} and x ν {\displaystyle x^{\nu }} are the hermitian generators of a noncommutative C ∗ {\displaystyle C^{*}} -algebra of "functions on spacetime". That means that (with any given set of axes), it is impossible to accurately measure the position of a particle with respect to more than one axis. In fact, this leads to an uncertainty relation for the coordinates analogous to the Heisenberg uncertainty principle. Various lower limits have been claimed for the noncommutative scale, (i.e. how accurately positions can be measured) but there is currently no experimental evidence in favour of such a theory or grounds for ruling them out. One of the novel features of noncommutative field theories is the UV/IR mixing phenomenon in which the physics at high energies affects the physics at low energies which does not occur in quantum field theories in which the coordinates commute. Other features include violation of Lorentz invariance due to the preferred direction of noncommutativity. Relativistic invariance can however be retained in the sense of twisted Poincaré invariance of the theory. The causality condition is modified from that of the commutative theories. History and motivation Heisenberg was the first to suggest extending noncommutativity to the coordinates as a possible way of removing the infinite quantities appearing in field theories before the renormalization procedure was developed and had gained acceptance. The first paper on the subject was published in 1947 by Hartland Snyder. The success of the renormalization method resulted in little attention being paid to the subject for some time. In the 1980s, mathematicians, most notably Alain Connes, developed noncommutative geometry. Among other things, this work generalized the notion of differential structure to a noncommutative setting. This led to an operator algebraic description of noncommutative space-times, with the problem that it classically corresponds to a manifold with positively defined metric tensor, so that there is no description of (noncommutative) causality in this approach. However it also led to the development of a Yang–Mills theory on a noncommutative torus. The particle physics community became interested in the noncommutative approach because of a paper by Nathan Seiberg and Edward Witten. They argued in the context of string theory that the coordinate functions of the endpoints of open strings constrained to a D-brane in the presence of a constant Neveu–Schwarz B-field—equivalent to a constant magnetic field on the brane—would satisfy the noncommutative algebra set out above. The implication is that a quantum field theory on noncommutative spacetime can be interpreted as a low energy limit of the theory of open strings. Two papers, one by Sergio Doplicher, Klaus Fredenhagen and John Roberts and the other by D. V. Ahluwalia, set out another motivation for the possible noncommutativity of space-time. The arguments go as follows: According to general relativity, when the energy density grows sufficiently large, a black hole is formed. On the other hand, according to the Heisenberg uncertainty principle, a measurement of a space-time separation causes an uncertainty in momentum inversely proportional to the extent of the separation. Thus energy whose scale corresponds to the uncertainty in momentum is localized in the system within a region corresponding to the uncertainty in position. When the separation is small enough, the Schwarzschild radius of the system is reached and a black hole is formed, which prevents any information from escaping the system. Thus there is a lower bound for the measurement of length. A sufficient condition for preventing gravitational collapse can be expressed as an uncertainty relation for the coordinates. This relation can in turn be derived from a commutation relation for the coordinates. It is worth stressing that, differently from other approaches, in particular those relying upon Connes' ideas, here the noncommutative spacetime is a proper spacetime, i.e. it extends the idea of a four-dimensional pseudo-Riemannian manifold. On the other hand, differently from Connes' noncommutative geometry, the proposed model turns out to be coordinate-dependent from scratch. In Doplicher Fredenhagen Roberts' paper noncommutativity of coordinates concerns all four spacetime coordinates and not only spatial ones. See also Moyal product Noncommutative geometry Noncommutative standard model Wigner–Weyl transform Footnotes Further reading Grensing, Gerhard (2013). Structural Aspects of Quantum Field Theory and Noncommutative Geometry. World Scientific. doi:10.1142/8771. ISBN 978-981-4472-69-2. M. R. Douglas and N. A. Nekrasov, (2001). Noncommutative field theory. Rev. Mod. Phys., 73(4), 977. Richard J. Szabo (2003) "Quantum Field Theory on Noncommutative Spaces," Physics Reports 378: 207-99. An expository article on noncommutative quantum field theories. Noncommutative quantum field theory, see statistics on arxiv.org Valter Moretti (2003), "Aspects of noncommutative Lorentzian geometry for globally hyperbolic spacetimes," Rev. Math. Phys. 15: 1171-1218. An expository paper (also) on the difficulties to extend non-commutative geometry to the Lorentzian case describing causality In condensed matter physics, the Laughlin wavefunction is an ansatz, proposed by Robert Laughlin for the ground state of a two-dimensional electron gas placed in a uniform background magnetic field in the presence of a uniform jellium background when the filling factor of the lowest Landau level is ν = 1 / n {\displaystyle \nu =1/n} where n {\displaystyle n} is an odd positive integer. It was constructed to explain the observation of the ν = 1 / 3 {\displaystyle \nu =1/3} fractional quantum Hall effect (FQHE), and predicted the existence of additional ν = 1 / n {\displaystyle \nu =1/n} states as well as quasiparticle excitations with fractional electric charge e / n {\displaystyle e/n} , both of which were later experimentally observed. Laughlin received one third of the Nobel Prize in Physics in 1998 for this discovery. Context and analytical expression If we ignore the jellium and mutual Coulomb repulsion between the electrons as a zeroth order approximation, we have an infinitely degenerate lowest Landau level (LLL) and with a filling factor of 1/n, we'd expect that all of the electrons would lie in the LLL. Turning on the interactions, we can make the approximation that all of the electrons lie in the LLL. If ψ 0 {\displaystyle \psi _{0}} is the single particle wavefunction of the LLL state with the lowest orbital angular momenta, then the Laughlin ansatz for the multiparticle wavefunction is ⟨ z 1 , z 2 , z 3 , … , z N ∣ n , N ⟩ = ψ n , N ( z 1 , z 2 , z 3 , … , z N ) = D [ ∏ N ⩾ i > j ⩾ 1 ( z i − z j ) n ] ∏ k = 1 N exp ⁡ ( − ∣ z k ∣ 2 ) {\displaystyle \langle z_{1},z_{2},z_{3},\ldots ,z_{N}\mid n,N\rangle =\psi _{n,N}(z_{1},z_{2},z_{3},\ldots ,z_{N})=D\left[\prod _{N\geqslant i>j\geqslant 1}\left(z_{i}-z_{j}\right)^{n}\right]\prod _{k=1}^{N}\exp \left(-\mid z_{k}\mid ^{2}\right)} where position is denoted by z = 1 2 l B ( x + i y ) {\displaystyle z={1 \over 2{\mathit {l}}_{B}}\left(x+iy\right)} in (Gaussian units) l B = ℏ c e B {\displaystyle {\mathit {l}}_{B}={\sqrt {\hbar c \over eB}}} and x {\displaystyle x} and y {\displaystyle y} are coordinates in the x–y plane. Here ℏ {\displaystyle \hbar } is the reduced Planck constant, e {\displaystyle e} is the electron charge, N {\displaystyle N} is the total number of particles, and B {\displaystyle B} is the magnetic field, which is perpendicular to the xy plane. The subscripts on z identify the particle. In order for the wavefunction to describe fermions, n must be an odd integer. This forces the wavefunction to be antisymmetric under particle interchange. The angular momentum for this state is n ℏ {\displaystyle n\hbar } . True ground state in FQHE at ν = 1/3 Consider n = 3 {\displaystyle n=3} above: resultant Ψ L ( z 1 , z 2 , z 3 , … , z N ) ∝ Π i < j ( z i − z j ) 3 {\displaystyle \Psi _{L}(z_{1},z_{2},z_{3},\ldots ,z_{N})\propto \Pi _{i<j}(z_{i}-z_{j})^{3}} is a trial wavefunction; it is not exact, but qualitatively, it reproduces many features of the exact solution and quantitatively, it has very high overlaps with the exact ground state for small systems. Assuming Coulomb repulsion between any two electrons, that ground state Ψ E D {\displaystyle \Psi _{ED}} can be determined using exact diagonalisation and the overlaps have been calculated to be close to one. Moreover, with short-range interaction (Haldane pseudopotentials for m > 3 {\displaystyle m>3} set to zero), Laughlin wavefunction becomes exact, i.e. ⟨ Ψ E D | Ψ L ⟩ = 1 {\displaystyle \langle \Psi _{ED}|\Psi _{L}\rangle =1} . Parent Hamiltonian and Haldane Pseudopotentials While the Laughlin wave function was initially proposed as a highly successful ansatz, its central role in the theory of the fractional quantum Hall effect was cemented by F. Duncan Haldane, who demonstrated that it is the unique, exact zero-energy ground state of a specific "parent" Hamiltonian. This approach reverse-engineers the Hamiltonian from the known properties of the wave function, providing a powerful theoretical framework and a benchmark for numerical studies. The construction is based on the properties of interacting particles in the lowest Landau level. In a strong magnetic field, the kinetic energy is quenched, and the physics is dominated by the interaction potential. The states of two interacting particles can be decomposed into states of definite relative angular momentum, l. The core insight lies in the structure of the Laughlin wave function itself: due to the Jastrow factor ( z i − z j ) m {\displaystyle (z_{i}-z_{j})^{m}} , the probability of finding two electrons with a relative angular momentum l less than m is exactly zero. The wave function is constructed to keep particles far apart in a very specific way, encoding correlations in the relative angular momentum channels. Haldane's idea was to build a Hamiltonian that penalizes any pair of particles that has a relative angular momentum less than m. This is achieved using Haldane pseudopotentials, which can be thought of as a projection of the interaction potential onto states of definite relative angular momentum. The parent Hamiltonian is constructed as a sum of projection operators: H parent = ∑ i < j ∑ l = 0 m − 1 V l P i j ( l ) {\displaystyle H_{\text{parent}}=\sum _{i<j}\sum _{l=0}^{m-1}V_{l}P_{ij}(l)} where: The sum is over all pairs of particles (i,j). P i j ( l ) {\displaystyle P_{ij}(l)} is the operator that projects the pair (i,j) onto a state with relative angular momentum l. V l {\displaystyle V_{l}} are positive coefficients ( V l > 0 {\displaystyle V_{l}>0} ) representing the energy cost for a pair to be found in the l-th relative angular momentum channel. For the parent Hamiltonian, only the V l {\displaystyle V_{l}} for l < m {\displaystyle l<m} need to be non-zero. This Hamiltonian is a sum of positive semi-definite operators, so its energy eigenvalues are always non-negative. A ground state with zero energy can only exist if it is annihilated by every term in the sum. This means the ground state wavefunction |Ψ₀⟩ must satisfy: P i j ( l ) | Ψ 0 ⟩ = 0 for all pairs ( i , j ) and all l < m {\displaystyle P_{ij}(l)|\Psi _{0}\rangle =0\quad {\text{for all pairs }}(i,j){\text{ and all }}l<m} The Laughlin ν = 1/m state, by its very construction, perfectly satisfies this condition. It contains no components where any pair of particles has relative angular momentum less than m. Therefore, the Laughlin state is an exact zero-energy eigenstate of this parent Hamiltonian. Furthermore, for a given number of particles, it can be shown that the Laughlin state is the unique, densest state (i.e., the state with the most particles per unit of magnetic flux) that satisfies this set of conditions. This makes it the unique ground state of this idealized Hamiltonian. This formalism is extremely powerful. It proves that there exists a local Hamiltonian for which the Laughlin state is the exact ground state, solidifying its physical relevance. It also provides a crucial tool for exact diagonalization studies. The ground state of a more realistic interaction, like the Coulomb potential, can be computed numerically and its overlap with the ideal Laughlin state can be calculated. A large overlap indicates that the Laughlin state is an excellent approximation to the true ground state of the system. Energy of interaction for two particles The Laughlin wavefunction is the multiparticle wavefunction for quasiparticles. The expectation value of the interaction energy for a pair of quasiparticles is ⟨ V ⟩ = ⟨ n , N ∣ V ∣ n , N ⟩ , N = 2 {\displaystyle \langle V\rangle =\langle n,N\mid V\mid n,N\rangle ,\;\;\;N=2} where the screened potential is (see Static forces and virtual-particle exchange § Coulomb potential between two current loops embedded in a magnetic field) V ( r 12 ) = ( 2 e 2 L B ) ∫ 0 ∞ k d k k 2 + k B 2 r B 2 M ( l + 1 , 1 , − k 2 4 ) M ( l ′ + 1 , 1 , − k 2 4 ) J 0 ( k r 12 r B ) {\displaystyle V\left(r_{12}\right)=\left({2e^{2} \over L_{B}}\right)\int _{0}^{\infty }{{k\;dk\;} \over k^{2}+k_{B}^{2}r_{B}^{2}}\;M\left({\mathit {l}}+1,1,-{k^{2} \over 4}\right)\;M\left({\mathit {l}}^{\prime }+1,1,-{k^{2} \over 4}\right)\;{\mathcal {J}}_{0}\left(k{r_{12} \over r_{B}}\right)} where M {\displaystyle M} is a confluent hypergeometric function and J 0 {\displaystyle {\mathcal {J}}_{0}} is a Bessel function of the first kind. Here, r 12 {\displaystyle r_{12}} is the distance between the centers of two current loops, e {\displaystyle e} is the magnitude of the electron charge, r B = 2 l B {\displaystyle r_{B}={\sqrt {2}}{\mathit {l}}_{B}} is the quantum version of the Larmor radius, and L B {\displaystyle L_{B}} is the thickness of the electron gas in the direction of the magnetic field. The angular momenta of the two individual current loops are l ℏ {\displaystyle {\mathit {l}}\hbar } and l ′ ℏ {\displaystyle {\mathit {l}}^{\prime }\hbar } where l + l ′ = n {\displaystyle {\mathit {l}}+{\mathit {l}}^{\prime }=n} . The inverse screening length is given by (Gaussian units) k B 2 = 4 π e 2 ℏ ω c A L B {\displaystyle k_{B}^{2}={4\pi e^{2} \over \hbar \omega _{c}AL_{B}}} where ω c {\displaystyle \omega _{c}} is the cyclotron frequency, and A {\displaystyle A} is the area of the electron gas in the xy plane. The interaction energy evaluates to: To obtain this result we have made the change of integration variables u 12 = z 1 − z 2 2 {\displaystyle u_{12}={z_{1}-z_{2} \over {\sqrt {2}}}} and v 12 = z 1 + z 2 2 {\displaystyle v_{12}={z_{1}+z_{2} \over {\sqrt {2}}}} and noted (see Common integrals in quantum field theory) 1 ( 2 π ) 2 2 2 n n ! ∫ d 2 z 1 d 2 z 2 ∣ z 1 − z 2 ∣ 2 n exp ⁡ [ − 2 ( ∣ z 1 ∣ 2 + ∣ z 2 ∣ 2 ) ] J 0 ( 2 k ∣ z 1 − z 2 ∣ ) = {\displaystyle {1 \over \left(2\pi \right)^{2}\;2^{2n}\;n!}\int d^{2}z_{1}\;d^{2}z_{2}\;\mid z_{1}-z_{2}\mid ^{2n}\;\exp \left[-2\left(\mid z_{1}\mid ^{2}+\mid z_{2}\mid ^{2}\right)\right]\;{\mathcal {J}}_{0}\left({\sqrt {2}}\;{k\mid z_{1}-z_{2}\mid }\right)=} 1 ( 2 π ) 2 2 n n ! ∫ d 2 u 12 d 2 v 12 ∣ u 12 ∣ 2 n exp ⁡ [ − 2 ( ∣ u 12 ∣ 2 + ∣ v 12 ∣ 2 ) ] J 0 ( 2 k ∣ u 12 ∣ ) = {\displaystyle {1 \over \left(2\pi \right)^{2}\;2^{n}\;n!}\int d^{2}u_{12}\;d^{2}v_{12}\;\mid u_{12}\mid ^{2n}\;\exp \left[-2\left(\mid u_{12}\mid ^{2}+\mid v_{12}\mid ^{2}\right)\right]\;{\mathcal {J}}_{0}\left({2}k\mid u_{12}\mid \right)=} M ( n + 1 , 1 , − k 2 2 ) . {\displaystyle M\left(n+1,1,-{k^{2} \over 2}\right).} The interaction energy has minima for (Figure 1) l n = 1 3 , 2 5 , 3 7 , etc., {\displaystyle {{\mathit {l}} \over n}={1 \over 3},{2 \over 5},{3 \over 7},{\mbox{etc.,}}} and l n = 2 3 , 3 5 , 4 7 , etc. {\displaystyle {{\mathit {l}} \over n}={2 \over 3},{3 \over 5},{4 \over 7},{\mbox{etc.}}} For these values of the ratio of angular momenta, the energy is plotted in Figure 2 as a function of n {\displaystyle n} . References See also Landau level Fractional quantum Hall effect Coulomb potential between two current loops embedded in a magnetic field The UK Molecular R-Matrix codes are a set of software routines used to calculate the effects of collision of electrons with atoms and molecules. The R-matrix method is used in computational quantum mechanics to study scattering of positrons and electrons by atomic and molecular targets. The fundamental idea was originally introduced by Eugene Wigner and Leonard Eisenbud in the 1940s. The method uses the fixed nuclei approximation, where the molecule's nuclei are considered fixed when collision occurs and the electronic part of the problem is solved. This information is then plugged into calculations which take into account nuclear motion. The UK Molecular R-Matrix codes were developed by the Collaborative Computational Project Q (CCPQ). Software The CCPQ and CCP2 have supported various incarnations of the UK Molecular R-matrix project for almost 40 years. The UK Molecular R-Matrix Group is actually a subgroup of CCP2, and their codes are maintained by Professor Jonathan Tennyson and his group of researchers. Advances in research have shown that the UK Molecular R-matrix codes can be used to explain scattering problems involving light molecular targets. Quantemol-N (QN) is software that allows the UK molecular R-matrix codes, which is used to model electron-polyatomic molecule interactions, to be employed quickly with reduced set-up times. QN is an interface that simplifies the process of using the sophisticated UK molecular R-Matrix codes. See also Quantemol == References == In physics, the cluster decomposition property states that experiments carried out far from each other cannot influence each other. Usually applied to quantum field theory, it requires that vacuum expectation values of operators localized in bounded regions factorize whenever these regions becomes sufficiently distant from each other. First formulated by Eyvind Wichmann and James H. Crichton in 1963 in the context of the S-matrix, it was conjectured by Steven Weinberg that in the low energy limit the cluster decomposition property, together with Lorentz invariance and quantum mechanics, inevitably lead to quantum field theory. String theory satisfies all three of the conditions and so provides a counter-example against this being true at all energy scales. Formulation The S-matrix S β α {\displaystyle S_{\beta \alpha }} describes the amplitude for a process with an initial state α {\displaystyle \alpha } evolving into a final state β {\displaystyle \beta } . If the initial and final states consist of two clusters, with α 1 {\displaystyle \alpha _{1}} and β 1 {\displaystyle \beta _{1}} close to each other but far from the pair α 2 {\displaystyle \alpha _{2}} and β 2 {\displaystyle \beta _{2}} , then the cluster decomposition property requires the S-matrix to factorize S β α → S β 1 α 1 S β 2 α 2 {\displaystyle S_{\beta \alpha }\rightarrow S_{\beta _{1}\alpha _{1}}S_{\beta _{2}\alpha _{2}}} as the distance between the two clusters increases. The physical interpretation of this is that any two spatially well separated experiments α 1 → β 1 {\displaystyle \alpha _{1}\rightarrow \beta _{1}} and α 2 → β 2 {\displaystyle \alpha _{2}\rightarrow \beta _{2}} cannot influence each other. This condition is fundamental to the ability to doing physics without having to know the state of the entire universe. By expanding the S-matrix into a sum of a product of connected S-matrix elements S β α c {\displaystyle S_{\beta \alpha }^{c}} , which at the perturbative level are equivalent to connected Feynman diagrams, the cluster decomposition property can be restated as demanding that connected S-matrix elements must vanish whenever some of its clusters of particles are far apart from each other. This position space formulation can also be reformulated in terms of the momentum space S-matrix S ~ β α c {\displaystyle {\tilde {S}}_{\beta \alpha }^{c}} . Since its Fourier transformation gives the position space connected S-matrix, this only depends on position through the exponential terms. Therefore, performing a uniform translation in a direction a {\displaystyle {\boldsymbol {a}}} on a subset of particles will effectively change the momentum space S-matrix as S ~ β α c → x i → x i + a e i a ⋅ ( ∑ i p i ) S ~ β α c . {\displaystyle {\tilde {S}}_{\beta \alpha }^{c}\xrightarrow {{\boldsymbol {x}}_{i}\rightarrow {\boldsymbol {x}}_{i}+{\boldsymbol {a}}} e^{i{\boldsymbol {a}}\cdot (\sum _{i}{\boldsymbol {p}}_{i})}{\tilde {S}}_{\beta \alpha }^{c}.} By translational invariance, a translation of all particles cannot change the S-matrix, therefore S ~ β α {\displaystyle {\tilde {S}}_{\beta \alpha }} must be proportional to a momentum conserving delta function δ ( Σ p ) {\displaystyle \delta (\Sigma {\boldsymbol {p}})} to ensure that the translation exponential factor vanishes. If there is an additional delta function of only a subset of momenta corresponding to some cluster of particles, then this cluster can be moved arbitrarily far through a translation without changing the S-matrix, which would violate cluster decomposition. This means that in momentum space the property requires that the S-matrix only has a single delta function. Cluster decomposition can also be formulated in terms of correlation functions, where for any two operators O 1 ( x ) {\displaystyle {\mathcal {O}}_{1}(x)} and O 2 ( x ) {\displaystyle {\mathcal {O}}_{2}(x)} localized to some region, the vacuum expectation values factorize as the two operators become distantly separated lim | x | → ∞ ⟨ O 1 ( x ) O 2 ( 0 ) ⟩ → ⟨ O 1 ⟩ ⟨ O 2 ⟩ . {\displaystyle \lim _{|{\boldsymbol {x}}|\rightarrow \infty }\langle {\mathcal {O}}_{1}({\boldsymbol {x}}){\mathcal {O}}_{2}(0)\rangle \rightarrow \langle {\mathcal {O}}_{1}\rangle \langle {\mathcal {O}}_{2}\rangle .} This formulation allows for the property to be applied to theories that lack an S-matrix such as conformal field theories. It is in terms of these Wightman functions that the property is usually formulated in axiomatic quantum field theory. In some formulations, such as Euclidean constructive field theory, it is explicitly introduced as an axiom. Properties If a theory is constructed from creation and annihilation operators, then the cluster decomposition property automatically holds. This can be seen by expanding out the S-matrix as a sum of Feynman diagrams which allows for the identification of connected S-matrix elements with connected Feynman diagrams. Vertices arise whenever creation and annihilation operators commute past each other leaving behind a single momentum delta function. In any connected diagram with V vertices, I internal lines and L loops, I-L of the delta functions go into fixing internal momenta, leaving V-(I-L) delta functions unfixed. A form of Euler's formula states that any graph with C disjoint connected components satisfies C = V-I+L. Since the connected S-matrix elements correspond to C=1 diagrams, these only have a single delta function and thus the cluster decomposition property, as formulated above in momentum space in terms of delta functions, holds. Microcausality, the locality condition requiring commutation relations of local operators to vanish for spacelike separations, is a sufficient condition for the S-matrix to satisfy cluster decomposition. In this sense cluster decomposition serves a similar purpose for the S-matrix as microcausality does for fields, preventing causal influence from propagating between regions that are distantly separated. However, cluster decomposition is weaker than having no superluminal causation since it can be formulated for classical theories as well. One key requirement for cluster decomposition is that it requires a unique vacuum state, with it failing if the vacuum state is a mixed state. The rate at which the correlation functions factorize depends on the spectrum of the theory, where if it has mass gap of mass m {\displaystyle m} then there is an exponential falloff ⟨ ϕ ( x ) ϕ ( 0 ) ⟩ ∼ e − m | x | {\displaystyle \langle \phi (x)\phi (0)\rangle \sim e^{-m|x|}} while if there are massless particles present then it can be as slow as 1 / | x | 2 {\displaystyle 1/|x|^{2}} . == References == In lattice field theory, the Ginsparg–Wilson equation generalizes chiral symmetry on the lattice in a way that approaches the continuum formulation in the continuum limit. The class of fermions whose Dirac operators satisfy this equation are known as Ginsparg–Wilson fermions, with notable examples being overlap, domain wall and fixed point fermions. They are a means to avoid the fermion doubling problem, widely used for instance in lattice QCD calculations. The equation was discovered by Paul Ginsparg and Kenneth Wilson in 1982, however it was quickly forgotten about since there were no known solutions. It was only in 1997 and 1998 that the first solutions were found in the form of the overlap and fixed point fermions, at which point the equation entered prominence. Ginsparg–Wilson fermions do not contradict the Nielsen–Ninomiya theorem because they explicitly violate chiral symmetry. More precisely, the continuum chiral symmetry relation D γ 5 + γ 5 D = 0 {\displaystyle D\gamma _{5}+\gamma _{5}D=0} (where D {\displaystyle D} is the massless Dirac operator) is replaced by the Ginsparg–Wilson equation D γ 5 + γ 5 D = a D γ 5 D {\displaystyle D\gamma _{5}+\gamma _{5}D=a\,D\gamma _{5}D\,} which recovers the correct continuum expression as the lattice spacing a {\displaystyle a} goes to zero. In contrast to Wilson fermions, Ginsparg–Wilson fermions do not modify the inverse fermion propagator additively but multiplicatively, thus lifting the unphysical poles at p μ = π / a {\displaystyle p_{\mu }=\pi /a} . The exact form of this modification depends on the individual realisation. == References == Niels Henrik David Bohr (US: , UK: ; Danish: [ˈne̝ls ˈpoɐ̯ˀ]; 7 October 1885 – 18 November 1962) was a Danish theoretical physicist who made foundational contributions to understanding atomic structure and quantum theory, for which he received the Nobel Prize in Physics in 1922. Bohr was also a philosopher and a promoter of scientific research. Bohr developed the Bohr model of the atom, in which he proposed that energy levels of electrons are discrete and that the electrons revolve in stable orbits around the atomic nucleus but can jump from one energy level (or orbit) to another. Although the Bohr model has been supplanted by other models, its underlying principles remain valid. He conceived the principle of complementarity: that items could be separately analysed in terms of contradictory properties, like behaving as a wave or a stream of particles. The notion of complementarity dominated Bohr's thinking in both science and philosophy. Bohr founded the Institute of Theoretical Physics at the University of Copenhagen, now known as the Niels Bohr Institute, which opened in 1920. Bohr mentored and collaborated with physicists including Hans Kramers, Oskar Klein, George de Hevesy, and Werner Heisenberg. He predicted the properties of a new zirconium-like element, which was named hafnium, after the Latin name for Copenhagen, where it was discovered. Later, the synthetic element bohrium was named after him because of his groundbreaking work on the structure of atoms. During the 1930s, Bohr helped refugees from Nazism. After Denmark was occupied by the Germans, he met with Heisenberg, who had become the head of the German nuclear weapon project. In September 1943 word reached Bohr that he was about to be arrested by the Germans, so he fled to Sweden. From there, he was flown to Britain, where he joined the British Tube Alloys nuclear weapons project, and was part of the British mission to the Manhattan Project. After the war, Bohr called for international cooperation on nuclear energy. He was involved with the establishment of CERN and the Research Establishment Risø of the Danish Atomic Energy Commission and became the first chairman of the Nordic Institute for Theoretical Physics in 1957. Early life Niels Henrik David Bohr was born in Copenhagen, Denmark, on 7 October 1885, the second of three children of Christian Bohr, a professor of physiology at the University of Copenhagen, and his wife Ellen née Adler, who came from a wealthy Jewish banking family. He had an elder sister, Jenny, and a younger brother Harald. Jenny became a teacher, while Harald became a mathematician and footballer who played for the Danish national team at the 1908 Summer Olympics in London. Niels was a passionate footballer as well, and the two brothers played several matches for the Copenhagen-based Akademisk Boldklub (Academic Football Club), with Niels as goalkeeper. Bohr was educated at Gammelholm Latin School, starting when he was seven. In 1903, Bohr enrolled as an undergraduate at Copenhagen University. His major was physics, which he studied under Professor Christian Christiansen, the university's only professor of physics at that time. He also studied astronomy and mathematics under Professor Thorvald Thiele, and philosophy under Professor Harald Høffding, a friend of his father. In 1905 a gold medal competition was sponsored by the Royal Danish Academy of Sciences and Letters to investigate a method for measuring the surface tension of liquids that had been proposed by Lord Rayleigh in 1879. This involved measuring the frequency of oscillation of the radius of a water jet. Bohr conducted a series of experiments using his father's laboratory in the university; the university itself had no physics laboratory. To complete his experiments, he had to make his own glassware, creating test tubes with the required elliptical cross-sections. He went beyond the original task, incorporating improvements into both Rayleigh's theory and his method, by taking into account the viscosity of the water, and by working with finite amplitudes instead of just infinitesimal ones. His essay, which he submitted at the last minute, won the prize. He later submitted an improved version of the paper to the Royal Society in London for publication in the Philosophical Transactions of the Royal Society. Harald became the first of the two Bohr brothers to earn a master's degree, which he earned for mathematics in April 1909. Niels took another nine months to earn his on the electron theory of metals, a topic assigned by his supervisor, Christiansen. Bohr subsequently elaborated his master's thesis into his much-larger Doctor of Philosophy thesis. He surveyed the literature on the subject, settling on a model postulated by Paul Drude and elaborated by Hendrik Lorentz, in which the electrons in a metal are considered to behave like a gas. Bohr extended Lorentz's model, but was still unable to account for phenomena like the Hall effect, and concluded that electron theory could not fully explain the magnetic properties of metals. The thesis was accepted in April 1911, and Bohr conducted his formal defence on 13 May. Harald had received his doctorate the previous year. Bohr's thesis was groundbreaking, but attracted little interest outside Scandinavia because it was written in Danish, a Copenhagen University requirement at the time. In 1921, the Dutch physicist Hendrika Johanna van Leeuwen would independently derive a theorem in Bohr's thesis that is today known as the Bohr–Van Leeuwen theorem. In 1910, Bohr met Margrethe Nørlund, the sister of the mathematician Niels Erik Nørlund. Bohr resigned his membership in the Church of Denmark on 16 April 1912, and he and Margrethe were married in a civil ceremony at the town hall in Slagelse on 1 August. Years later, his brother Harald similarly left the church before getting married. Bohr and Margrethe had six sons. The oldest, Christian, died in a boating accident in 1934, and another, Harald, was severely mentally disabled. He was placed in an institution away from his family's home at the age of four and died from childhood meningitis six years later. Aage Bohr became a successful physicist, and in 1975 was awarded the Nobel Prize in physics, like his father. A son of Aage, Vilhelm A. Bohr, is a scientist affiliated with the University of Copenhagen and the National Institute on Aging in the U.S. Hans became a physician; Erik, a chemical engineer; and Ernest, a lawyer. Like his uncle Harald, Ernest Bohr became an Olympic athlete, playing field hockey for Denmark at the 1948 Summer Olympics in London. Physics Bohr model In September 1911, Bohr, supported by a fellowship from the Carlsberg Foundation, travelled to England, where most of the theoretical work on the structure of atoms and molecules was being done. He met J. J. Thomson of the Cavendish Laboratory and Trinity College, Cambridge. He attended lectures on electromagnetism given by James Jeans and Joseph Larmor, and did some research on cathode rays, but failed to impress Thomson. He had more success with younger physicists like the Australian William Lawrence Bragg, and New Zealand's Ernest Rutherford, whose 1911 small central nucleus Rutherford model of the atom had challenged Thomson's 1904 plum pudding model. Bohr received an invitation from Rutherford to conduct post-doctoral work at Victoria University of Manchester, where Bohr met George de Hevesy and Charles Galton Darwin (whom Bohr referred to as "the grandson of the real Darwin"). Bohr returned to Denmark in July 1912 for his wedding, and travelled around England and Scotland on his honeymoon. On his return, he became a privatdocent at the University of Copenhagen, giving lectures on thermodynamics. Martin Knudsen put Bohr's name forward for a docent, which was approved in July 1913, and Bohr then began teaching medical students. His three papers, which later became famous as "the trilogy", were published in Philosophical Magazine in July, September and November of that year. He adapted Rutherford's nuclear structure to Max Planck's quantum theory and so created his Bohr model of the atom. Planetary models of atoms were not new, but Bohr's treatment was. Taking the 1912 paper by Darwin on the role of electrons in the interaction of alpha particles with a nucleus as his starting point, he advanced the theory of electrons travelling in orbits of quantised "stationary states" around the atom's nucleus in order to stabilise the atom, but it wasn't until his 1921 paper that he showed that the chemical properties of each element were largely determined by the number of electrons in the outer orbits of its atoms. He introduced the idea that an electron could drop from a higher-energy orbit to a lower one, in the process emitting a quantum of discrete energy. This became a basis for what is now known as the old quantum theory. In 1885, Johann Balmer had come up with his Balmer series to describe the visible spectral lines of a hydrogen atom: 1 λ = R H ( 1 2 2 − 1 n 2 ) for n = 3 , 4 , 5 , . . . {\displaystyle {\frac {1}{\lambda }}=R_{\mathrm {H} }\left({\frac {1}{2^{2}}}-{\frac {1}{n^{2}}}\right)\quad {\text{for}}\ n=3,4,5,...} where λ is the wavelength of the absorbed or emitted light and RH is the Rydberg constant. Balmer's formula was corroborated by the discovery of additional spectral lines, but for thirty years, no one could explain why it worked. In the first paper of his trilogy, Bohr was able to derive it from his model: R Z = 2 π 2 m e Z 2 e 4 h 3 {\displaystyle R_{Z}={2\pi ^{2}m_{e}Z^{2}e^{4} \over h^{3}}} where me is the electron's mass, e is its charge, h is the Planck constant and Z is the atom's atomic number (1 for hydrogen). The model's first hurdle was the Pickering series, lines that did not fit Balmer's formula. When challenged on this by Alfred Fowler, Bohr replied that they were caused by ionised helium, helium atoms with only one electron. The Bohr model was found to work for such ions. Many older physicists, like Thomson, Rayleigh and Hendrik Lorentz, did not like the trilogy, but the younger generation, including Rutherford, David Hilbert, Albert Einstein, Enrico Fermi, Max Born and Arnold Sommerfeld saw it as a breakthrough. Einstein called Bohr's model "the highest form of musicality in the sphere of thought." The trilogy's acceptance was entirely due to its ability to explain phenomena that stymied other models, and to predict results that were subsequently verified by experiments. Today, the Bohr model of the atom has been superseded, but is still the best known model of the atom, as it often appears in high school physics and chemistry texts. Bohr did not enjoy teaching medical students. He later admitted that he was not a good lecturer, because he needed a balance between clarity and truth, between "Klarheit und Wahrheit". He decided to return to Manchester, where Rutherford had offered him a job as a reader in place of Darwin, whose tenure had expired. Bohr accepted. He took a leave of absence from the University of Copenhagen, which he started by taking a holiday in Tyrol with his brother Harald and aunt Hanna Adler. There, he visited the University of Göttingen and the Ludwig Maximilian University of Munich, where he met Sommerfeld and conducted seminars on the trilogy. The First World War broke out while they were in Tyrol, greatly complicating the trip back to Denmark and Bohr's subsequent voyage with Margrethe to England, where he arrived in October 1914. They stayed until July 1916, by which time he had been appointed to the Chair of Theoretical Physics at the University of Copenhagen, a position created especially for him. His docentship was abolished at the same time, so he still had to teach physics to medical students. New professors were formally introduced to King Christian X, who expressed his delight at meeting such a famous football player. Institute of Physics In April 1917, Bohr began a campaign to establish an Institute of Theoretical Physics. He gained the support of the Danish government and the Carlsberg Foundation, and sizeable contributions were also made by industry and private donors, many of them Jewish. Legislation establishing the institute was passed in November 1918. Now known as the Niels Bohr Institute, it opened on 3 March 1921, with Bohr as its director. His family moved into an apartment on the first floor. Bohr's institute served as a focal point for researchers into quantum mechanics and related subjects in the 1920s and 1930s, when most of the world's best-known theoretical physicists spent some time in his company. Early arrivals included Hans Kramers from the Netherlands, Oskar Klein from Sweden, George de Hevesy from Hungary, Wojciech Rubinowicz from Poland, and Svein Rosseland from Norway. Bohr became widely appreciated as their congenial host and eminent colleague. Klein and Rosseland produced the institute's first publication even before it opened. The Bohr model worked well for hydrogen and ionized single-electron helium, which impressed Einstein but could not explain more complex elements. By 1919, Bohr was moving away from the idea that electrons orbited the nucleus and developed heuristics to describe them. The rare-earth elements posed a particular classification problem for chemists because they were so chemically similar. An important development came in 1924 with Wolfgang Pauli's discovery of the Pauli exclusion principle, which put Bohr's models on a firm theoretical footing. Bohr was then able to declare that the as-yet-undiscovered element 72 was not a rare-earth element but an element with chemical properties similar to those of zirconium. (Elements had been predicted and discovered since 1871 by chemical properties), and Bohr was immediately challenged by the French chemist Georges Urbain, who claimed to have discovered a rare-earth element 72, which he called "celtium". At the Institute in Copenhagen, Dirk Coster and George de Hevesy took up the challenge of proving Bohr right and Urbain wrong. Starting with a clear idea of the chemical properties of the unknown element greatly simplified the search process. They went through samples from Copenhagen's Museum of Mineralogy looking for a zirconium-like element and soon found it. The element, which they named hafnium (hafnia being the Latin name for Copenhagen), turned out to be more common than gold. In 1922, Bohr was awarded the Nobel Prize in Physics "for his services in the investigation of the structure of atoms and of the radiation emanating from them". The award thus recognised both the trilogy and his early leading work in the emerging field of quantum mechanics. For his Nobel lecture, Bohr gave his audience a comprehensive survey of what was then known about the structure of the atom, including the correspondence principle, which he had formulated. This states that the behaviour of systems described by quantum theory reproduces classical physics in the limit of large quantum numbers. The discovery of Compton scattering by Arthur Holly Compton in 1923 convinced most physicists that light was composed of photons and that energy and momentum were conserved in collisions between electrons and photons. In 1924, Bohr, Kramers, and John C. Slater, an American physicist working at the Institute in Copenhagen, proposed the Bohr–Kramers–Slater theory (BKS). It was more of a program than a full physical theory, as the ideas it developed were not worked out quantitatively. The BKS theory became the final attempt at understanding the interaction of matter and electromagnetic radiation on the basis of the old quantum theory, in which quantum phenomena were treated by imposing quantum restrictions on a classical wave description of the electromagnetic field. Modelling atomic behaviour under incident electromagnetic radiation using "virtual oscillators" at the absorption and emission frequencies, rather than the (different) apparent frequencies of the Bohr orbits, led Max Born, Werner Heisenberg and Kramers to explore different mathematical models. They led to the development of matrix mechanics, the first form of modern quantum mechanics. The BKS theory also generated discussion of, and renewed attention to, difficulties in the foundations of the old quantum theory. The most provocative element of BKS – that momentum and energy would not necessarily be conserved in each interaction, but only statistically – was soon shown to be in conflict with experiments conducted by Walther Bothe and Hans Geiger. In light of these results, Bohr informed Darwin that "there is nothing else to do than to give our revolutionary efforts as honourable a funeral as possible". Quantum mechanics The introduction of spin by George Uhlenbeck and Samuel Goudsmit in November 1925 was a milestone. The next month, Bohr travelled to Leiden to attend celebrations of the 50th anniversary of Hendrick Lorentz receiving his doctorate. When his train stopped in Hamburg, he was met by Wolfgang Pauli and Otto Stern, who asked for his opinion of the spin theory. Bohr pointed out that he had concerns about the interaction between electrons and magnetic fields. When he arrived in Leiden, Paul Ehrenfest and Albert Einstein informed Bohr that Einstein had resolved this problem using relativity. Bohr then had Uhlenbeck and Goudsmit incorporate this into their paper. Thus, when he met Werner Heisenberg and Pascual Jordan in Göttingen on the way back, he had become, in his own words, "a prophet of the electron magnet gospel". Heisenberg first came to Copenhagen in 1924, then returned to Göttingen in June 1925, shortly thereafter developing the mathematical foundations of quantum mechanics. When he showed his results to Max Born in Göttingen, Born realised that they could best be expressed using matrices. This work attracted the attention of the British physicist Paul Dirac, who came to Copenhagen for six months in September 1926. Austrian physicist Erwin Schrödinger also visited in 1926. His attempt at explaining quantum physics in classical terms using wave mechanics impressed Bohr, who believed it contributed "so much to mathematical clarity and simplicity that it represents a gigantic advance over all previous forms of quantum mechanics". When Kramers left the institute in 1926 to take up a chair as professor of theoretical physics at the Utrecht University, Bohr arranged for Heisenberg to return and take Kramers's place as a lektor at the University of Copenhagen. Heisenberg worked in Copenhagen as a university lecturer and assistant to Bohr from 1926 to 1927. Bohr became convinced that light behaved like both waves and particles and, in 1927, experiments confirmed the de Broglie hypothesis that matter (like electrons) also behaved like waves. He conceived the philosophical principle of complementarity: that items could have apparently mutually exclusive properties, such as being a wave or a stream of particles, depending on the experimental framework. He felt that it was not fully understood by professional philosophers. In February 1927, Heisenberg developed the first version of the uncertainty principle, presenting it using a thought experiment where an electron was observed through a gamma-ray microscope. Bohr was dissatisfied with Heisenberg's argument, since it required only that a measurement disturb properties that already existed, rather than the more radical idea that the electron's properties could not be discussed at all apart from the context they were measured in. In a paper presented at the Como Conference in September 1927, Bohr emphasised that Heisenberg's uncertainty relations could be derived from classical considerations about the resolving power of optical instruments. Understanding the true meaning of complementarity would, Bohr believed, require "closer investigation". Einstein preferred the determinism of classical physics over the probabilistic new quantum physics to which he himself had contributed. Philosophical issues that arose from the novel aspects of quantum mechanics became widely celebrated subjects of discussion. Einstein and Bohr had good-natured arguments over such issues throughout their lives. In 1914 Carl Jacobsen, the heir to Carlsberg breweries, bequeathed his mansion (the Carlsberg Honorary Residence, currently known as Carlsberg Academy) to be used for life by the Dane who had made the most prominent contribution to science, literature or the arts, as an honorary residence (Danish: Æresbolig). Harald Høffding had been the first occupant, and upon his death in July 1931, the Royal Danish Academy of Sciences and Letters gave Bohr occupancy. He and his family moved there in 1932. He was elected president of the Academy on 17 March 1939. By 1929 the phenomenon of beta decay prompted Bohr to again suggest that the law of conservation of energy be abandoned, but Wolfgang Pauli's hypothetical neutrino and the subsequent 1932 discovery of the neutron provided another explanation. This prompted Bohr to create a new theory of the compound nucleus in 1936, which explained how neutrons could be captured by the nucleus. In this model, the nucleus could be deformed like a drop of liquid. He worked on this with a new collaborator, the Danish physicist Fritz Kalckar, who died suddenly in 1938. The discovery of nuclear fission by Otto Hahn in December 1938 (and its theoretical explanation by Lise Meitner) generated intense interest among physicists. Bohr brought the news to the United States where he opened the fifth Washington Conference on Theoretical Physics with Fermi on 26 January 1939. When Bohr told George Placzek that this resolved all the mysteries of transuranic elements, Placzek told him that one remained: the neutron capture energies of uranium did not match those of its decay. Bohr thought about it for a few minutes and then announced to Placzek, Léon Rosenfeld and John Wheeler that "I have understood everything." Based on his liquid drop model of the nucleus, Bohr concluded that it was the uranium-235 isotope and not the more abundant uranium-238 that was primarily responsible for fission with thermal neutrons. In April 1940, John R. Dunning demonstrated that Bohr was correct. In the meantime, Bohr and Wheeler developed a theoretical treatment, which they published in a September 1939 paper on "The Mechanism of Nuclear Fission". Philosophy Heisenberg said of Bohr that he was "primarily a philosopher, not a physicist". Bohr read the 19th-century Danish Christian existentialist philosopher Søren Kierkegaard. Richard Rhodes argued in The Making of the Atomic Bomb that Bohr was influenced by Kierkegaard through Høffding. In 1909, Bohr sent his brother Kierkegaard's Stages on Life's Way as a birthday gift. In the enclosed letter, Bohr wrote, "It is the only thing I have to send home; but I do not believe that it would be very easy to find anything better ... I even think it is one of the most delightful things I have ever read." Bohr enjoyed Kierkegaard's language and literary style, but mentioned that he had some disagreement with Kierkegaard's philosophy. Some of Bohr's biographers suggested that this disagreement stemmed from Kierkegaard's advocacy of Christianity, while Bohr was an atheist. There has been some dispute over the extent to which Kierkegaard influenced Bohr's philosophy and science. David Favrholdt argued that Kierkegaard had minimal influence over Bohr's work, taking Bohr's statement about disagreeing with Kierkegaard at face value, while Jan Faye argued that one can disagree with the content of a theory while accepting its general premises and structure. Bohr sat on the Board of Editors of the book series World Perspectives which published a variety of books on philosophy. Quantum physics There has been much subsequent debate and discussion about Bohr's views and philosophy of quantum mechanics. Regarding his ontological interpretation of the quantum world, Bohr has been seen as an anti-realist, an instrumentalist, a phenomenological realist or some other kind of realist. Furthermore, though some have seen Bohr as being a subjectivist or a positivist, most philosophers agree that this is a misunderstanding of Bohr as he never argued for verificationism or for the idea that the subject had a direct impact on the outcome of a measurement. Bohr has often been quoted saying that there is "no quantum world" but only an "abstract quantum physical description". This was not publicly said by Bohr, but rather a private statement attributed to Bohr by Aage Petersen in a reminiscence after his death. N. David Mermin recalled Victor Weisskopf declaring that Bohr wouldn't have said anything of the sort and exclaiming, "Shame on Aage Petersen for putting those ridiculous words in Bohr's mouth!" Numerous scholars have argued that the philosophy of Immanuel Kant had a strong influence on Bohr. Like Kant, Bohr thought distinguishing between the subject's experience and the object was an important condition for attaining knowledge. This can only be done through the use of causal and spatial-temporal concepts to describe the subject's experience. Thus, according to Jan Faye, Bohr thought that it is because of "classical" concepts like "space", "position", "time", "causation", and "momentum" that one can talk about objects and their objective existence. Bohr held that basic concepts like "time" are built in to our ordinary language and that the concepts of classical physics are merely a refinement of them. Therefore, for Bohr, classical concepts need to be used to describe experiments that deal with the quantum world. Bohr writes: [T]he account of all evidence must be expressed in classical terms. The argument is simply that by the word 'experiment' we refer to a situation where we can tell to others what we have done and what we have learned and that, therefore, the account of the experimental arrangement and of the results of the observations must be expressed in unambiguous language with suitable application of the terminology of classical physics (APHK, p. 39). According to Faye, there are various explanations for why Bohr believed that classical concepts were necessary for describing quantum phenomena. Faye groups explanations into five frameworks: empiricism (i.e. logical positivism); Kantianism (or Neo-Kantian models of epistemology); Pragmatism (which focus on how human beings experientially interact with atomic systems according to their needs and interests); Darwinianism (i.e. we are adapted to use classical type concepts, which Léon Rosenfeld said that we evolved to use); and Experimentalism (which focuses strictly on the function and outcome of experiments that thus must be described classically). These explanations are not mutually exclusive, and at times Bohr seems to emphasise some of these aspects while at other times he focuses on other elements. According to Faye "Bohr thought of the atom as real. Atoms are neither heuristic nor logical constructions." However, according to Faye, he did not believe "that the quantum mechanical formalism was true in the sense that it gave us a literal ('pictorial') rather than a symbolic representation of the quantum world." Therefore, Bohr's theory of complementarity "is first and foremost a semantic and epistemological reading of quantum mechanics that carries certain ontological implications". As Faye explains, Bohr's indefinability thesis is that [T]he truth conditions of sentences ascribing a certain kinematic or dynamic value to an atomic object are dependent on the apparatus involved, in such a way that these truth conditions have to include reference to the experimental setup as well as the actual outcome of the experiment. Faye notes that Bohr's interpretation makes no reference to a "collapse of the wave function during measurements" (and indeed, he never mentioned this idea). Instead, Bohr "accepted the Born statistical interpretation because he believed that the ψ-function has only a symbolic meaning and does not represent anything real". Since for Bohr, the ψ-function is not a literal pictorial representation of reality, there can be no real collapse of the wavefunction. A much debated point in recent literature is what Bohr believed about atoms and their reality and whether they are something else than what they seem to be. Some like Henry Folse argue that Bohr saw a distinction between observed phenomena and a transcendental reality. Jan Faye disagrees with this position and holds that for Bohr, the quantum formalism and complementarity was the only thing we could say about the quantum world and that "there is no further evidence in Bohr's writings indicating that Bohr would attribute intrinsic and measurement-independent state properties to atomic objects [...] in addition to the classical ones being manifested in measurement." Second World War Assistance to refugee scholars The rise of Nazism in Germany prompted many scholars to flee their countries, either because they were Jewish or because they were political opponents of the Nazi regime. In 1933, the Rockefeller Foundation created a fund to help support refugee academics, and Bohr discussed this programme with the President of the Rockefeller Foundation, Max Mason, in May 1933 during a visit to the United States. Bohr offered the refugees temporary jobs at the institute, provided them with financial support, arranged for them to be awarded fellowships from the Rockefeller Foundation, and ultimately found them places at institutions around the world. Those that he helped included Guido Beck, Felix Bloch, James Franck, George de Hevesy, Otto Frisch, Hilde Levi, Lise Meitner, George Placzek, Eugene Rabinowitch, Stefan Rozental, Erich Ernst Schneider, Edward Teller, Arthur von Hippel and Victor Weisskopf. In April 1940, early in the Second World War, Nazi Germany invaded and occupied Denmark. To prevent the Germans from discovering Max von Laue's and James Franck's gold Nobel medals, Bohr had de Hevesy dissolve them in aqua regia. In this form, they were stored on a shelf at the Institute until after the war, when the gold was precipitated and the medals re-struck by the Nobel Foundation. Bohr's own medal had been donated to an auction to the Finnish Relief Fund, and was auctioned off in March 1940, along with the medal of August Krogh. The buyer later donated the two medals to the Danish Historical Museum in Frederiksborg Castle, where they are still kept, although Bohr's medal temporarily went to space with Andreas Mogensen on ISS Expedition 70 in 2023-2024. Bohr kept the Institute running, but all the foreign scholars departed. Meeting with Heisenberg Bohr was aware of the possibility of using uranium-235 to construct an atomic bomb, referring to it in lectures in Britain and Denmark shortly before and after the war started, but he did not believe that it was technically feasible to extract a sufficient quantity of uranium-235. In September 1941, Heisenberg, who had become head of the German nuclear energy project, visited Bohr in Copenhagen. During this meeting the two men took a private moment outside, the content of which has caused much speculation, as both gave differing accounts. According to Heisenberg, he began to address nuclear energy, morality and the war, to which Bohr seems to have reacted by terminating the conversation abruptly while not giving Heisenberg hints about his own opinions. Ivan Supek, one of Heisenberg's students and friends, claimed that the main subject of the meeting was Carl Friedrich von Weizsäcker, who had proposed trying to persuade Bohr to mediate peace between Britain and Germany. In 1957, Heisenberg wrote to Robert Jungk, who was then working on the book Brighter than a Thousand Suns: A Personal History of the Atomic Scientists. Heisenberg explained that he had visited Copenhagen to communicate to Bohr the views of several German scientists, that production of a nuclear weapon was possible with great efforts, and this raised enormous responsibilities on the world's scientists on both sides. When Bohr saw Jungk's depiction in the Danish translation of the book, he drafted (but never sent) a letter to Heisenberg, stating that he deeply disagreed with Heisenberg's account of the meeting, that he recalled Heisenberg's visit as being to encourage cooperation with the inevitably victorious Nazis and that he was shocked that Germany was pursuing nuclear weapons under Heisenberg's leadership. Michael Frayn's 1998 play Copenhagen explores what might have happened at the 1941 meeting between Heisenberg and Bohr. A television film version of the play by the BBC was first screened on 26 September 2002, with Stephen Rea as Bohr. With the subsequent release of Bohr's letters, the play has been criticised by historians as being a "grotesque oversimplification and perversion of the actual moral balance" due to adopting a pro-Heisenberg perspective. The same meeting had previously been dramatised by the BBC's Horizon science documentary series in 1992, with Anthony Bate as Bohr, and Philip Anthony as Heisenberg. The meeting is also dramatised in the Norwegian/Danish/British miniseries The Heavy Water War. Manhattan Project In September 1943, word reached Bohr and his brother Harald that the Nazis considered their family to be Jewish, since their mother was Jewish, and that they were therefore in danger of being arrested. The Danish resistance helped Bohr and his wife escape by sea to Sweden on 29 September. The next day, Bohr persuaded King Gustaf V of Sweden to make public Sweden's willingness to provide asylum to Jewish refugees. On 2 October 1943, Swedish radio broadcast that Sweden was ready to offer asylum, and the mass rescue of the Danish Jews by their countrymen followed swiftly thereafter. Some historians claim that Bohr's actions led directly to the mass rescue, while others say that, though Bohr did all that he could for his countrymen, his actions were not a decisive influence on the wider events. Eventually, over 7,000 Danish Jews escaped to Sweden. When the news of Bohr's escape reached Britain, Lord Cherwell sent a telegram to Bohr asking him to come to Britain. Bohr arrived in Scotland on 6 October in a de Havilland Mosquito operated by the British Overseas Airways Corporation (BOAC). The Mosquitos were unarmed high-speed bomber aircraft that had been converted to carry small, valuable cargoes or important passengers. By flying at high speed and high altitude, they could cross German-occupied Norway, and yet avoid German fighters. Bohr, equipped with parachute, flying suit and oxygen mask, spent the three-hour flight lying on a mattress in the aircraft's bomb bay. During the flight, Bohr did not wear his flying helmet as it was too small, and consequently did not hear the pilot's intercom instruction to turn on his oxygen supply when the aircraft climbed to high altitude to overfly Norway. He passed out from oxygen starvation and only revived when the aircraft descended to lower altitude over the North Sea. Bohr's son Aage followed his father to Britain on another flight a week later, and became his personal assistant. Bohr was warmly received by James Chadwick and Sir John Anderson, but for security reasons Bohr was kept out of sight. He was given an apartment at St James's Palace and an office with the British Tube Alloys nuclear weapons development team. Bohr was astonished at the amount of progress that had been made. Chadwick arranged for Bohr to visit the United States as a Tube Alloys consultant, with Aage as his assistant. On 8 December 1943, Bohr arrived in Washington, D.C., where he met with the director of the Manhattan Project, Brigadier General Leslie R. Groves Jr. He visited Einstein and Pauli at the Institute for Advanced Study in Princeton, New Jersey, and went to Los Alamos in New Mexico, where the nuclear weapons were being designed. For security reasons, he went under the name of "Nicholas Baker" in the United States, while Aage became "James Baker". In May 1944 the Danish resistance newspaper De frie Danske reported that they had learned that 'the famous son of Denmark Professor Niels Bohr' in October the previous year had fled his country via Sweden to London and from there travelled to Moscow from where he could be assumed to support the war effort. Bohr did not remain at Los Alamos, but paid a series of extended visits over the course of the next two years. Robert Oppenheimer credited Bohr with acting "as a scientific father figure to the younger men", most notably Richard Feynman. Bohr is quoted as saying, "They didn't need my help in making the atom bomb." Oppenheimer gave Bohr credit for an important contribution to the work on modulated neutron initiators. "This device remained a stubborn puzzle", Oppenheimer noted, "but in early February 1945 Niels Bohr clarified what had to be done". Bohr recognised early that nuclear weapons would change international relations. In April 1944, he received a letter from Peter Kapitza, written some months before when Bohr was in Sweden, inviting him to come to the Soviet Union. The letter convinced Bohr that the Soviets were aware of the Anglo-American project, and would strive to catch up. He sent Kapitza a non-committal response, which he showed to the authorities in Britain before posting. Bohr met Churchill on 16 May 1944, but found that "we did not speak the same language". Churchill disagreed with the idea of openness towards the Russians to the point that he wrote in a letter: "It seems to me Bohr ought to be confined or at any rate made to see that he is very near the edge of mortal crimes." Oppenheimer suggested that Bohr visit President Franklin D. Roosevelt to convince him that the Manhattan Project should be shared with the Soviets in the hope of speeding up its results. Bohr's friend, Supreme Court Justice Felix Frankfurter, informed President Roosevelt about Bohr's opinions, and a meeting between them took place on 26 August 1944. Roosevelt suggested that Bohr return to the United Kingdom to try to win British approval. When Churchill and Roosevelt met at Hyde Park on 19 September 1944, they rejected the idea of informing the world about the project, and the aide-mémoire of their conversation contained a rider that "enquiries should be made regarding the activities of Professor Bohr and steps taken to ensure that he is responsible for no leakage of information, particularly to the Russians". In June 1950, Bohr addressed an "Open Letter" to the United Nations calling for international cooperation on nuclear energy. In the 1950s, after the Soviet Union's first nuclear weapon test in 1949, the International Atomic Energy Agency was created along the lines of Bohr's suggestion. In 1957 he received the first ever Atoms for Peace Award. Later years Following the ending of the war, Bohr returned to Copenhagen on 25 August 1945, and was re-elected President of the Royal Danish Academy of Arts and Sciences on 21 September. At a memorial meeting of the Academy on 17 October 1947 for King Christian X, who had died in April, the new king, Frederik IX, announced that he was conferring the Order of the Elephant on Bohr. This award was normally awarded only to royalty and heads of state, but the king said that it honoured not just Bohr personally, but Danish science. Bohr designed his own coat of arms, which featured a taijitu (symbol of yin and yang) and a motto in Latin: contraria sunt complementa, "opposites are complementary". The Second World War demonstrated that science, and physics in particular, now required considerable financial and material resources. To avoid a brain drain to the United States, twelve European countries banded together to create CERN, a research organisation along the lines of the national laboratories in the United States, designed to undertake Big Science projects beyond the resources of any one of them alone. Questions soon arose regarding the best location for the facilities. Bohr and Kramers felt that the Institute in Copenhagen would be the ideal site. Pierre Auger, who organised the preliminary discussions, disagreed; he felt that both Bohr and his Institute were past their prime, and that Bohr's presence would overshadow others. After a long debate, Bohr pledged his support to CERN in February 1952, and Geneva was chosen as the site in October. The CERN Theory Group was based in Copenhagen until their new accommodation in Geneva was ready in 1957. Victor Weisskopf, who later became the Director General of CERN, summed up Bohr's role, saying that "there were other personalities who started and conceived the idea of CERN. The enthusiasm and ideas of the other people would not have been enough, however, if a man of his stature had not supported it." Meanwhile, Scandinavian countries formed the Nordic Institute for Theoretical Physics in 1957, with Bohr as its chairman. He was also involved with the founding of the Research Establishment Risø of the Danish Atomic Energy Commission, and served as its first chairman from February 1956. Bohr died of heart failure at his home in Carlsberg on 18 November 1962. He was cremated, and his ashes were buried in the family plot in the Assistens Cemetery in the Nørrebro section of Copenhagen, along with those of his parents, his brother Harald, and his son Christian. Years later, his wife's ashes were also interred there. On 7 October 1965, on what would have been his 80th birthday, the Institute for Theoretical Physics at the University of Copenhagen was officially renamed to what it had been called unofficially for many years: the Niels Bohr Institute. Accolades Bohr received numerous honours and accolades. In addition to the Nobel Prize, he received the Hughes Medal in 1921, the Matteucci Medal in 1923, the Franklin Medal in 1926, the Copley Medal in 1938, the Order of the Elephant in 1947, the Atoms for Peace Award in 1957 and the Sonning Prize in 1961. He became foreign member of the Finnish Society of Sciences an Letters in 1922, and of the Royal Netherlands Academy of Arts and Sciences in 1923, an international member of the United States National Academy of Sciences in 1925, a member of the Royal Society in 1926, an international member of the American Philosophical Society in 1940, and an international honorary member of the American Academy of Arts and Sciences in 1945. The Bohr model's semicentennial was commemorated in Denmark on 21 November 1963 with a postage stamp depicting Bohr, the hydrogen atom and the formula for the difference of any two hydrogen energy levels: h ν = ϵ 2 − ϵ 1 {\displaystyle h\nu =\epsilon _{2}-\epsilon _{1}} . Several other countries have also issued postage stamps depicting Bohr. In 1997, the Danish National Bank began circulating the 500-krone banknote with the portrait of Bohr smoking a pipe. On 7 October 2012, in celebration of Niels Bohr's 127th birthday, a Google Doodle depicting the Bohr model of the hydrogen atom appeared on Google's home page. An asteroid, 3948 Bohr, was named after him, as was the Bohr lunar crater, and bohrium, the chemical element with atomic number 107, in acknowledgement of his work on the structure of atoms. Bibliography See also Einstein–Podolsky–Rosen paradox Notes References Further reading External links Niels Bohr Archive Author profile in the database zbMATH Works by Niels Bohr at Project Gutenberg Niels Bohr at IMDb Newspaper clippings about Niels Bohr in the 20th Century Press Archives of the ZBW Niels Bohr on Nobelprize.org including the Nobel Lecture, 11 December 1922 The Structure of the Atom Oral history interview transcript for Niels Bohr on 31 October 1962, American Institute of Physics, Niels Bohr Library & Archives – interviews conducted by Thomas S. Kuhn, Leon Rosenfeld, Erik Rudinger, and Aage Petersen Oral history interview transcript for Niels Bohr on 1 November 1962, American Institute of Physics, Niels Bohr Library & Archives Oral history interview transcript for Niels Bohr on 7 November 1962, American Institute of Physics, Niels Bohr Library & Archives Oral history interview transcript for Niels Bohr on 14 November 1962, American Institute of Physics, Niels Bohr Library & Archives Oral history interview transcript for Niels Bohr on 17 November 1962, American Institute of Physics, Niels Bohr Library & Archives "The Bohr-Heisenberg meeting in September 1941". American Institute of Physics. Archived from the original on 4 July 2011. Retrieved 2 March 2013. "Resources for Frayn's Copenhagen: Niels Bohr". Massachusetts Institute of Technology. Retrieved 9 October 2013. "Video – Niels Bohr (1962): Atomic Physics and Human Knowledge". Lindau Nobel Laureate Meetings. Retrieved 9 July 2014. A hallmark of Albert Einstein's career was his use of visualized thought experiments (German: Gedankenexperiment) as a fundamental tool for understanding physical issues and for elucidating his concepts to others. Einstein's thought experiments took diverse forms. In his youth, he mentally chased beams of light. For special relativity, he employed moving trains and flashes of lightning to explain his theory. For general relativity, he considered a person falling off a roof, accelerating elevators, blind beetles crawling on curved surfaces and the like. In his debates with Niels Bohr on the nature of reality, he proposed imaginary devices that attempted to show, at least in concept, how the Heisenberg uncertainty principle might be evaded. In a contribution to the literature on quantum mechanics, Einstein considered two particles briefly interacting and then flying apart so that their states are correlated, anticipating the phenomenon known as quantum entanglement. Introduction A thought experiment is a logical argument or mental model cast within the context of an imaginary (hypothetical or even counterfactual) scenario. A scientific thought experiment, in particular, may examine the implications of a theory, law, or set of principles with the aid of fictive and/or natural particulars (demons sorting molecules, cats whose lives hinge upon a radioactive disintegration, men in enclosed elevators) in an idealized environment (massless trapdoors, absence of friction). They describe experiments that, except for some specific and necessary idealizations, could conceivably be performed in the real world. As opposed to physical experiments, thought experiments do not report new empirical data. They can only provide conclusions based on deductive or inductive reasoning from their starting assumptions. Thought experiments invoke particulars that are irrelevant to the generality of their conclusions. It is the invocation of these particulars that give thought experiments their experiment-like appearance. A thought experiment can always be reconstructed as a straightforward argument, without the irrelevant particulars. John D. Norton, a well-known philosopher of science, has noted that "a good thought experiment is a good argument; a bad thought experiment is a bad argument." When effectively used, the irrelevant particulars that convert a straightforward argument into a thought experiment can act as "intuition pumps" that stimulate readers' ability to apply their intuitions to their understanding of a scenario. Thought experiments have a long history. Perhaps the best known in the history of modern science is Galileo's demonstration that falling objects must fall at the same rate regardless of their masses. This has sometimes been taken to be an actual physical demonstration, involving his climbing up the Leaning Tower of Pisa and dropping two heavy weights off it. In fact, it was a logical demonstration described by Galileo in Discorsi e dimostrazioni matematiche (1638). Einstein had a highly visual understanding of physics. His work in the patent office "stimulated [him] to see the physical ramifications of theoretical concepts." These aspects of his thinking style inspired him to fill his papers with vivid practical detail making them quite different from, say, the papers of Lorentz or Maxwell. This included his use of thought experiments.: 26–27, 121–127 Special relativity Pursuing a beam of light Late in life, Einstein recalled ...a paradox upon which I had already hit at the age of sixteen: If I pursue a beam of light with the velocity c (velocity of light in a vacuum), I should observe such a beam of light as an electromagnetic field at rest though spatially oscillating. There seems to be no such thing, however, neither on the basis of experience nor according to Maxwell's equations. From the very beginning it appeared to me intuitively clear that, judged from the standpoint of such an observer, everything would have to happen according to the same laws as for an observer who, relative to the earth, was at rest. For how should the first observer know or be able to determine, that he is in a state of fast uniform motion? One sees in this paradox the germ of the special relativity theory is already contained.: 52–53 Einstein's recollections of his youthful musings are widely cited because of the hints they provide of his later great discovery. However, Norton has noted that Einstein's reminiscences were probably colored by a half-century of hindsight. Norton lists several problems with Einstein's recounting, both historical and scientific: 1. At 16 years old and a student at the Gymnasium in Aarau, Einstein would have had the thought experiment in late 1895 to early 1896. But various sources note that Einstein did not learn Maxwell's theory until 1898, in university. 2. A 19th century aether theorist would have had no difficulties with the thought experiment. Einstein's statement, "...there seems to be no such thing...on the basis of experience," would not have counted as an objection, but would have represented a mere statement of fact, since no one had ever traveled at such speeds. 3. An aether theorist would have regarded "...nor according to Maxwell's equations" as simply representing a misunderstanding on Einstein's part. Unfettered by any notion that the speed of light represents a cosmic limit, the aether theorist would simply have set velocity equal to c, noted that yes indeed, the light would appear to be frozen, and then thought no more of it. Rather than the thought experiment being at all incompatible with aether theories (which it is not), the youthful Einstein appears to have reacted to the scenario out of an intuitive sense of wrongness. He felt that the laws of optics should obey the principle of relativity. As he grew older, his early thought experiment acquired deeper levels of significance: Einstein felt that Maxwell's equations should be the same for all observers in inertial motion. From Maxwell's equations, one can deduce a single speed of light, and there is nothing in this computation that depends on an observer's speed. Einstein sensed a conflict between Newtonian mechanics and the constant speed of light determined by Maxwell's equations.: 114–115 Regardless of the historical and scientific issues described above, Einstein's early thought experiment was part of the repertoire of test cases that he used to check on the viability of physical theories. Norton suggests that the real importance of the thought experiment was that it provided a powerful objection to emission theories of light, which Einstein had worked on for several years prior to 1905. Magnet and conductor In the first paragraph of Einstein's 1905 work introducing special relativity, he writes: It is well known that Maxwell's electrodynamics—as usually understood at present—when applied to moving bodies, leads to asymmetries that do not seem to attach to the phenomena. Let us recall, for example, the electrodynamic interaction between a magnet and a conductor. The observable phenomenon depends here only on the relative motion of conductor and magnet, while according to the customary conception the two cases, in which, respectively, either the one or the other of the two bodies is the one in motion, are to be strictly differentiated from each other. For if the magnet is in motion and the conductor is at rest, there arises in the surroundings of the magnet an electric field endowed with a certain energy value that produces a current in the places where parts of the conductor are located. But if the magnet is at rest and the conductor is in motion, no electric field arises in the surroundings of the magnet, while in the conductor an electromotive force will arise, to which in itself there does not correspond any energy, but which, provided that the relative motion in the two cases considered is the same, gives rise to electrical currents that have the same magnitude and the same course as those produced by the electric forces in the first-mentioned case. This opening paragraph recounts well-known experimental results obtained by Michael Faraday in 1831. The experiments describe what appeared to be two different phenomena: the motional EMF generated when a wire moves through a magnetic field (see Lorentz force), and the transformer EMF generated by a changing magnetic field (due to the Maxwell–Faraday equation).: 135–157 James Clerk Maxwell himself drew attention to this fact in his 1861 paper On Physical Lines of Force. In the latter half of Part II of that paper, Maxwell gave a separate physical explanation for each of the two phenomena. Although Einstein calls the asymmetry "well-known", there is no evidence that any of Einstein's contemporaries considered the distinction between motional EMF and transformer EMF to be in any way odd or pointing to a lack of understanding of the underlying physics. Maxwell, for instance, had repeatedly discussed Faraday's laws of induction, stressing that the magnitude and direction of the induced current was a function only of the relative motion of the magnet and the conductor, without being bothered by the clear distinction between conductor-in-motion and magnet-in-motion in the underlying theoretical treatment.: 135–138 Yet Einstein's reflection on this experiment represented the decisive moment in his long and tortuous path to special relativity. Although the equations describing the two scenarios are entirely different, there is no measurement that can distinguish whether the magnet is moving, the conductor is moving, or both. In a 1920 review on the Fundamental Ideas and Methods of the Theory of Relativity (unpublished), Einstein related how disturbing he found this asymmetry: The idea that these two cases should essentially be different was unbearable to me. According to my conviction, the difference between the two could only lie in the choice of the point of view, but not in a real difference <in the reality of nature>.: 20 Einstein needed to extend the relativity of motion that he perceived between magnet and conductor in the above thought experiment to a full theory. For years, however, he did not know how this might be done. The exact path that Einstein took to resolve this issue is unknown. We do know, however, that Einstein spent several years pursuing an emission theory of light, encountering difficulties that eventually led him to give up the attempt. Gradually I despaired of the possibility of discovering the true laws by means of constructive efforts based on known facts. The longer and more desperately I tried, the more I came to the conviction that only the discovery of a universal formal principle could lead us to assured results.: 49 That decision ultimately led to his development of special relativity as a theory founded on two postulates. Einstein's original expression of these postulates was: "The laws governing the changes of the state of any physical system do not depend on which one of two coordinate systems in uniform translational motion relative to each other these changes of the state are referred to. Each ray of light moves in the coordinate system "at rest" with the definite velocity V independent of whether this ray of light is emitted by a body at rest or a body in motion." In their modern form: 1. The laws of physics take the same form in all inertial frames. 2. In any given inertial frame, the velocity of light c is the same whether the light be emitted by a body at rest or by a body in uniform motion. [Emphasis added by editor]: 140–141 Einstein's wording of the first postulate was one with which nearly all theorists of his day could agree. His second postulate expresses a new idea about the character of light. Modern textbooks combine the two postulates. One popular textbook expresses the second postulate as, "The speed of light in free space has the same value c in all directions and in all inertial reference frames." Trains, embankments, and lightning flashes The topic of how Einstein arrived at special relativity has been a fascinating one to many scholars: A twenty-six year old patent officer (third class), largely self-taught in physics and completely divorced from mainstream research, nevertheless in the year 1905 produced four extraordinary works (Annus Mirabilis papers), only one of which (his paper on Brownian motion) appeared related to anything that he had ever published before. Einstein's paper, On the Electrodynamics of Moving Bodies, is a polished work that bears few traces of its gestation. Documentary evidence concerning the development of the ideas that went into it consist of, quite literally, only two sentences in a handful of preserved early letters, and various later historical remarks by Einstein himself, some of them known only second-hand and at times contradictory. In regards to the relativity of simultaneity, Einstein's 1905 paper develops the concept vividly by carefully considering the basics of how time may be disseminated through the exchange of signals between clocks. In his popular work, Relativity: The Special and General Theory, Einstein translates the formal presentation of his paper into a thought experiment using a train, a railway embankment, and lightning flashes. The essence of the thought experiment is as follows: Observer M stands on an embankment, while observer M' rides on a rapidly traveling train. At the precise moment that M and M' coincide in their positions, lightning strikes points A and B equidistant from M and M'. Light from these two flashes reach M at the same time, from which M concludes that the bolts were synchronous. The combination of Einstein's first and second postulates implies that, despite the rapid motion of the train relative to the embankment, M' measures exactly the same speed of light as does M. Since M' was equidistant from A and B when lightning struck, the fact that M' receives light from B before light from A means that to M', the bolts were not synchronous. Instead, the bolt at B struck first.: 29–31 A routine supposition among historians of science is that, in accordance with the analysis given in his 1905 special relativity paper and in his popular writings, Einstein discovered the relativity of simultaneity by thinking about how clocks could be synchronized by light signals. The Einstein synchronization convention was originally developed by telegraphers in the middle 19th century. The dissemination of precise time was an increasingly important topic during this period. Trains needed accurate time to schedule use of track, cartographers needed accurate time to determine longitude, while astronomers and surveyors dared to consider the worldwide dissemination of time to accuracies of thousandths of a second.: 132–144, 183–187 Following this line of argument, Einstein's position in the patent office, where he specialized in evaluating electromagnetic and electromechanical patents, would have exposed him to the latest developments in time technology, which would have guided him in his thoughts towards understanding the relativity of simultaneity.: 243–263 However, all of the above is supposition. In later recollections, when Einstein was asked about what inspired him to develop special relativity, he would mention his riding a light beam and his magnet and conductor thought experiments. He would also mention the importance of the Fizeau experiment and the observation of stellar aberration. "They were enough", he said. He never mentioned thought experiments about clocks and their synchronization. The routine analyses of the Fizeau experiment and of stellar aberration, that treat light as Newtonian corpuscles, do not require relativity. But problems arise if one considers light as waves traveling through an aether, which are resolved by applying the relativity of simultaneity. It is entirely possible, therefore, that Einstein arrived at special relativity through a different path than that commonly assumed, through Einstein's examination of Fizeau's experiment and stellar aberration. We therefore do not know just how important clock synchronization and the train and embankment thought experiment were to Einstein's development of the concept of the relativity of simultaneity. We do know, however, that the train and embankment thought experiment was the preferred means whereby he chose to teach this concept to the general public.: 29–31 Relativistic center-of-mass theorem Einstein proposed the equivalence of mass and energy in his final Annus Mirabilis paper. Over the next several decades, the understanding of energy and its relationship with momentum were further developed by Einstein and other physicists including Max Planck, Gilbert N. Lewis, Richard C. Tolman, Max von Laue (who in 1911 gave a comprehensive proof of M0 = E0/c2 from the stress–energy tensor), and Paul Dirac (whose investigations of negative solutions in his 1928 formulation of the energy–momentum relation led to the 1930 prediction of the existence of antimatter). Einstein's relativistic center-of-mass theorem of 1906 is a case in point. In 1900, Henri Poincaré had noted a paradox in modern physics as it was then understood: When he applied well-known results of Maxwell's equations to the equality of action and reaction, he could describe a cyclic process which would result in creation of a reactionless drive, i.e. a device which could displace its center of mass without the exhaust of a propellant, in violation of the conservation of momentum. Poincaré resolved this paradox by imagining electromagnetic energy to be a fluid having a given density, which is created and destroyed with a given momentum as energy is absorbed and emitted. The motions of this fluid would oppose displacement of the center of mass in such fashion as to preserve the conservation of momentum. Einstein demonstrated that Poincaré's artifice was superfluous. Rather, he argued that mass-energy equivalence was a necessary and sufficient condition to resolve the paradox. In his demonstration, Einstein provided a derivation of mass-energy equivalence that was distinct from his original derivation. Einstein began by recasting Poincaré's abstract mathematical argument into the form of a thought experiment: Einstein considered (a) an initially stationary, closed, hollow cylinder free-floating in space, of mass M {\displaystyle M} and length L {\displaystyle L} , (b) with some sort of arrangement for sending a quantity of radiative energy (a burst of photons) E {\displaystyle E} from the left to the right. The radiation has momentum E / c . {\displaystyle E/c.} Since the total momentum of the system is zero, the cylinder recoils with a speed v = − E / ( M c ) . {\displaystyle v=-E/(Mc).} (c) The radiation hits the other end of the cylinder in time Δ t = L / c , {\displaystyle \Delta t=L/c,} (assuming v << c {\displaystyle v<<c} ), bringing the cylinder to a stop after it has moved through a distance Δ x = − E L M c 2 . {\displaystyle \Delta x=-{\frac {EL}{Mc^{2}}}.} (d) The energy deposited on the right wall of the cylinder is transferred to a massless shuttle mechanism k , {\displaystyle k,} (e) which transports the energy to the left wall (f) and then returns to re-create the starting configuration of the system, except with the cylinder displaced to the left. The cycle may then be repeated. The reactionless drive described here violates the laws of mechanics, according to which the center of mass of a body at rest cannot be displaced in the absence of external forces. Einstein argued that the shuttle k {\displaystyle k} cannot be massless while transferring energy from the right to the left. If energy E {\displaystyle E} possesses the inertia m = E / c 2 , {\displaystyle m=E/c^{2},} the contradiction disappears. Modern analysis suggests that neither Einstein's original 1905 derivation of mass-energy equivalence nor the alternate derivation implied by his 1906 center-of-mass theorem are definitively correct. For instance, the center-of-mass thought experiment regards the cylinder as a completely rigid body. In reality, the impulse provided to the cylinder by the burst of light in step (b) cannot travel faster than light, so that when the burst of photons reaches the right wall in step (c), the wall has not yet begun to move. Ohanian has credited von Laue (1911) as having provided the first truly definitive derivation of M0 = E0/c2. Impossibility of faster-than-light signaling In 1907, Einstein noted that from the composition law for velocities, one could deduce that there cannot exist an effect that allows faster-than-light signaling. Einstein imagined a strip of material that allows propagation of signals at the faster-than-light speed of W {\displaystyle W} (as viewed from the material strip). Imagine two observers, A and B, standing on the x-axis and separated by the distance L {\displaystyle L} . They stand next to the material strip, which is not at rest, but rather is moving in the negative x-direction with speed v {\displaystyle v} . A uses the strip to send a signal to B. From the velocity composition formula, the signal propagates from A to B with speed ( W − v ) / ( 1 − ( W v / c 2 ) ) {\displaystyle {(W-v)/(1-(Wv/c^{2}))}} . The time T {\displaystyle T} required for the signal to propagate from A to B is given by T = L 1 − ( W v / c 2 ) W − v . {\displaystyle T=L{1-(Wv/c^{2}) \over W-v}.} The strip can move at any speed v < c {\displaystyle v<c} . Given the starting assumption W > c {\displaystyle W>c} , one can always set the strip moving at a speed v {\displaystyle v} such that T < 0 {\displaystyle T<0} . In other words, given the existence of a means of transmitting signals faster-than-light, scenarios can be envisioned whereby the recipient of a signal will receive the signal before the transmitter has transmitted it. About this thought experiment, Einstein wrote: Even though this result, in my opinion, does not contain any contradiction from a purely logical point of view, it conflicts with the character of all our experience to such an extent that this seems sufficient to prove the impossibility of the assumption W > c {\displaystyle W>c} . General relativity Falling painters and accelerating elevators In his unpublished 1920 review, Einstein related the genesis of his thoughts on the equivalence principle: When I was busy (in 1907) writing a summary of my work on the theory of special relativity for the Jahrbuch der Radioaktivität und Elektronik [Yearbook for Radioactivity and Electronics], I also had to try to modify the Newtonian theory of gravitation such as to fit its laws into the theory. While attempts in this direction showed the practicability of this enterprise, they did not satisfy me because they would have had to be based upon unfounded physical hypotheses. At that moment I got the happiest thought of my life in the following form: In an example worth considering, the gravitational field has a relative existence only in a manner similar to the electric field generated by magneto-electric induction. Because for an observer in free-fall from the roof of a house there is during the fall—at least in his immediate vicinity—no gravitational field. Namely, if the observer lets go of any bodies, they remain relative to him, in a state of rest or uniform motion, independent of their special chemical or physical nature. The observer, therefore, is justified in interpreting his state as being "at rest.": 20–21 The realization "startled" Einstein, and inspired him to begin an eight-year quest that led to what is considered to be his greatest work, the theory of general relativity. Over the years, the story of the falling man has become an iconic one, much embellished by other writers. In most retellings of Einstein's story, the falling man is identified as a painter. In some accounts, Einstein was inspired after he witnessed a painter falling from the roof of a building adjacent to the patent office where he worked. This version of the story leaves unanswered the question of why Einstein might consider his observation of such an unfortunate accident to represent the happiest thought in his life.: 145 Einstein later refined his thought experiment to consider a man inside a large enclosed chest or elevator falling freely in space. While in free fall, the man would consider himself weightless, and any loose objects that he emptied from his pockets would float alongside him. Then Einstein imagined a rope attached to the roof of the chamber. A powerful "being" of some sort begins pulling on the rope with constant force. The chamber begins to move "upwards" with a uniformly accelerated motion. Within the chamber, all of the man's perceptions are consistent with his being in a uniform gravitational field. Einstein asked, "Ought we to smile at the man and say that he errs in his conclusion?" Einstein answered no. Rather, the thought experiment provided "good grounds for extending the principle of relativity to include bodies of reference which are accelerated with respect to each other, and as a result we have gained a powerful argument for a generalised postulate of relativity.": 75–79 : 145–147 Through this thought experiment, Einstein addressed an issue that was so well known, scientists rarely worried about it or considered it puzzling: Objects have "gravitational mass," which determines the force with which they are attracted to other objects. Objects also have "inertial mass," which determines the relationship between the force applied to an object and how much it accelerates. Newton had pointed out that, even though they are defined differently, gravitational mass and inertial mass always seem to be equal. But until Einstein, no one had conceived a good explanation as to why this should be so. From the correspondence revealed by his thought experiment, Einstein concluded that "it is impossible to discover by experiment whether a given system of coordinates is accelerated, or whether...the observed effects are due to a gravitational field." This correspondence between gravitational mass and inertial mass is the equivalence principle.: 147 An extension to his accelerating observer thought experiment allowed Einstein to deduce that "rays of light are propagated curvilinearly in gravitational fields.": 83–84 : 190 Early applications of the equivalence principle Einstein's formulation of special relativity was in terms of kinematics (the study of moving bodies without reference to forces). Late in 1907, his former mathematics professor, Hermann Minkowski, presented an alternative, geometric interpretation of special relativity in a lecture to the Göttingen Mathematical society, introducing the concept of spacetime. Einstein was initially dismissive of Minkowski's geometric interpretation, regarding it as überflüssige Gelehrsamkeit (superfluous learnedness). As with special relativity, Einstein's early results in developing what was ultimately to become general relativity were accomplished using kinematic analysis rather than geometric techniques of analysis. In his 1907 Jahrbuch paper, Einstein first addressed the question of whether the propagation of light is influenced by gravitation, and whether there is any effect of a gravitational field on clocks. In 1911, Einstein returned to this subject, in part because he had realized that certain predictions of his nascent theory were amenable to experimental test. By the time of his 1911 paper, Einstein and other scientists had offered several alternative demonstrations that the inertial mass of a body increases with its energy content: If the energy increase of the body is E {\displaystyle E} , then the increase in its inertial mass is E / c 2 . {\displaystyle E/c^{2}.} Einstein asked whether there is an increase of gravitational mass corresponding to the increase in inertial mass, and if there is such an increase, is the increase in gravitational mass precisely the same as its increase in inertial mass? Using the equivalence principle, Einstein concluded that this must be so. To show that the equivalence principle necessarily implies the gravitation of energy, Einstein considered a light source S 2 {\displaystyle S_{2}} separated along the z-axis by a distance h {\displaystyle h} above a receiver S 1 {\displaystyle S_{1}} in a homogeneous gravitational field having a force per unit mass of 1 g . {\displaystyle g.} A certain amount of electromagnetic energy E {\displaystyle E} is emitted by S 2 {\displaystyle S_{2}} towards S 1 . {\displaystyle S_{1}.} According to the equivalence principle, this system is equivalent to a gravitation-free system which moves with uniform acceleration g {\displaystyle g} in the direction of the positive z-axis, with S 2 {\displaystyle S_{2}} separated by a constant distance h {\displaystyle h} from S 1 . {\displaystyle S_{1}.} In the accelerated system, light emitted from S 2 {\displaystyle S_{2}} takes (to a first approximation) h / c {\displaystyle h/c} to arrive at S 1 . {\displaystyle S_{1}.} But in this time, the velocity of S 1 {\displaystyle S_{1}} will have increased by v = g h / c {\displaystyle v=gh/c} from its velocity when the light was emitted. The energy arriving at S 1 {\displaystyle S_{1}} will therefore not be the energy E 2 , {\displaystyle E_{2},} but the greater energy E 1 {\displaystyle E_{1}} given by E 1 ≈ E 2 ( 1 + v c ) = E 2 ( 1 + g h c 2 ) . {\displaystyle E_{1}\approx E_{2}\left(1+{\frac {v}{c}}\right)=E_{2}\left(1+{\frac {gh}{c^{2}}}\right).} According to the equivalence principle, the same relation holds for the non-accelerated system in a gravitational field, where we replace g h {\displaystyle gh} by the gravitational potential difference Φ {\displaystyle \Phi } between S 2 {\displaystyle S_{2}} and S 1 {\displaystyle S_{1}} so that E 1 = E 2 + E 2 c 2 Φ . {\displaystyle E_{1}=E_{2}+{\frac {E_{2}}{c^{2}}}\Phi .} The energy E 1 {\displaystyle E_{1}} arriving at S 1 {\displaystyle S_{1}} is greater than the energy E 2 {\displaystyle E_{2}} emitted by S 2 {\displaystyle S_{2}} by the potential energy of the mass E 2 / c 2 {\displaystyle E_{2}/c^{2}} in the gravitational field. Hence E / c 2 {\displaystyle E/c^{2}} corresponds to the gravitational mass as well as the inertial mass of a quantity of energy. To further clarify that the energy of gravitational mass must equal the energy of inertial mass, Einstein proposed the following cyclic process: (a) A light source S 2 {\displaystyle S_{2}} is situated a distance h {\displaystyle h} above a receiver S 1 {\displaystyle S_{1}} in a uniform gravitational field. A movable mass M {\displaystyle M} can shuttle between S 2 {\displaystyle S_{2}} and S 1 . {\displaystyle S_{1}.} (b) A pulse of electromagnetic energy E {\displaystyle E} is sent from S 2 {\displaystyle S_{2}} to S 1 . {\displaystyle S_{1}.} The energy E ( 1 + g h / c 2 ) {\displaystyle E(1+gh/c^{2})} is absorbed by S 1 . {\displaystyle S_{1}.} (c) Mass M {\displaystyle M} is lowered from S 2 {\displaystyle S_{2}} to S 1 , {\displaystyle S_{1},} releasing an amount of work equal to M g h . {\displaystyle Mgh.} (d) The energy absorbed by S 1 {\displaystyle S_{1}} is transferred to M . {\displaystyle M.} This increases the gravitational mass of M {\displaystyle M} to a new value M ′ . {\displaystyle M'.} (e) The mass is lifted back to S 2 {\displaystyle S_{2}} , requiring the input of work M ′ g h . {\displaystyle M'gh.} (e) The energy carried by the mass is then transferred to S 2 , {\displaystyle S_{2},} completing the cycle. Conservation of energy demands that the difference in work between raising the mass and lowering the mass, M ′ g h − M g h , {\displaystyle M'gh-Mgh,} , must equal E g h / c 2 , {\displaystyle Egh/c^{2},} or one could potentially define a perpetual motion machine. Therefore, M ′ − M = E c 2 . {\displaystyle M'-M={\frac {E}{c^{2}}}.} In other words, the increase in gravitational mass predicted by the above arguments is precisely equal to the increase in inertial mass predicted by special relativity. Einstein then considered sending a continuous electromagnetic beam of frequency v 2 {\displaystyle v_{2}} (as measured at S 2 {\displaystyle S_{2}} ) from S 2 {\displaystyle S_{2}} to S 1 {\displaystyle S_{1}} in a homogeneous gravitational field. The frequency of the light as measured at S 1 {\displaystyle S_{1}} will be a larger value v 1 {\displaystyle v_{1}} given by v 1 = v 2 ( 1 + Φ c 2 ) . {\displaystyle v_{1}=v_{2}\left(1+{\frac {\Phi }{c^{2}}}\right).} Einstein noted that the above equation seemed to imply something absurd: Given that the transmission of light from S 2 {\displaystyle S_{2}} to S 1 {\displaystyle S_{1}} is continuous, how could the number of periods emitted per second from S 2 {\displaystyle S_{2}} be different from that received at S 1 ? {\displaystyle S_{1}?} It is impossible for wave crests to appear on the way down from S 2 {\displaystyle S_{2}} to S 1 {\displaystyle S_{1}} . The simple answer is that this question presupposes an absolute nature of time, when in fact there is nothing that compels us to assume that clocks situated at different gravitational potentials must be conceived of as going at the same rate. The principle of equivalence implies gravitational time dilation. It is important to realize that Einstein's arguments predicting gravitational time dilation are valid for any theory of gravity that respects the principle of equivalence. This includes Newtonian gravitation.: 16 Experiments such as the Pound–Rebka experiment, which have firmly established gravitational time dilation, therefore do not serve to distinguish general relativity from Newtonian gravitation. In the remainder of Einstein's 1911 paper, he discussed the bending of light rays in a gravitational field, but given the incomplete nature of Einstein's theory as it existed at the time, the value that he predicted was half the value that would later be predicted by the full theory of general relativity. Non-Euclidean geometry and the rotating disk By 1912, Einstein had reached an impasse in his kinematic development of general relativity, realizing that he needed to go beyond the mathematics that he knew and was familiar with. Stachel has identified Einstein's analysis of the rigid relativistic rotating disk as being key to this realization. The rigid rotating disk had been a topic of lively discussion since Max Born and Paul Ehrenfest, in 1909, both presented analyses of rigid bodies in special relativity. An observer on the edge of a rotating disk experiences an apparent ("fictitious" or "pseudo") force called "centrifugal force". By 1912, Einstein had become convinced of a close relationship between gravitation and pseudo-forces such as centrifugal force:Such a system K, according to the equivalence principle, is strictly equivalent to a system at rest in which a matter-free static gravitational field of a certain kind exists. In the accompanying illustration, A represents a circular disk of 10 units diameter at rest in an inertial reference frame. The circumference of the disk is π {\displaystyle \pi } times the diameter, and the illustration shows 31.4 rulers laid out along the circumference. B represents a circular disk of 10 units diameter that is spinning rapidly. According to a non-rotating observer, each of the rulers along the circumference is length-contracted along its line of motion. More rulers are required to cover the circumference, while the number of rulers required to span the diameter is unchanged. Note that we have not stated that we set A spinning to get B. In special relativity, it is not possible to set spinning a disk that is "rigid" in Born's sense of the term. Since spinning up disk A would cause the material to contract in the circumferential direction but not in the radial direction, a rigid disk would become fragmented from the induced stresses. In later years, Einstein repeatedly stated that consideration of the rapidly rotating disk was of "decisive importance" to him because it showed that a gravitational field causes non-Euclidean arrangements of measuring rods. Einstein realized that he did not have the mathematical skills to describe the non-Euclidean view of space and time that he envisioned, so he turned to his mathematician friend, Marcel Grossmann, for help. After researching in the library, Grossman found a review article by Ricci and Levi-Civita on absolute differential calculus (tensor calculus). Grossman tutored Einstein on the subject, and in 1913 and 1914, they published two joint papers describing an initial version of a generalized theory of gravitation. Over the next several years, Einstein used these mathematical tools to generalize Minkowski's geometric approach to relativity so as to encompass curved spacetime. Quantum mechanics Background: Einstein and the quantum Many myths have grown up about Einstein's relationship with quantum mechanics. Freshman physics students are aware that Einstein explained the photoelectric effect and introduced the concept of the photon. But students who have grown up with the photon may not be aware of how revolutionary the concept was for his time. The best-known factoids about Einstein's relationship with quantum mechanics are his statement, "God does not play dice with the universe" and the indisputable fact that he just did not like the theory in its final form. This has led to the general impression that, despite his initial contributions, Einstein was out of touch with quantum research and played at best a secondary role in its development.: 1–4 Concerning Einstein's estrangement from the general direction of physics research after 1925, his scientific biographer, Abraham Pais, wrote: Einstein is the only scientist to be justly held equal to Newton. That comparison is based exclusively on what he did before 1925. In the remaining 30 years of his life he remained active in research but his fame would be undiminished, if not enhanced, had he gone fishing instead.: 43 In hindsight, we know that Pais was incorrect in his assessment. Einstein was arguably the greatest single contributor to the "old" quantum theory. In his 1905 paper on light quanta, Einstein created the quantum theory of light. His proposal that light exists as tiny packets (photons) was so revolutionary, that even such major pioneers of quantum theory as Planck and Bohr refused to believe that it could be true.: 70–79, 282–284 Bohr, in particular, was a passionate disbeliever in light quanta, and repeatedly argued against them until 1925, when he yielded in the face of overwhelming evidence for their existence. In his 1906 theory of specific heats, Einstein was the first to realize that quantized energy levels explained the specific heat of solids. In this manner, he found a rational justification for the third law of thermodynamics (i.e. the entropy of any system approaches zero as the temperature approaches absolute zero): at very cold temperatures, atoms in a solid do not have enough thermal energy to reach even the first excited quantum level, and so cannot vibrate.: 141–148 Einstein proposed the wave–particle duality of light. In 1909, using a rigorous fluctuation argument based on a thought experiment and drawing on his previous work on Brownian motion, he predicted the emergence of a "fusion theory" that would combine the two views.: 136–140 Basically, he demonstrated that the Brownian motion experienced by a mirror in thermal equilibrium with black-body radiation would be the sum of two terms, one due to the wave properties of radiation, the other due to its particulate properties. Although Planck is justly hailed as the father of quantum mechanics, his derivation of the law of black-body radiation rested on fragile ground, since it required ad hoc assumptions of an unreasonable character. Furthermore, Planck's derivation represented an analysis of classical harmonic oscillators merged with quantum assumptions in an improvised fashion.: 184 In his 1916 theory of radiation, Einstein was the first to create a purely quantum explanation. This paper, well known for broaching the possibility of stimulated emission (the basis of the laser), changed the nature of the evolving quantum theory by introducing the fundamental role of random chance.: 181–192 In 1924, Einstein received a short manuscript by an unknown Indian professor, Satyendra Nath Bose, outlining a new method of deriving the law of blackbody radiation. Einstein was intrigued by Bose's peculiar method of counting the number of distinct ways of putting photons into the available states, a method of counting that Bose apparently did not realize was unusual. Einstein, however, understood that Bose's counting method implied that photons are, in a deep sense, indistinguishable. He translated the paper into German and had it published. Einstein then followed Bose's paper with an extension to Bose's work which predicted Bose–Einstein condensation, one of the fundamental research topics of condensed matter physics.: 215–240 While trying to develop a mathematical theory of light which would fully encompass its wavelike and particle-like aspects, Einstein developed the concept of "ghost fields". A guiding wave obeying Maxwell's classical laws would propagate following the normal laws of optics, but would not transmit any energy. This guiding wave, however, would govern the appearance of quanta of energy h ν {\displaystyle h\nu } on a statistical basis, so that the appearance of these quanta would be proportional to the intensity of the interference radiation. These ideas became widely known in the physics community, and through Born's work in 1926, later became a key concept in the modern quantum theory of radiation and matter.: 193–203 Therefore, Einstein before 1925 originated most of the key concepts of quantum theory: light quanta, wave–particle duality, the fundamental randomness of physical processes, the concept of indistinguishability, and the probability density interpretation of the wave equation. In addition, Einstein can arguably be considered the father of solid state physics and condensed matter physics. He provided a correct derivation of the blackbody radiation law and sparked the notion of the laser. In 1935, working with two younger colleagues, Einstein issued a final challenge to quantum mechanics, attempting to show that it could not represent a final solution. Despite the questions raised by this paper, it made little or no difference to how physicists employed quantum mechanics in their work. Of this paper, Pais was to write: The only part of this article that will ultimately survive, I believe, is this last phrase [i.e. "No reasonable definition of reality could be expect to permit this" where "this" refers to the instantaneous transmission of information over a distance], which so poignantly summarizes Einstein's views on quantum mechanics in his later years....This conclusion has not affected subsequent developments in physics, and it is doubtful that it ever will.: 454–457 In contrast to Pais' negative assessment, this paper, outlining the EPR paradox, has become one of the most widely cited articles in the entire physics literature.: 23 It is considered the centerpiece of the development of quantum information theory, which has been termed the "third quantum revolution." Wave–particle duality All of Einstein's major contributions to the old quantum theory were arrived at via statistical argument. This includes his 1905 paper arguing that light has particle properties, his 1906 work on specific heats, his 1909 introduction of the concept of wave–particle duality, his 1916 work presenting an improved derivation of the blackbody radiation formula, and his 1924 work that introduced the concept of indistinguishability.: 56 Einstein's 1909 arguments for the wave–particle duality of light were based on a thought experiment. Einstein imagined a mirror in a cavity containing particles of an ideal gas and filled with black-body radiation, with the entire system in thermal equilibrium. The mirror is constrained in its motions to a direction perpendicular to its surface. The mirror jiggles from Brownian motion due to collisions with the gas molecules. Since the mirror is in a radiation field, the moving mirror transfers some of its kinetic energy to the radiation field as a result of the difference in the radiation pressure between its forwards and reverse surfaces. This implies that there must be fluctuations in the black-body radiation field, and hence fluctuations in the black-body radiation pressure. Reversing the argument shows that there must be a route for the return of energy from the fluctuating black-body radiation field back to the gas molecules. Given the known shape of the radiation field given by Planck's law, Einstein could calculate the mean square energy fluctuation of the black-body radiation. He found the root mean square energy fluctuation ⟨ ϵ 2 ⟩ {\displaystyle \left\langle \epsilon ^{2}\right\rangle } in a small volume v {\displaystyle v} of a cavity filled with thermal radiation in the frequency interval between ν {\displaystyle \nu } and ν + d ν {\displaystyle \nu +d\nu } to be a function of frequency and temperature: ⟨ ϵ 2 ( ν , T ) ⟩ = ( h ν ρ + c 3 8 π ν 2 ρ 2 ) v d ν , {\displaystyle \left\langle \epsilon ^{2}(\nu ,T)\right\rangle =\left(h\nu \rho +{\frac {c^{3}}{8\pi \nu ^{2}}}\rho ^{2}\right)vd\nu ,} where ρ v d ν {\displaystyle \rho vd\nu } would be the average energy of the volume in contact with the thermal bath. The above expression has two terms, the second corresponding to the classical Rayleigh-Jeans law (i.e. a wavelike term), and the first corresponding to the Wien distribution law (which from Einstein's 1905 analysis, would result from point-like quanta with energy h ν {\displaystyle h\nu } ). From this, Einstein concluded that radiation had simultaneous wave and particle aspects.: 402–404 Bubble paradox From 1905 to 1923, Einstein was virtually the only physicist who took light-quanta seriously. Throughout most of this period, the physics community treated the light-quanta hypothesis with "skepticism bordering on derision": 357 and maintained this attitude even after Einstein's photoelectric law was validated. The citation for Einstein's 1922 Nobel Prize very deliberately avoided all mention of light-quanta, instead stating that it was being awarded for "his services to theoretical physics and especially for his discovery of the law of the photoelectric effect".: 386 This dismissive stance contrasts sharply with the enthusiastic manner in which Einstein's other major contributions were accepted, including his work on Brownian motion, special relativity, general relativity, and his numerous other contributions to the "old" quantum theory. Various explanations have been given for this neglect on the part of the physics community. First and foremost was wave theory's long and indisputable success in explaining purely optical phenomena. Second was the fact that his 1905 paper, which pointed out that certain phenomena would be more readily explained under the assumption that light is particulate, presented the hypothesis only as a "heuristic viewpoint". The paper offered no compelling, comprehensive alternative to existing electromagnetic theory. Third was the fact that his 1905 paper introducing light quanta and his two 1909 papers that argued for a wave–particle fusion theory approached their subjects via statistical arguments that his contemporaries "might accept as theoretical exercise—crazy, perhaps, but harmless".: 142–144 Most of Einstein's contemporaries adopted the position that light is ultimately a wave, but appears particulate in certain circumstances only because atoms absorb wave energy in discrete units.: 88 Among the thought experiments that Einstein presented in his 1909 lecture on the nature and constitution of radiation was one that he used to point out the implausibility of the above argument. He used this thought experiment to argue that atoms emit light as discrete particles rather than as continuous waves: (a) An electron in a cathode ray beam strikes an atom in a target. The intensity of the beam is set so low that we can consider one electron at a time as impinging on the target. (b) The atom emits a spherically radiating electromagnetic wave. (c) This wave excites an atom in a secondary target, causing it to release an electron of energy comparable to that of the original electron. The energy of the secondary electron depends only on the energy of the original electron and not at all on the distance between the primary and secondary targets. All the energy spread around the circumference of the radiating electromagnetic wave would appear to be instantaneously focused on the target atom, an action that Einstein considered implausible. Far more plausible would be to say that the first atom emitted a particle in the direction of the second atom. Although Einstein originally presented this thought experiment as an argument for light having a particulate nature, it has been noted that this thought experiment, which has been termed the "bubble paradox", foreshadows the famous 1935 EPR paper. In his 1927 Solvay debate with Bohr, Einstein employed this thought experiment to illustrate that according to the Copenhagen interpretation of quantum mechanics that Bohr championed, the quantum wavefunction of a particle would abruptly collapse like a "popped bubble" no matter how widely dispersed the wavefunction. The transmission of energy from opposite sides of the bubble to a single point would occur faster than light, violating the principle of locality.: 87–90 In the end, it was experiment, not any theoretical argument, that finally enabled the concept of the light quantum to prevail. In 1923, Arthur Compton was studying the scattering of high energy X-rays from a graphite target. Unexpectedly, he found that the scattered X-rays were shifted in wavelength, corresponding to inelastic scattering of the X-rays by the electrons in the target. His observations were totally inconsistent with wave behavior, but instead could only be explained if the X-rays acted as particles. This observation of the Compton effect rapidly brought about a change in attitude, and by 1926, the concept of the "photon" was generally accepted by the physics community.: 569–570 Einstein's light box Einstein did not like the direction in which quantum mechanics had turned after 1925. Although excited by Heisenberg's matrix mechanics, Schroedinger's wave mechanics, and Born's clarification of the meaning of the Schroedinger wave equation (i.e. that the absolute square of the wave function is to be interpreted as a probability density), his instincts told him that something was missing.: 326–335 In a letter to Born, he wrote: Quantum mechanics is very impressive. But an inner voice tells me that it is not yet the real thing. The theory produces a good deal but hardly brings us closer to the secret of the Old One.: 440–443 The Solvay Debates between Bohr and Einstein began in dining-room discussions at the Fifth Solvay International Conference on Electrons and Photons in 1927. Einstein's issue with the new quantum mechanics was not just that, with the probability interpretation, it rendered invalid the notion of rigorous causality. After all, as noted above, Einstein himself had introduced random processes in his 1916 theory of radiation. Rather, by defining and delimiting the maximum amount of information obtainable in a given experimental arrangement, the Heisenberg uncertainty principle denied the existence of any knowable reality in terms of a complete specification of the momenta and description of individual particles, an objective reality that would exist whether or not we could ever observe it.: 325–326 : 443–446 Over dinner, during after-dinner discussions, and at breakfast, Einstein debated with Bohr and his followers on the question whether quantum mechanics in its present form could be called complete. Einstein illustrated his points with increasingly clever thought experiments intended to prove that position and momentum could in principle be simultaneously known to arbitrary precision. For example, one of his thought experiments involved sending a beam of electrons through a shuttered screen, recording the positions of the electrons as they struck a photographic screen. Bohr and his allies would always be able to counter Einstein's proposal, usually by the end of the same day.: 344–347 On the final day of the conference, Einstein revealed that the uncertainty principle was not the only aspect of the new quantum mechanics that bothered him. Quantum mechanics, at least in the Copenhagen interpretation, appeared to allow action at a distance, the ability for two separated objects to communicate at speeds greater than light. By 1928, the consensus was that Einstein had lost the debate, and even his closest allies during the Fifth Solvay Conference, for example Louis de Broglie, conceded that quantum mechanics appeared to be complete.: 346–347 At the Sixth Solvay International Conference on Magnetism (1930), Einstein came armed with a new thought experiment. This involved a box with a shutter that operated so quickly, it would allow only one photon to escape at a time. The box would first be weighed exactly. Then, at a precise moment, the shutter would open, allowing a photon to escape. The box would then be re-weighed. The well-known relationship between mass and energy E = m c 2 {\displaystyle E=mc^{2}} would allow the energy of the particle to be precisely determined. With this gadget, Einstein believed that he had demonstrated a means to obtain, simultaneously, a precise determination of the energy of the photon as well as its exact time of departure from the system.: 346–347 : 446–448 Bohr was shaken by this thought experiment. Unable to think of a refutation, he went from one conference participant to another, trying to convince them that Einstein's thought experiment could not be true, that if it were true, it would literally mean the end of physics. After a sleepless night, he finally worked out a response which, ironically, depended on Einstein's general relativity.: 348–349 Consider the illustration of Einstein's light box:: 446–448 1. After emitting a photon, the loss of weight causes the box to rise in the gravitational field. 2. The observer returns the box to its original height by adding weights until the pointer points to its initial position. It takes a certain amount of time t {\displaystyle t} for the observer to perform this procedure. How long it takes depends on the strength of the spring and on how well-damped the system is. If undamped, the box will bounce up and down forever. If over-damped, the box will return to its original position sluggishly (See Damped spring-mass system). 3. The longer that the observer allows the damped spring-mass system to settle, the closer the pointer will reach its equilibrium position. At some point, the observer will conclude that his setting of the pointer to its initial position is within an allowable tolerance. There will be some residual error Δ q {\displaystyle \Delta q} in returning the pointer to its initial position. Correspondingly, there will be some residual error Δ m {\displaystyle \Delta m} in the weight measurement. 4. Adding the weights imparts a momentum p {\displaystyle p} to the box which can be measured with an accuracy Δ p {\displaystyle \Delta p} delimited by Δ p Δ q ≈ h . {\displaystyle \Delta p\Delta q\approx h.} It is clear that Δ p < g t Δ m , {\displaystyle \Delta p<gt\Delta m,} where g {\displaystyle g} is the gravitational constant. Plugging in yields g t Δ m Δ q > h . {\displaystyle gt\Delta m\Delta q>h.} 5. General relativity informs us that while the box has been at a height different than its original height, it has been ticking at a rate different than its original rate. The red shift formula informs us that there will be an uncertainty Δ t = c − 2 g t Δ q {\displaystyle \Delta t=c^{-2}gt\Delta q} in the determination of t 0 , {\displaystyle t_{0},} the emission time of the photon. 6. Hence, c 2 Δ m Δ t = Δ E Δ t > h . {\displaystyle c^{2}\Delta m\Delta t=\Delta E\Delta t>h.} The accuracy with which the energy of the photon is measured restricts the precision with which its moment of emission can be measured, following the Heisenberg uncertainty principle. After finding his last attempt at finding a loophole around the uncertainty principle refuted, Einstein quit trying to search for inconsistencies in quantum mechanics. Instead, he shifted his focus to the other aspects of quantum mechanics with which he was uncomfortable, focusing on his critique of action at a distance. His next paper on quantum mechanics foreshadowed his later paper on the EPR paradox.: 448 Einstein was gracious in his defeat. The following September, Einstein nominated Heisenberg and Schroedinger for the Nobel Prize, stating, "I am convinced that this theory undoubtedly contains a part of the ultimate truth.": 448 EPR paradox Both Bohr and Einstein were subtle men. Einstein tried very hard to show that quantum mechanics was inconsistent; Bohr, however, was always able to counter his arguments. But in his final attack Einstein pointed to something so deep, so counterintuitive, so troubling, and yet so exciting, that at the beginning of the twenty-first century it has returned to fascinate theoretical physicists. Bohr's only answer to Einstein's last great discovery—the discovery of entanglement—was to ignore it. Einstein's fundamental dispute with quantum mechanics was not about whether God rolled dice, whether the uncertainty principle allowed simultaneous measurement of position and momentum, or even whether quantum mechanics was complete. It was about reality. Does a physical reality exist independent of our ability to observe it? To Bohr and his followers, such questions were meaningless. All that we can know are the results of measurements and observations. It makes no sense to speculate about an ultimate reality that exists beyond our perceptions.: 460–461 Einstein's beliefs had evolved over the years from those that he had held when he was young, when, as a logical positivist heavily influenced by his reading of David Hume and Ernst Mach, he had rejected such unobservable concepts as absolute time and space. Einstein believed:: 460–461 1. A reality exists independent of our ability to observe it. 2. Objects are located at distinct points in spacetime and have their own independent, real existence. In other words, he believed in separability and locality. 3. Although at a superficial level, quantum events may appear random, at some ultimate level, strict causality underlies all processes in nature. Einstein considered that realism and localism were fundamental underpinnings of physics. After leaving Nazi Germany and settling in Princeton at the Institute for Advanced Study, Einstein began writing up a thought experiment that he had been mulling over since attending a lecture by Léon Rosenfeld in 1933. Since the paper was to be in English, Einstein enlisted the help of the 46-year-old Boris Podolsky, a fellow who had moved to the institute from Caltech; he also enlisted the help of the 26-year-old Nathan Rosen, also at the institute, who did much of the math. The result of their collaboration was the four page EPR paper, which in its title asked the question Can Quantum-Mechanical Description of Physical Reality be Considered Complete?: 448–450 After seeing the paper in print, Einstein found himself unhappy with the result. His clear conceptual visualization had been buried under layers of mathematical formalism.: 448–450 Einstein's thought experiment involved two particles that have collided or which have been created in such a way that they have properties which are correlated. The total wave function for the pair links the positions of the particles as well as their linear momenta.: 450–453 The figure depicts the spreading of the wave function from the collision point. However, observation of the position of the first particle allows us to determine precisely the position of the second particle no matter how far the pair have separated. Likewise, measuring the momentum of the first particle allows us to determine precisely the momentum of the second particle. "In accordance with our criterion for reality, in the first case we must consider the quantity P as being an element of reality, in the second case the quantity Q is an element of reality." Einstein concluded that the second particle, which we have never directly observed, must have at any moment a position that is real and a momentum that is real. Quantum mechanics does not account for these features of reality. Therefore, quantum mechanics is not complete.: 451 It is known, from the uncertainty principle, that position and momentum cannot be measured at the same time. But even though their values can only be determined in distinct contexts of measurement, can they both be definite at the same time? Einstein concluded that the answer must be yes. The only alternative, claimed Einstein, would be to assert that measuring the first particle instantaneously affected the reality of the position and momentum of the second particle.: 451 "No reasonable definition of reality could be expected to permit this." Bohr was stunned when he read Einstein's paper and spent more than six weeks framing his response, which he gave exactly the same title as the EPR paper. The EPR paper forced Bohr to make a major revision in his understanding of complementarity in the Copenhagen interpretation of quantum mechanics. Prior to EPR, Bohr had maintained that disturbance caused by the act of observation was the physical explanation for quantum uncertainty. In the EPR thought experiment, however, Bohr had to admit that "there is no question of a mechanical disturbance of the system under investigation." On the other hand, he noted that the two particles were one system described by one quantum function. Furthermore, the EPR paper did nothing to dispel the uncertainty principle.: 454–457 Later commentators have questioned the strength and coherence of Bohr's response. As a practical matter, however, physicists for the most part did not pay much attention to the debate between Bohr and Einstein, since the opposing views did not affect one's ability to apply quantum mechanics to practical problems, but only affected one's interpretation of the quantum formalism. If they thought about the problem at all, most working physicists tended to follow Bohr's leadership. In 1964, John Stewart Bell made the groundbreaking discovery that Einstein's local realist world view made experimentally verifiable predictions that would be in conflict with those of quantum mechanics. Bell's discovery shifted the Einstein–Bohr debate from philosophy to the realm of experimental physics. Bell's theorem showed that, for any local realist formalism, there exist limits on the predicted correlations between pairs of particles in an experimental realization of the EPR thought experiment. In 1972, the first experimental tests were carried out that demonstrated violation of these limits. Successive experiments improved the accuracy of observation and closed loopholes. To date, it is virtually certain that local realist theories have been falsified. The EPR paper has recently been recognized as prescient, since it identified the phenomenon of quantum entanglement, which has inspired approaches to quantum mechanics different from the Copenhagen interpretation, and has been at the forefront of major technological advances in quantum computing, quantum encryption, and quantum information theory. Notes Primary sources References External links NOVA: Inside Einstein's Mind (2015) — Retrace the thought experiments that inspired his theory on the nature of reality. The Rydberg–Klein–Rees method is a procedure used in the analysis of rotational-vibrational spectra of diatomic molecules to obtain a potential energy curve from the experimentally-known line positions. == References == Ignazio Licata (born 1958) is an Italian theoretical physicist, professor and scientific director of the Institute for Scientific Methodology, Italy. Education and career Licata has studied with David Bohm, Jean-Pierre Vigier, Abdus Salam and Giuseppe Arcidiacono. Licata is editor-in-chief of the Electronic Journal of Theoretical Physics (EJTP) and scientific director of the interdisciplinary Institute for Scientific Methodology (ISEM), located in Bagheria, Province of Palermo, Italy. He has worked on quantum field theory, interpretation of quantum mechanics, and recently quantum cosmology. His further topics of research include the foundation of quantum mechanics, space-time at Planck scale, the group approach in quantum cosmology, systems theory, non-linear dynamics, as well as computation in physical, biological and cognitive systems (logical openness, sub and super Turing systems). Licata has developed a new approach to quantum cosmology ("Archaic Universe") based on de Sitter invariant special relativity. Awards "Le Veneri per la Scienza" prize (Lecce, 2008) for "the high merit in research and cultural diffusion" "Targa Pirandello" (Agrigento, 2010) "International Prize Conference on Time" (Al Ain, 2012) Publications Books (as author) Ignazio Licata, Davide Fiscaletti: Quantum potential: Physics, Geometry and Algebra, AMC, Springer, 2013, ISBN 978-3-319-00332-0 (print) / ISBN 978-3-319-00333-7 (online) Ignazio Licata: Complessità. Un'introduzione semplice, :Duepunti, 2011, ISBN 978-8889987674 Ignazio Licata: La Logica Aperta della Mente, Codice Edizioni, Torino, 2008, ISBN 978-8875780906 Ignazio Licata: Osservando la Sfinge. La realtà virtuale della fisica quantistica, Di Renzo, Roma, 2003, ISBN 978-8883230639 Books (other) Ignazio Licata, Ammar Sakaji (eds.): Vision of Oneness, Aracne (2011), ISBN 978-88-548-44629 Ignazio Licata, Sara Felloni, Ammar J. Sakaji, Jatinder Singh (eds.): New Trends in Quantum Information, Aracne, 2010, ISBN 978-8854834118 Ignazio Licata, Ammar Sakaji (eds.): Crossing in Complexity: Interdisciplinary Application of Physics in Biological and Social Systems, Nova Science Publishers, 2010, ISBN 9781616680374 Ammar Sakaji, Ignazio Licata (eds.): Lev Davidovich Landau and His Impact on Contemporary Theoretical Physics (Horizons in World Physics), Nova Science Publishers, 2009, ISBN 978-1606929087 Unexpected Connections: Art–Science Crossing, Politi Publ., Milan, 2009 Ignazio Licata, Ammar Sakaji (eds.): Physics of Emergence and Organization, EJTP and World Scientific, 2008, ISBN 978-9812779946 Ignazio Licata: Majorana Legacy in Contemporary Physics, EJTP and Di Renzo, Roma 2006, ISBN 9788883231520 Informazione & Complessità, Edizioni Andromeda, Bologna, 1998 Articles (selection) Articles by Ignazio Licata at arXiv.org and at PhilPapers Ignazio Licata: Vision as Adaptive Epistemology, in G. Minati Ed., Methods, models, simulations and approaches. Towards a general theory of change, World Scientific, 2012 Ignazio Licata, Gianfranco Minati: "Meta-structural properties in collective behaviours", International Journal of General Systems, vol. 41, nr. 3, pp. 289–311 (2012) Ignazio Licata, Leonardo Chiatti: "Archaic universe and cosmological model: "big-bang" as nucleation by vacuum", International Journal of Theoretical Physics, vol. 49, nr. 10, pp. 2379–2402 (2010), DOI: 10.1007/s10773-010-0424-0 (abstract) Ignazio Licata: Almost-anywhere theories: reductionism and universality of emergence, Complexity, vol. 15, n. 6, pp. 11–19 (2010) Ignazio Licata, Leonardo Chiatti: The archaic universe: big bang, cosmological term and the quantum origin of time in projective cosmology, International Journal of Theoretical Physics, vol. 48, nr. 4 (2009), pp. 1003–1018, DOI: 10.1007/s10773-008-9874-z (abstract) Ignazio Licata: A dynamical model for information retrieval and emergence of scale-free clusters in a long term memory network, in Complexity: Emergence and Organization (E:CO), vol. 11, nr. 1, 2009, pp. 48-57, arXiv:0801.0887 (submitted 6 January 2008) Ignazio Licata: Logical openness in cognitive models, Epistemologia XXXVI (2008), pp. 177–192 (full text) Ignazio Licata, Luigi Lella: A new model for the organizational knowledge life cycle in G. Minati, E. Pessa (eds.): Processes of emergence of systems and systemic properties. Towards a general theory of emergence, World Scientific, 2008 Ignazio Licata, Luigi Lella: Evolutionary neural gas (ENG): A model of self rrganizing network from input categorization (2007) EJTP, vol. 4, nr. 14 Ignazio Licata: Universe without singularities. A group approach to de Sitter cosmology, EJTP, vol. 3 nr. 10 (2006), pp. 211-224 References External links Ignazio Licata, curriculum Ignazio Licata, manifesto della scienza semplice (manifest of simple science) In the fields of atomic, molecular, and optical science, the term light dressed state refers to a quantum state of an atomic or molecular system interacting with a laser light in terms of the Floquet picture, i.e. roughly like an atom or a molecule plus a photon. The Floquet picture is based on the Floquet theorem in differential equations with periodic coefficients. Mathematical formulation The Hamiltonian of a system of charged particles interacting with a laser light can be expressed as where A {\displaystyle \mathbf {A} } is the vector potential of the electromagnetic field of the laser; A {\displaystyle \mathbf {A} } is periodic in time as A ( t + T ) = A ( t ) {\displaystyle \mathbf {A} (t+T)=\mathbf {A} (t)} . The position and momentum of the i {\displaystyle i\,} -th particle are denoted as r i {\displaystyle \mathbf {r} _{i}\,} and p i {\displaystyle \mathbf {p} _{i}\,} , respectively, while its mass and charge are symbolized as m i {\displaystyle m_{i}\,} and z i {\displaystyle z_{i}\,} , respectively. c {\displaystyle c\,} is the speed of light. By virtue of this time-periodicity of the laser field, the total Hamiltonian is also periodic in time as H ( t + T ) = H ( t ) . {\displaystyle H(t+T)=H(t)\,.} The Floquet theorem guarantees that any solution ψ ( { r i } , t ) {\displaystyle \psi (\{\mathbf {r} _{i}\},t)} of the Schrödinger equation with this type of Hamiltonian, i ℏ ∂ ∂ t ψ ( { r i } , t ) = H ( t ) ψ ( { r i } , t ) {\displaystyle i\hbar {\frac {\partial }{\partial t}}\psi (\{\mathbf {r} _{i}\},t)=H(t)\psi (\{\mathbf {r} _{i}\},t)} can be expressed in the form ψ ( { r i } , t ) = exp ⁡ [ − i E t / ℏ ] ϕ ( { r i } , t ) {\displaystyle \psi (\{\mathbf {r} _{i}\},t)=\exp[-iEt/\hbar ]\phi (\{\mathbf {r} _{i}\},t)} where ϕ {\displaystyle \phi \,} has the same time-periodicity as the Hamiltonian, ϕ ( { r i } , t + T ) = ϕ ( { r i } , t ) . {\displaystyle \phi (\{\mathbf {r} _{i}\},t+T)=\phi (\{\mathbf {r} _{i}\},t).} Therefore, this part can be expanded in a Fourier series, obtaining where ω ( = 2 π / T ) {\displaystyle \omega (=2\pi /T)\,} is the frequency of the laser field. This expression (2) reveals that a quantum state of the system governed by the Hamiltonian (1) can be specified by a real number E {\displaystyle E\,} and an integer n {\displaystyle n\,} . The integer n {\displaystyle n\,} in eq. (2) can be regarded as the number of photons absorbed from (or emitted to) the laser field. In order to prove this statement, we clarify the correspondence between the solution (2), which is derived from the classical expression of the electromagnetic field where there is no concept of photons, and one which is derived from a quantized electromagnetic field (see quantum field theory). (It can be verified that n {\displaystyle n\,} is equal to the expectation value of the absorbed photon number at the limit of n ≪ N {\displaystyle n\ll N\,} , where N {\displaystyle N\,} is the initial number of total photons.) References Shirley, Jon H. (1965). "Solution of the Schrödinger Equation with a Hamiltonian Periodic in Time". Physical Review. 138 (4B): B979 – B987. Bibcode:1965PhRv..138..979S. doi:10.1103/PhysRev.138.B979. ISSN 0031-899X. Sambe, Hideo (1973). "Steady States and Quasienergies of a Quantum-Mechanical System in an Oscillating Field". Physical Review A. 7 (6): 2203–2213. Bibcode:1973PhRvA...7.2203S. doi:10.1103/PhysRevA.7.2203. ISSN 0556-2791. Guérin, S; Monti, F; Dupont, J-M; Jauslin, H R (1997). "On the relation between cavity-dressed states, Floquet states, RWA and semiclassical models". Journal of Physics A: Mathematical and General. 30 (20): 7193–7215. Bibcode:1997JPhA...30.7193G. doi:10.1088/0305-4470/30/20/020. ISSN 0305-4470. Cardoso, G.C.; Tabosa, J.W.R. (2000). "Four-wave mixing in dressed cold cesium atoms". Optics Communications. 185 (4–6): 353–358. Bibcode:2000OptCo.185..353C. doi:10.1016/S0030-4018(00)01033-6. ISSN 0030-4018. Guérin, S.; Jauslin, H. R. (2003). "Control of Quantum Dynamics by Laser Pulses: Adiabatic Floquet Theory". Advances in Chemical Physics. pp. 147–267. doi:10.1002/0471428027.ch3. ISBN 9780471214526. ISSN 1934-4791. F.H.M. Faisal, Theory of Multiphoton Processes, Plenum (New York) 1987 ISBN 0-306-42317-0. See also Quantum optics Hamiltonian (quantum mechanics) In molecular physics, the Axilrod–Teller potential (also known as the Axilrod–Teller–Muto or ATM potential) describes the interaction potential (energy) between three atoms, capturing effects beyond simple pairwise attractions. It builds on the London dispersion forces, using perturbation theory to correct for the influence of a third atom. Its strength depends on properties of the atoms and their arrangement in space. This potential is often used in models of rare gases and molecular dynamics simulations to study complex attraction effects. Formula The ATM potential is given by: V i j k = E 0 [ 1 + 3 cos ⁡ γ i cos ⁡ γ j cos ⁡ γ k ( r i j r j k r i k ) 3 ] {\displaystyle V_{ijk}=E_{0}\left[{\frac {1+3\cos \gamma _{i}\cos \gamma _{j}\cos \gamma _{k}}{\left(r_{ij}r_{jk}r_{ik}\right)^{3}}}\right]} where r i j {\displaystyle r_{ij}} is the distance between atoms i {\displaystyle i} and j {\displaystyle j} , and γ i {\displaystyle \gamma _{i}} is the angle between the vectors r i j {\displaystyle \mathbf {r} _{ij}} and r i k {\displaystyle \mathbf {r} _{ik}} . The coefficient E 0 {\displaystyle E_{0}} is positive and of the order V α 3 {\displaystyle V\alpha ^{3}} , where V {\displaystyle V} is the ionization energy and α {\displaystyle \alpha } is the mean atomic polarizability; the exact value of E 0 {\displaystyle E_{0}} depends on the magnitudes of the dipole matrix elements and on the energies of the p {\displaystyle p} orbitals. References Axilrod, B. M.; Teller, E. (1943). "Interaction of the van der Waals Type Between Three Atoms". Journal of Chemical Physics. 11 (6): 299. Bibcode:1943JChPh..11..299A. doi:10.1063/1.1723844. Muto (1943). "Force between nonpolar molecules". Journal of the Physical Society of Japan. 17: 629. doi:10.11429/subutsukaishi1927.17.10-11-12_629. A bound state is a composite of two or more fundamental building blocks, such as particles, atoms, or bodies, that behaves as a single object and in which energy is required to split them. In quantum physics, a bound state is a quantum state of a particle subject to a potential such that the particle has a tendency to remain localized in one or more regions of space. The potential may be external or it may be the result of the presence of another particle; in the latter case, one can equivalently define a bound state as a state representing two or more particles whose interaction energy exceeds the total energy of each separate particle. One consequence is that, given a potential vanishing at infinity, negative-energy states must be bound. The energy spectrum of the set of bound states are most commonly discrete, unlike scattering states of free particles, which have a continuous spectrum. Although not bound states in the strict sense, metastable states with a net positive interaction energy, but long decay time, are often considered unstable bound states as well and are called "quasi-bound states". Examples include radionuclides and Rydberg atoms. In relativistic quantum field theory, a stable bound state of n particles with masses { m k } k = 1 n {\displaystyle \{m_{k}\}_{k=1}^{n}} corresponds to a pole in the S-matrix with a center-of-mass energy less than ∑ k m k {\displaystyle \textstyle \sum _{k}m_{k}} . An unstable bound state shows up as a pole with a complex center-of-mass energy. Examples A proton and an electron can move separately; when they do, the total center-of-mass energy is positive, and such a pair of particles can be described as an ionized atom. Once the electron starts to "orbit" the proton, the energy becomes negative, and a bound state – namely the hydrogen atom – is formed. Only the lowest-energy bound state, the ground state, is stable. Other excited states are unstable and will decay into stable (but not other unstable) bound states with less energy by emitting a photon. A positronium "atom" is an unstable bound state of an electron and a positron. It decays into photons. Any state in the quantum harmonic oscillator is bound, but has positive energy. Note that lim x → ± ∞ V QHO ( x ) = ∞ {\displaystyle \lim _{x\to \pm \infty }{V_{\text{QHO}}(x)}=\infty } , so the below does not apply. A nucleus is a bound state of protons and neutrons (nucleons). The proton itself is a bound state of three quarks (two up and one down; one red, one green and one blue). However, unlike the case of the hydrogen atom, the individual quarks can never be isolated. See confinement. The Hubbard and Jaynes–Cummings–Hubbard (JCH) models support similar bound states. In the Hubbard model, two repulsive bosonic atoms can form a bound pair in an optical lattice. The JCH Hamiltonian also supports two-polariton bound states when the photon-atom interaction is sufficiently strong. Definition Let σ-finite measure space ( X , A , μ ) {\displaystyle (X,{\mathcal {A}},\mu )} be a probability space associated with separable complex Hilbert space H {\displaystyle H} . Define a one-parameter group of unitary operators ( U t ) t ∈ R {\displaystyle (U_{t})_{t\in \mathbb {R} }} , a density operator ρ = ρ ( t 0 ) {\displaystyle \rho =\rho (t_{0})} and an observable T {\displaystyle T} on H {\displaystyle H} . Let μ ( T , ρ ) {\displaystyle \mu (T,\rho )} be the induced probability distribution of T {\displaystyle T} with respect to ρ {\displaystyle \rho } . Then the evolution ρ ( t 0 ) ↦ [ U t ( ρ ) ] ( t 0 ) = ρ ( t 0 + t ) {\displaystyle \rho (t_{0})\mapsto [U_{t}(\rho )](t_{0})=\rho (t_{0}+t)} is bound with respect to T {\displaystyle T} if lim R → ∞ sup t ≥ t 0 μ ( T , ρ ( t ) ) ( R > R ) = 0 {\displaystyle \lim _{R\rightarrow \infty }{\sup _{t\geq t_{0}}{\mu (T,\rho (t))(\mathbb {R} _{>R})}}=0} , where R > R = { x ∈ R ∣ x > R } {\displaystyle \mathbb {R} _{>R}=\lbrace x\in \mathbb {R} \mid x>R\rbrace } . A quantum particle is in a bound state if at no point in time it is found “too far away" from any finite region R ⊂ X {\displaystyle R\subset X} . Using a wave function representation, for example, this means 0 = lim R → ∞ P ( particle measured inside X ∖ R ) = lim R → ∞ ∫ X ∖ R | ψ ( x ) | 2 d μ ( x ) , {\displaystyle {\begin{aligned}0&=\lim _{R\to \infty }{\mathbb {P} ({\text{particle measured inside }}X\setminus R)}\\&=\lim _{R\to \infty }{\int _{X\setminus R}|\psi (x)|^{2}\,d\mu (x)},\end{aligned}}} such that ∫ X | ψ ( x ) | 2 d μ ( x ) < ∞ . {\displaystyle \int _{X}{|\psi (x)|^{2}\,d\mu (x)}<\infty .} In general, a quantum state is a bound state if and only if it is finitely normalizable for all times t ∈ R {\displaystyle t\in \mathbb {R} } and remains spatially localized. Furthermore, a bound state lies within the pure point part of the spectrum of T {\displaystyle T} if and only if it is an eigenvector of T {\displaystyle T} . More informally, "boundedness" results foremost from the choice of domain of definition and characteristics of the state rather than the observable. For a concrete example: let H := L 2 ( R ) {\displaystyle H:=L^{2}(\mathbb {R} )} and let T {\displaystyle T} be the position operator. Given compactly supported ρ = ρ ( 0 ) ∈ H {\displaystyle \rho =\rho (0)\in H} and [ − 1 , 1 ] ⊆ S u p p ( ρ ) {\displaystyle [-1,1]\subseteq \mathrm {Supp} (\rho )} . If the state evolution of ρ {\displaystyle \rho } "moves this wave package to the right", e.g., if [ t − 1 , t + 1 ] ∈ S u p p ( ρ ( t ) ) {\displaystyle [t-1,t+1]\in \mathrm {Supp} (\rho (t))} for all t ≥ 0 {\displaystyle t\geq 0} , then ρ {\displaystyle \rho } is not bound state with respect to position. If ρ {\displaystyle \rho } does not change in time, i.e., ρ ( t ) = ρ {\displaystyle \rho (t)=\rho } for all t ≥ 0 {\displaystyle t\geq 0} , then ρ {\displaystyle \rho } is bound with respect to position. More generally: If the state evolution of ρ {\displaystyle \rho } "just moves ρ {\displaystyle \rho } inside a bounded domain", then ρ {\displaystyle \rho } is bound with respect to position. Properties As finitely normalizable states must lie within the pure point part of the spectrum, bound states must lie within the pure point part. However, as Neumann and Wigner pointed out, it is possible for the energy of a bound state to be located in the continuous part of the spectrum. This phenomenon is referred to as bound state in the continuum. Position-bound states Consider the one-particle Schrödinger equation. If a state has energy E < max ( lim x → ∞ V ( x ) , lim x → − ∞ V ( x ) ) {\textstyle E<\max {\left(\lim _{x\to \infty }{V(x)},\lim _{x\to -\infty }{V(x)}\right)}} , then the wavefunction ψ satisfies, for some X > 0 {\displaystyle X>0} ψ ′ ′ ψ = 2 m ℏ 2 ( V ( x ) − E ) > 0 for x > X {\displaystyle {\frac {\psi ^{\prime \prime }}{\psi }}={\frac {2m}{\hbar ^{2}}}(V(x)-E)>0{\text{ for }}x>X} so that ψ is exponentially suppressed at large x. This behaviour is well-studied for smoothly varying potentials in the WKB approximation for wavefunction, where an oscillatory behaviour is observed if the right hand side of the equation is negative and growing/decaying behaviour if it is positive. Hence, negative energy-states are bound if V ( x ) {\displaystyle V(x)} vanishes at infinity. Non-degeneracy in one-dimensional bound states One-dimensional bound states can be shown to be non-degenerate in energy for well-behaved wavefunctions that decay to zero at infinities. This need not hold true for wavefunctions in higher dimensions. Due to the property of non-degenerate states, one-dimensional bound states can always be expressed as real wavefunctions. Node theorem Node theorem states that n th {\displaystyle n{\text{th}}} bound wavefunction ordered according to increasing energy has exactly n − 1 {\displaystyle n-1} nodes, i.e., points x = a {\displaystyle x=a} where ψ ( a ) = 0 ≠ ψ ′ ( a ) {\displaystyle \psi (a)=0\neq \psi '(a)} . Due to the form of Schrödinger's time independent equations, it is not possible for a physical wavefunction to have ψ ( a ) = 0 = ψ ′ ( a ) {\displaystyle \psi (a)=0=\psi '(a)} since it corresponds to ψ ( x ) = 0 {\displaystyle \psi (x)=0} solution. Requirements A boson with mass mχ mediating a weakly coupled interaction produces an Yukawa-like interaction potential, V ( r ) = ± α χ r e − r λ χ {\displaystyle V(r)=\pm {\frac {\alpha _{\chi }}{r}}e^{-{\frac {r}{\lambda \!\!\!{\frac {}{\ }}_{\chi }}}}} , where α χ = g 2 / 4 π {\displaystyle \alpha _{\chi }=g^{2}/4\pi } , g is the gauge coupling constant, and ƛi = ⁠ℏ/mic⁠ is the reduced Compton wavelength. A scalar boson produces a universally attractive potential, whereas a vector attracts particles to antiparticles but repels like pairs. For two particles of mass m1 and m2, the Bohr radius of the system becomes a 0 = λ _ 1 + λ _ 2 α χ {\displaystyle a_{0}={\frac {{\lambda \!\!\!^{{}^{\underline {\ \ }}}}_{1}+{\lambda \!\!\!^{{}^{\underline {\ \ }}}}_{2}}{\alpha _{\chi }}}} and yields the dimensionless number D = λ _ χ a 0 = α χ λ _ χ λ _ 1 + λ _ 2 = α χ m 1 + m 2 m χ {\displaystyle D={\frac {{\lambda \!\!\!^{{}^{\underline {\ \ }}}}_{\chi }}{a_{0}}}=\alpha _{\chi }{\frac {{\lambda \!\!\!^{{}^{\underline {\ \ }}}}_{\chi }}{{\lambda \!\!\!^{{}^{\underline {\ \ }}}}_{1}+{\lambda \!\!\!^{{}^{\underline {\ \ }}}}_{2}}}=\alpha _{\chi }{\frac {m_{1}+m_{2}}{m_{\chi }}}} . In order for the first bound state to exist at all, D ≳ 0.8 {\displaystyle D\gtrsim 0.8} . Because the photon is massless, D is infinite for electromagnetism. For the weak interaction, the Z boson's mass is 91.1876±0.0021 GeV/c2, which prevents the formation of bound states between most particles, as it is 97.2 times the proton's mass and 178,000 times the electron's mass. Note, however, that, if the Higgs interaction did not break electroweak symmetry at the electroweak scale, then the SU(2) weak interaction would become confining. See also Bethe–Salpeter equation Bound state in the continuum Composite field Cooper pair Exciton Resonance (particle physics) Levinson's theorem Remarks References Further reading Blanchard, Philippe; Brüning, Edward (2015). "Some Applications of the Spectral Representation". Mathematical Methods in Physics: Distributions, Hilbert Space Operators, Variational Methods, and Applications in Quantum Physics (2nd ed.). Switzerland: Springer International Publishing. p. 431. ISBN 978-3-319-14044-5. Polarizability usually refers to the tendency of matter, when subjected to an electric field, to acquire an electric dipole moment in proportion to that applied field. It is a property of particles with an electric charge. When subject to an electric field, the negatively charged electrons and positively charged atomic nuclei are subject to opposite forces and undergo charge separation. Polarizability is responsible for a material's dielectric constant and, at high (optical) frequencies, its refractive index. The polarizability of an atom or molecule is defined as the ratio of its induced dipole moment to the local electric field; in a crystalline solid, one considers the dipole moment per unit cell. Note that the local electric field seen by a molecule is generally different from the macroscopic electric field that would be measured externally. This discrepancy is taken into account by the Clausius–Mossotti relation (below) which connects the bulk behaviour (polarization density due to an external electric field according to the electric susceptibility χ = ε r − 1 {\displaystyle \chi =\varepsilon _{\mathrm {r} }-1} ) with the molecular polarizability α {\displaystyle \alpha } due to the local field. Magnetic polarizability likewise refers to the tendency for a magnetic dipole moment to appear in proportion to an external magnetic field. Electric and magnetic polarizabilities determine the dynamical response of a bound system (such as a molecule or crystal) to external fields, and provide insight into a molecule's internal structure. "Polarizability" should not be confused with the intrinsic magnetic or electric dipole moment of an atom, molecule, or bulk substance; these do not depend on the presence of an external field. Electric polarizability Definition Electric polarizability is the relative tendency of a charge distribution, like the electron cloud of an atom or molecule, to be distorted from its normal shape by an external electric field. The polarizability α {\displaystyle \alpha } in isotropic media is defined as the ratio of the induced dipole moment p {\displaystyle \mathbf {p} } of an atom to the electric field E {\displaystyle \mathbf {E} } that produces this dipole moment. α = | p | | E | {\displaystyle \alpha ={\frac {|\mathbf {p} |}{|\mathbf {E} |}}} Polarizability has the SI units of C·m2·V−1 = A2·s4·kg−1 while its cgs unit is cm3. Usually it is expressed in cgs units as a so-called polarizability volume, sometimes expressed in Å3 = 10−24 cm3. One can convert from SI units ( α {\displaystyle \alpha } ) to cgs units ( α ′ {\displaystyle \alpha '} ) as follows: α ′ ( c m 3 ) = 10 6 4 π ε 0 α ( C ⋅ m 2 ⋅ V − 1 ) = 10 6 4 π ε 0 α ( F ⋅ m 2 ) {\displaystyle \alpha '(\mathrm {cm} ^{3})={\frac {10^{6}}{4\pi \varepsilon _{0}}}\alpha (\mathrm {C{\cdot }m^{2}{\cdot }V^{-1}} )={\frac {10^{6}}{4\pi \varepsilon _{0}}}\alpha (\mathrm {F{\cdot }m^{2}} )} ≃ 8.988×1015 × α ( F ⋅ m 2 ) {\displaystyle \alpha (\mathrm {F{\cdot }m^{2}} )} where ε 0 {\displaystyle \varepsilon _{0}} , the vacuum permittivity, is ≈8.854 × 10−12 (F/m). If the polarizability volume in cgs units is denoted α ′ {\displaystyle \alpha '} the relation can be expressed generally (in SI) as α = 4 π ε 0 α ′ {\displaystyle \alpha =4\pi \varepsilon _{0}\alpha '} . The polarizability of individual particles is related to the average electric susceptibility of the medium by the Clausius–Mossotti relation: R = ( 4 π 3 ) N A α c = ( M p ) ( ε r − 1 ε r + 2 ) {\displaystyle R={\displaystyle \left({\frac {4\pi }{3}}\right)N_{\text{A}}\alpha _{c}=\left({\frac {M}{p}}\right)\left({\frac {\varepsilon _{\mathrm {r} }-1}{\varepsilon _{\mathrm {r} }+2}}\right)}} where R is the molar refractivity, N A {\displaystyle N_{\text{A}}} is the Avogadro constant, α c {\displaystyle \alpha _{c}} is the electronic polarizability, p is the density of molecules, M is the molar mass, and ε r = ϵ / ϵ 0 {\displaystyle \varepsilon _{\mathrm {r} }=\epsilon /\epsilon _{0}} is the material's relative permittivity or dielectric constant (or in optics, the square of the refractive index). Polarizability for anisotropic or non-spherical media cannot in general be represented as a scalar quantity. Defining α {\displaystyle \alpha } as a scalar implies both that applied electric fields can only induce polarization components parallel to the field and that the x , y {\displaystyle x,y} and z {\displaystyle z} directions respond in the same way to the applied electric field. For example, an electric field in the x {\displaystyle x} -direction can only produce an x {\displaystyle x} component in p {\displaystyle \mathbf {p} } and if that same electric field were applied in the y {\displaystyle y} -direction the induced polarization would be the same in magnitude but appear in the y {\displaystyle y} component of p {\displaystyle \mathbf {p} } . Many crystalline materials have directions that are easier to polarize than others and some even become polarized in directions perpendicular to the applied electric field, and the same thing happens with non-spherical bodies. Some molecules and materials with this sort of anisotropy are optically active, or exhibit linear birefringence of light. Tensor To describe anisotropic media a polarizability rank two tensor or 3 × 3 {\displaystyle 3\times 3} matrix α {\displaystyle \alpha } is defined, α = [ α x x α x y α x z α y x α y y α y z α z x α z y α z z ] {\displaystyle \mathbb {\alpha } ={\begin{bmatrix}\alpha _{xx}&\alpha _{xy}&\alpha _{xz}\\\alpha _{yx}&\alpha _{yy}&\alpha _{yz}\\\alpha _{zx}&\alpha _{zy}&\alpha _{zz}\\\end{bmatrix}}} so that: p = α E {\displaystyle \mathbf {p} =\mathbb {\alpha } \mathbf {E} } The elements describing the response parallel to the applied electric field are those along the diagonal. A large value of α y x {\displaystyle \alpha _{yx}} here means that an electric-field applied in the x {\displaystyle x} -direction would strongly polarize the material in the y {\displaystyle y} -direction. Explicit expressions for α {\displaystyle \alpha } have been given for homogeneous anisotropic ellipsoidal bodies. Application in crystallography The matrix above can be used with the molar refractivity equation and other data to produce density data for crystallography. Each polarizability measurement along with the refractive index associated with its direction will yield a direction specific density that can be used to develop an accurate three dimensional assessment of molecular stacking in the crystal. This relationship was first observed by Linus Pauling. Polarizability and molecular property are related to refractive index and bulk property. In crystalline structures, the interactions between molecules are considered by comparing a local field to the macroscopic field. Analyzing a cubic crystal lattice, we can imagine an isotropic spherical region to represent the entire sample. Giving the region the radius a {\displaystyle a} , the field would be given by the volume of the sphere times the dipole moment per unit volume P . {\displaystyle \mathbf {P} .} p {\displaystyle \mathbf {p} } = 4 π a 3 3 {\displaystyle {\frac {4\pi a^{3}}{3}}} P . {\displaystyle \mathbf {P} .} We can call our local field F {\displaystyle \mathbf {F} } , our macroscopic field E {\displaystyle \mathbf {E} } , and the field due to matter within the sphere, E i n = − P 3 ε 0 {\displaystyle \mathbf {E} _{\mathrm {in} }={\tfrac {-\mathbf {P} }{3\varepsilon _{0}}}} We can then define the local field as the macroscopic field without the contribution of the internal field: F = E − E i n = E + P 3 ε 0 {\displaystyle \mathbf {F} =\mathbf {E} -\mathbf {E} _{\mathrm {in} }=\mathbf {E} +{\frac {\mathbf {P} }{3\varepsilon _{0}}}} The polarization is proportional to the macroscopic field by P = ε 0 ( ε r − 1 ) E = χ e ε 0 E {\displaystyle \mathbf {P} =\varepsilon _{0}(\varepsilon _{r}-1)\mathbf {E} =\chi _{\text{e}}\varepsilon _{0}\mathbf {E} } where ε 0 {\displaystyle \varepsilon _{0}} is the electric permittivity constant and χ e {\displaystyle \chi _{\text{e}}} is the electric susceptibility. Using this proportionality, we find the local field as F = 1 3 ( ε r + 2 ) E {\displaystyle \mathbf {F} ={\tfrac {1}{3}}(\varepsilon _{\mathrm {r} }+2)\mathbf {E} } which can be used in the definition of polarization P = N α V F = N α 3 V ( ε r + 2 ) E {\displaystyle \mathbf {P} ={\frac {N\alpha }{V}}\mathbf {F} ={\frac {N\alpha }{3V}}(\varepsilon _{\mathrm {r} }+2)\mathbf {E} } and simplified with ε r = 1 + N α ε 0 V {\displaystyle \varepsilon _{\mathrm {r} }=1+{\tfrac {N\alpha }{\varepsilon _{0}V}}} to get P = ε 0 ( ε r − 1 ) E {\displaystyle \mathbf {P} =\varepsilon _{0}(\varepsilon _{\mathrm {r} }-1)\mathbf {E} } . These two terms can both be set equal to the other, eliminating the E {\displaystyle \mathbf {E} } term giving us ε r − 1 ε r + 2 = N α 3 ε 0 V {\displaystyle {\frac {\varepsilon _{\mathrm {r} }-1}{\varepsilon _{\mathrm {r} }+2}}={\frac {N\alpha }{3\varepsilon _{0}V}}} . We can replace the relative permittivity ε r {\displaystyle \varepsilon _{\mathrm {r} }} with refractive index n {\displaystyle n} , since ε r = n 2 {\displaystyle \varepsilon _{\mathrm {r} }=n^{2}} for a low-pressure gas. The number density can be related to the molecular weight M {\displaystyle M} and mass density ρ {\displaystyle \rho } through N V = N A ρ M {\displaystyle {\tfrac {N}{V}}={\tfrac {N_{\mathrm {A} }\rho }{M}}} , adjusting the final form of our equation to include molar refractivity: R M = N A α 3 ε 0 = ( M ρ ) n 2 − 1 n 2 + 2 {\displaystyle R_{\mathrm {M} }={\frac {N_{\mathrm {A} }\alpha }{3\varepsilon _{0}}}=\left({\frac {M}{\rho }}\right){\frac {n^{2}-1}{n^{2}+2}}} This equation allows us to relate bulk property (refractive index) to the molecular property (polarizability) as a function of frequency. Atomic and molecular polarizability Generally, polarizability increases as the volume occupied by electrons increases. In atoms, this occurs because larger atoms have more loosely held electrons in contrast to smaller atoms with tightly bound electrons. On rows of the periodic table, polarizability therefore decreases from left to right. Polarizability increases down on columns of the periodic table. Likewise, larger molecules are generally more polarizable than smaller ones. Water is a very polar molecule, but alkanes and other hydrophobic molecules are more polarizable. Water with its permanent dipole is less likely to change shape due to an external electric field. Alkanes are the most polarizable molecules. Although alkenes and arenes are expected to have larger polarizability than alkanes because of their higher reactivity compared to alkanes, alkanes are in fact more polarizable. This results because of alkene's and arene's more electronegative sp2 carbons to the alkane's less electronegative sp3 carbons. Ground state electron configuration models often describe molecular or bond polarization during chemical reactions poorly, because reactive intermediates may be excited, or be the minor, alternate structures in a chemical equilibrium with the initial reactant. Magnetic polarizability Magnetic polarizability defined by spin interactions of nucleons is an important parameter of deuterons and hadrons. In particular, measurement of tensor polarizabilities of nucleons yields important information about spin-dependent nuclear forces. The method of spin amplitudes uses quantum mechanics formalism to more easily describe spin dynamics. Vector and tensor polarization of particle/nuclei with spin S ≥ 1 are specified by the unit polarization vector p {\displaystyle \mathbf {p} } and the polarization tensor P`. Additional tensors composed of products of three or more spin matrices are needed only for the exhaustive description of polarization of particles/nuclei with spin S ≥ 3⁄2. See also Dielectric Electric susceptibility Hyperpolarizability Polarization density MOSCED, an estimation method for activity coefficients which uses polarizability as one of its parameters == References == A qubit field theory is a quantum field theory in which the canonical commutation relations involved in the quantisation of pairs of observables are relaxed. Specifically, it is a quantum field theory in which, unlike most other quantum field theories, the pair of observables is not required to always commute. Theory In many ordinary quantum field theories, constraining one observable to a fixed value results in the uncertainty of the other observable being infinite (cf. uncertainty principle), and as a consequence there is potentially an infinite amount of information involved. In the situation of the standard position-momentum commutation (where the uncertainty principle is most commonly cited), this implies that a fixed, finite, volume of space has an infinite capacity to store information. However, Bekenstein's bound hints that the information storage capacity ought to be finite. Qubit field theory seeks to resolve this issue by removing the commutation restriction, allowing the capacity to store information to be finite; hence the name qubit, which derives from quantum-bit or quantised-bit. David Deutsch has presented a group of qubit field theories which, despite not requiring commutation of certain observables, still presents the same observable results as ordinary quantum field theory. J. Hruby has presented a supersymmetric extension. References External links Qubit Field Theory by David Deutsch A fermionic condensate (or Fermi–Dirac condensate) is a superfluid phase formed by fermionic particles at low temperatures. It is closely related to the Bose–Einstein condensate, a superfluid phase formed by bosonic atoms under similar conditions. Examples of fermionic condensates include superconductors and the superfluid phase of helium-3. The first fermionic condensate in dilute atomic gases was created by a team led by Deborah S. Jin using potassium-40 atoms at the University of Colorado Boulder in 2003. Background Superfluidity Fermionic condensates are attained at lower temperatures than Bose–Einstein condensates. Fermionic condensates are a type of superfluid. As the name suggests, a superfluid possesses fluid properties similar to those possessed by ordinary liquids and gases, such as the lack of a definite shape and the ability to flow in response to applied forces. However, superfluids possess some properties that do not appear in ordinary matter. For instance, they can flow at high velocities without dissipating any energy—i.e. zero viscosity. At lower velocities, energy is dissipated by the formation of quantized vortices, which act as "holes" in the medium where superfluidity breaks down. Superfluidity was originally discovered in liquid helium-4 whose atoms are bosons, not fermions. Fermionic superfluids It is far more difficult to produce a fermionic superfluid than a bosonic one, because the Pauli exclusion principle prohibits fermions from occupying the same quantum state. However, there is a well-known mechanism by which a superfluid may be formed from fermions: That mechanism is the BCS transition, discovered in 1957 by J. Bardeen, L.N. Cooper, and R. Schrieffer for describing superconductivity. These authors showed that, below a certain temperature, electrons (which are fermions) can pair up to form bound pairs now known as Cooper pairs. As long as collisions with the ionic lattice of the solid do not supply enough energy to break the Cooper pairs, the electron fluid will be able to flow without dissipation. As a result, it becomes a superfluid, and the material through which it flows a superconductor. The BCS theory was phenomenally successful in describing superconductors. Soon after the publication of the BCS paper, several theorists proposed that a similar phenomenon could occur in fluids made up of fermions other than electrons, such as helium-3 atoms. These speculations were confirmed in 1971, when experiments performed by D.D. Osheroff showed that helium-3 becomes a superfluid below 0.0025 K. It was soon verified that the superfluidity of helium-3 arises from a BCS-like mechanism. Condensates of fermionic atoms When Eric Cornell and Carl Wieman produced a Bose–Einstein condensate from rubidium atoms in 1995, there naturally arose the prospect of creating a similar sort of condensate made from fermionic atoms, which would form a superfluid by the BCS mechanism. However, early calculations indicated that the temperature required for producing Cooper pairing in atoms would be too cold to achieve. In 2001, Murray Holland at JILA suggested a way of bypassing this difficulty. He speculated that fermionic atoms could be coaxed into pairing up by subjecting them to a strong magnetic field. In 2003, working on Holland's suggestion, Deborah Jin at JILA, Rudolf Grimm at the University of Innsbruck, and Wolfgang Ketterle at MIT managed to coax fermionic atoms into forming molecular bosons, which then underwent Bose–Einstein condensation. However, this was not a true fermionic condensate. On December 16, 2003, Jin managed to produce a condensate out of fermionic atoms for the first time. The experiment involved 500,000 potassium-40 atoms cooled to a temperature of 5×10−8 K, subjected to a time-varying magnetic field. Examples Chiral condensate A chiral condensate is an example of a fermionic condensate that appears in theories of massless fermions with chiral symmetry breaking, such as the theory of quarks in Quantum Chromodynamics. BCS theory The BCS theory of superconductivity has a fermion condensate. A pair of electrons in a metal with opposite spins can form a scalar bound state called a Cooper pair. The bound states themselves then form a condensate. Since the Cooper pair has electric charge, this fermion condensate breaks the electromagnetic gauge symmetry of a superconductor, giving rise to the unusual electromagnetic properties of such states. QCD In quantum chromodynamics (QCD) the chiral condensate is also called the quark condensate. This property of the QCD vacuum is partly responsible for giving masses to hadrons (along with other condensates like the gluon condensate). In an approximate version of QCD, which has vanishing quark masses for N quark flavours, there is an exact chiral SU(N) × SU(N) symmetry of the theory. The QCD vacuum breaks this symmetry to SU(N) by forming a quark condensate. The existence of such a fermion condensate was first shown explicitly in the lattice formulation of QCD. The quark condensate is therefore an order parameter of transitions between several phases of quark matter in this limit. This is very similar to the BCS theory of superconductivity. The Cooper pairs are analogous to the pseudoscalar mesons. However, the vacuum carries no charge. Hence all the gauge symmetries are unbroken. Corrections for the masses of the quarks can be incorporated using chiral perturbation theory. Helium-3 superfluid A helium-3 atom is a fermion and at very low temperatures, they form two-atom Cooper pairs which are bosonic and condense into a superfluid. These Cooper pairs are substantially larger than the interatomic separation. See also Fermi gas Bose gas Footnotes References === Sources === Francesca Ferlaino (born 1977) is an Italian-Austrian experimental physicist known for her research on quantum matter. She is a professor of physics at the University of Innsbruck. Biography Francesca Ferlaino was born in Naples, Italy. She studied physics at the University of Naples Federico II (1995–2000) and was an undergraduate research fellow at the International School for Advanced Studies (SISSA) in Trieste (1999–2000). She did a PhD in physics at the University of Florence and the European Laboratory for Non-Linear Spectroscopy (LENS) (2001–2004). In 2007 she moved to the University of Innsbruck, Austria, where she was a research and teaching associate and started her own research group. In 2014 she became a professor of physics at the University of Innsbruck and research director at the Institute for Quantum Optics and Quantum Information (IQOQI) of the Austrian Academy of Sciences. Work Her research activity explores quantum phenomena in atomic gases at ultralow temperatures with contributions spanning topics including quantum matter of atoms and molecules and few-body and scattering physics. Over the last years, she focuses specifically on the strongly magnetic, and rather unexplored, Erbium and Dysprosium atomic species, realizing in 2012 world's first Bose-Einstein condensation of Erbium, and in 2018 the first dipolar quantum mixture of Erbium and Dysprosium. In 2019, she was able to prepare the first long-lived supersolid state, an elusive and paradoxical state where superfluid flow and crystal rigidity coexist. With these systems, she has explored a variety of many-body quantum phenomena dictated by the long-range and anisotropic dipolar interaction among the atoms. In 2021 she created supersolid states along two dimensions. In 2024 her team reported the observation of quantum vortices in the supersolid phase Awards and fellowships Her work has earned her multiple awards, including the Grand Prix de Physique "Cécile-DeWitt Morette/École de Physique des Houches" from the French Academy of Sciences (2019), the Junior BEC Award (2019), the Feltrinelli Prize (2017) and the Erwin Schrödinger Prize (2017), the highest award of the Austrian Academy of Sciences. In addition, she is the recipient of an Alexander von Humboldt Professorship (2013), a START-Prize (2009) and three ERC Grants (Starting 2010, Consolidator 2016 and Advanced 2022) She was elected as a Fellow of the American Physical Society (APS) in 2019, after a nomination from the APS Division of Atomic, Molecular and Optical Physics, "for ground-breaking experiments on dipolar quantum gases of erbium atoms, including the attainment of quantum degeneracy of bosons and fermions, studies on quantum-chaotical scattering, the formation of quantum droplets, and investigations on the roton spectrum". References External links Dipolar Quantum Gas Group Research group of Francesca Ferlaino In quantum chromodynamics (QCD), the gluon condensate is a non-perturbative property of the QCD vacuum which could be partly responsible for giving masses to light mesons. If the gluon field tensor is represented as Gμν, then the gluon condensate is the vacuum expectation value ⟨ G μ ν G μ ν ⟩ {\displaystyle \langle G_{\mu \nu }G^{\mu \nu }\rangle } . It is not clear yet whether this condensate is related to any of the known phase changes in quark matter. There have been scattered studies of other types of gluon condensates, involving a different number of gluon fields. For more on the context in which this quantity occurs, see the article on the QCD vacuum. See also Quantum chromodynamics QCD vacuum and chiral condensates Vacuum in quantum field theory Quark–gluon plasma QCD matter == References == In quantum field theory, the term moduli (sg.: modulus; more properly moduli fields) is sometimes used to refer to scalar fields whose potential energy function has continuous families of global minima. Such potential functions frequently occur in supersymmetric systems. The term "modulus" is borrowed from mathematics (or more specifically, moduli space is borrowed from algebraic geometry), where it is used synonymously with "parameter". The word moduli (Moduln in German) first appeared in 1857 in Bernhard Riemann's celebrated paper "Theorie der Abel'schen Functionen". Moduli spaces in quantum field theories In quantum field theories, the possible vacua are usually labeled by the vacuum expectation values of scalar fields, as Lorentz invariance forces the vacuum expectation values of any higher spin fields to vanish. These vacuum expectation values can take any value for which the potential function is a minimum. Consequently, when the potential function has continuous families of global minima, the space of vacua for the quantum field theory is a manifold (or orbifold), usually called the vacuum manifold. This manifold is often called the moduli space of vacua, or just the moduli space, for short. The term moduli is also used in string theory to refer to various continuous parameters that label possible string backgrounds: the expectation value of the dilaton field, the parameters (e.g. the radius and complex structure) which govern the shape of the compactification manifold, et cetera. These parameters are represented, in the quantum field theory that approximates the string theory at low energies, by the vacuum expectation values of massless scalar fields, making contact with the usage described above. In string theory, the term "moduli space" is often used specifically to refer to the space of all possible string backgrounds. Moduli spaces of supersymmetric gauge theories In general quantum field theories, even if the classical potential energy is minimized over a large set of possible expectation values, once quantum corrections are included it is generically the case that nearly all of these configurations cease to minimize the energy. The result is that the set of vacua of the quantum theory is generally much smaller than that of the classical theory. A notable exception occurs when the various vacua in question are related by a symmetry which guarantees that their energy levels remain exactly degenerate. The situation is very different in supersymmetric quantum field theories. In general, these possess large moduli spaces of vacua which are not related by any symmetry, for example, the masses of the various excitations may differ at various points on the moduli space. The moduli spaces of supersymmetric gauge theories are in general easier to calculate than those of nonsupersymmetric theories because supersymmetry restricts the allowed geometries of the moduli space even when quantum corrections are included. Allowed moduli spaces of 4-dimensional theories The more supersymmetry there is, the stronger the restriction on the vacuum manifold. Therefore, if a restriction appears below for a given number N of spinors of supercharges, then it also holds for all greater values of N. N=1 Theories The first restriction on the geometry of a moduli space was found in 1979 by Bruno Zumino and published in the article "Supersymmetry and Kähler Manifolds". He considered an N=1 theory in 4-dimensions with global supersymmetry. N=1 means that the fermionic components of the supersymmetry algebra can be assembled into a single Majorana supercharge. The only scalars in such a theory are the complex scalars of the chiral superfields. He found that the vacuum manifold of allowed vacuum expectation values for these scalars is not only complex but also a Kähler manifold. If gravity is included in the theory, so that there is local supersymmetry, then the resulting theory is called a supergravity theory and the restriction on the geometry of the moduli space becomes stronger. The moduli space must not only be Kähler, but also the Kähler form must lift to integral cohomology. Such manifolds are called Hodge manifolds. The first example appeared in the 1979 article "Spontaneous Symmetry Breaking and Higgs Effect in Supergravity Without Cosmological Constant" and the general statement appeared 3 years later in "Quantization of Newton's Constant in Certain Supergravity Theories". N=2 Theories In extended 4-dimensional theories with N=2 supersymmetry, corresponding to a single Dirac spinor supercharge, the conditions are stronger. The N=2 supersymmetry algebra contains two representations with scalars, the vector multiplet which contains a complex scalar and the hypermultiplet which contains two complex scalars. The moduli space of the vector multiplets is called the Coulomb branch while that of the hypermultiplets is called the Higgs branch. The total moduli space is locally a product of these two branches, as nonrenormalization theorems imply that the metric of each is independent of the fields of the other multiplet.(See for example Argyres, Non-Perturbative Dynamics Of Four-Dimensional Supersymmetric Field Theories, pp. 6–7, for further discussion of the local product structure.) In the case of global N=2 supersymmetry, in other words in the absence of gravity, the Coulomb branch of the moduli space is a special Kähler manifold. The first example of this restriction appeared in the 1984 article Potentials and Symmetries of General Gauged N=2 Supergravity: Yang-Mills Models by Bernard de Wit and Antoine Van Proeyen, while a general geometric description of the underlying geometry, called special geometry, was presented by Andrew Strominger in his 1990 paper Special Geometry. The Higgs branch is a hyperkähler manifold as was shown by Luis Alvarez-Gaume and Daniel Freedman in their 1981 paper Geometrical Structure and Ultraviolet Finiteness in the Supersymmetric Sigma Model. Including gravity the supersymmetry becomes local. Then one needs to add the same Hodge condition to the special Kahler Coulomb branch as in the N=1 case. Jonathan Bagger and Edward Witten demonstrated in their 1982 paper Matter Couplings in N=2 Supergravity that in this case, the Higgs branch must be a quaternionic Kähler manifold. N>2 Supersymmetry In extended supergravities with N>2 the moduli space must always be a symmetric space. References Andrianopoli, L.; Bertolini, M.; Ceresole, A.; D'Auria, R.; Ferrara, S.; Fré, P.; Magri, T. (Sep 1997). "N = 2 supergravity and N = 2 super Yang-Mills theory on general scalar manifolds: Symplectic covariance gaugings and the momentum map". Journal of Geometry and Physics. 23 (2): 111–189. arXiv:hep-th/9605032. Bibcode:1997JGP....23..111A. doi:10.1016/S0393-0440(97)00002-8, contains a review of restrictions on moduli spaces in various supersymmetric gauge theories. In mathematics, weak convergence in a Hilbert space is the convergence of a sequence of points in the weak topology. Definition A sequence of points ( x n ) {\displaystyle (x_{n})} in a Hilbert space H is said to converge weakly to a point x in H if lim n → ∞ ⟨ x n , y ⟩ = ⟨ x , y ⟩ {\displaystyle \lim _{n\to \infty }\langle x_{n},y\rangle =\langle x,y\rangle } for all y in H. Here, ⟨ ⋅ , ⋅ ⟩ {\displaystyle \langle \cdot ,\cdot \rangle } is understood to be the inner product on the Hilbert space. The notation x n ⇀ x {\displaystyle x_{n}\rightharpoonup x} is sometimes used to denote this kind of convergence. Properties If a sequence converges strongly (that is, if it converges in norm), then it converges weakly as well. Since every closed and bounded set is weakly relatively compact (its closure in the weak topology is compact), every bounded sequence x n {\displaystyle x_{n}} in a Hilbert space H contains a weakly convergent subsequence. Note that closed and bounded sets are not in general weakly compact in Hilbert spaces (consider the set consisting of an orthonormal basis in an infinite-dimensional Hilbert space which is closed and bounded but not weakly compact since it doesn't contain 0). However, bounded and weakly closed sets are weakly compact so as a consequence every convex bounded closed set is weakly compact. As a consequence of the principle of uniform boundedness, every weakly convergent sequence is bounded. The norm is (sequentially) weakly lower-semicontinuous: if x n {\displaystyle x_{n}} converges weakly to x, then ‖ x ‖ ≤ lim inf n → ∞ ‖ x n ‖ , {\displaystyle \Vert x\Vert \leq \liminf _{n\to \infty }\Vert x_{n}\Vert ,} and this inequality is strict whenever the convergence is not strong. For example, infinite orthonormal sequences converge weakly to zero, as demonstrated below. If x n → x {\displaystyle x_{n}\to x} weakly and ‖ x n ‖ → ‖ x ‖ {\displaystyle \lVert x_{n}\rVert \to \lVert x\rVert } , then x n → x {\displaystyle x_{n}\to x} strongly: ⟨ x − x n , x − x n ⟩ = ⟨ x , x ⟩ + ⟨ x n , x n ⟩ − ⟨ x n , x ⟩ − ⟨ x , x n ⟩ → 0. {\displaystyle \langle x-x_{n},x-x_{n}\rangle =\langle x,x\rangle +\langle x_{n},x_{n}\rangle -\langle x_{n},x\rangle -\langle x,x_{n}\rangle \rightarrow 0.} If the Hilbert space is finite-dimensional, i.e. a Euclidean space, then weak and strong convergence are equivalent. Example The Hilbert space L 2 [ 0 , 2 π ] {\displaystyle L^{2}[0,2\pi ]} is the space of the square-integrable functions on the interval [ 0 , 2 π ] {\displaystyle [0,2\pi ]} equipped with the inner product defined by ⟨ f , g ⟩ = ∫ 0 2 π f ( x ) ⋅ g ( x ) d x , {\displaystyle \langle f,g\rangle =\int _{0}^{2\pi }f(x)\cdot g(x)\,dx,} (see Lp space). The sequence of functions f 1 , f 2 , … {\displaystyle f_{1},f_{2},\ldots } defined by f n ( x ) = sin ⁡ ( n x ) {\displaystyle f_{n}(x)=\sin(nx)} converges weakly to the zero function in L 2 [ 0 , 2 π ] {\displaystyle L^{2}[0,2\pi ]} , as the integral ∫ 0 2 π sin ⁡ ( n x ) ⋅ g ( x ) d x . {\displaystyle \int _{0}^{2\pi }\sin(nx)\cdot g(x)\,dx.} tends to zero for any square-integrable function g {\displaystyle g} on [ 0 , 2 π ] {\displaystyle [0,2\pi ]} when n {\displaystyle n} goes to infinity, which is by Riemann–Lebesgue lemma, i.e. ⟨ f n , g ⟩ → ⟨ 0 , g ⟩ = 0. {\displaystyle \langle f_{n},g\rangle \to \langle 0,g\rangle =0.} Although f n {\displaystyle f_{n}} has an increasing number of 0's in [ 0 , 2 π ] {\displaystyle [0,2\pi ]} as n {\displaystyle n} goes to infinity, it is of course not equal to the zero function for any n {\displaystyle n} . Note that f n {\displaystyle f_{n}} does not converge to 0 in the L ∞ {\displaystyle L_{\infty }} or L 2 {\displaystyle L_{2}} norms. This dissimilarity is one of the reasons why this type of convergence is considered to be "weak." Weak convergence of orthonormal sequences Consider a sequence e n {\displaystyle e_{n}} which was constructed to be orthonormal, that is, ⟨ e n , e m ⟩ = δ m n {\displaystyle \langle e_{n},e_{m}\rangle =\delta _{mn}} where δ m n {\displaystyle \delta _{mn}} equals one if m = n and zero otherwise. We claim that if the sequence is infinite, then it converges weakly to zero. A simple proof is as follows. For x ∈ H, we have ∑ n | ⟨ e n , x ⟩ | 2 ≤ ‖ x ‖ 2 {\displaystyle \sum _{n}|\langle e_{n},x\rangle |^{2}\leq \|x\|^{2}} (Bessel's inequality) where equality holds when {en} is a Hilbert space basis. Therefore | ⟨ e n , x ⟩ | 2 → 0 {\displaystyle |\langle e_{n},x\rangle |^{2}\rightarrow 0} (since the series above converges, its corresponding sequence must go to zero) i.e. ⟨ e n , x ⟩ → 0. {\displaystyle \langle e_{n},x\rangle \rightarrow 0.} Banach–Saks theorem The Banach–Saks theorem states that every bounded sequence x n {\displaystyle x_{n}} contains a subsequence x n k {\displaystyle x_{n_{k}}} and a point x such that 1 N ∑ k = 1 N x n k {\displaystyle {\frac {1}{N}}\sum _{k=1}^{N}x_{n_{k}}} converges strongly to x as N goes to infinity. Generalizations The definition of weak convergence can be extended to Banach spaces. A sequence of points ( x n ) {\displaystyle (x_{n})} in a Banach space B is said to converge weakly to a point x in B if f ( x n ) → f ( x ) {\displaystyle f(x_{n})\to f(x)} for any bounded linear functional f {\displaystyle f} defined on B {\displaystyle B} , that is, for any f {\displaystyle f} in the dual space B ′ {\displaystyle B'} . If B {\displaystyle B} is an Lp space on Ω {\displaystyle \Omega } and p < + ∞ {\displaystyle p<+\infty } , then any such f {\displaystyle f} has the form f ( x ) = ∫ Ω x y d μ {\displaystyle f(x)=\int _{\Omega }x\,y\,d\mu } for some y ∈ L q ( Ω ) {\displaystyle y\in \,L^{q}(\Omega )} , where μ {\displaystyle \mu } is the measure on Ω {\displaystyle \Omega } and 1 p + 1 q = 1 {\displaystyle {\frac {1}{p}}+{\frac {1}{q}}=1} are conjugate indices. In the case where B {\displaystyle B} is a Hilbert space, then, by the Riesz representation theorem, f ( ⋅ ) = ⟨ ⋅ , y ⟩ {\displaystyle f(\cdot )=\langle \cdot ,y\rangle } for some y {\displaystyle y} in B {\displaystyle B} , so one obtains the Hilbert space definition of weak convergence. See also Dual topology Operator topologies – topologies on the set of operators on a Hilbert space == References == Hooke's atom, also known as harmonium or hookium, refers to an artificial helium-like atom where the Coulombic electron-nucleus interaction potential is replaced by a harmonic potential. This system is of significance as it is, for certain values of the force constant defining the harmonic containment, an exactly solvable ground-state many-electron problem that explicitly includes electron correlation. As such it can provide insight into quantum correlation (albeit in the presence of a non-physical nuclear potential) and can act as a test system for judging the accuracy of approximate quantum chemical methods for solving the Schrödinger equation. The name "Hooke's atom" arises because the harmonic potential used to describe the electron-nucleus interaction is a consequence of Hooke's law. Definition Employing atomic units, the Hamiltonian defining the Hooke's atom is H ^ = − 1 2 ∇ 1 2 − 1 2 ∇ 2 2 + 1 2 k ( r 1 2 + r 2 2 ) + 1 | r 1 − r 2 | . {\displaystyle {\hat {H}}=-{\frac {1}{2}}\nabla _{1}^{2}-{\frac {1}{2}}\nabla _{2}^{2}+{\frac {1}{2}}k(r_{1}^{2}+r_{2}^{2})+{\frac {1}{|\mathbf {r} _{1}-\mathbf {r} _{2}|}}.} As written, the first two terms are the kinetic energy operators of the two electrons, the third term is the harmonic electron-nucleus potential, and the final term the electron-electron interaction potential. The non-relativistic Hamiltonian of the helium atom differs only in the replacement: − 2 r → 1 2 k r 2 . {\displaystyle -{\frac {2}{r}}\rightarrow {\frac {1}{2}}kr^{2}.} Solution The equation to be solved is the two electron Schrödinger equation: H ^ Ψ ( r 1 , r 2 ) = E Ψ ( r 1 , r 2 ) . {\displaystyle {\hat {H}}\Psi (\mathbf {r} _{1},\mathbf {r} _{2})=E\Psi (\mathbf {r} _{1},\mathbf {r} _{2}).} For arbitrary values of the force constant, k, the Schrödinger equation does not have an analytic solution. However, for a countably infinite number of values, such as k=¼, simple closed form solutions can be derived. Given the artificial nature of the system this restriction does not hinder the usefulness of the solution. To solve, the system is first transformed from the Cartesian electronic coordinates, (r1,r2), to the center of mass coordinates, (R,u), defined as R = 1 2 ( r 1 + r 2 ) , u = r 2 − r 1 . {\displaystyle \mathbf {R} ={\frac {1}{2}}(\mathbf {r} _{1}+\mathbf {r} _{2}),\mathbf {u} =\mathbf {r} _{2}-\mathbf {r} _{1}.} Under this transformation, the Hamiltonian becomes separable – that is, the |r1 - r2| term coupling the two electrons is removed (and not replaced by some other form) allowing the general separation of variables technique to be applied to further a solution for the wave function in the form Ψ ( r 1 , r 2 ) = χ ( R ) Φ ( u ) {\displaystyle \Psi (\mathbf {r} _{1},\mathbf {r} _{2})=\chi (\mathbf {R} )\Phi (\mathbf {u} )} . The original Schrödinger equation is then replaced by: ( − 1 4 ∇ R 2 + k R 2 ) χ ( R ) = E R χ ( R ) , {\displaystyle \left(-{\frac {1}{4}}\nabla _{\mathbf {R} }^{2}+kR^{2}\right)\chi (\mathbf {R} )=E_{\mathbf {R} }\chi (\mathbf {R} ),} ( − ∇ u 2 + 1 4 k u 2 + 1 u ) Φ ( u ) = E u Φ ( u ) . {\displaystyle \left(-\nabla _{\mathbf {u} }^{2}+{\frac {1}{4}}ku^{2}+{\frac {1}{u}}\right)\Phi (\mathbf {u} )=E_{\mathbf {u} }\Phi (\mathbf {u} ).} The first equation for χ ( R ) {\displaystyle \chi (\mathbf {R} )} is the Schrödinger equation for an isotropic quantum harmonic oscillator with ground-state energy E R = ( 3 / 2 ) k E h {\displaystyle E_{\mathbf {R} }=(3/2){\sqrt {k}}E_{\mathrm {h} }} and (unnormalized) wave function χ ( R ) = e − k R 2 . {\displaystyle \chi (\mathbf {R} )=e^{-{\sqrt {k}}R^{2}}.} Asymptotically, the second equation again behaves as a harmonic oscillator of the form exp ⁡ ( − ( k / 4 ) u 2 ) {\displaystyle \exp(-({\sqrt {k}}/4)u^{2})\,} and the rotationally invariant ground state can be expressed, in general, as Φ ( u ) = f ( u ) exp ⁡ ( − ( k / 4 ) u 2 ) {\displaystyle \Phi (\mathbf {u} )=f(u)\exp(-({\sqrt {k}}/4)u^{2})\,} for some function f ( u ) {\displaystyle f(u)\,} . It was long noted that f(u) is very well approximated by a linear function in u. Thirty years after the proposal of the model an exact solution was discovered for k=¼, and it was seen that f(u)=1+u/2. It was later shown that there are many values of k which lead to an exact solution for the ground state, as will be shown in the following. Decomposing Φ ( u ) = R l ( u ) Y l m {\displaystyle \Phi (\mathbf {u} )=R_{l}(u)Y_{lm}} and expressing the Laplacian in spherical coordinates, ( − 1 u 2 ∂ ∂ u ( u 2 ∂ ∂ u ) + L ^ 2 u 2 + 1 4 k u 2 + 1 u ) R l ( u ) Y l m ( u ^ ) = E l R l ( u ) Y l m ( u ^ ) , {\displaystyle \left(-{\frac {1}{u^{2}}}{\frac {\partial }{\partial u}}\left(u^{2}{\frac {\partial }{\partial u}}\right)+{\frac {{\hat {L}}^{2}}{u^{2}}}+{\frac {1}{4}}ku^{2}+{\frac {1}{u}}\right)R_{l}(u)Y_{lm}({\hat {\mathbf {u} }})=E_{l}R_{l}(u)Y_{lm}({\hat {\mathbf {u} }}),} one further decomposes the radial wave function as R l ( u ) = S l ( u ) / u {\displaystyle R_{l}(u)=S_{l}(u)/u\,} which removes the first derivative to yield − ∂ 2 S l ( u ) ∂ u 2 + ( l ( l + 1 ) u 2 + 1 4 k u 2 + 1 u ) S l ( u ) = E l S l ( u ) . {\displaystyle -{\frac {\partial ^{2}S_{l}(u)}{\partial u^{2}}}+\left({\frac {l(l+1)}{u^{2}}}+{\frac {1}{4}}ku^{2}+{\frac {1}{u}}\right)S_{l}(u)=E_{l}S_{l}(u).} The asymptotic behavior S l ( u ) ∼ e − k 4 u 2 {\displaystyle S_{l}(u)\sim e^{-{\frac {\sqrt {k}}{4}}u^{2}}\,} encourages a solution of the form S l ( u ) = e − k 4 u 2 T l ( u ) . {\displaystyle S_{l}(u)=e^{-{\frac {\sqrt {k}}{4}}u^{2}}T_{l}(u).} The differential equation satisfied by T l ( u ) {\displaystyle T_{l}(u)\,} is − ∂ 2 T l ( u ) ∂ u 2 + k u ∂ T l ( u ) ∂ u + ( l ( l + 1 ) u 2 + 1 u + ( k 2 − E l ) ) T l ( u ) = 0. {\displaystyle -{\frac {\partial ^{2}T_{l}(u)}{\partial u^{2}}}+{\sqrt {k}}u{\frac {\partial T_{l}(u)}{\partial u}}+\left({\frac {l(l+1)}{u^{2}}}+{\frac {1}{u}}+\left({\frac {\sqrt {k}}{2}}-E_{l}\right)\right)T_{l}(u)=0.} This equation lends itself to a solution by way of the Frobenius method. That is, T l ( u ) {\displaystyle T_{l}(u)\,} is expressed as T l ( u ) = u m ∑ k = 0 ∞ a k u k . {\displaystyle T_{l}(u)=u^{m}\sum _{k=0}^{\infty }\ a_{k}u^{k}.} for some m {\displaystyle m\,} and { a k } k = 0 k = ∞ {\displaystyle \{a_{k}\}_{k=0}^{k=\infty }\,} which satisfy: m ( m − 1 ) = l ( l + 1 ) , {\displaystyle m(m-1)=l(l+1)\,,} a 0 ≠ 0 {\displaystyle a_{0}\neq 0\,} a 1 = a 0 2 ( l + 1 ) , {\displaystyle a_{1}={\frac {a_{0}}{2(l+1)}},} a 2 = a 1 + ( k ( l + 3 2 ) − E l ) a 0 2 ( 2 l + 3 ) = a 0 2 ( 2 l + 3 ) ( 1 2 ( l + 1 ) + k ( l + 3 2 ) − E l ) , {\displaystyle a_{2}={\frac {a_{1}+\left({\sqrt {k}}(l+{\frac {3}{2}})-E_{l}\right)a_{0}}{2(2l+3)}}={\frac {a_{0}}{2(2l+3)}}\left({\frac {1}{2(l+1)}}+{\sqrt {k}}\left(l+{\frac {3}{2}}\right)-E_{l}\right),} a 3 = a 2 + ( k ( l + 5 2 ) − E l ) a 1 6 ( l + 2 ) , {\displaystyle a_{3}={\frac {a_{2}+\left({\sqrt {k}}(l+{\frac {5}{2}})-E_{l}\right)a_{1}}{6(l+2)}},} a n + 1 = a n + ( k ( l + 1 2 + n ) − E l ) a n − 1 ( n + 1 ) ( 2 l + 2 + n ) . {\displaystyle a_{n+1}={\frac {a_{n}+\left({\sqrt {k}}(l+{\frac {1}{2}}+n)-E_{l}\right)a_{n-1}}{(n+1)(2l+2+n)}}.} The two solutions to the indicial equation are m = l + 1 {\displaystyle m=l+1} and m = − l {\displaystyle m=-l} of which the former is taken as it yields the regular (bounded, normalizable) wave function. For a simple solution to exist, the infinite series is sought to terminate and it is here where particular values of k are exploited for an exact closed-form solution. Terminating the polynomial at any particular order can be accomplished with different values of k defining the Hamiltonian. As such there exists an infinite number of systems, differing only in the strength of the harmonic containment, with exact ground-state solutions. Most simply, to impose ak = 0 for k ≥ 2, two conditions must be satisfied: 1 2 ( l + 1 ) + k ( l + 3 2 ) − E l = 0 , {\displaystyle {\frac {1}{2(l+1)}}+{\sqrt {k}}\left(l+{\frac {3}{2}}\right)-E_{l}=0,} k ( l + 5 2 ) = E l . {\displaystyle {\sqrt {k}}(l+{\frac {5}{2}})=E_{l}.} These directly force a2 = 0 and a3 = 0 respectively, and as a consequence of the three term recession, all higher coefficients also vanish. Solving for k {\displaystyle {\sqrt {k}}\,} and E l {\displaystyle E_{l}\,} yields k = 1 2 ( l + 1 ) , {\displaystyle {\sqrt {k}}={\frac {1}{2(l+1)}},} E l = 2 l + 5 4 ( l + 1 ) , {\displaystyle E_{l}={\frac {2l+5}{4(l+1)}},} and the radial wave function T l = u l + 1 ( a 0 + a 0 2 ( l + 1 ) u ) . {\displaystyle T_{l}=u^{l+1}\left(a_{0}+{\frac {a_{0}}{2(l+1)}}u\right).} Transforming back to R l ( u ) {\displaystyle R_{l}(u)\,} R l ( u ) = T l ( u ) e − k 4 u 2 u = u l ( 1 + 1 2 ( l + 1 ) u ) e − k 4 u 2 , {\displaystyle R_{l}(u)={\frac {T_{l}(u)e^{-{\frac {\sqrt {k}}{4}}u^{2}}}{u}}=u^{l}\left(1+{\frac {1}{2(l+1)}}u\right)e^{-{\frac {\sqrt {k}}{4}}u^{2}},} the ground-state (with l = 0 {\displaystyle l=0\,} and energy 5 / 4 E h {\displaystyle 5/4E_{\mathrm {h} }\,} ) is finally Φ ( u ) = ( 1 + u 2 ) e − u 2 / 8 . {\displaystyle \Phi (\mathbf {u} )=\left(1+{\frac {u}{2}}\right)e^{-u^{2}/8}.} Combining, normalizing, and transforming back to the original coordinates yields the ground state wave function: Ψ ( r 1 , r 2 ) = 1 2 8 π 5 / 2 + 5 π 3 ( 1 + 1 2 | r 1 − r 2 | ) exp ⁡ ( − 1 4 ( r 1 2 + r 2 2 ) ) . {\displaystyle \Psi (\mathbf {r} _{1},\mathbf {r} _{2})={\frac {1}{2{\sqrt {8\pi ^{5/2}+5\pi ^{3}}}}}\left(1+{\frac {1}{2}}|\mathbf {r} _{1}-\mathbf {r} _{2}|\right)\exp \left(-{\frac {1}{4}}{\big (}r_{1}^{2}+r_{2}^{2}{\big )}\right).} The corresponding ground-state total energy is then E = E R + E u = 3 4 + 5 4 = 2 E h {\displaystyle E=E_{R}+E_{u}={\frac {3}{4}}+{\frac {5}{4}}=2E_{\mathrm {h} }} . Remarks The exact ground state electronic density of the Hooke atom for the special case k = 1 / 4 {\displaystyle k=1/4} is ρ ( r ) = 2 π 3 / 2 ( 8 + 5 π ) e − ( 1 / 2 ) r 2 ( ( π 2 ) 1 / 2 ( 7 4 + 1 4 r 2 + ( r + 1 r ) e r f ( r 2 ) ) + e − ( 1 / 2 ) r 2 ) . {\displaystyle \rho (\mathbf {r} )={\frac {2}{\pi ^{3/2}(8+5{\sqrt {\pi }})}}e^{-(1/2)r^{2}}\left(\left({\frac {\pi }{2}}\right)^{1/2}\left({\frac {7}{4}}+{\frac {1}{4}}r^{2}+\left(r+{\frac {1}{r}}\right)\mathrm {erf} \left({\frac {r}{\sqrt {2}}}\right)\right)+e^{-(1/2)r^{2}}\right).} From this we see that the radial derivative of the density vanishes at the nucleus. This is in stark contrast to the real (non-relativistic) helium atom where the density displays a cusp at the nucleus as a result of the unbounded Coulomb potential. See also List of quantum-mechanical systems with analytical solutions References Further reading Cioslowski, Jerzy; Pernal, Katarzyna (2000). "The Ground State of Harmonium". The Journal of Chemical Physics. 113 (19): 8434–8443. Bibcode:2000JChPh.113.8434C. doi:10.1063/1.1318767. O’Neill, Darragh P.; Gill, Peter M. W. (2003). "Wave functions and two-electron probability distributions of the Hooke's-law atom and helium" (PDF). Physical Review A. 68 (2): 022505. Bibcode:2003PhRvA..68b2505O. doi:10.1103/PhysRevA.68.022505. In theoretical physics, a logarithmic conformal field theory is a conformal field theory in which the correlators of the basic fields are allowed to be logarithmic at short distance, instead of being powers of the fields' distance. Equivalently, the dilation operator is not diagonalizable. Examples of logarithmic conformal field theories include critical percolation. In two dimensions Just like conformal field theory in general, logarithmic conformal field theory has been particularly well-studied in two dimensions. Some two-dimensional logarithmic CFTs have been solved: The Gaberdiel–Kausch CFT at central charge c = − 2 {\displaystyle c=-2} , which is rational with respect to its extended symmetry algebra, namely the triplet algebra. The G L ( 1 | 1 ) {\displaystyle GL(1|1)} Wess–Zumino–Witten model, based on the simplest non-trivial supergroup. The triplet model at c = 0 {\displaystyle c=0} is also rational with respect to the triplet algebra. == References == In quantum mechanics, a doublet is a composite quantum state of a system with an effective spin of 1/2, such that there are two allowed values of the spin component, −1/2 and +1/2. Quantum systems with two possible states are sometimes called two-level systems. Essentially all occurrences of doublets in nature arise from rotational symmetry; spin 1/2 is associated with the fundamental representation of the Lie group SU(2). History and applications The term "doublet" dates back to the early 19th century, when it was observed that certain spectral lines of an ionized, excited gas would split into two under the influence of a strong magnetic field, in an effect known as the anomalous Zeeman effect. Such spectral lines were observed not only in the laboratory, but also in astronomical spectroscopy observations, allowing astronomers to deduce the existence of, and measure the strength of magnetic fields around the Sun, stars and galaxies. Conversely, it was the observation of doublets in spectroscopy that allowed physicists to deduce that the electron had a spin, and that furthermore, the magnitude of the spin had to be 1/2. See the history section of the article on Spin (physics) for greater detail. Doublets continue to play an important role in physics. For example, the healthcare technology of magnetic resonance imaging is based on nuclear magnetic resonance. In this technology, a spectroscopic doublet occurs in a spin-1/2 atomic nucleus, whose doublet splitting is in the radio-frequency range. By applying both a magnetic field and carefully tuning a radio-frequency transmitter, the nuclear spins will flip and re-emit radiation, in an effect known as the Rabi cycle. The strength and frequency of the emitted radio waves allow the concentration of such nuclei to be measured. Another potential application is the use of doublets as the emitting layer in light-emitting diodes (LEDs). These materials have the advantage of having 100% theoretical quantum efficiency based on spin statistics whereas singlet systems and triplet systems have significantly lower efficiencies or rely on noble metals such as Pt and Ir to emit light. See also Singlet state Triplet state Spin multiplicity == References == In quantum field theory, a branch of theoretical physics, crossing is the property of scattering amplitudes that allows antiparticles to be interpreted as particles going backwards in time. Crossing states that the same formula that determines the S-matrix elements and scattering amplitudes for particle A {\displaystyle \mathrm {A} } to scatter with X {\displaystyle \mathrm {X} } and produce particle B {\displaystyle \mathrm {B} } and Y {\displaystyle \mathrm {Y} } will also give the scattering amplitude for A + B ¯ + X {\displaystyle \mathrm {A} +{\bar {\mathrm {B} }}+\mathrm {X} } to go into Y {\displaystyle \mathrm {Y} } , or for B ¯ {\displaystyle {\bar {\mathrm {B} }}} to scatter with X {\displaystyle \mathrm {X} } to produce Y + A ¯ {\displaystyle \mathrm {Y} +{\bar {\mathrm {A} }}} . The only difference is that the value of the energy is negative for the antiparticle. The formal way to state this property is that the antiparticle scattering amplitudes are the analytic continuation of particle scattering amplitudes to negative energies. The interpretation of this statement is that the antiparticle is in every way a particle going backwards in time. History Murray Gell-Mann and Marvin Leonard Goldberger introduced crossing symmetry in 1954. Crossing had already been implicit in the work of Richard Feynman, but came to its own in the 1950s and 1960s as part of the analytic S-matrix program. Overview Consider an amplitude M ( ϕ ( p ) + . . . → . . . ) {\displaystyle {\mathcal {M}}(\phi (p)+...\ \rightarrow ...)} . We concentrate our attention on one of the incoming particles with momentum p. The quantum field ϕ ( p ) {\displaystyle \phi (p)} , corresponding to the particle is allowed to be either bosonic or fermionic. Crossing symmetry states that we can relate the amplitude of this process to the amplitude of a similar process with an outgoing antiparticle ϕ ¯ ( − p ) {\displaystyle {\bar {\phi }}(-p)} replacing the incoming particle ϕ ( p ) {\displaystyle \phi (p)} : M ( ϕ ( p ) + . . . → . . . ) = M ( . . . → . . . + ϕ ¯ ( − p ) ) {\displaystyle {\mathcal {M}}(\phi (p)+...\rightarrow ...)={\mathcal {M}}(...\rightarrow ...+{\bar {\phi }}(-p))} . In the bosonic case, the idea behind crossing symmetry can be understood intuitively using Feynman diagrams. Consider any process involving an incoming particle with momentum p. For the particle to give a measurable contribution to the amplitude, it has to interact with a number of different particles with momenta q 1 , q 2 , . . . , q n {\displaystyle q_{1},q_{2},...,q_{n}} via a vertex. Conservation of momentum implies ∑ k = 1 n q k = p {\displaystyle \sum _{k=1}^{n}q_{k}=p} . In case of an outgoing particle, conservation of momentum reads as ∑ k = 1 n q k = − p {\displaystyle \sum _{k=1}^{n}q_{k}=-p} . Thus, replacing an incoming boson with an outgoing antiboson with opposite momentum yields the same S-matrix element. In fermionic case, one can apply the same argument but now the relative phase convention for the external spinors must be taken into account. Examples For example, the annihilation of an electron with a positron into two photons is related to an elastic scattering of an electron with a photon (Compton scattering) by crossing symmetry. This relation allows to calculate the scattering amplitude of one process from the amplitude for the other process if negative values of energy of some particles are substituted. Bhabha scattering (electron–positron scattering) and Møller scattering (electron-electron scattering) are also related by crossing symmetry. See also Feynman–Stueckelberg interpretation Feynman diagram Regge theory Detailed balance References Further reading Peskin, M.; Schroeder, D. (1995). An Introduction to Quantum Field Theory. Westview Press. p. 155. ISBN 0-201-50397-2. Griffiths, David (1987). An Introduction to Elementary Particles (1st ed.). John Wiley & Sons. p. 21. ISBN 0-471-60386-4. In physics, a hidden-variable theory is a deterministic model which seeks to explain the probabilistic nature of quantum mechanics by introducing additional, possibly inaccessible, variables. The mathematical formulation of quantum mechanics assumes that the state of a system prior to measurement is indeterminate; quantitative bounds on this indeterminacy are expressed by the Heisenberg uncertainty principle. Most hidden-variable theories are attempts to avoid this indeterminacy, but possibly at the expense of requiring that nonlocal interactions be allowed. One notable hidden-variable theory is the de Broglie–Bohm theory. In their 1935 EPR paper, Albert Einstein, Boris Podolsky, and Nathan Rosen argued that quantum entanglement might imply that quantum mechanics is an incomplete description of reality. John Stewart Bell in 1964, in his eponymous theorem proved that correlations between particles under any local hidden variable theory must obey certain constraints. Subsequently, Bell test experiments have demonstrated broad violation of these constraints, ruling out such theories. Bell's theorem, however, does not rule out the possibility of nonlocal theories or superdeterminism; these therefore cannot be falsified by Bell tests. Motivation Macroscopic physics requires classical mechanics which allows accurate predictions of mechanical motion with reproducible, high precision. Quantum phenomena require quantum mechanics, which allows accurate predictions of statistical averages only. If quantum states had hidden-variables awaiting ingenious new measurement technologies, then the latter (statistical results) might be convertible to a form of the former (classical-mechanical motion). This classical mechanics description would eliminate unsettling characteristics of quantum theory like the uncertainty principle. More fundamentally however, a successful model of quantum phenomena with hidden variables implies quantum entities with intrinsic values independent of measurements. Existing quantum mechanics asserts that state properties can only be known after a measurement. As N. David Mermin puts it:It is a fundamental quantum doctrine that a measurement does not, in general, reveal a pre-existing value of the measured property. On the contrary, the outcome of a measurement is brought into being by the act of measurement itself... In other words, whereas a hidden-variable theory would imply intrinsic particle properties, in quantum mechanics an electron has no definite position and velocity to even be revealed. History "God does not play dice" In June 1926, Max Born published a paper, in which he was the first to clearly enunciate the probabilistic interpretation of the quantum wave function, which had been introduced by Erwin Schrödinger earlier in the year. Born concluded the paper as follows:Here the whole problem of determinism comes up. From the standpoint of our quantum mechanics there is no quantity which in any individual case causally fixes the consequence of the collision; but also experimentally we have so far no reason to believe that there are some inner properties of the atom which conditions a definite outcome for the collision. Ought we to hope later to discover such properties ... and determine them in individual cases? Or ought we to believe that the agreement of theory and experiment—as to the impossibility of prescribing conditions for a causal evolution—is a pre-established harmony founded on the nonexistence of such conditions? I myself am inclined to give up determinism in the world of atoms. But that is a philosophical question for which physical arguments alone are not decisive.Born's interpretation of the wave function was criticized by Schrödinger, who had previously attempted to interpret it in real physical terms, but Albert Einstein's response became one of the earliest and most famous assertions that quantum mechanics is incomplete:Quantum mechanics is very worthy of respect. But an inner voice tells me this is not the genuine article after all. The theory delivers much but it hardly brings us closer to the Old One's secret. In any event, I am convinced that He is not playing dice.Niels Bohr reportedly replied to Einstein's later expression of this sentiment by advising him to "stop telling God what to do." Early attempts at hidden-variable theories Shortly after making his famous "God does not play dice" comment, Einstein attempted to formulate a deterministic counter proposal to quantum mechanics, presenting a paper at a meeting of the Academy of Sciences in Berlin, on 5 May 1927, titled "Bestimmt Schrödinger's Wellenmechanik die Bewegung eines Systems vollständig oder nur im Sinne der Statistik?" ("Does Schrödinger's wave mechanics determine the motion of a system completely or only in the statistical sense?"). However, as the paper was being prepared for publication in the academy's journal, Einstein decided to withdraw it, possibly because he discovered that, contrary to his intention, his use of Schrödinger's field to guide localized particles allowed just the kind of non-local influences he intended to avoid. At the Fifth Solvay Congress, held in Belgium in October 1927 and attended by all the major theoretical physicists of the era, Louis de Broglie presented his own version of a deterministic hidden-variable theory, apparently unaware of Einstein's aborted attempt earlier in the year. In his theory, every particle had an associated, hidden "pilot wave" which served to guide its trajectory through space. The theory was subject to criticism at the Congress, particularly by Wolfgang Pauli, which de Broglie did not adequately answer; de Broglie abandoned the theory shortly thereafter. Declaration of completeness of quantum mechanics, and the Bohr–Einstein debates Also at the Fifth Solvay Congress, Max Born and Werner Heisenberg made a presentation summarizing the recent tremendous theoretical development of quantum mechanics. At the conclusion of the presentation, they declared:[W]hile we consider ... a quantum mechanical treatment of the electromagnetic field ... as not yet finished, we consider quantum mechanics to be a closed theory, whose fundamental physical and mathematical assumptions are no longer susceptible of any modification... On the question of the 'validity of the law of causality' we have this opinion: as long as one takes into account only experiments that lie in the domain of our currently acquired physical and quantum mechanical experience, the assumption of indeterminism in principle, here taken as fundamental, agrees with experience.Although there is no record of Einstein responding to Born and Heisenberg during the technical sessions of the Fifth Solvay Congress, he did challenge the completeness of quantum mechanics at various times. In his tribute article for Born's retirement he discussed the quantum representation of a macroscopic ball bouncing elastically between rigid barriers. He argues that such a quantum representation does not represent a specific ball, but "time ensemble of systems". As such the representation is correct, but incomplete because it does not represent the real individual macroscopic case. Einstein considered quantum mechanics incomplete "because the state function, in general, does not even describe the individual event/system". Von Neumann's proof John von Neumann in his 1932 book Mathematical Foundations of Quantum Mechanics had presented a proof that there could be no "hidden parameters" in quantum mechanics. The validity of von Neumann's proof was questioned by Grete Hermann in 1935, who found a flaw in the proof. The critical issue concerned averages over ensembles. Von Neumann assumed that a relation between the expected values of different observable quantities holds for each possible value of the "hidden parameters", rather than only for a statistical average over them. However Hermann's work went mostly unnoticed until its rediscovery by John Stewart Bell more than 30 years later. The validity and definitiveness of von Neumann's proof were also questioned by Hans Reichenbach, and possibly in conversation though not in print by Albert Einstein. Reportedly, in a conversation circa 1938 with his assistants Peter Bergmann and Valentine Bargmann, Einstein pulled von Neumann's book off his shelf, pointed to the same assumption critiqued by Hermann and Bell, and asked why one should believe in it. Simon Kochen and Ernst Specker rejected von Neumann's key assumption as early as 1961, but did not publish a criticism of it until 1967. EPR paradox Einstein argued that quantum mechanics could not be a complete theory of physical reality. He wrote, Consider a mechanical system consisting of two partial systems A and B which interact with each other only during a limited time. Let the ψ function [i.e., wavefunction] before their interaction be given. Then the Schrödinger equation will furnish the ψ function after the interaction has taken place. Let us now determine the physical state of the partial system A as completely as possible by measurements. Then quantum mechanics allows us to determine the ψ function of the partial system B from the measurements made, and from the ψ function of the total system. This determination, however, gives a result which depends upon which of the physical quantities (observables) of A have been measured (for instance, coordinates or momenta). Since there can be only one physical state of B after the interaction which cannot reasonably be considered to depend on the particular measurement we perform on the system A separated from B it may be concluded that the ψ function is not unambiguously coordinated to the physical state. This coordination of several ψ functions to the same physical state of system B shows again that the ψ function cannot be interpreted as a (complete) description of a physical state of a single system. Together with Boris Podolsky and Nathan Rosen, Einstein published a paper that gave a related but distinct argument against the completeness of quantum mechanics. They proposed a thought experiment involving a pair of particles prepared in what would later become known as an entangled state. Einstein, Podolsky, and Rosen pointed out that, in this state, if the position of the first particle were measured, the result of measuring the position of the second particle could be predicted. If instead the momentum of the first particle were measured, then the result of measuring the momentum of the second particle could be predicted. They argued that no action taken on the first particle could instantaneously affect the other, since this would involve information being transmitted faster than light, which is impossible according to the theory of relativity. They invoked a principle, later known as the "EPR criterion of reality", positing that: "If, without in any way disturbing a system, we can predict with certainty (i.e., with probability equal to unity) the value of a physical quantity, then there exists an element of reality corresponding to that quantity." From this, they inferred that the second particle must have a definite value of both position and of momentum prior to either quantity being measured. But quantum mechanics considers these two observables incompatible and thus does not associate simultaneous values for both to any system. Einstein, Podolsky, and Rosen therefore concluded that quantum theory does not provide a complete description of reality. Bohr answered the Einstein–Podolsky–Rosen challenge as follows: [The argument of] Einstein, Podolsky and Rosen contains an ambiguity as regards the meaning of the expression "without in any way disturbing a system." ... [E]ven at this stage [i.e., the measurement of, for example, a particle that is part of an entangled pair], there is essentially the question of an influence on the very conditions which define the possible types of predictions regarding the future behavior of the system. Since these conditions constitute an inherent element of the description of any phenomenon to which the term "physical reality" can be properly attached, we see that the argumentation of the mentioned authors does not justify their conclusion that quantum-mechanical description is essentially incomplete." Bohr is here choosing to define a "physical reality" as limited to a phenomenon that is immediately observable by an arbitrarily chosen and explicitly specified technique, using his own special definition of the term 'phenomenon'. He wrote in 1948: As a more appropriate way of expression, one may strongly advocate limitation of the use of the word phenomenon to refer exclusively to observations obtained under specified circumstances, including an account of the whole experiment. This was, of course, in conflict with the EPR criterion of reality. Bell's theorem In 1964, John Stewart Bell showed through his famous theorem that if local hidden variables exist, certain experiments could be performed involving quantum entanglement where the result would satisfy a Bell inequality. If, on the other hand, statistical correlations resulting from quantum entanglement could not be explained by local hidden variables, the Bell inequality would be violated. Another no-go theorem concerning hidden-variable theories is the Kochen–Specker theorem. Physicists such as Alain Aspect and Paul Kwiat have performed experiments that have found violations of these inequalities up to 242 standard deviations. This rules out local hidden-variable theories, but does not rule out non-local ones. Theoretically, there could be experimental problems that affect the validity of the experimental findings. Gerard 't Hooft has disputed the validity of Bell's theorem on the basis of the superdeterminism loophole and proposed some ideas to construct local deterministic models. Bohm's hidden-variable theory In 1952, David Bohm proposed a hidden variable theory. Bohm unknowingly rediscovered (and extended) the idea that Louis de Broglie's pilot wave theory had proposed in 1927 (and abandoned) – hence this theory is commonly called "de Broglie-Bohm theory". Assuming the validity of Bell's theorem, any deterministic hidden-variable theory that is consistent with quantum mechanics would have to be non-local, maintaining the existence of instantaneous or faster-than-light relations (correlations) between physically separated entities. Bohm posited both the quantum particle, e.g. an electron, and a hidden 'guiding wave' that governs its motion. Thus, in this theory electrons are quite clearly particles. When a double-slit experiment is performed, the electron goes through either one of the slits. Also, the slit passed through is not random but is governed by the (hidden) pilot wave, resulting in the wave pattern that is observed. In Bohm's interpretation, the (non-local) quantum potential constitutes an implicate (hidden) order which organizes a particle, and which may itself be the result of yet a further implicate order: a superimplicate order which organizes a field. Nowadays Bohm's theory is considered to be one of many interpretations of quantum mechanics. Some consider it the simplest theory to explain quantum phenomena. Nevertheless, it is a hidden-variable theory, and necessarily so. The major reference for Bohm's theory today is his book with Basil Hiley, published posthumously. A possible weakness of Bohm's theory is that some (including Einstein, Pauli, and Heisenberg) feel that it looks contrived. (Indeed, Bohm thought this of his original formulation of the theory.) Bohm said he considered his theory to be unacceptable as a physical theory due to the guiding wave's existence in an abstract multi-dimensional configuration space, rather than three-dimensional space. Recent developments In August 2011, Roger Colbeck and Renato Renner published a proof that any extension of quantum mechanical theory, whether using hidden variables or otherwise, cannot provide a more accurate prediction of outcomes, assuming that observers can freely choose the measurement settings. Colbeck and Renner write: "In the present work, we have ... excluded the possibility that any extension of quantum theory (not necessarily in the form of local hidden variables) can help predict the outcomes of any measurement on any quantum state. In this sense, we show the following: under the assumption that measurement settings can be chosen freely, quantum theory really is complete". In January 2013, Giancarlo Ghirardi and Raffaele Romano described a model which, "under a different free choice assumption [...] violates [the statement by Colbeck and Renner] for almost all states of a bipartite two-level system, in a possibly experimentally testable way". See also References Bibliography Peres, Asher; Zurek, Wojciech (1982). "Is quantum theory universally valid?". American Journal of Physics. 50 (9): 807–810. Bibcode:1982AmJPh..50..807P. doi:10.1119/1.13086. Jammer, Max (1985). "The EPR Problem in Its Historical Development". In Lahti, P.; Mittelstaedt, P. (eds.). Symposium on the Foundations of Modern Physics: 50 years of the Einstein–Podolsky–Rosen Gedankenexperiment. Singapore: World Scientific. pp. 129–149. Fine, Arthur (1986). The Shaky Game: Einstein Realism and the Quantum Theory. Chicago: University of Chicago Press. Quantum networks form an important element of quantum computing and quantum communication systems. Quantum networks facilitate the transmission of information in the form of quantum bits, also called qubits, between physically separated quantum processors. A quantum processor is a machine able to perform quantum circuits on a certain number of qubits. Quantum networks work in a similar way to classical networks. The main difference is that quantum networking, like quantum computing, is better at solving certain problems, such as modeling quantum systems. Basics Quantum networks for computation Networked quantum computing or distributed quantum computing works by linking multiple quantum processors through a quantum network by sending qubits in between them. Doing this creates a quantum computing cluster and therefore creates more computing potential. Less powerful computers can be linked in this way to create one more powerful processor. This is analogous to connecting several classical computers to form a computer cluster in classical computing. Like classical computing, this system is scalable by adding more and more quantum computers to the network. Currently quantum processors are only separated by short distances. Quantum networks for communication In the realm of quantum communication, one wants to send qubits from one quantum processor to another over long distances. This way, local quantum networks can be intra connected into a quantum internet. A quantum internet supports many applications, which derive their power from the fact that by creating quantum entangled qubits, information can be transmitted between the remote quantum processors. Most applications of a quantum internet require only very modest quantum processors. For most quantum internet protocols, such as quantum key distribution in quantum cryptography, it is sufficient if these processors are capable of preparing and measuring only a single qubit at a time. This is in contrast to quantum computing where interesting applications can be realized only if the (combined) quantum processors can easily simulate more qubits than a classical computer (around 60). Quantum internet applications require only small quantum processors, often just a single qubit, because quantum entanglement can already be realized between just two qubits. A simulation of an entangled quantum system on a classical computer cannot simultaneously provide the same security and speed. Overview of the elements of a quantum network The basic structure of a quantum network and more generally a quantum internet is analogous to a classical network. First, we have end nodes on which applications are ultimately run. These end nodes are quantum processors of at least one qubit. Some applications of a quantum internet require quantum processors of several qubits as well as a quantum memory at the end nodes. Second, to transport qubits from one node to another, we need communication lines. For the purpose of quantum communication, standard telecom fibers can be used. For networked quantum computing, in which quantum processors are linked at short distances, different wavelengths are chosen depending on the exact hardware platform of the quantum processor. Third, to make maximum use of communication infrastructure, one requires optical switches capable of delivering qubits to the intended quantum processor. These switches need to preserve quantum coherence, which makes them more challenging to realize than standard optical switches. Finally, one requires a quantum repeater to transport qubits over long distances. Repeaters appear in between end nodes. Since qubits cannot be copied (No-cloning theorem), classical signal amplification is not possible. By necessity, a quantum repeater works in a fundamentally different way than a classical repeater. Elements of a quantum network End nodes: quantum processors End nodes can both receive and emit information. Telecommunication lasers and parametric down-conversion combined with photodetectors can be used for quantum key distribution. In this case, the end nodes can in many cases be very simple devices consisting only of beamsplitters and photodetectors. However, for many protocols more sophisticated end nodes are desirable. These systems provide advanced processing capabilities and can also be used as quantum repeaters. Their chief advantage is that they can store and retransmit quantum information without disrupting the underlying quantum state. The quantum state being stored can either be the relative spin of an electron in a magnetic field or the energy state of an electron. They can also perform quantum logic gates. One way of realizing such end nodes is by using color centers in diamond, such as the nitrogen-vacancy center. This system forms a small quantum processor featuring several qubits. NV centers can be utilized at room temperatures. Small scale quantum algorithms and quantum error correction has already been demonstrated in this system, as well as the ability to entangle two and three quantum processors, and perform deterministic quantum teleportation. Another possible platform are quantum processors based on ion traps, which utilize radio-frequency magnetic fields and lasers. In a multispecies trapped-ion node network, photons entangled with a parent atom are used to entangle different nodes. Also, cavity quantum electrodynamics (Cavity QED) is one possible method of doing this. In Cavity QED, photonic quantum states can be transferred to and from atomic quantum states stored in single atoms contained in optical cavities. This allows for the transfer of quantum states between single atoms using optical fiber in addition to the creation of remote entanglement between distant atoms. Communication lines: physical layer Over long distances, the primary method of operating quantum networks is to use optical networks and photon-based qubits. This is due to optical networks having a reduced chance of decoherence. Optical networks have the advantage of being able to re-use existing optical fiber. Alternately, free space networks can be implemented that transmit quantum information through the atmosphere or through a vacuum. Fiber optic networks Optical networks using existing telecommunication fiber can be implemented using hardware similar to existing telecommunication equipment. This fiber can be either single-mode or multi-mode, with single-mode allowing for more precise communication. At the sender, a single photon source can be created by heavily attenuating a standard telecommunication laser such that the mean number of photons per pulse is less than 1. For receiving, an avalanche photodetector can be used. Various methods of phase or polarization control can be used such as interferometers and beam splitters. In the case of entanglement based protocols, entangled photons can be generated through spontaneous parametric down-conversion. In both cases, the telecom fiber can be multiplexed to send non-quantum timing and control signals. In 2020 a team of researchers affiliated with several institutions in China has succeeded in sending entangled quantum memories over a 50-kilometer coiled fiber cable. Free space networks Free space quantum networks operate similar to fiber optic networks but rely on line of sight between the communicating parties instead of using a fiber optic connection. Free space networks can typically support higher transmission rates than fiber optic networks and do not have to account for polarization scrambling caused by optical fiber. However, over long distances, free space communication is subject to an increased chance of environmental disturbance on the photons. Free space communication is also possible from a satellite to the ground. A quantum satellite capable of entanglement distribution over a distance of 1,203 km has been demonstrated. The experimental exchange of single photons from a global navigation satellite system at a slant distance of 20,000 km has also been reported. These satellites can play an important role in linking smaller ground-based networks over larger distances. In free-space networks, atmospheric conditions such as turbulence, scattering, and absorption present challenges that affect the fidelity of transmitted quantum states. To mitigate these effects, researchers employ adaptive optics, advanced modulation schemes, and error correction techniques. The resilience of QKD protocols against eavesdropping plays a crucial role in ensuring the security of the transmitted data. Specifically, protocols like BB84 and decoy-state schemes have been adapted for free-space environments to improve robustness against potential security vulnerabilities. Repeaters Long-distance communication is hindered by the effects of signal loss and decoherence inherent to most transport mediums such as optical fiber. In classical communication, amplifiers can be used to boost the signal during transmission, but in a quantum network amplifiers cannot be used since qubits cannot be copied – known as the no-cloning theorem. That is, to implement an amplifier, the complete state of the flying qubit would need to be determined, something which is both unwanted and impossible. Trusted repeaters An intermediary step which allows the testing of communication infrastructure are trusted repeaters. Importantly, a trusted repeater cannot be used to transmit qubits over long distances. Instead, a trusted repeater can only be used to perform quantum key distribution with the additional assumption that the repeater is trusted. Consider two end nodes A and B, and a trusted repeater R in the middle. A and R now perform quantum key distribution to generate a key k A R {\displaystyle k_{AR}} . Similarly, R and B run quantum key distribution to generate a key k R B {\displaystyle k_{RB}} . A and B can now obtain a key k A B {\displaystyle k_{AB}} between themselves as follows: A sends k A B {\displaystyle k_{AB}} to R encrypted with the key k A R {\displaystyle k_{AR}} . R decrypts to obtain k A B {\displaystyle k_{AB}} . R then re-encrypts k A B {\displaystyle k_{AB}} using the key k R B {\displaystyle k_{RB}} and sends it to B. B decrypts to obtain k A B {\displaystyle k_{AB}} . A and B now share the key k A B {\displaystyle k_{AB}} . The key is secure from an outside eavesdropper, but clearly the repeater R also knows k A B {\displaystyle k_{AB}} . This means that any subsequent communication between A and B does not provide end to end security, but is only secure as long as A and B trust the repeater R. Quantum repeaters A true quantum repeater allows the end to end generation of quantum entanglement, and thus – by using quantum teleportation – the end to end transmission of qubits. In quantum key distribution protocols one can test for such entanglement. This means that when making encryption keys, the sender and receiver are secure even if they do not trust the quantum repeater. Any other application of a quantum internet also requires the end to end transmission of qubits, and thus a quantum repeater. Quantum repeaters allow entanglement and can be established at distant nodes without physically sending an entangled qubit the entire distance. In this case, the quantum network consists of many short distance links of perhaps tens or hundreds of kilometers. In the simplest case of a single repeater, two pairs of entangled qubits are established: | A ⟩ {\displaystyle |A\rangle } and | R a ⟩ {\displaystyle |R_{a}\rangle } located at the sender and the repeater, and a second pair | R b ⟩ {\displaystyle |R_{b}\rangle } and | B ⟩ {\displaystyle |B\rangle } located at the repeater and the receiver. These initial entangled qubits can be easily created, for example through parametric down conversion, with one qubit physically transmitted to an adjacent node. At this point, the repeater can perform a Bell measurement on the qubits | R a ⟩ {\displaystyle |R_{a}\rangle } and | R b ⟩ {\displaystyle |R_{b}\rangle } thus teleporting the quantum state of | R a ⟩ {\displaystyle |R_{a}\rangle } onto | B ⟩ {\displaystyle |B\rangle } . This has the effect of "swapping" the entanglement such that | A ⟩ {\displaystyle |A\rangle } and | B ⟩ {\displaystyle |B\rangle } are now entangled at a distance twice that of the initial entangled pairs. It can be seen that a network of such repeaters can be used linearly or in a hierarchical fashion to establish entanglement over great distances. Hardware platforms suitable as end nodes above can also function as quantum repeaters. However, there are also hardware platforms specific only to the task of acting as a repeater, without the capabilities of performing quantum gates. Error correction Error correction can be used in quantum repeaters. Due to technological limitations, however, the applicability is limited to very short distances as quantum error correction schemes capable of protecting qubits over long distances would require an extremely large amount of qubits and hence extremely large quantum computers. Errors in communication can be broadly classified into two types: Loss errors (due to optical fiber/environment) and operation errors (such as depolarization, dephasing etc.). While redundancy can be used to detect and correct classical errors, redundant qubits cannot be created due to the no-cloning theorem. As a result, other types of error correction must be introduced such as the Shor code or one of a number of more general and efficient codes. All of these codes work by distributing the quantum information across multiple entangled qubits so that operation errors as well as loss errors can be corrected. In addition to quantum error correction, classical error correction can be employed by quantum networks in special cases such as quantum key distribution. In these cases, the goal of the quantum communication is to securely transmit a string of classical bits. Traditional error correction codes such as Hamming codes can be applied to the bit string before encoding and transmission on the quantum network. Entanglement purification Quantum decoherence can occur when one qubit from a maximally entangled bell state is transmitted across a quantum network. Entanglement purification allows for the creation of nearly maximally entangled qubits from a large number of arbitrary weakly entangled qubits, and thus provides additional protection against errors. Entanglement purification (also known as Entanglement distillation) has already been demonstrated in Nitrogen-vacancy centers in diamond. Applications A quantum internet supports numerous applications, enabled by quantum entanglement. In general, quantum entanglement is well suited for tasks that require coordination, synchronization or privacy. Examples of such applications include quantum key distribution, clock stabilization, protocols for distributed system problems such as leader election or Byzantine agreement, extending the baseline of telescopes, as well as position verification, secure identification and two-party cryptography in the noisy-storage model. A quantum internet also enables secure access to a quantum computer in the cloud. Specifically, a quantum internet enables very simple quantum devices to connect to a remote quantum computer in such a way that computations can be performed there without the quantum computer finding out what this computation actually is (the input and output quantum states can not be measured without destroying the computation, but the circuit composition used for the calculation will be known). Secure communications When it comes to communicating in any form the largest issue has always been keeping these communications private. Quantum networks would allow for information to be created, stored and transmitted, potentially achieving "a level of privacy, security and computational clout that is impossible to achieve with today’s Internet." By applying a quantum operator that the user selects to a system of information the information can then be sent to the receiver without a chance of an eavesdropper being able to accurately be able to record the sent information without either the sender or receiver knowing. Unlike classical information that is transmitted in bits and assigned either a 0 or 1 value, the quantum information used in quantum networks uses quantum bits (qubits), which can have both 0 and 1 value at the same time, being in a state of superposition. This works because if a listener tries to listen in then they will change the information in an unintended way by listening, thereby tipping their hand to the people on whom they are attacking. Secondly, without the proper quantum operator to decode the information they will corrupt the sent information without being able to use it themselves. Furthermore, qubits can be encoded in a variety of materials, including in the polarization of photons or the spin states of electrons. Current status Quantum internet One example of a prototype quantum communication network is the eight-user city-scale quantum network described in a paper published in September 2020. The network located in Bristol used already deployed fibre-infrastructure and worked without active switching or trusted nodes. In 2022, Researchers at the University of Science and Technology of China and Jinan Institute of Quantum Technology demonstrated quantum entanglement between two memory devices located at 12.5 km apart from each other within an urban environment. In the same year, Physicist at the Delft University of Technology in Netherlands has taken a significant step toward the network of the future by using a technique called quantum teleportation that sends data to three physical locations which was previously only possible with two locations. In 2024, researchers in the U.K and Germany achieved a first by producing, storing, and retrieving quantum information. This milestone involved interfacing a quantum dot light source and a quantum memory system, paving the way for practical applications despite challenges like quantum information loss over long distances. In February 2025, researchers from Oxford University experimentally demonstrated the distribution of quantum computations between two photonically interconnected trapped-ion modules. Each module contained dedicated network and circuit qubits, and they were separated by approximately two meters. The team achieved deterministic teleportation of a controlled-Z gate between two circuit qubits located in separate modules, attaining an 86% fidelity. This experiment also marked the first implementation of a distributed quantum algorithm comprising multiple non-local two-qubit gates, specifically Grover's search algorithm, which was executed with a 71% success rate. These advancements represented significant progress toward scalable quantum computing and the development of a quantum internet. Quantum networks for computation In 2021, researchers at the Max Planck Institute of Quantum Optics in Germany reported a first prototype of quantum logic gates for distributed quantum computers. Experimental quantum modems A research team at the Max-Planck-Institute of Quantum Optics in Garching, Germany is finding success in transporting quantum data from flying and stable qubits via infrared spectrum matching. This requires a sophisticated, super-cooled yttrium silicate crystal to sandwich erbium in a mirrored environment to achieve resonance matching of infrared wavelengths found in fiber optic networks. The team successfully demonstrated the device works without data loss. Mobile quantum networks In 2021, researchers in China reported the successful transmission of entangled photons between drones, used as nodes for the development of mobile quantum networks or flexible network extensions. This could be the first work in which entangled particles were sent between two moving devices. Also, it has been researched the application of quantum communications to improve 6G mobile networks for joint detection and data transfer with quantum entanglement, where there are possible advantages such as security and energy efficiency. Quantum key distribution networks Several test networks have been deployed that are tailored to the task of quantum key distribution either at short distances (but connecting many users), or over larger distances by relying on trusted repeaters. These networks do not yet allow for the end to end transmission of qubits or the end to end creation of entanglement between far away nodes. DARPA Quantum Network Starting in the early 2000s, DARPA began sponsorship of a quantum network development project with the aim of implementing secure communication. The DARPA Quantum Network became operational within the BBN Technologies laboratory in late 2003 and was expanded further in 2004 to include nodes at Harvard and Boston Universities. The network consists of multiple physical layers including fiber optics supporting phase-modulated lasers and entangled photons as well free-space links. SECOQC Vienna QKD network From 2003 to 2008 the Secure Communication based on Quantum Cryptography (SECOQC) project developed a collaborative network between a number of European institutions. The architecture chosen for the SECOQC project is a trusted repeater architecture which consists of point-to-point quantum links between devices where long distance communication is accomplished through the use of repeaters. Chinese hierarchical network In May 2009, a hierarchical quantum network was demonstrated in Wuhu, China. The hierarchical network consists of a backbone network of four nodes connecting a number of subnets. The backbone nodes are connected through an optical switching quantum router. Nodes within each subnet are also connected through an optical switch and are connected to the backbone network through a trusted relay. Geneva area network (SwissQuantum) The SwissQuantum network developed and tested between 2009 and 2011 linked facilities at CERN with the University of Geneva and hepia in Geneva. The SwissQuantum program focused on transitioning the technologies developed in the SECOQC and other research quantum networks into a production environment. In particular the integration with existing telecommunication networks, and its reliability and robustness. Tokyo QKD network In 2010, a number of organizations from Japan and the European Union setup and tested the Tokyo QKD network. The Tokyo network build upon existing QKD technologies and adopted a SECOQC like network architecture. For the first time, one-time-pad encryption was implemented at high enough data rates to support popular end-user application such as secure voice and video conferencing. Previous large-scale QKD networks typically used classical encryption algorithms such as AES for high-rate data transfer and use the quantum-derived keys for low rate data or for regularly re-keying the classical encryption algorithms. Beijing-Shanghai Trunk Line In September 2017, a 2000-km quantum key distribution network between Beijing and Shanghai, China, was officially opened. This trunk line will serve as a backbone connecting quantum networks in Beijing, Shanghai, Jinan in Shandong province and Hefei in Anhui province. During the opening ceremony, two employees from the Bank of Communications completed a transaction from Shanghai to Beijing using the network. The State Grid Corporation of China is also developing a managing application for the link. The line uses 32 trusted nodes as repeaters. A quantum telecommunication network has been also put into service in Wuhan, capital of central China's Hubei Province, which will be connected to the trunk. Other similar city quantum networks along the Yangtze River are planned to follow. In 2021, researchers working on this network of networks reported that they combined over 700 optical fibers with two QKD-ground-to-satellite links using a trusted relay structure for a total distance between nodes of up to ~4,600 km, which makes it Earth's largest integrated quantum communication network. IQNET IQNET (Intelligent Quantum Networks and Technologies) was founded in 2017 by Caltech and AT&T. Together, they are collaborating with the Fermi National Accelerator Laboratory, and the Jet Propulsion Laboratory. In December 2020, IQNET published a work in PRX Quantum that reported a successful teleportation of time-bin qubits across 44 km of fiber. For the first time, the published work includes a theoretical modelling of the experimental setup. The two test beds for performed measurements were the Caltech Quantum Network and the Fermilab Quantum Network. This research represents an important step in establishing a quantum internet of the future, which would revolutionise the fields of secure communication, data storage, precision sensing, and computing. See also Quantum mechanics Quantum computer Quantum bus References Further reading External links https://web.archive.org/web/20090716121402/http://itvibe.com/news/2583/ http://www.vnunet.com/vnunet/news/2125164/first-quantum-computr-network-goes-online Elliott, Chip (2004). "The DARPA Quantum Network". arXiv:quant-ph/0412029. http://www.cse.wustl.edu/~jain/cse571-07/ftp/quantum/ https://web.archive.org/web/20141229113448/http://www.ipod.org.uk/reality/reality_quantum_entanglement.asp The Wigner–Araki–Yanase theorem, also known as the WAY theorem, is a result in quantum physics establishing that the presence of a conservation law limits the accuracy with which observables that fail to commute with the conserved quantity can be measured. It is named for the physicists Eugene Wigner, Huzihiro Araki and Mutsuo Yanase. The theorem can be illustrated with a particle coupled to a measuring apparatus.: 421 If the position operator of the particle is q {\displaystyle q} and its momentum operator is p {\displaystyle p} , and if the position and momentum of the apparatus are Q {\displaystyle Q} and P {\displaystyle P} respectively, assuming that the total momentum p + P {\displaystyle p+P} is conserved implies that, in a suitably quantified sense, the particle's position itself cannot be measured. The measurable quantity is its position relative to the measuring apparatus, represented by the operator q − Q {\displaystyle q-Q} . The Wigner–Araki–Yanase theorem generalizes this to the case of two arbitrary observables A {\displaystyle A} and B {\displaystyle B} for the system and an observable C {\displaystyle C} for the apparatus, satisfying the condition that B + C {\displaystyle B+C} is conserved. Mikko Tukiainen gave a generalized version of the WAY theorem, which makes no use of conservation laws, but uses quantum incompatibility instead. Yui Kuramochi and Hiroyasu Tajima proved a generalized form of the theorem for possibly unbounded and continuous conserved observables. == References == In theoretical physics, Nielsen–Olesen string is a one-dimensional object or equivalently a classical solution of certain equations of motion. The solution does not depend on the direction along the string; the dependence on the other two, transverse dimensions is identical as in the case of a Nielsen–Olesen vortex. == References == Carlo Rovelli (born 3 May 1956) is an Italian theoretical physicist and writer who has worked in Italy, the United States, France, and Canada. He is currently Emeritus Professor at the Centre de Physique Theorique of Marseille in France, a Distinguished Visiting Research Chair at the Perimeter Institute, core member of the Rotman Institute of Philosophy of Western University in Canada, and Fractal Faculty of the Santa Fe Institute in The United States. Rovelli works mainly in the field of quantum gravity and is a founder of the theory of loop quantum gravity. He has also worked in the history and philosophy of science, formulating the relational quantum mechanics and the notion of thermal time. He collaborates with several Italian newspapers, including the cultural supplements of the Corriere della Sera, Il Sole 24 Ore, and La Repubblica. His popular science book, Seven Brief Lessons on Physics, was originally published in Italian in 2014. It has sold over two million copies worldwide. In 2019, he was included by Foreign Policy magazine in the list of the 100 most influential global thinkers. In 2021, he was included by Prospect magazine in the list of the 50 world's top thinkers. Life and career Carlo Rovelli was born in Verona, Italy, on 3 May 1956. He attended the Liceo Classico Scipione Maffei in Verona. In the 1970s, he participated in the student political movements in Italian universities. He was involved with the free political radio stations Radio Alice in Bologna and Radio Anguana in Verona, which he helped found. In conjunction with his political activity, he was charged, but later released, for crimes of opinion related to the book Fatti Nostri, which he co-authored with Enrico Palandri, Maurizio Torrealta, and Claudio Piersanti. Rovelli has credited his use of LSD at this time with sparking his interest in theoretical physics, saying of his experience: "it was an extraordinarily strong experience that touched me also intellectually... Among the strange phenomena was the sense of time stopping. Things were happening in my mind but the clock was not going ahead; the flow of time was not passing any more... And I thought: ‘Well, it's a chemical that is changing things in my brain. But how do I know that the usual perception is right, and this is wrong? If these two ways of perceiving are so different, what does it mean that one is the correct one?" In 1981, Rovelli graduated with a BS/MS in physics from the University of Bologna, and in 1986 he obtained his PhD at the University of Padova, Italy. Rovelli refused military service, which was compulsory in Italy at the time, and was therefore briefly detained in 1977. He held postdoctoral positions at the University of Rome, the International School for Advanced Studies in Trieste, and Yale University. Rovelli was on the faculty of the University of Pittsburgh from 1990 to 2000, where he was also affiliated with the Department of History and Philosophy of Science. Since 2000 he has been a professor at the Centre de Physique Théorique de Luminy of Aix-Marseille University in France. Main contributions Loop quantum gravity In 1988, Rovelli, Lee Smolin and Abhay Ashtekar introduced a theory of quantum gravity called loop quantum gravity. In 1995, Rovelli and Smolin obtained a basis of states of quantum gravity, labelled by Penrose's spin networks, and using this basis they were able to show that the theory predicts that area and volume are quantized. This result indicates the existence of a discrete structure of space on a very small scale. In 1997, Rovelli and Michael Reisenberger introduced a "sum over surfaces" formulation of the theory, which has since evolved into the currently covariant "spin foam" version of loop quantum gravity. In 2008, in collaboration with Jonathan Engle and Roberto Pereira, he has introduced the spin foam vertex amplitude which is the basis of the current definition of the loop quantum gravity covariant dynamics. Loop theory is today considered a candidate for a quantum theory of gravity. It finds applications in quantum cosmology, spinfoam cosmology and quantum black hole physics. Physics without time In his 2004 book, Quantum Gravity, Rovelli developed a formulation of classical and quantum mechanics that does not make explicit reference to the notion of time. The first step towards a theory of quantum gravity without a time variable is described by Wheeler–DeWitt equation. The timeless formalism is used to describe the world in the regimes where the quantum properties of the gravitational field cannot be disregarded. This is because the quantum fluctuation of spacetime itself makes the notion of time unsuitable for writing physical laws in the conventional form of evolution laws in time. This position led him to face the following problem: if time is not part of the fundamental theory of the world, then how does time emerge? In 1993, in collaboration with Alain Connes, Rovelli proposed a solution to this problem called the thermal time hypothesis. According to this hypothesis, time emerges only in a thermodynamic or statistical context. If this is correct, the flow of time is not fundamental, deriving from the incompleteness of knowledge. Similar conclusions had been reached earlier in the context of nonequilibrium statistical mechanics, in particular in the work of Robert Zwanzig, and in Caldeira-Leggett models used in quantum dissipation. Relational quantum mechanics In 1994, Rovelli introduced the relational interpretation of quantum mechanics, based on the idea that the quantum state of a system must always be interpreted relative to another physical system (like the "velocity of an object" is always relative to another object, in classical mechanics). The idea has been developed and analyzed in particular by Bas van Fraassen and by Michel Bitbol. Among other important consequences, it provides a solution of the EPR paradox that does not violate locality. Rovelli has expressed the main idea of relational quantum mechanics in the popular book Helgoland. Relative information Rovelli won the second prize in the 2013 FQXi contest "It From Bit or Bit From It?" for his essay about "relative information". His paper, Relative Information at the Foundation of Physics, discusses how "Shannon's notion of relative information between two physical systems can function as [a] foundation for statistical mechanics and quantum mechanics, without referring to subjectivism or idealism...[This approach can] represent a key missing element in the foundation of the naturalistic picture of the world." In 2017, Rovelli elaborated further upon the subject of relative information, writing that: In nature, variables are not independent; for instance, in any magnet, the two ends have opposite polarities. Knowing one amounts to knowing the other. So we can say that each end “has information” about the other. There is nothing mental in this; it is just a way of saying that there is a necessary relation between the polarities of the two ends. We say that there is "relative information" between two systems anytime the state of one is constrained by the state of the other. In this precise sense, physical systems may be said to have information about one another, with no need for a mind to play any role. Such "relative information" is ubiquitous in nature: The colour of the light carries information about the object the light has bounced from; a virus has information about the cell it may attach, and neurons have information about one another. Since the world is a knit tangle of interacting events, it teems with relative information. When this information is exploited for survival, extensively elaborated by our brain, and may be coded in a language understood by a community, it becomes mental, and it acquires the semantic weight that we commonly attribute to the notion of information. But the basic ingredient is down there in the physical world: physical correlation between distinct variables. The physical world is not a set of self-absorbed entities that do their selfish things. It is a tightly knitted net of relative information, where everybody's state reflects somebody else's state. We understand physical, chemical, biological, social, political, astrophysical, and cosmological systems in terms of these nets of relations, not in terms of individual behaviour. Physical relative information is a powerful basic concept for describing the world. Before “energy,” “matter,” or even “entity.” History and philosophy of science Rovelli has written a book on the Greek philosopher Anaximander, published in France, Italy, US and Brazil. The book analyses the main aspects of scientific thinking and articulates Rovelli's views on science. Anaximander is presented in the book as a main initiator of scientific thinking. For Rovelli, science is a continuous process of exploring novel possible views of the world; this happens via a "learned rebellion", which always builds and relies on previous knowledge but at the same time continuously questions aspects of this received knowledge. The foundation of science, therefore, is not certainty but the very opposite, a radical uncertainty about our own knowledge, or equivalently, an acute awareness of the extent of our ignorance. Religious views Rovelli defines himself as "serenely atheist". He discussed his religious views in several articles and in his book on Anaximander. He argues that the conflict between rational/scientific thinking and structured religion may find periods of truce ("there is no contradiction between solving Maxwell's equations and believing that God created Heaven and Earth"), but it is ultimately unsolvable, because most religions demand the acceptance of some unquestionable truths, while scientific thinking is based on the continuous questioning of any truth. Thus, for Rovelli, the source of the conflict is not the pretense of science to give answers – for Rovelli, the universe is full of mystery and a source of awe and emotions – but, on the contrary, the source of the conflict is the acceptance of our ignorance at the foundation of science, which clashes with religions' pretense to be depositories of certain knowledge. Political engagement, pacifism, and controversies Rovelli's first book was on the Italian student political movements in the 1970s. He later refused Italy's compulsory military draft and was briefly detained. In 2021, he coordinated the Global Peace Dividend, an open letter signed by more that 50 Nobel Laureates, including the Dalai Lama, calling for all countries to negotiate a balanced cut on their military spending by 2% a year for the next five years, and put half the saved money in a UN fund to combat pandemics, the climate crisis, and extreme poverty. On 1 May 2023, Rovelli gave a political speech at the large Italian Labour Day concert in Rome, inviting the youth to engage politically for the environment, economical equality and peace, and criticizing the Italian Defence Minister Guido Crosetto for what he claims was his direct involvement with what he calls the industrial military complex. The speech raised a large controversy. As a consequence, his invitation to represent Italy at the 2024 Frankfurt Book Fair was cancelled; the cancellation itself was widely criticized, leading to his re-invitation, and the resignation of the Italian Commissary for the Buchmesse. Rovelli repeated his call for reduced military spending and improved international cooperation following the outbreak of the Gaza war. That same year, he was one of the signatories of the International Peace Conference manifesto, which accuses, falsely, and without evidence, the so-called West (e.g., the United States, the European Union, and NATO) of attacking "Russia with the clear goal to destroy it as a sovereign state". The manifesto goes as far as to claim that this Western aggression was the driving force behind the Russian invasion of Ukraine. Main awards 2024 Lewis Thomas Prize for Writing About Science ("In the conflicted world of 2024, the abiding, idealistic voice of Rovelli’s essay collection There Are Places in the World Where Rules Are Less Important Than Kindness feels especially valuable.") 1995 International Xanthopoulos Award of the International Society for General Relativity and Gravitation, "for outstanding contributions to theoretical physics" Senior member of the Institut Universitaire de France Laurea Honoris Causa National University of General San Martín Honorary Professor of the Beijing Normal University in China Member of the Académie Internationale de Philosophie des Sciences Honorary member of the Accademia di Scienze Arti e Lettere di Verona 2009 First "community" prize of the FQXi contest on the "nature of time" 2013 Second prize of the FQXi contest on the "relation between physics and information" 2014 Premio Letterario Merck for the book Reality Is Not What It Seems: The Journey to Quantum Gravity 2015 Premio Pagine di Scienza di Rosignano for the book Reality Is Not What It Seems: The Journey to Quantum Gravity 2015 Premio Alassio centolibri per l’informazione culturale 2015 Premio Larderello 2015 Premio letterario Galileo per la divulgazione scientifica for the book Reality Is Not What It Seems: The Journey to Quantum Gravity In popular culture Rovelli has appeared as a Disney character in a Mickey Mouse story in the Italian Disney publication of Topolino. In November 2022 Carlo Rovelli and rock band Belladonna release the single Nothing Shines Unless It Burns. In October 2023 the song enters the Grammy Awards ballot in the Best Rock Performance category. In the science fiction novel Mars Trilogy by Kim Stanley Robinson, set in a future century, Rovelli and Lee Smolin appear as historical characters in the history of physics. In the novel, Loop quantum gravity has merged to string theory to give a comprehensive physical theory of the world. The book The Order of Time has been published in audiobook format read by the British actor Benedict Cumberbatch. In Treacle Walker (2021) Alan Garner chose a quote from Rovelli's The Order of Time (L'ordine del tempo, 2017) as the epigraph for his book. The 2023 film The Order of Time, directed by Liliana Cavani, is inspired by Rovelli's book of the same title. Rovelli collaborated with the screenwriting. Interviews on BBC radio: The BBC Radio 4 show Desert Island Discs in summer 2017. The BBC Radio 4 show The Life Scientific in 2018 (discussing his career in science). The BBC Radio 3 show Private Passions in 2020 (discussing time in music and science) The BBC Radio 4 show A Good Read in 2020 (discussing books). In 2022 Rovelli appeared in the Netflix documentary A Trip to Infinity, discussing the mathematical implications of infinity. He appeared on BBC Radio 4's The Museum of Curiosity in February 2023. His hypothetical donation to this imaginary museum was a white hole. Books and articles Rovelli has written more than 200 scientific articles published in international journals. He has published two monographs on loop quantum gravity and several popular science books. His book, Seven Brief Lessons on Physics, has been translated into 41 languages. Scientific books Quantum Gravity, Cambridge University Press, 2004, ISBN 0-521-83733-2 With Francesca Vidotto, Covariant Loop Quantum Gravity: An Elementary Introduction to Quantum Gravity and Spinfoam Theory, Cambridge University Press, 2014, ISBN 978-1107069626 Popular books Anaximander: And the Birth of Science, Penguin Random House, 2023 (republication of The First Scientist: Anaximander and his legacy, Westholme Publishing, 2011) Helgoland, Penguin Random House 2021 / Helgoland, Adelphi, 2020. There Are Places in the World Where Rules Are Less Important Than Kindness, Penguin Random House, 2020 / Ci sono luoghi al mondo dove più che le regole è importante la gentilezza, Solferino, 2020. The Order of Time, Penguin Random House, 2018 / L'ordine del tempo, Adelphi, 2017. Reality Is Not What It Seems: The Journey to Quantum Gravity, Penguin Random House, 2016 / La realtà non è come ci appare: La struttura elementare delle cose, Raffaello Cortina Editore, 2014. Seven Brief Lessons on Physics, Penguin Random House, 2015 / Sette brevi lezioni di fisica, Adelphi, 2014. Marion Lignana Rosenberg (translator), The First Scientist: Anaximander and his legacy, Westholme Publishing, 2011 / Che cos'è la Scienza. La rivoluzione di Anassimandro., Mondadori, 2012. What is time, what is space? (interview), Di Renzo Editore, 2006 / Che cos'è il tempo, che cos'é lo spazio?, Di Renzo Editore, 2004. Bologna, marzo 1977 ...fatti nostri..., a cura di e con Enrico Palandri, Claudio Piersanti, Maurizio Torrealta et alii, Verona, Bertani, 1977; Rimini, NdA press, 2007, ISBN 978-88-89035-17-7. General Relativity: The Essentials, Cambridge University Press, 2021. White Holes, Penguin Random House, 2023 References External links Homepage Introduction to Loop Quantum Gravity, online course by Carlo Rovelli. Carlo Rovelli, A Dialog on Quantum Gravity, preprint available as hep-th/0310077 Loop Quantum Gravity by Carlo Rovelli Quantum Gravity, draft of the book Quantum Gravity Curriculum Vitae et Studiorum Archived 13 August 2021 at the Wayback Machine In quantum field theory, the Klein transformation is a redefinition of the fields to amend the spin-statistics theorem. Bose–Einstein Suppose φ and χ are fields such that, if x and y are spacelike-separated points and i and j represent the spinor/tensor indices, [ φ i ( x ) , φ j ( y ) ] = [ χ i ( x ) , χ j ( y ) ] = { φ i ( x ) , χ j ( y ) } = 0. {\displaystyle [\varphi _{i}(x),\varphi _{j}(y)]=[\chi _{i}(x),\chi _{j}(y)]=\{\varphi _{i}(x),\chi _{j}(y)\}=0.} Also suppose χ is invariant under the Z2 parity (nothing to do with spatial reflections!) mapping χ to −χ but leaving φ invariant. Free field theories always satisfy this property. Then, the Z2 parity of the number of χ particles is well defined and is conserved in time. Let's denote this parity by the operator Kχ which maps χ-even states to itself and χ-odd states into their negative. Then, Kχ is involutive, Hermitian and unitary. The fields φ and χ above don't have the proper statistics relations for either a boson or a fermion. This means that they are bosonic with respect to themselves but fermionic with respect to each other. Their statistical properties, when viewed on their own, have exactly the same statistics as the Bose–Einstein statistics because: Define two new fields φ' and χ' as follows: φ ′ = i K χ φ {\displaystyle \varphi '=iK_{\chi }\varphi \,} and χ ′ = K χ χ . {\displaystyle \chi '=K_{\chi }\chi .\,} This redefinition is invertible (because Kχ is). The spacelike commutation relations become [ φ i ′ ( x ) , φ j ′ ( y ) ] = [ χ i ′ ( x ) , χ j ′ ( y ) ] = [ φ i ′ ( x ) , χ j ′ ( y ) ] = 0. {\displaystyle [\varphi '_{i}(x),\varphi '_{j}(y)]=[\chi '_{i}(x),\chi '_{j}(y)]=[\varphi '_{i}(x),\chi '_{j}(y)]=0.\,} Fermi–Dirac Consider the example where { ϕ i ( x ) , ϕ j ( y ) } = { χ i ( x ) , χ j ( y ) } = [ ϕ i ( x ) , χ j ( y ) ] = 0 {\displaystyle \{\phi ^{i}(x),\phi ^{j}(y)\}=\{\chi ^{i}(x),\chi ^{j}(y)\}=[\phi ^{i}(x),\chi ^{j}(y)]=0} (spacelike-separated as usual). Assume you have a Z2 conserved parity operator Kχ acting upon χ alone. Let ϕ ′ = i K χ ϕ {\displaystyle \phi '=iK_{\chi }\phi \,} and χ ′ = K χ χ . {\displaystyle \chi '=K_{\chi }\chi .\,} Then { ϕ ′ i ( x ) , ϕ ′ j ( y ) } = { χ ′ i ( x ) , χ ′ j ( y ) } = { ϕ ′ i ( x ) , χ ′ j ( y ) } = 0. {\displaystyle \{\phi '^{i}(x),\phi '^{j}(y)\}=\{\chi '^{i}(x),\chi '^{j}(y)\}=\{\phi '^{i}(x),\chi '^{j}(y)\}=0.} References See also Jordan–Schwinger transformation Jordan–Wigner transformation Bogoliubov–Valatin transformation Holstein–Primakoff transformation In quantum field theory and statistical mechanics, loop integrals are the integrals which appear when evaluating the Feynman diagrams with one or more loops by integrating over the internal momenta. These integrals are used to determine counterterms, which in turn allow evaluation of the beta function, which encodes the dependence of coupling g {\displaystyle g} for an interaction on an energy scale μ {\displaystyle \mu } . One-loop integral Generic formula A generic one-loop integral, for example those appearing in one-loop renormalization of QED or QCD may be written as a linear combination of terms in the form ∫ d d k ( 2 π ) d k μ 1 ⋯ k μ n ( ( k + q 1 ) 2 + m 1 2 ) ⋯ ( ( k + q b ) 2 + m b 2 ) {\displaystyle \int {\frac {d^{d}k}{(2\pi )^{d}}}{\frac {k_{\mu _{1}}\cdots k_{\mu _{n}}}{((k+q_{1})^{2}+m_{1}^{2})\cdots ((k+q_{b})^{2}+m_{b}^{2})}}} where the q i {\displaystyle q_{i}} are 4-momenta which are linear combinations of the external momenta, and the m i {\displaystyle m_{i}} are masses of interacting particles. This expression uses Euclidean signature. In Lorentzian signature the denominator would instead be a product of expressions of the form ( k + q ) 2 − m 2 + i ϵ {\displaystyle (k+q)^{2}-m^{2}+i\epsilon } . Using Feynman parametrization, this can be rewritten as a linear combination of integrals of the form ∫ d d l ( 2 π ) d l μ 1 ⋯ l μ n ( l 2 + Δ ) b , {\displaystyle \int {\frac {d^{d}l}{(2\pi )^{d}}}{\frac {l_{\mu _{1}}\cdots l_{\mu _{n}}}{(l^{2}+\Delta )^{b}}},} where the 4-vector l {\displaystyle l} and Δ {\displaystyle \Delta } are functions of the q i , m i {\displaystyle q_{i},m_{i}} and the Feynman parameters. This integral is also integrated over the domain of the Feynman parameters. The integral is an isotropic tensor and so can be written as an isotropic tensor without l {\displaystyle l} dependence (but possibly dependent on the dimension d {\displaystyle d} ), multiplied by the integral ∫ d d l ( 2 π ) d ( l 2 ) a ( l 2 + Δ ) b . {\displaystyle \int {\frac {d^{d}l}{(2\pi )^{d}}}{\frac {(l^{2})^{a}}{(l^{2}+\Delta )^{b}}}.} Note that if n {\displaystyle n} were odd, then the integral vanishes, so we can define n = 2 a {\displaystyle n=2a} . Regularizing the integral Cutoff regularization In Wilsonian renormalization, the integral is made finite by specifying a cutoff scale Λ > 0 {\displaystyle \Lambda >0} . The integral to be evaluated is then ∫ Λ d d l ( 2 π ) d ( l 2 ) a ( l 2 + Δ ) b , {\displaystyle \int ^{\Lambda }{\frac {d^{d}l}{(2\pi )^{d}}}{\frac {(l^{2})^{a}}{(l^{2}+\Delta )^{b}}},} where ∫ Λ {\displaystyle \int ^{\Lambda }} is shorthand for integration over the domain { l ∈ R d : | l | < Λ } {\displaystyle \{l\in \mathbb {R} ^{d}:|l|<\Lambda \}} . The expression is finite, but in general as Λ → ∞ {\displaystyle \Lambda \rightarrow \infty } , the expression diverges. Dimensional regularization The integral without a momentum cutoff may be evaluated as I d ( b , a , Δ ) := ∫ R d d d l ( 2 π ) d ( l 2 ) a ( l 2 + Δ ) b = 1 ( 4 π ) d / 2 1 Γ ( d / 2 ) B ( b − a − d 2 , a + d 2 ) Δ − ( b − a − d / 2 ) , {\displaystyle I_{d}(b,a,\Delta ):=\int _{\mathbb {R} ^{d}}{\frac {d^{d}l}{(2\pi )^{d}}}{\frac {(l^{2})^{a}}{(l^{2}+\Delta )^{b}}}={\frac {1}{(4\pi )^{d/2}}}{\frac {1}{\Gamma (d/2)}}B\left(b-a-{\frac {d}{2}},a+{\frac {d}{2}}\right)\Delta ^{-(b-a-d/2)},} where B {\displaystyle B} is the Beta function. For calculations in the renormalization of QED or QCD, a {\displaystyle a} takes values 0 , 1 {\displaystyle 0,1} and 2 {\displaystyle 2} . For loop integrals in QFT, B {\displaystyle B} actually has a pole for relevant values of a , b {\displaystyle a,b} and d {\displaystyle d} . For example in scalar ϕ 4 {\displaystyle \phi ^{4}} theory in 4 dimensions, the loop integral in the calculation of one-loop renormalization of the interaction vertex has ( a , b , d ) = ( 0 , 2 , 4 ) {\displaystyle (a,b,d)=(0,2,4)} . We use the 'trick' of dimensional regularization, analytically continuing d {\displaystyle d} to d = 4 − ϵ {\displaystyle d=4-\epsilon } with ϵ {\displaystyle \epsilon } a small parameter. For calculation of counterterms, the loop integral should be expressed as a Laurent series in ϵ {\displaystyle \epsilon } . To do this, it is necessary to use the Laurent expansion of the Gamma function, Γ ( ϵ ) = 1 ϵ − γ + O ( ϵ ) {\displaystyle \Gamma (\epsilon )={\frac {1}{\epsilon }}-\gamma +{\mathcal {O}}(\epsilon )} where γ {\displaystyle \gamma } is the Euler–Mascheroni constant. In practice the loop integral generally diverges as ϵ → 0 {\displaystyle \epsilon \rightarrow 0} . For full evaluation of the Feynman diagram, there may be algebraic factors which must be evaluated. For example in QED, the tensor indices of the integral may be contracted with Gamma matrices, and identities involving these are needed to evaluate the integral. In QCD, there may be additional Lie algebra factors, such as the quadratic Casimir of the adjoint representation as well as of any representations that matter (scalar or spinor fields) in the theory transform under. Examples Scalar field theory φ4 theory The starting point is the action for ϕ 4 {\displaystyle \phi ^{4}} theory in R d {\displaystyle \mathbb {R} ^{d}} is S [ ϕ 0 ] = ∫ d d x 1 2 ( ∂ ϕ 0 ) 2 + 1 2 m 0 ϕ 0 2 + 1 4 ! λ 0 ϕ 0 4 . {\displaystyle S[\phi _{0}]=\int d^{d}x{\frac {1}{2}}(\partial \phi _{0})^{2}+{\frac {1}{2}}m_{0}\phi _{0}^{2}+{\frac {1}{4!}}\lambda _{0}\phi _{0}^{4}.} Where ( ∂ ϕ 0 ) 2 = ∇ ϕ 0 ⋅ ∇ ϕ 0 = ∑ i = 1 d ∂ i ϕ 0 ∂ i ϕ 0 {\displaystyle (\partial \phi _{0})^{2}=\nabla \phi _{0}\cdot \nabla \phi _{0}=\sum _{i=1}^{d}\partial _{i}\phi _{0}\partial _{i}\phi _{0}} . The domain is purposefully left ambiguous, as it varies depending on regularisation scheme. The Euclidean signature propagator in momentum space is 1 p 2 + m 0 2 . {\displaystyle {\frac {1}{p^{2}+m_{0}^{2}}}.} The one-loop contribution to the two-point correlator ⟨ ϕ ( x ) ϕ ( y ) ⟩ {\displaystyle \langle \phi (x)\phi (y)\rangle } (or rather, to the momentum space two-point correlator or Fourier transform of the two-point correlator) comes from a single Feynman diagram and is λ 0 2 ∫ d d k ( 2 π ) d 1 k 2 + m 0 2 . {\displaystyle {\frac {\lambda _{0}}{2}}\int {\frac {d^{d}k}{(2\pi )^{d}}}{\frac {1}{k^{2}+m_{0}^{2}}}.} This is an example of a loop integral. If d ≥ 2 {\displaystyle d\geq 2} and the domain of integration is R d {\displaystyle \mathbb {R} ^{d}} , this integral diverges. This is typical of the puzzle of divergences which plagued quantum field theory historically. To obtain finite results, we choose a regularization scheme. For illustration, we give two schemes. Cutoff regularization: fix Λ > 0 {\displaystyle \Lambda >0} . The regularized loop integral is the integral over the domain k = | k | < Λ , {\displaystyle k=|\mathbf {k} |<\Lambda ,} and it is typical to denote this integral by λ 0 2 ∫ Λ d d k ( 2 π ) d 1 k 2 + m 0 2 . {\displaystyle {\frac {\lambda _{0}}{2}}\int ^{\Lambda }{\frac {d^{d}k}{(2\pi )^{d}}}{\frac {1}{k^{2}+m_{0}^{2}}}.} This integral is finite and in this case can be evaluated. Dimensional regularization: we integrate over all of R d {\displaystyle \mathbb {R} ^{d}} , but instead of considering d {\displaystyle d} to be a positive integer, we analytically continue d {\displaystyle d} to d = n − ϵ {\displaystyle d=n-\epsilon } , where ϵ {\displaystyle \epsilon } is small. By the computation above, we showed that the integral can be written in terms of expressions which have a well-defined analytic continuation from integers n {\displaystyle n} to functions on C {\displaystyle \mathbb {C} } : specifically the gamma function has an analytic continuation and taking powers, x d {\displaystyle x^{d}} , is an operation which can be analytically continued. See also Regularization (physics) Renormalization References Further reading Vladimir A. Smirnov: "Evaluating Feynman Integrals", Springer,ISBN 978-3-540239338 (2004). Vladimir A. Smirnov: "Feynman Integral Calculus", Springer, ISBN 978-3-540306108 (2006). Vladimir A. Smirnov: "Analytic Tools for Feynman Integrals", Springer, ISBN 978-3642348853 (2013). Johannes Blümlein and Carsten Schneider (Eds.): "Anti-Differentiation and the Calculation of Feynman Amplitudes", Springer, ISBN 978-3-030-80218-9 (2021). Stefan Weinzierl: "Feynman Integrals: A Comprehensive Treatment for Students and Researchers", Springer, ISBN 978-3-030-99560-7 (2023). Shot noise or Poisson noise is a type of noise which can be modeled by a Poisson process. In electronics shot noise originates from the discrete nature of electric charge. Shot noise also occurs in photon counting in optical devices, where shot noise is associated with the particle nature of light. Origin In a statistical experiment such as tossing a fair coin and counting the occurrences of heads and tails, the numbers of heads and tails after many throws will differ by only a tiny percentage, while after only a few throws outcomes with a significant excess of heads over tails or vice versa are common; if an experiment with a few throws is repeated over and over, the outcomes will fluctuate a lot. From the law of large numbers, one can show that the relative fluctuations reduce as the reciprocal square root of the number of throws, a result valid for all statistical fluctuations, including shot noise. Shot noise exists because phenomena such as light and electric current consist of the movement of discrete (also called "quantized") 'packets'. Consider light—a stream of discrete photons—coming out of a laser pointer and hitting a wall to create a visible spot. The fundamental physical processes that govern light emission are such that these photons are emitted from the laser at random times; but the many billions of photons needed to create a spot are so many that the brightness, the number of photons per unit of time, varies only infinitesimally with time. However, if the laser brightness is reduced until only a handful of photons hit the wall every second, the relative fluctuations in number of photons, i.e., brightness, will be significant, just as when tossing a coin a few times. These fluctuations are shot noise. The concept of shot noise was first introduced in 1918 by Walter Schottky who studied fluctuations of current in vacuum tubes. Shot noise may be dominant when the finite number of particles that carry energy (such as electrons in an electronic circuit or photons in an optical device) is sufficiently small so that uncertainties due to the Poisson distribution, which describes the occurrence of independent random events, are significant. It is important in electronics, telecommunications, optical detection, and fundamental physics. The term can also be used to describe any noise source, even if solely mathematical, of similar origin. For instance, particle simulations may produce a certain amount of "noise", where because of the small number of particles simulated, the simulation exhibits undue statistical fluctuations which don't reflect the real-world system. The magnitude of shot noise increases according to the square root of the expected number of events, such as the electric current or intensity of light. But since the strength of the signal itself increases more rapidly, the relative proportion of shot noise decreases and the signal-to-noise ratio (considering only shot noise) increases anyway. Thus shot noise is most frequently observed with small currents or low light intensities that have been amplified. Signal-to-Noise For large numbers, the Poisson distribution approaches a normal distribution about its mean, and the elementary events (photons, electrons, etc.) are no longer individually observed, typically making shot noise in actual observations indistinguishable from true Gaussian noise. Since the standard deviation of shot noise is equal to the square root of the average number of events N, the signal-to-noise ratio (SNR) is given by: S N R = N N = N . {\displaystyle \mathrm {SNR} ={\frac {N}{\sqrt {N}}}={\sqrt {N}}.\,} Thus when N is very large, the signal-to-noise ratio is very large as well, and any relative fluctuations in N due to other sources are more likely to dominate over shot noise. However, when the other noise source is at a fixed level, such as thermal noise, or grows slower than N {\displaystyle {\sqrt {N}}} , increasing N (the DC current or light level, etc.) can lead to dominance of shot noise. Properties Electronic devices Shot noise in electronic circuits consists of random fluctuations of DC current, which is due to electric current being the flow of discrete charges (electrons). Because the electron has such a tiny charge, however, shot noise is of relative insignificance in many (but not all) cases of electrical conduction. For instance 1 ampere of current consists of about 6.24×1018 electrons per second; even though this number will randomly vary by several billion in any given second, such a fluctuation is minuscule compared to the current itself. In addition, shot noise is often less significant as compared with two other noise sources in electronic circuits, flicker noise and Johnson–Nyquist noise. However, shot noise is temperature and frequency independent, in contrast to Johnson–Nyquist noise, which is proportional to temperature, and flicker noise, with the spectral density decreasing with increasing frequency. Therefore, at high frequencies and low temperatures shot noise may become the dominant source of noise. With very small currents and considering shorter time scales (thus wider bandwidths) shot noise can be significant. For instance, a microwave circuit operates on time scales of less than a nanosecond and if we were to have a current of 16 nanoamperes that would amount to only 100 electrons passing every nanosecond. According to Poisson statistics the actual number of electrons in any nanosecond would vary by 10 electrons rms, so that one sixth of the time fewer than 90 electrons would pass a point and one sixth of the time more than 110 electrons would be counted in a nanosecond. Now with this small current viewed on this time scale, the shot noise amounts to 1/10 of the DC current itself. The result by Schottky, based on the assumption that the statistics of electrons passage is Poissonian, reads for the spectral noise density at the frequency f {\displaystyle f} , S ( f ) = 2 e | I | , {\displaystyle S(f)=2e\vert I\vert \ ,} where e {\displaystyle e} is the electron charge, and I {\displaystyle I} is the average current of the electron stream. The noise spectral power is frequency independent, which means the noise is white. This can be combined with the Landauer formula, which relates the average current with the transmission eigenvalues T n {\displaystyle T_{n}} of the contact through which the current is measured ( n {\displaystyle n} labels transport channels). In the simplest case, these transmission eigenvalues can be taken to be energy independent and so the Landauer formula is I = e 2 π ℏ V ∑ n T n , {\displaystyle I={\frac {e^{2}}{\pi \hbar }}V\sum _{n}T_{n}\ ,} where V {\displaystyle V} is the applied voltage. This provides for S = 2 e 3 π ℏ | V | ∑ n T n , {\displaystyle S={\frac {2e^{3}}{\pi \hbar }}\vert V\vert \sum _{n}T_{n}\ ,} commonly referred to as the Poisson value of shot noise, S P {\displaystyle S_{P}} . This is a classical result in the sense that it does not take into account that electrons obey Fermi–Dirac statistics. The correct result takes into account the quantum statistics of electrons and reads (at zero temperature) S = 2 e 3 π ℏ | V | ∑ n T n ( 1 − T n ) . {\displaystyle S={\frac {2e^{3}}{\pi \hbar }}\vert V\vert \sum _{n}T_{n}(1-T_{n})\ .} It was obtained in the 1990s by Viktor Khlus, Gordey Lesovik (independently the single-channel case), and Markus Büttiker (multi-channel case). This noise is white and is always suppressed with respect to the Poisson value. The degree of suppression, F = S / S P {\displaystyle F=S/S_{P}} , is known as the Fano factor. Noises produced by different transport channels are independent. Fully open ( T n = 1 {\displaystyle T_{n}=1} ) and fully closed ( T n = 0 {\displaystyle T_{n}=0} ) channels produce no noise, since there are no irregularities in the electron stream. At finite temperature, a closed expression for noise can be written as well. It interpolates between shot noise (zero temperature) and Nyquist-Johnson noise (high temperature). Examples Tunnel junction is characterized by low transmission in all transport channels, therefore the electron flow is Poissonian, and the Fano factor equals one. Quantum point contact is characterized by an ideal transmission in all open channels, therefore it does not produce any noise, and the Fano factor equals zero. The exception is the step between plateaus, when one of the channels is partially open and produces noise. A metallic diffusive wire has a Fano factor of 1/3 regardless of the geometry and the details of the material. In 2DEG exhibiting fractional quantum Hall effect electric current is carried by quasiparticles moving at the sample edge whose charge is a rational fraction of the electron charge. The first direct measurement of their charge was through the shot noise in the current. Effects of interactions While this is the result when the electrons contributing to the current occur completely randomly, unaffected by each other, there are important cases in which these natural fluctuations are largely suppressed due to a charge build up. Take the previous example in which an average of 100 electrons go from point A to point B every nanosecond. During the first half of a nanosecond we would expect 50 electrons to arrive at point B on the average, but in a particular half nanosecond there might well be 60 electrons which arrive there. This will create a more negative electric charge at point B than average, and that extra charge will tend to repel the further flow of electrons from leaving point A during the remaining half nanosecond. Thus the net current integrated over a nanosecond will tend more to stay near its average value of 100 electrons rather than exhibiting the expected fluctuations (10 electrons rms) we calculated. This is the case in ordinary metallic wires and in metal film resistors, where shot noise is almost completely cancelled due to this anti-correlation between the motion of individual electrons, acting on each other through the coulomb force. However this reduction in shot noise does not apply when the current results from random events at a potential barrier which all the electrons must overcome due to a random excitation, such as by thermal activation. This is the situation in p-n junctions, for instance. A semiconductor diode is thus commonly used as a noise source by passing a particular DC current through it. In other situations interactions can lead to an enhancement of shot noise, which is the result of a super-poissonian statistics. For example, in a resonant tunneling diode the interplay of electrostatic interaction and of the density of states in the quantum well leads to a strong enhancement of shot noise when the device is biased in the negative differential resistance region of the current-voltage characteristics. Shot noise is distinct from voltage and current fluctuations expected in thermal equilibrium; this occurs without any applied DC voltage or current flowing. These fluctuations are known as Johnson–Nyquist noise or thermal noise and increase in proportion to the Kelvin temperature of any resistive component. However both are instances of white noise and thus cannot be distinguished simply by observing them even though their origins are quite dissimilar. Since shot noise is a Poisson process due to the finite charge of an electron, one can compute the root mean square current fluctuations as being of a magnitude σ i = 2 q I Δ f {\displaystyle \sigma _{i}={\sqrt {2qI\,\Delta f}}} where q is the elementary charge of an electron, Δf is the single-sided bandwidth in hertz over which the noise is considered, and I is the DC current flowing. For a current of 100 mA, measuring the current noise over a bandwidth of 1 Hz, we obtain σ i = 0.18 n A . {\displaystyle \sigma _{i}=0.18\,\mathrm {nA} \;.} If this noise current is fed through a resistor a noise voltage of σ v = σ i R {\displaystyle \sigma _{v}=\sigma _{i}\,R} would be generated. Coupling this noise through a capacitor, one could supply a noise power of P = 1 2 q I Δ f R . {\displaystyle P={\frac {1}{2}}qI\,\Delta fR.} to a matched load. Photodetectors The mean number of incident photons in time interval Δ t {\displaystyle \Delta t} on a photodetector is N ¯ = P Δ t h c λ {\displaystyle {\bar {N}}={\frac {P\,\Delta t}{\frac {hc}{\lambda }}}} where c is the speed of light, h is the Planck constant, P is the average power, and λ {\displaystyle \lambda } is the photon wavelength. Following Poisson statistics, the standard deviation of photon number is σ N = N ¯ {\displaystyle \sigma _{N}={\sqrt {\bar {N}}}} The spectral density ( W 2 / H z {\displaystyle W^{2}/Hz} ) of shot noise limited light is S ( f ) = 2 h c λ P {\displaystyle S(f)=2{\frac {hc}{\lambda }}P} The SNR for a CCD camera can be calculated from the following equation: S N R = I ⋅ Q E ⋅ t I ⋅ Q E ⋅ t + N d ⋅ t + N r 2 , {\displaystyle \mathrm {SNR} ={\frac {I\cdot QE\cdot t}{\sqrt {I\cdot QE\cdot t+N_{d}\cdot t+N_{r}^{2}}}},} where: I = photon flux (photons/pixel/second), QE = quantum efficiency, t = integration time (seconds), Nd = dark current (electrons/pixel/sec), Nr = read noise (electrons). Optics In optics, shot noise describes the fluctuations of the number of photons detected (or simply counted in the abstract) because they occur independently of each other. This is therefore another consequence of discretization, in this case of the energy in the electromagnetic field in terms of photons. In the case of photon detection, the relevant process is the random conversion of photons into photo-electrons for instance, thus leading to a larger effective shot noise level when using a detector with a quantum efficiency below unity. Only in an exotic squeezed coherent state can the number of photons measured per unit time have fluctuations smaller than the square root of the expected number of photons counted in that period of time. Of course there are other mechanisms of noise in optical signals which often dwarf the contribution of shot noise. When these are absent, however, optical detection is said to be "photon noise limited" as only the shot noise (also known as "quantum noise" or "photon noise" in this context) remains. Shot noise is easily observable in the case of photomultipliers and avalanche photodiodes used in the Geiger mode, where individual photon detections are observed. However the same noise source is present with higher light intensities measured by any photo detector, and is directly measurable when it dominates the noise of the subsequent electronic amplifier. Just as with other forms of shot noise, the fluctuations in a photo-current due to shot noise scale as the square-root of the average intensity: ( Δ I ) 2 = d e f ⟨ ( I − ⟨ I ⟩ ) 2 ⟩ ∝ I . {\displaystyle (\Delta I)^{2}\ {\stackrel {\mathrm {def} }{=}}\ \langle \left(I-\langle I\rangle \right)^{2}\rangle \propto I.} The shot noise of a coherent optical beam (having no other noise sources) is a fundamental physical phenomenon, reflecting quantum fluctuations in the electromagnetic field. In optical homodyne detection, the shot noise in the photodetector can be attributed to either the zero point fluctuations of the quantised electromagnetic field, or to the discrete nature of the photon absorption process. However, shot noise itself is not a distinctive feature of quantised field and can also be explained through semiclassical theory. What the semiclassical theory does not predict, however, is the squeezing of shot noise. Shot noise also sets a lower bound on the noise introduced by quantum amplifiers which preserve the phase of an optical signal. See also Johnson–Nyquist noise or thermal noise 1/f noise Burst noise Contact resistance Image noise Quantum efficiency References This article incorporates public domain material from Federal Standard 1037C. General Services Administration. Archived from the original on 2022-01-22. (in support of MIL-STD-188). Michelle Yvonne Simmons (born 14 July 1967) is an Australian quantum physicist, recognised for her foundational contributions to the field of atomic electronics. She is founding director of the Australian Research Council's Centre of Excellence for Quantum Computation & Communication Technology, and as of 2023 is Scientia Professor of Quantum Physics in the Faculty of Science at the University of New South Wales. She has twice been an Australian Research Council Federation Fellow, and is an Australian Research Council Laureate Fellow. In January 2018, Simmons was named Australian of the Year for her work and dedication to quantum information science, and in June 2019, she was appointed an Officer of the Order of Australia in the Queen's Birthday Honours in recognition of her "distinguished service to science education as a leader in quantum and atomic electronics, and as a role model". Early life and education Michelle Yvonne Simmons was born on 14 July 1967 in London, to a mother who worked as a bank manager and a father who worked as a policeman. Simmons grew up in South-East London with an older brother. Between 1985 and 1988 she undertook her undergraduate degree at Trevelyan College, Durham University, where she studied physics and chemistry of materials. As a postgraduate at St Aidan's College, Durham she was awarded a PhD in 1992 for her thesis "The characterisation of CdTe-based epitaxial solar cell structures fabricated by MOVPE", with research supervised by Andrew W. Brinkman. Career and research From 1992 to 1998 Simmons worked as a research fellow in quantum electronics with Michael Pepper at the Cavendish Laboratory in the UK, where she gained an international reputation for her work in the discovery of the 0.7 feature and the development of 'hole' transistors. In 1999, she was awarded an Australian Research Council (ARC) QEII Fellowship and went to Australia, conducting research for four years under this fellowship. She was a founding member of the ARC Centre of Excellence for Quantum Computer Technology, and as of 2023 remains director of the centre. Simmons founded Silicon Quantum Computing, a Sydney based quantum computing company, in 2017 and is the current CEO. She has held several other positions over the course of her career, including: 2000: Director, Atomic Fabrication Facility, UNSW 2000: Manager, Atomic Fabrication and Crystal Growth Program, Centre for Quantum Computer Technology, School of Physics, UNSW 2003: Chair, New South Wales Branch, Australian Institute of Physics 2003: Member, C8 Commission, International Union of Pure and Applied Physics 2003: Australian Representative for Nanotechnology, International Union for Vacuum Science, Technique and Applications 2005: Member, Expert Advisory Committee for Physics, Chemistry and Geosciences, Australian Research Council 2007: Associate editor, IEEE Journal of Nanotechnology 2007: Chair, National Committee for Physics, Australian Academy of Science As of 2023 Simmons is Scientia Professor of Quantum Physics in the Faculty of Science at the University of New South Wales. In 2025 Simmons was elected to the Board of Directors of the Tech Council of Australia. Achievements Simmons is well-known internationally for creating the field of atomic electronics, that is, building electronic devices at the atomic scale. Her research team at ARC created the first precision single atom transistor and the narrowest conducting wires in silicon, among other achievements. Since 2000 she has established a large research group dedicated to the fabrication of atomic scale devices in silicon and germanium using the atomic precision of scanning tunnelling microscopy. Her research group is the only group worldwide that can create atomically precise devices in silicon—they were also the first team in the world to develop a working "perfect" single-atom transistor and the narrowest conducting doped wires in silicon. Publications and other activities Simmons has published over 450 peer-reviewed journal papers, amassing over 13,000 citations, written five book chapters, and published a book on nanotechnology. She has also filed 44 patents and delivered over 400 invited and plenary presentations at international conferences. She was the inaugural editor-in-chief of npj Quantum Information, an academic journal publishing articles in the emerging field of quantum information science launched in 2015. She gave the Australia Day address for New South Wales in 2017, in which she spoke about the importance of setting high expectations for students. Simmons delivered the 2023 Boyer Lecture in four parts, titled The Atomic Revolution. Recognition and awards 1999: Australian Research Council QEII Fellowship 2004: Australian Research Council Federation Fellowship 2005: Australian Academy of Science's Pawsey Medal 2006: Fellow of the Australian Academy of Science (FAA) 2009: Australian Research Council Federation Fellowship 2011: NSW Scientist of the Year (named by the NSW Government Office of the Chief Scientist & Engineer) 2013: Australian Laureate Fellowship, Australian Research Council 2013: Royal Society of New South Wales Walter Burfitt Prize 2014: Fellow of the American Academy of Arts and Sciences 2015: Thomas Ranken Lyle Medal 2015: Fellow of the Australian Academy of Technological Sciences and Engineering (FTSE) 2015: Feynman Prize in Nanotechnology, Foresight Institute, for the abrication of single-atom transistors 2015: Eureka Prize for Leadership in Science, CSIRO 2017: George R. Stibitz Computing Pioneer Award, American Computer Museum 2017: L'Oréal-UNESCO Awards for Women in Science Asia-Pacific Laureate 2017: Profiled in a short documentary on France24 TV 2018: Australian of the Year, for her work in quantum physics 2018: Fellow of the Royal Society (FRS) 2019: Officer of the Order of Australia (AO) as part of the 2019 Queen's Birthday Honours 2020: Chair, Division of Quantum Information, American Physical Society 2021: Fellow, American Physical Society 2022: Royal Society Bakerian Medal 2023: Prime Minister's Prize for Science 2023: Erna Hamburger Prize As of 2017, Simmons was an elected trustee of Sydney Grammar School. Personal life and views Simmons has resided in Australia since 1999, taking citizenship in 2007. She is married to Thomas Barlow, formerly, a Financial Times columnist and a Fellow of MIT and Balliol College, Oxford, now a novelist and business analyst. They have three children. She says she enjoys "planning expeditions and keeping fit. But the thing that brings me the most joy is my funny husband and three adorable children". Her heroes in science are Michael Faraday and Nobel Laureate John Bardeen. Views on education In her 2017 Australia Day address, Simmons criticised the lowering of standards in physics education in the HSC (Higher School Certificate) curriculum, in which an effort has been made to make physics more appealing to girls by substituting mathematical problem-solving with qualitative responses, remarking that the curriculum had a "feminised nature". When Simmons was made Australian of the Year in 2018, she spoke about the importance of not being defined by other people's expectations of you. She said, "Don't live your life according to what other people think. Go out there and do what you really want to do". She is passionate about encouraging girls to pursue a career in science and technology. "Seeing women in leadership roles and competing internationally is important. It gives them the sense that anything is possible", she said. == References == In chemical bonds, an orbital overlap is the concentration of orbitals on adjacent atoms in the same regions of space. Orbital overlap can lead to bond formation. The general principle for orbital overlap is that, the greater the overlap between orbitals, the greater the bond strength. Linus Pauling explained the importance of orbital overlap in the molecular bond angles observed through experimentation; it is the basis for orbital hybridization. As s orbitals are spherical (and have no directionality) and p orbitals are oriented 90° to each other, a theory was needed to explain why molecules such as methane (CH4) had observed bond angles of 109.5°. Pauling proposed that s and p orbitals on the carbon atom can combine to form hybrid orbitals (sp3 in the case of methane) which are directed toward the hydrogen atoms. The carbon hybrid orbitals have greater overlap with the hydrogen orbitals, and can therefore form stronger C–H bonds. A quantitative measure of the overlap of two atomic orbitals ΨA and ΨB on atoms A and B is their overlap integral, defined as S A B = ∫ Ψ A ∗ Ψ B d V , {\displaystyle \mathbf {S} _{\mathrm {AB} }=\int \Psi _{\mathrm {A} }^{*}\Psi _{\mathrm {B} }\,dV,} where the integration extends over all space. The star on the first orbital wavefunction indicates the function's complex conjugate, which in general may be complex-valued. Overlap matrix The overlap matrix is a square matrix, used in quantum chemistry to describe the inter-relationship of a set of basis vectors of a quantum system, such as an atomic orbital basis set used in molecular electronic structure calculations. In particular, if the vectors are orthogonal to one another, the overlap matrix will be diagonal. In addition, if the basis vectors form an orthonormal set, the overlap matrix will be the identity matrix. The overlap matrix is always n×n, where n is the number of basis functions used. It is a kind of Gramian matrix. In general, each overlap matrix element is defined as an overlap integral: S j k = ⟨ b j | b k ⟩ = ∫ Ψ j ∗ Ψ k d τ {\displaystyle \mathbf {S} _{jk}=\left\langle b_{j}|b_{k}\right\rangle =\int \Psi _{j}^{*}\Psi _{k}\,d\tau } where | b j ⟩ {\displaystyle \left|b_{j}\right\rangle } is the j-th basis ket (vector), and Ψ j {\displaystyle \Psi _{j}} is the j-th wavefunction, defined as : Ψ j ( x ) = ⟨ x | b j ⟩ {\displaystyle \Psi _{j}(x)=\left\langle x|b_{j}\right\rangle } . In particular, if the set is normalized (though not necessarily orthogonal) then the diagonal elements will be identically 1 and the magnitude of the off-diagonal elements less than or equal to one with equality if and only if there is linear dependence in the basis set as per the Cauchy–Schwarz inequality. Moreover, the matrix is always positive definite; that is to say, the eigenvalues are all strictly positive. See also Roothaan equations Hartree–Fock method Pi bond Sigma bond References Quantum Chemistry: Fifth Edition, Ira N. Levine, 2000 In chemistry, resonance, also called mesomerism, is a way of describing bonding in certain molecules or polyatomic ions by the combination of several contributing structures (or forms, also variously known as resonance structures or canonical structures) into a resonance hybrid (or hybrid structure) in valence bond theory. It has particular value for analyzing delocalized electrons where the bonding cannot be expressed by one single Lewis structure. The resonance hybrid is the accurate structure for a molecule or ion; it is an average of the theoretical (or hypothetical) contributing structures. Overview Under the framework of valence bond theory, resonance is an extension of the idea that the bonding in a chemical species can be described by a Lewis structure. For many chemical species, a single Lewis structure, consisting of atoms obeying the octet rule, possibly bearing formal charges, and connected by bonds of positive integer order, is sufficient for describing the chemical bonding and rationalizing experimentally determined molecular properties like bond lengths, angles, and dipole moment. However, in some cases, more than one Lewis structure could be drawn, and experimental properties are inconsistent with any one structure. In order to address this type of situation, several contributing structures are considered together as an average, and the molecule is said to be represented by a resonance hybrid in which several Lewis structures are used collectively to describe its true structure. For instance, in NO2–, nitrite anion, the two N–O bond lengths are equal, even though no single Lewis structure has two N–O bonds with the same formal bond order. However, its measured structure is consistent with a description as a resonance hybrid of the two major contributing structures shown above: it has two equal N–O bonds of 125 pm, intermediate in length between a typical N–O single bond (145 pm in hydroxylamine, H2N–OH) and N–O double bond (115 pm in nitronium ion, [O=N=O]+). According to the contributing structures, each N–O bond is an average of a formal single and formal double bond, leading to a true bond order of 1.5. By virtue of this averaging, the Lewis description of the bonding in NO2– is reconciled with the experimental fact that the anion has equivalent N–O bonds. The resonance hybrid represents the actual molecule as the "average" of the contributing structures, with bond lengths and partial charges taking on intermediate values compared to those expected for the individual Lewis structures of the contributors, were they to exist as "real" chemical entities. The contributing structures differ only in the formal apportionment of electrons to the atoms, and not in the actual physically and chemically significant electron or spin density. While contributing structures may differ in formal bond orders and in formal charge assignments, all contributing structures must have the same number of valence electrons and the same spin multiplicity. Because electron delocalization lowers the potential energy of a system, any species represented by a resonance hybrid is more stable than any of the (hypothetical) contributing structures. Electron delocalization stabilizes a molecule because the electrons are more evenly spread out over the molecule, decreasing electron-electron repulsion. The difference in potential energy between the actual species and the (computed) energy of the contributing structure with the lowest potential energy is called the resonance energy or delocalization energy. The magnitude of the resonance energy depends on assumptions made about the hypothetical "non-stabilized" species and the computational methods used and does not represent a measurable physical quantity, although comparisons of resonance energies computed under similar assumptions and conditions may be chemically meaningful. Molecules with an extended π system such as linear polyenes and polyaromatic compounds are well described by resonance hybrids as well as by delocalised orbitals in molecular orbital theory. Resonance vs isomerism Resonance is to be distinguished from isomerism. Isomers are molecules with the same chemical formula but are distinct chemical species with different arrangements of atomic nuclei in space. Resonance contributors of a molecule, on the other hand, can only differ in the way electrons are formally assigned to atoms in the Lewis structure depictions of the molecule. Specifically, when a molecular structure is said to be represented by a resonance hybrid, it does not mean that electrons of the molecule are "resonating" or shifting back and forth between several sets of positions, each one represented by a Lewis structure. Rather, it means that the set of contributing structures represents an intermediate structure (a weighted average of the contributors), with a single, well-defined geometry and distribution of electrons. It is incorrect to regard resonance hybrids as rapidly interconverting isomers, even though the term "resonance" might evoke such an image. (As described below, the term "resonance" originated as a classical physics analogy for a quantum mechanical phenomenon, so it should not be construed too literally.) Symbolically, the double headed arrow A ⟷ B {\displaystyle {\ce {A<->B}}} is used to indicate that A and B are contributing forms of a single chemical species (as opposed to an equilibrium arrow, e.g., A ↽ − − ⇀ B {\displaystyle {\ce {A <=> B}}} ; see below for details on usage). A non-chemical analogy is illustrative: one can describe the characteristics of a real animal, the narwhal, in terms of the characteristics of two mythical creatures: the unicorn, a creature with a single horn on its head, and the leviathan, a large, whale-like creature. The narwhal is not a creature that goes back and forth between being a unicorn and being a leviathan, nor do the unicorn and leviathan have any physical existence outside the collective human imagination. Nevertheless, describing the narwhal in terms of these imaginary creatures provides a reasonably good description of its physical characteristics. Due to confusion with the physical meaning of the word resonance, as no entities actually physically "resonate", it has been suggested that the term resonance be abandoned in favor of delocalization and resonance energy abandoned in favor of delocalization energy. A resonance structure becomes a contributing structure and the resonance hybrid becomes the hybrid structure. The double headed arrows would be replaced by commas to illustrate a set of structures, as arrows of any type may suggest that a chemical change is taking place. Representation in diagrams In diagrams, contributing structures are typically separated by double-headed arrows (↔). The arrow should not be confused with the right and left pointing equilibrium arrow (⇌). All structures together may be enclosed in large square brackets, to indicate they picture one single molecule or ion, not different species in a chemical equilibrium. Alternatively to the use of contributing structures in diagrams, a hybrid structure can be used. In a hybrid structure, pi bonds that are involved in resonance are usually pictured as curves or dashed lines, indicating that these are partial rather than normal complete pi bonds. In benzene and other aromatic rings, the delocalized pi-electrons are sometimes pictured as a solid circle. History The concept first appeared in 1899 in Johannes Thiele's "Partial Valence Hypothesis" to explain the unusual stability of benzene which would not be expected from August Kekulé's structure proposed in 1865 with alternating single and double bonds. Benzene undergoes substitution reactions, rather than addition reactions as typical for alkenes. He proposed that the carbon-carbon bond in benzene is intermediate of a single and double bond. The resonance proposal also helped explain the number of isomers of benzene derivatives. For example, Kekulé's structure would predict four dibromobenzene isomers, including two ortho isomers with the brominated carbon atoms joined by either a single or a double bond. In reality there are only three dibromobenzene isomers and only one is ortho, in agreement with the idea that there is only one type of carbon-carbon bond, intermediate between a single and a double bond. The mechanism of resonance was introduced into quantum mechanics by Werner Heisenberg in 1926 in a discussion of the quantum states of the helium atom. He compared the structure of the helium atom with the classical system of resonating coupled harmonic oscillators. In the classical system, the coupling produces two modes, one of which is lower in frequency than either of the uncoupled vibrations; quantum mechanically, this lower frequency is interpreted as a lower energy. Linus Pauling used this mechanism to explain the partial valence of molecules in 1928, and developed it further in a series of papers in 1931-1933. The alternative term mesomerism popular in German and French publications with the same meaning was introduced by C. K. Ingold in 1938, but did not catch on in the English literature. The current concept of mesomeric effect has taken on a related but different meaning. The double headed arrow was introduced by the German chemist Fritz Arndt who preferred the German phrase zwischenstufe or intermediate stage. Resonance theory dominated over competing Hückel method for two decades thanks to being relatively easier to understand for chemists without fundamental physics background, even if they couldn't grasp the concept of quantum superposition and confused it with tautomerism. Pauling and Wheland themselves characterized Erich Hückel's approach as "cumbersome" at the time, and his lack of communication skills contributed: when Robert Robinson sent him a friendly request, he responded arrogantly that he is not interested in organic chemistry. In the Soviet Union, resonance theory – especially as developed by Pauling – was attacked in the early 1950s as being contrary to the Marxist principles of dialectical materialism, and in June 1951 the Soviet Academy of Sciences under the leadership of Alexander Nesmeyanov convened a conference on the chemical structure of organic compounds, attended by 400 physicists, chemists, and philosophers, where "the pseudo-scientific essence of the theory of resonance was exposed and unmasked". Major and minor contributors One contributing structure may resemble the actual molecule more than another (in the sense of energy and stability). Structures with a low value of potential energy are more stable than those with high values and resemble the actual structure more. The most stable contributing structures are called major contributors. Energetically unfavourable and therefore less favorable structures are minor contributors. With rules listed in rough order of diminishing importance, major contributors are generally structures that obey as much as possible the octet rule (8 valence electrons around each atom rather than having deficiencies or surplus, or 2 electrons for Period 1 elements); have a maximum number of covalent bonds; carry a minimum of formally charged atoms, with the separation for unlike and like charges minimized and maximized, respectively; place negative charge, if any, on the most electronegative atoms and positive charge, if any, on the most electropositive; do not deviate substantially from idealized bond lengths and angles (e.g., the relative unimportance of Dewar-type resonance contributors for benzene); maintain aromatic substructures locally while avoiding anti-aromatic ones (see Clar sextet and biphenylene). A maximum of eight valence electrons is strict for the Period 2 elements Be, B, C, N, O, and F, as is a maximum of two for H and He and effectively for Li as well. The issue of expansion of the valence shell of third period and heavier main group elements is controversial. A Lewis structure in which a central atom has a valence electron count greater than eight traditionally implies the participation of d orbitals in bonding. However, the consensus opinion is that while they may make a marginal contribution, the participation of d orbitals is unimportant, and the bonding of so-called hypervalent molecules are, for the most part, better explained by charge-separated contributing forms that depict three-center four-electron bonding. Nevertheless, by tradition, expanded octet structures are still commonly drawn for functional groups like sulfoxides, sulfones, and phosphorus ylides, for example. Regarded as a formalism that does not necessarily reflect the true electronic structure, such depictions are preferred by the IUPAC over structures featuring partial bonds, charge separation, or dative bonds. Equivalent contributors contribute equally to the actual structure, while the importance of nonequivalent contributors is determined by the extent to which they conform to the properties listed above. A larger number of significant contributing structures and a more voluminous space available for delocalized electrons lead to stabilization (lowering of the energy) of the molecule. Examples Aromatic molecules In benzene the two cyclohexatriene Kekulé structures, first proposed by Kekulé, are taken together as contributing structures to represent the total structure. In the hybrid structure on the right, the dashed hexagon replaces three double bonds, and represents six electrons in a set of three molecular orbitals of π symmetry, with a nodal plane in the plane of the molecule. In furan a lone pair of the oxygen atom interacts with the π orbitals of the carbon atoms. The curved arrows depict the permutation of delocalized π electrons, which results in different contributors. Electron-rich molecules The ozone molecule is represented by two contributing structures. In reality the two terminal oxygen atoms are equivalent and the hybrid structure is drawn on the right with a charge of −1⁄2 on both oxygen atoms and partial double bonds with a full and dashed line and bond order 1+1⁄2. For hypervalent molecules, the rationalization described above can be applied to generate contributing structures to explain the bonding in such molecules. Shown below are the contributing structures of a 3c-4e bond in xenon difluoride. [ F − XeF − ⟷ F − Xe − F ] {\displaystyle {\ce {[{\mathsf {F-XeF^{-}<->F^{-}Xe-F}}]}}} Electron-deficient molecules The allyl cation has two contributing structures with a positive charge on the terminal carbon atoms. In the hybrid structure their charge is +1⁄2. The full positive charge can also be depicted as delocalized among three carbon atoms. The diborane molecule is described by contributing structures, each with electron-deficiency on different atoms. This reduces the electron-deficiency on each atom and stabilizes the molecule. Below are the contributing structures of an individual 3c-2e bond in diborane. Reactive intermediates Often, reactive intermediates such as carbocations and free radicals have more delocalized structure than their parent reactants, giving rise to unexpected products. The classical example is allylic rearrangement. When 1 mole of HCl adds to 1 mole of 1,3-butadiene, in addition to the ordinarily expected product 3-chloro-1-butene, we also find 1-chloro-2-butene. Isotope labelling experiments have shown that what happens here is that the additional double bond shifts from 1,2 position to 2,3 position in some of the product. This and other evidence (such as NMR in superacid solutions) shows that the intermediate carbocation must have a highly delocalized structure, different from its mostly classical (delocalization exists but is small) parent molecule. This cation (an allylic cation) can be represented using resonance, as shown above. This observation of greater delocalization in less stable molecules is quite general. The excited states of conjugated dienes are stabilised more by conjugation than their ground states, causing them to become organic dyes. A well-studied example of delocalization that does not involve π electrons (hyperconjugation) can be observed in the non-classical 2-Norbornyl cation Another example is methanium (CH+5). These can be viewed as containing three-center two-electron bonds and are represented either by contributing structures involving rearrangement of σ electrons or by a special notation, a Y that has the three nuclei at its three points. Delocalized electrons are important for several reasons; a major one is that an expected chemical reaction may not occur because the electrons delocalize to a more stable configuration, resulting in a reaction that happens at a different location. An example is the Friedel–Crafts alkylation of benzene with 1-chloro-2-methylpropane; the carbocation rearranges to a tert-butyl group stabilized by hyperconjugation, a particular form of delocalization. Benzene Bond lengths Comparing the two contributing structures of benzene, all single and double bonds are interchanged. Bond lengths can be measured, for example using X-ray diffraction. The average length of a C–C single bond is 154 pm; that of a C=C double bond is 133 pm. In localized cyclohexatriene, the carbon–carbon bonds should be alternating 154 and 133 pm. Instead, all carbon–carbon bonds in benzene are found to be about 139 pm, a bond length intermediate between single and double bond. This mixed single and double bond (or triple bond) character is typical for all molecules in which bonds have a different bond order in different contributing structures. Bond lengths can be compared using bond orders. For example, in cyclohexane the bond order is 1 while that in benzene is 1 + (3 ÷ 6) = 1+1⁄2. Consequently, benzene has more double bond character and hence has a shorter bond length than cyclohexane. Resonance energy Resonance (or delocalization) energy is the amount of energy needed to convert the true delocalized structure into that of the most stable contributing structure. The empirical resonance energy can be estimated by comparing the enthalpy change of hydrogenation of the real substance with that estimated for the contributing structure. The complete hydrogenation of benzene to cyclohexane via 1,3-cyclohexadiene and cyclohexene is exothermic; 1 mole of benzene delivers 208.4 kJ (49.8 kcal). Hydrogenation of one mole of double bonds delivers 119.7 kJ (28.6 kcal), as can be deduced from the last step, the hydrogenation of cyclohexene. In benzene, however, 23.4 kJ (5.6 kcal) are needed to hydrogenate one mole of double bonds. The difference, being 143.1 kJ (34.2 kcal), is the empirical resonance energy of benzene. Because 1,3-cyclohexadiene also has a small delocalization energy (7.6 kJ or 1.8 kcal/mol) the net resonance energy, relative to the localized cyclohexatriene, is a bit higher: 151 kJ or 36 kcal/mol. This measured resonance energy is also the difference between the hydrogenation energy of three 'non-resonance' double bonds and the measured hydrogenation energy: (3 × 119.7) − 208.4 = 150.7 kJ/mol (36 kcal). Regardless of their exact values, resonance energies of various related compounds provide insights into their bonding. The resonance energies for pyrrole, thiophene, and furan are, respectively, 88, 121, and 67 kJ/mol (21, 29, and 16 kcal/mol). Thus, these heterocycles are far less aromatic than benzene, as is manifested in the lability of these rings. Quantum mechanical description in valence bond (VB) theory Resonance has a deeper significance in the mathematical formalism of valence bond theory (VB). Quantum mechanics requires that the wavefunction of a molecule obey its observed symmetry. If a single contributing structure does not achieve this, resonance is invoked. For example, in benzene, valence bond theory begins with the two Kekulé structures which do not individually possess the sixfold symmetry of the real molecule. The theory constructs the actual wave function as a linear superposition of the wave functions representing the two structures. As both Kekulé structures have equal energy, they are equal contributors to the overall structure – the superposition is an equally weighted average, or a 1:1 linear combination of the two in the case of benzene. The symmetric combination gives the ground state, while the antisymmetric combination gives the first excited state, as shown. In general, the superposition is written with undetermined coefficients, which are then variationally optimized to find the lowest possible energy for the given set of basis wave functions. When more contributing structures are included, the molecular wave function becomes more accurate and more excited states can be derived from different combinations of the contributing structures. Comparison with molecular orbital (MO) theory In molecular orbital theory, the main alternative to valence bond theory, the molecular orbitals (MOs) are approximated as sums of all the atomic orbitals (AOs) on all the atoms; there are as many MOs as AOs. Each AOi has a weighting coefficient ci that indicates the AO's contribution to a particular MO. For example, in benzene, the MO model gives us 6 π MOs which are combinations of the 2pz AOs on each of the 6 C atoms. Thus, each π MO is delocalized over the whole benzene molecule and any electron occupying an MO will be delocalized over the whole molecule. This MO interpretation has inspired the picture of the benzene ring as a hexagon with a circle inside. When describing benzene, the VB concept of localized σ bonds and the MO concept of delocalized π orbitals are frequently combined in elementary chemistry courses. The contributing structures in the VB model are particularly useful in predicting the effect of substituents on π systems such as benzene. They lead to the models of contributing structures for an electron-withdrawing group and electron-releasing group on benzene. The utility of MO theory is that a quantitative indication of the charge from the π system on an atom can be obtained from the squares of the weighting coefficient ci on atom Ci. Charge qi ≈ c2i. The reason for squaring the coefficient is that if an electron is described by an AO, then the square of the AO gives the electron density. The AOs are adjusted (normalized) so that AO2 = 1, and qi ≈ (ciAOi)2 ≈ c2i. In benzene, qi = 1 on each C atom. With an electron-withdrawing group qi < 1 on the ortho and para C atoms and qi > 1 for an electron-releasing group. Coefficients Weighting of the contributing structures in terms of their contribution to the overall structure can be calculated in multiple ways, using "Ab initio" methods derived from Valence Bond theory, or else from the Natural Bond Orbitals (NBO) approaches of Weinhold NBO5 Archived 2008-02-08 at the Wayback Machine, or finally from empirical calculations based on the Hückel method. A Hückel method-based software for teaching resonance is available on the HuLiS Web site. Charge delocalization In the case of ions it is common to speak about delocalized charge (charge delocalization). An example of delocalized charge in ions can be found in the carboxylate group, wherein the negative charge is centered equally on the two oxygen atoms. Charge delocalization in anions is an important factor determining their reactivity (generally: the higher the extent of delocalization the lower the reactivity) and, specifically, the acid strength of their conjugate acids. As a general rule, the better delocalized is the charge in an anion the stronger is its conjugate acid. For example, the negative charge in perchlorate anion (ClO−4) is evenly distributed among the symmetrically oriented oxygen atoms (and a part of it is also kept by the central chlorine atom). This excellent charge delocalization combined with the high number of oxygen atoms (four) and high electronegativity of the central chlorine atom leads to perchloric acid being one of the strongest known acids with a pKa value of −10. The extent of charge delocalization in an anion can be quantitatively expressed via the WAPS (weighted average positive sigma) parameter parameter and an analogous WANS (weighted average negative sigma) parameter is used for cations. WAPS and WANS values are given in e/Å4. Larger values indicate more localized charge in the corresponding ion. See also Hückel molecular orbital theory Conjugated system Fluxional molecule Avoided crossing External links Goudard, N.; Carissan, Y.; Hagebaum-Reignier, D.; Humbel, S. (2008). "HuLiS : Java Applet − Simple Hückel Theory and Mesomery − program logiciel software" (in French). Retrieved 29 October 2010. == References == In physics, the Landé g-factor is a particular example of a g-factor, namely for an electron with both spin and orbital angular momenta. It is named after Alfred Landé, who first described it in 1921. In atomic physics, the Landé g-factor is a multiplicative term appearing in the expression for the energy levels of an atom in a weak magnetic field. The quantum states of electrons in atomic orbitals are normally degenerate in energy, with these degenerate states all sharing the same angular momentum. When the atom is placed in a weak magnetic field, however, the degeneracy is lifted. Description The factor comes about during the calculation of the first-order perturbation in the energy of an atom when a weak uniform magnetic field (that is, weak in comparison to the system's internal magnetic field) is applied to the system. Formally we can write the factor as, g J = g L J ( J + 1 ) − S ( S + 1 ) + L ( L + 1 ) 2 J ( J + 1 ) + g S J ( J + 1 ) + S ( S + 1 ) − L ( L + 1 ) 2 J ( J + 1 ) . {\displaystyle g_{J}=g_{L}{\frac {J(J+1)-S(S+1)+L(L+1)}{2J(J+1)}}+g_{S}{\frac {J(J+1)+S(S+1)-L(L+1)}{2J(J+1)}}.} The orbital g L {\displaystyle g_{L}} is equal to 1, and under the approximation g S = 2 {\displaystyle g_{S}=2} , the above expression simplifies to g J = 1 + J ( J + 1 ) + S ( S + 1 ) − L ( L + 1 ) 2 J ( J + 1 ) . {\displaystyle g_{J}=1+{\frac {J(J+1)+S(S+1)-L(L+1)}{2J(J+1)}}.} Here, J is the total electronic angular momentum, L is the orbital angular momentum, and S is the spin angular momentum. Because S = 1 / 2 {\displaystyle S=1/2} for electrons, one often sees this formula written with 3/4 in place of S ( S + 1 ) {\displaystyle S(S+1)} . The quantities gL and gS are other g-factors of an electron. For an S = 0 {\displaystyle S=0} atom, g J = 1 {\displaystyle g_{J}=1} and for an L = 0 {\displaystyle L=0} atom, g J = 2 {\displaystyle g_{J}=2} . If we wish to know the g-factor for an atom with total atomic angular momentum F → = I → + J → {\displaystyle {\vec {F}}={\vec {I}}+{\vec {J}}} (nucleus + electrons), such that the total atomic angular momentum quantum number can take values of F = J + I , J + I − 1 , … , | J − I | {\displaystyle F=J+I,J+I-1,\dots ,|J-I|} , giving g F = g J F ( F + 1 ) − I ( I + 1 ) + J ( J + 1 ) 2 F ( F + 1 ) + g I μ N μ B F ( F + 1 ) + I ( I + 1 ) − J ( J + 1 ) 2 F ( F + 1 ) ≈ g J F ( F + 1 ) − I ( I + 1 ) + J ( J + 1 ) 2 F ( F + 1 ) {\displaystyle {\begin{aligned}g_{F}&=g_{J}{\frac {F(F+1)-I(I+1)+J(J+1)}{2F(F+1)}}+g_{I}{\frac {\mu _{\text{N}}}{\mu _{\text{B}}}}{\frac {F(F+1)+I(I+1)-J(J+1)}{2F(F+1)}}\\&\approx g_{J}{\frac {F(F+1)-I(I+1)+J(J+1)}{2F(F+1)}}\end{aligned}}} Here μ B {\displaystyle \mu _{\text{B}}} is the Bohr magneton and μ N {\displaystyle \mu _{\text{N}}} is the nuclear magneton. This last approximation is justified because μ N {\displaystyle \mu _{\text{N}}} is smaller than μ B {\displaystyle \mu _{\text{B}}} by the ratio of the electron mass to the proton mass. A derivation The following working is a common derivation. Both orbital angular momentum and spin angular momentum of electron contribute to the magnetic moment. In particular, each of them alone contributes to the magnetic moment by the following form μ → L = − L → g L μ B / ℏ {\displaystyle {\vec {\mu }}_{L}=-{\vec {L}}g_{L}\mu _{\rm {B}}/\hbar } μ → S = − S → g S μ B / ℏ {\displaystyle {\vec {\mu }}_{S}=-{\vec {S}}g_{S}\mu _{\rm {B}}/\hbar } μ → J = μ → L + μ → S {\displaystyle {\vec {\mu }}_{J}={\vec {\mu }}_{L}+{\vec {\mu }}_{S}} where g L = 1 {\displaystyle g_{L}=1} g S ≈ 2 {\displaystyle g_{S}\approx 2} Note that negative signs in the above expressions are because an electron carries negative charge, and the value of g S {\displaystyle g_{S}} can be derived naturally from the Dirac equation. The total magnetic moment μ → J {\displaystyle {\vec {\mu }}_{J}} , as a vector operator, does not lie on the direction of total angular momentum J → = L → + S → {\displaystyle {\vec {J}}={\vec {L}}+{\vec {S}}} , because the g-factors for orbital and spin part are different. However, due to Wigner–Eckart theorem, its expectation value does effectively lie on the direction of J → {\displaystyle {\vec {J}}} which can be employed in the determination of the g-factor according to the rules of angular momentum coupling. In particular, the g-factor is defined as a consequence of the theorem itself ⟨ J , J z | μ → J | J , J z ′ ⟩ = − g J μ B ⟨ J , J z | J → | J , J z ′ ⟩ {\displaystyle \langle J,J_{\text{z}}|{\vec {\mu }}_{J}|J,J'_{\text{z}}\rangle =-g_{J}\mu _{\rm {B}}\langle J,J_{\text{z}}|{\vec {J}}|J,J'_{\text{z}}\rangle } Therefore, ⟨ J , J z | μ → J | J , J z ′ ⟩ ⋅ ⟨ J , J z ′ | J → | J , J z ⟩ = − g J μ B ⟨ J , J z | J → | J , J z ′ ⟩ ⋅ ⟨ J , J z ′ | J → | J , J z ⟩ {\displaystyle \langle J,J_{\text{z}}|{\vec {\mu }}_{J}|J,J'_{\text{z}}\rangle \cdot \langle J,J'_{\text{z}}|{\vec {J}}|J,J_{\text{z}}\rangle =-g_{J}\mu _{\rm {B}}\langle J,J_{\text{z}}|{\vec {J}}|J,J'_{\text{z}}\rangle \cdot \langle J,J'_{\text{z}}|{\vec {J}}|J,J_{\text{z}}\rangle } ∑ J z ′ ⟨ J , J z | μ → J | J , J z ′ ⟩ ⋅ ⟨ J , J z ′ | J → | J , J z ⟩ = − ∑ J z ′ g J μ B ⟨ J , J z | J → | J , J z ′ ⟩ ⋅ ⟨ J , J z ′ | J → | J , J z ⟩ {\displaystyle \sum _{J'_{\text{z}}}\langle J,J_{\text{z}}|{\vec {\mu }}_{J}|J,J'_{\text{z}}\rangle \cdot \langle J,J'_{\text{z}}|{\vec {J}}|J,J_{\text{z}}\rangle =-\sum _{J'_{\text{z}}}g_{J}\mu _{\rm {B}}\langle J,J_{\text{z}}|{\vec {J}}|J,J'_{\text{z}}\rangle \cdot \langle J,J'_{\text{z}}|{\vec {J}}|J,J_{\text{z}}\rangle } ⟨ J , J z | μ → J ⋅ J → | J , J z ⟩ = − g J μ B ⟨ J , J z | J → ⋅ J → | J , J z ⟩ = − g J μ B ℏ 2 J ( J + 1 ) {\displaystyle \langle J,J_{\text{z}}|{\vec {\mu }}_{J}\cdot {\vec {J}}|J,J_{\text{z}}\rangle =-g_{J}\mu _{\rm {B}}\langle J,J_{\text{z}}|{\vec {J}}\cdot {\vec {J}}|J,J_{\text{z}}\rangle =-g_{J}\mu _{\rm {B}}\quad \hbar ^{2}J(J+1)} One gets g J ⟨ J , J z | J → ⋅ J → | J , J z ⟩ = ⟨ J , J z | g L L → ⋅ J → + g S S → ⋅ J → | J , J z ⟩ = ⟨ J , J z | g L [ L → 2 + 1 2 ( J → 2 − L → 2 − S → 2 ) ] + g S [ S → 2 + 1 2 ( J → 2 − L → 2 − S → 2 ) ] | J , J z ⟩ = g L ℏ 2 2 [ J ( J + 1 ) + L ( L + 1 ) − S ( S + 1 ) ] + g S ℏ 2 2 [ J ( J + 1 ) − L ( L + 1 ) + S ( S + 1 ) ] g J = g L J ( J + 1 ) + L ( L + 1 ) − S ( S + 1 ) 2 J ( J + 1 ) + g S J ( J + 1 ) − L ( L + 1 ) + S ( S + 1 ) 2 J ( J + 1 ) {\displaystyle {\begin{aligned}g_{J}\langle J,J_{\text{z}}|{\vec {J}}\cdot {\vec {J}}|J,J_{\text{z}}\rangle &=\langle J,J_{\text{z}}|g_{L}{{\vec {L}}\cdot {\vec {J}}}+g_{S}{{\vec {S}}\cdot {\vec {J}}}|J,J_{\text{z}}\rangle \\&=\langle J,J_{\text{z}}|g_{L}{\left[{\vec {L}}^{2}+{\frac {1}{2}}\left({\vec {J}}^{2}-{\vec {L}}^{2}-{\vec {S}}^{2}\right)\right]}+g_{S}{\left[{\vec {S}}^{2}+{\frac {1}{2}}\left({\vec {J}}^{2}-{\vec {L}}^{2}-{\vec {S}}^{2}\right)\right]}|J,J_{\text{z}}\rangle \\&={\frac {g_{L}\hbar ^{2}}{2}}[J(J+1)+L(L+1)-S(S+1)]+{\frac {g_{S}\hbar ^{2}}{2}}[J(J+1)-L(L+1)+S(S+1)]\\g_{J}&=g_{L}{\frac {J(J+1)+L(L+1)-S(S+1)}{2J(J+1)}}+g_{S}{\frac {J(J+1)-L(L+1)+S(S+1)}{2J(J+1)}}\end{aligned}}} Table of values The following table gives the calculated Lande g-factors for some common term symbols in the approximation g S = 2 {\displaystyle g_{S}=2} . See also Einstein–de Haas effect, Zeeman effect, g-factor (physics). == References == In physics, specifically statistical mechanics, a population inversion occurs when a system (such as a group of atoms or molecules) exists in a state in which more members of the system are in higher, excited states than in lower, unexcited energy states. It is called an "inversion" because in many familiar and commonly encountered physical systems in thermal equilibrium, this is not possible. This concept is of fundamental importance in laser science because the production of a population inversion is a necessary step in the workings of a standard laser. Boltzmann distributions and thermal equilibrium To understand the concept of a population inversion, it is necessary to understand some thermodynamics and the way that light interacts with matter. To do so, it is useful to consider a very simple assembly of atoms forming a laser medium. Assume there is a group of N atoms, each of which is capable of being in one of two energy states: either The ground state, with energy E1; or The excited state, with energy E2, with E2 > E1. The number of these atoms which are in the ground state is given by N1, and the number in the excited state N2. Since there are N atoms in total, N 1 + N 2 = N {\displaystyle N_{1}+N_{2}=N} The energy difference between the two states, given by Δ E 12 = E 2 − E 1 , {\displaystyle \Delta E_{12}=E_{2}-E_{1},} determines the characteristic frequency ν 12 {\textstyle \nu _{12}} of light which will interact with the atoms; This is given by the relation E 2 − E 1 = Δ E 12 = h ν 12 , {\displaystyle E_{2}-E_{1}=\Delta E_{12}=h\nu _{12},} h being the Planck constant. If the group of atoms is in thermal equilibrium, it can be shown from Maxwell–Boltzmann statistics that the ratio of the number of atoms in each state is given by the ratio of two Boltzmann distributions, the Boltzmann factor: N 2 N 1 = g 2 g 1 exp ⁡ − ( E 2 − E 1 ) k T , {\displaystyle {\frac {N_{2}}{N_{1}}}={\frac {g_{2}}{g_{1}}}\exp {\frac {-(E_{2}-E_{1})}{kT}},} where T is the thermodynamic temperature of the group of atoms, k is the Boltzmann constant and g1 and g2 are the degeneracies of each state. Calculable is the ratio of the populations of the two states at room temperature (T ≈ 300 K) for an energy difference ΔE that corresponds to light of a frequency corresponding to visible light (ν ≈ 5×1014 Hz). In this case ΔE = E2 − E1 ≈ 2.07 eV, and kT ≈ 0.026 eV. Since E2 − E1 ≫ kT, it follows that the argument of the exponential in the equation above is a large negative number, and as such N2/N1 is vanishingly small; i.e., there are almost no atoms in the excited state. When in thermal equilibrium, then, it is seen that the lower energy state is more populated than the higher energy state, and this is the normal state of the system. As T increases, the number of electrons in the high-energy state (N2) increases, but N2 never exceeds N1 for a system at thermal equilibrium; rather, at infinite temperature, the populations N2 and N1 become equal. In other words, a population inversion (N2/N1 > 1) can never exist for a system at thermal equilibrium. To achieve population inversion therefore requires pushing the system into a non-equilibrated state. Interaction of light with matter There are three types of possible interactions between a system of atoms and light that are of interest: Absorption If light (photons) of frequency ν12 passes through the group of atoms, there is a possibility of the light being absorbed by electrons which are in the ground state, which will cause them to be excited to the higher energy state. The rate of absorption is proportional to the radiation density of the light, and also to the number of atoms currently in the ground state, N1. Spontaneous emission If atoms are in the excited state, spontaneous decay events to the ground state will occur at a rate proportional to N2, the number of atoms in the excited state. The energy difference between the two states ΔE21 is emitted from the atom as a photon of frequency ν21 as given by the frequency-energy relation above. The photons are emitted stochastically, and there is no fixed phase relationship between photons emitted from a group of excited atoms; in other words, spontaneous emission is incoherent. In the absence of other processes, the number of atoms in the excited state at time t, is given by N 2 ( t ) = N 2 ( 0 ) exp ⁡ − t τ 21 , {\displaystyle N_{2}(t)=N_{2}(0)\exp {\frac {-t}{\tau _{21}}},} where N2(0) is the number of excited atoms at time t = 0, and τ21 is the mean lifetime of the transition between the two states. Stimulated emission If an atom is already in the excited state, it may be agitated by the passage of a photon that has a frequency ν21 corresponding to the energy gap ΔE of the excited state to ground state transition. In this case, the excited atom relaxes to the ground state, and it produces a second photon of frequency ν21. The original photon is not absorbed by the atom, and so the result is two photons of the same frequency. This process is known as stimulated emission. Specifically, an excited atom will act like a small electric dipole which will oscillate with the external field provided. One of the consequences of this oscillation is that it encourages electrons to decay to the lowest energy state. When this happens due to the presence of the electromagnetic field from a photon, a photon is released in the same phase and direction as the "stimulating" photon, and is called stimulated emission. The rate at which stimulated emission occurs is proportional to the number of atoms N2 in the excited state, and the radiation density of the light. The base probability of a photon causing stimulated emission in a single excited atom was shown by Albert Einstein to be exactly equal to the probability of a photon being absorbed by an atom in the ground state. Therefore, when the numbers of atoms in the ground and excited states are equal, the rate of stimulated emission is equal to the rate of absorption for a given radiation density. The critical detail of stimulated emission is that the induced photon has the same frequency and phase as the incident photon. In other words, the two photons are coherent. It is this property that allows optical amplification, and the production of a laser system. During the operation of a laser, all three light-matter interactions described above are taking place. Initially, atoms are energized from the ground state to the excited state by a process called pumping, described below. Some of these atoms decay via spontaneous emission, releasing incoherent light as photons of frequency, ν. These photons are fed back into the laser medium, usually by an optical resonator. Some of these photons are absorbed by the atoms in the ground state, and the photons are lost to the laser process. However, some photons cause stimulated emission in excited-state atoms, releasing another coherent photon. In effect, this results in optical amplification. If the number of photons being amplified per unit time is greater than the number of photons being absorbed, then the net result is a continuously increasing number of photons being produced; the laser medium is said to have a gain of greater than unity. Recall from the descriptions of absorption and stimulated emission above that the rates of these two processes are proportional to the number of atoms in the ground and excited states, N1 and N2, respectively. If the ground state has a higher population than the excited state (N1 > N2), then the absorption process dominates, and there is a net attenuation of photons. If the populations of the two states are the same (N1 = N2), the rate of absorption of light exactly balances the rate of emission; the medium is then said to be optically transparent. If the higher energy state has a greater population than the lower energy state (N1 < N2), then the emission process dominates, and light in the system undergoes a net increase in intensity. It is thus clear that to produce a faster rate of stimulated emissions than absorptions, it is required that the ratio of the populations of the two states is such that N2/N1 > 1; In other words, a population inversion is required for laser operation. Selection rules Many transitions involving electromagnetic radiation are strictly forbidden under quantum mechanics. The allowed transitions are described by so-called selection rules, which describe the conditions under which a radiative transition is allowed. For instance, transitions are only allowed if ΔS = 0, S being the total spin angular momentum of the system. In real materials, other effects, such as interactions with the crystal lattice, intervene to circumvent the formal rules by providing alternate mechanisms. In these systems, the forbidden transitions can occur, but usually at slower rates than allowed transitions. A classic example is phosphorescence where a material has a ground state with S = 0, an excited state with S = 0, and an intermediate state with S = 1. The transition from the intermediate state to the ground state by emission of light is slow because of the selection rules. Thus emission may continue after the external illumination is removed. In contrast fluorescence in materials is characterized by emission which ceases when the external illumination is removed. Transitions that do not involve the absorption or emission of radiation are not affected by selection rules. The radiationless transition between levels, such as between the excited S = 0 and S = 1 states, may proceed quickly enough to siphon off a portion of the S = 0 population before it spontaneously returns to the ground state. The existence of intermediate states in materials is essential to the technique of optical pumping of lasers (see below). Creating a population inversion A population inversion is required for laser operation, but cannot be achieved in the above theoretical group of atoms with two energy-levels when they are in thermal equilibrium. In fact, any method by which the atoms are directly and continuously excited from the ground state to the excited state (such as optical absorption) will eventually reach equilibrium with the de-exciting processes of spontaneous and stimulated emission. At best, an equal population of the two states, N1 = N2 = N/2, can be achieved, resulting in optical transparency but no net optical gain. Three-level lasers To achieve lasting non-equilibrium conditions, an indirect method of populating the excited state must be used. To understand how this is done, consider a slightly more realistic model, that of a three-level laser. Again consider a group of N atoms, this time with each atom able to exist in any of three energy states, levels 1, 2 and 3, with energies E1, E2, and E3, and populations N1, N2, and N3, respectively. Assume E1 < E2 < E3; that is, the energy of level 2 lies between that of the ground state and level 3. Initially, the system of atoms is at thermal equilibrium, and the majority of the atoms will be in the ground state, i.e., N1 ≈ N, N2 ≈ N3 ≈ 0. If the atoms are subjected to light of a frequency ν 13 = 1 h ( E 3 − E 1 ) {\displaystyle \scriptstyle \nu _{13}\,=\,{\frac {1}{h}}\left(E_{3}-E_{1}\right)} , the process of optical absorption will excite electrons from the ground state to level 3. This process is called pumping, and does not necessarily always directly involve light absorption; other methods of exciting the laser medium, such as electrical discharge or chemical reactions, may be used. The level 3 is sometimes referred to as the pump level or pump band, and the energy transition E1 → E3 as the pump transition, which is shown as the arrow marked P in the diagram on the right. Upon pumping the medium, an appreciable number of atoms will transition to level 3, such that N3 > 0. To have a medium suitable for laser operation, it is necessary that these excited atoms quickly decay to level 2. The energy released in this transition may be emitted as a photon (spontaneous emission), however in practice the 3 → 2 transition called the Auger effect (labeled R in the diagram) is usually radiationless, with the energy being transferred to vibrational motion (heat) of the host material surrounding the atoms, without the generation of a photon. An electron in level 2 may decay by spontaneous emission to the ground state, releasing a photon of frequency ν12 (given by E2 − E1 = hν12), which is shown as the transition L, called the laser transition in the diagram. If the lifetime of this transition, τ21 is much longer than the lifetime of the radiationless 3 → 2 transition τ32 (if τ21 ≫ τ32, known as a favourable lifetime ratio), the population of the E3 will be essentially zero (N3 ≈ 0) and a population of excited state atoms will accumulate in level 2 (N2 > 0). If over half the N atoms can be accumulated in this state, this will exceed the population of the ground state N1. A population inversion (N2 > N1 ) has thus been achieved between level 1 and 2, and optical amplification at the frequency ν21 can be obtained. Because at least half the population of atoms must be excited from the ground state to obtain a population inversion, the laser medium must be very strongly pumped. This makes three-level lasers rather inefficient, despite being the first type of laser to be discovered (based on a ruby laser medium, by Theodore Maiman in 1960). A three-level system could also have a radiative transition between level 3 and 2, and a non-radiative transition between 2 and 1. In this case, the pumping requirements are weaker. In practice, most lasers are four-level lasers, described below. Four-level laser Here, there are four energy levels, energies E1, E2, E3, E4, and populations N1, N2, N3, N4, respectively. The energies of each level are such that E1 < E2 < E3 < E4. In this system, the pumping transition P excites the atoms in the ground state (level 1) into the pump band (level 4). From level 4, the atoms again decay by a fast, non-radiative transition Ra into the level 3. Since the lifetime of the laser transition L is long compared to that of Ra (τ32 ≫ τ43), a population accumulates in level 3 (the upper laser level), which may relax by spontaneous or stimulated emission into level 2 (the lower laser level). This level likewise has a fast, non-radiative decay Rb into the ground state. As before, the presence of a fast, radiationless decay transition results in the population of the pump band being quickly depleted (N4 ≈ 0). In a four-level system, any atom in the lower laser level E2 is also quickly de-excited, leading to a negligible population in that state (N2 ≈ 0). This is important, since any appreciable population accumulating in level 3, the upper laser level, will form a population inversion with respect to level 2. That is, as long as N3 > 0, then N3 > N2, and a population inversion is achieved. Thus optical amplification, and laser operation, can take place at a frequency of ν32 (E3 − E2 = hν32). Since only a few atoms must be excited into the upper laser level to form a population inversion, a four-level laser is much more efficient than a three-level one, and most practical lasers are of this type. In reality, many more than four energy levels may be involved in the laser process, with complex excitation and relaxation processes involved between these levels. In particular, the pump band may consist of several distinct energy levels, or a continuum of levels, which allow optical pumping of the medium over a wide range of wavelengths. Note that in both three- and four-level lasers, the energy of the pumping transition is greater than that of the laser transition. This means that, if the laser is optically pumped, the frequency of the pumping light must be greater than that of the resulting laser light. In other words, the pump wavelength is shorter than the laser wavelength. It is possible in some media to use multiple photon absorptions between multiple lower-energy transitions to reach the pump level; such lasers are called up-conversion lasers. While in many lasers the laser process involves the transition of atoms between different electronic energy states, as described in the model above, this is not the only mechanism that can result in laser action. For example, there are many common lasers (e.g., dye lasers, carbon dioxide lasers) where the laser medium consists of complete molecules, and energy states correspond to vibrational and rotational modes of oscillation of the molecules. This is the case with water masers, that occur in nature. In some media it is possible, by imposing an additional optical or microwave field, to use quantum coherence effects to reduce the likelihood of a ground-state to excited-state transition. This technique, known as lasing without inversion, allows optical amplification to take place without producing a population inversion between the two states. Other methods of creating a population inversion Stimulated emission was first observed in the microwave region of the electromagnetic spectrum, giving rise to the acronym MASER for Microwave Amplification by Stimulated Emission of Radiation. In the microwave region, the Boltzmann distribution of molecules among energy states is such that, at room temperature, all states are populated almost equally. To create a population inversion under these conditions, it is necessary to selectively remove some atoms or molecules from the system based on differences in properties. For instance, in a hydrogen maser, the well-known 21cm wave transition in atomic hydrogen, where the lone electron flips its spin state from parallel to the nuclear spin to antiparallel, can be used to create a population inversion because the parallel state has a magnetic moment and the antiparallel state does not. A strong inhomogeneous magnetic field will separate atoms in the higher energy state from a beam of mixed-state atoms. The separated population represents a population inversion that can exhibit stimulated emissions. See also Laser construction Negative temperature Quantum electronics References Svelto, Orazio (1998). Principles of Lasers, 4th ed. (trans. David Hanna), Springer. ISBN 0-306-45748-2 In quantum field theory a product of quantum fields, or equivalently their creation and annihilation operators, is usually said to be normal ordered (also called Wick order) when all creation operators are to the left of all annihilation operators in the product. The process of putting a product into normal order is called normal ordering (also called Wick ordering). The terms antinormal order and antinormal ordering are analogously defined, where the annihilation operators are placed to the left of the creation operators. Normal ordering of a product of quantum fields or creation and annihilation operators can also be defined in many other ways. Which definition is most appropriate depends on the expectation values needed for a given calculation. Most of this article uses the most common definition of normal ordering as given above, which is appropriate when taking expectation values using the vacuum state of the creation and annihilation operators. The process of normal ordering is particularly important for a quantum mechanical Hamiltonian. When quantizing a classical Hamiltonian there is some freedom when choosing the operator order, and these choices lead to differences in the ground state energy. That's why the process can also be used to eliminate the infinite vacuum energy of a quantum field. Notation If O ^ {\displaystyle {\hat {O}}} denotes an arbitrary product of creation and/or annihilation operators (or equivalently, quantum fields), then the normal ordered form of O ^ {\displaystyle {\hat {O}}} is denoted by : O ^ : {\displaystyle {\mathopen {:}}{\hat {O}}{\mathclose {:}}} . An alternative notation is N ( O ^ ) {\displaystyle {\mathcal {N}}({\hat {O}})} . Note that normal ordering is a concept that only makes sense for products of operators. Attempting to apply normal ordering to a sum of operators is not useful as normal ordering is not a linear operation. Bosons Bosons are particles which satisfy Bose–Einstein statistics. We will now examine the normal ordering of bosonic creation and annihilation operator products. Single bosons If we start with only one type of boson there are two operators of interest: b ^ † {\displaystyle {\hat {b}}^{\dagger }} : the boson's creation operator. b ^ {\displaystyle {\hat {b}}} : the boson's annihilation operator. These satisfy the commutator relationship [ b ^ † , b ^ † ] − = 0 {\displaystyle \left[{\hat {b}}^{\dagger },{\hat {b}}^{\dagger }\right]_{-}=0} [ b ^ , b ^ ] − = 0 {\displaystyle \left[{\hat {b}},{\hat {b}}\right]_{-}=0} [ b ^ , b ^ † ] − = 1 {\displaystyle \left[{\hat {b}},{\hat {b}}^{\dagger }\right]_{-}=1} where [ A , B ] − ≡ A B − B A {\displaystyle \left[A,B\right]_{-}\equiv AB-BA} denotes the commutator. We may rewrite the last one as: b ^ b ^ † = b ^ † b ^ + 1. {\displaystyle {\hat {b}}\,{\hat {b}}^{\dagger }={\hat {b}}^{\dagger }\,{\hat {b}}+1.} Examples 1. We'll consider the simplest case first. This is the normal ordering of b ^ † b ^ {\displaystyle {\hat {b}}^{\dagger }{\hat {b}}} : : b ^ † b ^ : = b ^ † b ^ . {\displaystyle {:\,}{\hat {b}}^{\dagger }\,{\hat {b}}{\,:}={\hat {b}}^{\dagger }\,{\hat {b}}.} The expression b ^ † b ^ {\displaystyle {\hat {b}}^{\dagger }\,{\hat {b}}} has not been changed because it is already in normal order - the creation operator ( b ^ † ) {\displaystyle ({\hat {b}}^{\dagger })} is already to the left of the annihilation operator ( b ^ ) {\displaystyle ({\hat {b}})} . 2. A more interesting example is the normal ordering of b ^ b ^ † {\displaystyle {\hat {b}}\,{\hat {b}}^{\dagger }} : : b ^ b ^ † : = b ^ † b ^ . {\displaystyle {:\,}{\hat {b}}\,{\hat {b}}^{\dagger }{\,:}={\hat {b}}^{\dagger }\,{\hat {b}}.} Here the normal ordering operation has reordered the terms by placing b ^ † {\displaystyle {\hat {b}}^{\dagger }} to the left of b ^ {\displaystyle {\hat {b}}} . These two results can be combined with the commutation relation obeyed by b ^ {\displaystyle {\hat {b}}} and b ^ † {\displaystyle {\hat {b}}^{\dagger }} to get b ^ b ^ † = b ^ † b ^ + 1 = : b ^ b ^ † : + 1. {\displaystyle {\hat {b}}\,{\hat {b}}^{\dagger }={\hat {b}}^{\dagger }\,{\hat {b}}+1={:\,}{\hat {b}}\,{\hat {b}}^{\dagger }{\,:}\;+1.} or b ^ b ^ † − : b ^ b ^ † : = 1. {\displaystyle {\hat {b}}\,{\hat {b}}^{\dagger }-{:\,}{\hat {b}}\,{\hat {b}}^{\dagger }{\,:}=1.} This equation is used in defining the contractions used in Wick's theorem. 3. An example with multiple operators is: : b ^ † b ^ b ^ b ^ † b ^ b ^ † b ^ : = b ^ † b ^ † b ^ † b ^ b ^ b ^ b ^ = ( b ^ † ) 3 b ^ 4 . {\displaystyle {:\,}{\hat {b}}^{\dagger }\,{\hat {b}}\,{\hat {b}}\,{\hat {b}}^{\dagger }\,{\hat {b}}\,{\hat {b}}^{\dagger }\,{\hat {b}}{\,:}={\hat {b}}^{\dagger }\,{\hat {b}}^{\dagger }\,{\hat {b}}^{\dagger }\,{\hat {b}}\,{\hat {b}}\,{\hat {b}}\,{\hat {b}}=({\hat {b}}^{\dagger })^{3}\,{\hat {b}}^{4}.} 4. A simple example shows that normal ordering cannot be extended by linearity from the monomials to all operators in a self-consistent way. Assume that we can apply the commutation relations to obtain: : b ^ b ^ † : = : 1 + b ^ † b ^ : . {\displaystyle {:\,}{\hat {b}}{\hat {b}}^{\dagger }{\,:}={:\,}1+{\hat {b}}^{\dagger }{\hat {b}}{\,:}.} Then, by linearity, : 1 + b ^ † b ^ : = : 1 : + : b ^ † b ^ : = 1 + b ^ † b ^ ≠ b ^ † b ^ = : b ^ b ^ † : , {\displaystyle {:\,}1+{\hat {b}}^{\dagger }{\hat {b}}{\,:}={:\,}1{\,:}+{:\,}{\hat {b}}^{\dagger }{\hat {b}}{\,:}=1+{\hat {b}}^{\dagger }{\hat {b}}\neq {\hat {b}}^{\dagger }{\hat {b}}={:\,}{\hat {b}}{\hat {b}}^{\dagger }{\,:},} a contradiction. The implication is that normal ordering is not a linear function on operators, but on the free algebra generated by the operators, i.e. the operators do not satisfy the canonical commutation relations while inside the normal ordering (or any other ordering operator like time-ordering, etc). Multiple bosons If we now consider N {\displaystyle N} different bosons there are 2 N {\displaystyle 2N} operators: b ^ i † {\displaystyle {\hat {b}}_{i}^{\dagger }} : the i t h {\displaystyle i^{th}} boson's creation operator. b ^ i {\displaystyle {\hat {b}}_{i}} : the i t h {\displaystyle i^{th}} boson's annihilation operator. Here i = 1 , … , N {\displaystyle i=1,\ldots ,N} . These satisfy the commutation relations: [ b ^ i † , b ^ j † ] − = 0 {\displaystyle \left[{\hat {b}}_{i}^{\dagger },{\hat {b}}_{j}^{\dagger }\right]_{-}=0} [ b ^ i , b ^ j ] − = 0 {\displaystyle \left[{\hat {b}}_{i},{\hat {b}}_{j}\right]_{-}=0} [ b ^ i , b ^ j † ] − = δ i j {\displaystyle \left[{\hat {b}}_{i},{\hat {b}}_{j}^{\dagger }\right]_{-}=\delta _{ij}} where i , j = 1 , … , N {\displaystyle i,j=1,\ldots ,N} and δ i j {\displaystyle \delta _{ij}} denotes the Kronecker delta. These may be rewritten as: b ^ i † b ^ j † = b ^ j † b ^ i † {\displaystyle {\hat {b}}_{i}^{\dagger }\,{\hat {b}}_{j}^{\dagger }={\hat {b}}_{j}^{\dagger }\,{\hat {b}}_{i}^{\dagger }} b ^ i b ^ j = b ^ j b ^ i {\displaystyle {\hat {b}}_{i}\,{\hat {b}}_{j}={\hat {b}}_{j}\,{\hat {b}}_{i}} b ^ i b ^ j † = b ^ j † b ^ i + δ i j . {\displaystyle {\hat {b}}_{i}\,{\hat {b}}_{j}^{\dagger }={\hat {b}}_{j}^{\dagger }\,{\hat {b}}_{i}+\delta _{ij}.} Examples 1. For two different bosons ( N = 2 {\displaystyle N=2} ) we have : b ^ 1 † b ^ 2 : = b ^ 1 † b ^ 2 {\displaystyle :{\hat {b}}_{1}^{\dagger }\,{\hat {b}}_{2}:\,={\hat {b}}_{1}^{\dagger }\,{\hat {b}}_{2}} : b ^ 2 b ^ 1 † : = b ^ 1 † b ^ 2 {\displaystyle :{\hat {b}}_{2}\,{\hat {b}}_{1}^{\dagger }:\,={\hat {b}}_{1}^{\dagger }\,{\hat {b}}_{2}} 2. For three different bosons ( N = 3 {\displaystyle N=3} ) we have : b ^ 1 † b ^ 2 b ^ 3 : = b ^ 1 † b ^ 2 b ^ 3 {\displaystyle :{\hat {b}}_{1}^{\dagger }\,{\hat {b}}_{2}\,{\hat {b}}_{3}:\,={\hat {b}}_{1}^{\dagger }\,{\hat {b}}_{2}\,{\hat {b}}_{3}} Notice that since (by the commutation relations) b ^ 2 b ^ 3 = b ^ 3 b ^ 2 {\displaystyle {\hat {b}}_{2}\,{\hat {b}}_{3}={\hat {b}}_{3}\,{\hat {b}}_{2}} the order in which we write the annihilation operators does not matter. : b ^ 2 b ^ 1 † b ^ 3 : = b ^ 1 † b ^ 2 b ^ 3 {\displaystyle :{\hat {b}}_{2}\,{\hat {b}}_{1}^{\dagger }\,{\hat {b}}_{3}:\,={\hat {b}}_{1}^{\dagger }\,{\hat {b}}_{2}\,{\hat {b}}_{3}} : b ^ 3 b ^ 2 b ^ 1 † : = b ^ 1 † b ^ 2 b ^ 3 {\displaystyle :{\hat {b}}_{3}{\hat {b}}_{2}\,{\hat {b}}_{1}^{\dagger }:\,={\hat {b}}_{1}^{\dagger }\,{\hat {b}}_{2}\,{\hat {b}}_{3}} Bosonic operator functions Normal ordering of bosonic operator functions f ( n ^ ) {\displaystyle f({\hat {n}})} , with occupation number operator n ^ = b ^ n ^ † b ^ {\displaystyle {\hat {n}}={\hat {b}}{\vphantom {\hat {n}}}^{\dagger }{\hat {b}}} , can be accomplished using (falling) factorial powers n ^ k _ = n ^ ( n ^ − 1 ) ⋯ ( n ^ − k + 1 ) {\displaystyle {\hat {n}}^{\underline {k}}={\hat {n}}({\hat {n}}-1)\cdots ({\hat {n}}-k+1)} and Newton series instead of Taylor series: It is easy to show that factorial powers n ^ k _ {\displaystyle {\hat {n}}^{\underline {k}}} are equal to normal-ordered (raw) powers n ^ k {\displaystyle {\hat {n}}^{k}} and are therefore normal ordered by construction, n ^ k _ = b ^ n ^ † k b ^ n ^ k = : n ^ k : , {\displaystyle {\hat {n}}^{\underline {k}}={\hat {b}}{\vphantom {\hat {n}}}^{\dagger k}{\hat {b}}{\vphantom {\hat {n}}}^{k}={:\,}{\hat {n}}^{k}{\,:},} such that the Newton series expansion f ~ ( n ^ ) = ∑ k = 0 ∞ Δ n k f ~ ( 0 ) n ^ k _ k ! {\displaystyle {\tilde {f}}({\hat {n}})=\sum _{k=0}^{\infty }\Delta _{n}^{k}{\tilde {f}}(0)\,{\frac {{\hat {n}}^{\underline {k}}}{k!}}} of an operator function f ~ ( n ^ ) {\displaystyle {\tilde {f}}({\hat {n}})} , with k {\displaystyle k} -th forward difference Δ n k f ~ ( 0 ) {\displaystyle \Delta _{n}^{k}{\tilde {f}}(0)} at n = 0 {\displaystyle n=0} , is always normal ordered. Here, the eigenvalue equation n ^ | n ⟩ = n | n ⟩ {\displaystyle {\hat {n}}|n\rangle =n|n\rangle } relates n ^ {\displaystyle {\hat {n}}} and n {\displaystyle n} . As a consequence, the normal-ordered Taylor series of an arbitrary function f ( n ^ ) {\displaystyle f({\hat {n}})} is equal to the Newton series of an associated function f ~ ( n ^ ) {\displaystyle {\tilde {f}}({\hat {n}})} , fulfilling f ~ ( n ^ ) = : f ( n ^ ) : , {\displaystyle {\tilde {f}}({\hat {n}})={:\,}f({\hat {n}}){\,:},} if the series coefficients of the Taylor series of f ( x ) {\displaystyle f(x)} , with continuous x {\displaystyle x} , match the coefficients of the Newton series of f ~ ( n ) {\displaystyle {\tilde {f}}(n)} , with integer n {\displaystyle n} , f ( x ) = ∑ k = 0 ∞ F k x k k ! , f ~ ( n ) = ∑ k = 0 ∞ F k n k _ k ! , F k = ∂ x k f ( 0 ) = Δ n k f ~ ( 0 ) , {\displaystyle {\begin{aligned}f(x)&=\sum _{k=0}^{\infty }F_{k}\,{\frac {x^{k}}{k!}},\\{\tilde {f}}(n)&=\sum _{k=0}^{\infty }F_{k}\,{\frac {n^{\underline {k}}}{k!}},\\F_{k}&=\partial _{x}^{k}f(0)=\Delta _{n}^{k}{\tilde {f}}(0),\end{aligned}}} with k {\displaystyle k} -th partial derivative ∂ x k f ( 0 ) {\displaystyle \partial _{x}^{k}f(0)} at x = 0 {\displaystyle x=0} . The functions f {\displaystyle f} and f ~ {\displaystyle {\tilde {f}}} are related through the so-called normal-order transform N [ f ] {\displaystyle {\mathcal {N}}[f]} according to f ~ ( n ) = N x [ f ( x ) ] ( n ) = 1 Γ ( − n ) ∫ − ∞ 0 d x e x f ( x ) ( − x ) − ( n + 1 ) = 1 Γ ( − n ) M − x [ e x f ( x ) ] ( − n ) , {\displaystyle {\begin{aligned}{\tilde {f}}(n)&={\mathcal {N}}_{x}[f(x)](n)\\&={\frac {1}{\Gamma (-n)}}\int _{-\infty }^{0}\mathrm {d} x\,e^{x}\,f(x)\,(-x)^{-(n+1)}\\&={\frac {1}{\Gamma (-n)}}{\mathcal {M}}_{-x}[e^{x}f(x)](-n),\end{aligned}}} which can be expressed in terms of the Mellin transform M {\displaystyle {\mathcal {M}}} , see for details. Fermions Fermions are particles which satisfy Fermi–Dirac statistics. We will now examine the normal ordering of fermionic creation and annihilation operator products. Single fermions For a single fermion there are two operators of interest: f ^ † {\displaystyle {\hat {f}}^{\dagger }} : the fermion's creation operator. f ^ {\displaystyle {\hat {f}}} : the fermion's annihilation operator. These satisfy the anticommutator relationships [ f ^ † , f ^ † ] + = 0 {\displaystyle \left[{\hat {f}}^{\dagger },{\hat {f}}^{\dagger }\right]_{+}=0} [ f ^ , f ^ ] + = 0 {\displaystyle \left[{\hat {f}},{\hat {f}}\right]_{+}=0} [ f ^ , f ^ † ] + = 1 {\displaystyle \left[{\hat {f}},{\hat {f}}^{\dagger }\right]_{+}=1} where [ A , B ] + ≡ A B + B A {\displaystyle \left[A,B\right]_{+}\equiv AB+BA} denotes the anticommutator. These may be rewritten as f ^ † f ^ † = 0 {\displaystyle {\hat {f}}^{\dagger }\,{\hat {f}}^{\dagger }=0} f ^ f ^ = 0 {\displaystyle {\hat {f}}\,{\hat {f}}=0} f ^ f ^ † = 1 − f ^ † f ^ . {\displaystyle {\hat {f}}\,{\hat {f}}^{\dagger }=1-{\hat {f}}^{\dagger }\,{\hat {f}}.} To define the normal ordering of a product of fermionic creation and annihilation operators we must take into account the number of interchanges between neighbouring operators. We get a minus sign for each such interchange. Examples 1. We again start with the simplest cases: : f ^ † f ^ : = f ^ † f ^ {\displaystyle :{\hat {f}}^{\dagger }\,{\hat {f}}:\,={\hat {f}}^{\dagger }\,{\hat {f}}} This expression is already in normal order so nothing is changed. In the reverse case, we introduce a minus sign because we have to change the order of two operators: : f ^ f ^ † : = − f ^ † f ^ {\displaystyle :{\hat {f}}\,{\hat {f}}^{\dagger }:\,=-{\hat {f}}^{\dagger }\,{\hat {f}}} These can be combined, along with the anticommutation relations, to show f ^ f ^ † = 1 − f ^ † f ^ = 1 + : f ^ f ^ † : {\displaystyle {\hat {f}}\,{\hat {f}}^{\dagger }\,=1-{\hat {f}}^{\dagger }\,{\hat {f}}=1+:{\hat {f}}\,{\hat {f}}^{\dagger }:} or f ^ f ^ † − : f ^ f ^ † := 1. {\displaystyle {\hat {f}}\,{\hat {f}}^{\dagger }-:{\hat {f}}\,{\hat {f}}^{\dagger }:=1.} This equation, which is in the same form as the bosonic case above, is used in defining the contractions used in Wick's theorem. 2. The normal order of any more complicated cases gives zero because there will be at least one creation or annihilation operator appearing twice. For example: : f ^ f ^ † f ^ f ^ † : = − f ^ † f ^ † f ^ f ^ = 0 {\displaystyle :{\hat {f}}\,{\hat {f}}^{\dagger }\,{\hat {f}}{\hat {f}}^{\dagger }:\,=-{\hat {f}}^{\dagger }\,{\hat {f}}^{\dagger }\,{\hat {f}}\,{\hat {f}}=0} Multiple fermions For N {\displaystyle N} different fermions there are 2 N {\displaystyle 2N} operators: f ^ i † {\displaystyle {\hat {f}}_{i}^{\dagger }} : the i t h {\displaystyle i^{th}} fermion's creation operator. f ^ i {\displaystyle {\hat {f}}_{i}} : the i t h {\displaystyle i^{th}} fermion's annihilation operator. Here i = 1 , … , N {\displaystyle i=1,\ldots ,N} . These satisfy the anti-commutation relations: [ f ^ i † , f ^ j † ] + = 0 {\displaystyle \left[{\hat {f}}_{i}^{\dagger },{\hat {f}}_{j}^{\dagger }\right]_{+}=0} [ f ^ i , f ^ j ] + = 0 {\displaystyle \left[{\hat {f}}_{i},{\hat {f}}_{j}\right]_{+}=0} [ f ^ i , f ^ j † ] + = δ i j {\displaystyle \left[{\hat {f}}_{i},{\hat {f}}_{j}^{\dagger }\right]_{+}=\delta _{ij}} where i , j = 1 , … , N {\displaystyle i,j=1,\ldots ,N} and δ i j {\displaystyle \delta _{ij}} denotes the Kronecker delta. These may be rewritten as: f ^ i † f ^ j † = − f ^ j † f ^ i † {\displaystyle {\hat {f}}_{i}^{\dagger }\,{\hat {f}}_{j}^{\dagger }=-{\hat {f}}_{j}^{\dagger }\,{\hat {f}}_{i}^{\dagger }} f ^ i f ^ j = − f ^ j f ^ i {\displaystyle {\hat {f}}_{i}\,{\hat {f}}_{j}=-{\hat {f}}_{j}\,{\hat {f}}_{i}} f ^ i f ^ j † = δ i j − f ^ j † f ^ i . {\displaystyle {\hat {f}}_{i}\,{\hat {f}}_{j}^{\dagger }=\delta _{ij}-{\hat {f}}_{j}^{\dagger }\,{\hat {f}}_{i}.} When calculating the normal order of products of fermion operators we must take into account the number of interchanges of neighbouring operators required to rearrange the expression. It is as if we pretend the creation and annihilation operators anticommute and then we reorder the expression to ensure the creation operators are on the left and the annihilation operators are on the right - all the time taking account of the anticommutation relations. Examples 1. For two different fermions ( N = 2 {\displaystyle N=2} ) we have : f ^ 1 † f ^ 2 : = f ^ 1 † f ^ 2 {\displaystyle :{\hat {f}}_{1}^{\dagger }\,{\hat {f}}_{2}:\,={\hat {f}}_{1}^{\dagger }\,{\hat {f}}_{2}} Here the expression is already normal ordered so nothing changes. : f ^ 2 f ^ 1 † : = − f ^ 1 † f ^ 2 {\displaystyle :{\hat {f}}_{2}\,{\hat {f}}_{1}^{\dagger }:\,=-{\hat {f}}_{1}^{\dagger }\,{\hat {f}}_{2}} Here we introduce a minus sign because we have interchanged the order of two operators. : f ^ 2 f ^ 1 † f ^ 2 † : = f ^ 1 † f ^ 2 † f ^ 2 = − f ^ 2 † f ^ 1 † f ^ 2 {\displaystyle :{\hat {f}}_{2}\,{\hat {f}}_{1}^{\dagger }\,{\hat {f}}_{2}^{\dagger }:\,={\hat {f}}_{1}^{\dagger }\,{\hat {f}}_{2}^{\dagger }\,{\hat {f}}_{2}=-{\hat {f}}_{2}^{\dagger }\,{\hat {f}}_{1}^{\dagger }\,{\hat {f}}_{2}} Note that the order in which we write the operators here, unlike in the bosonic case, does matter. 2. For three different fermions ( N = 3 {\displaystyle N=3} ) we have : f ^ 1 † f ^ 2 f ^ 3 : = f ^ 1 † f ^ 2 f ^ 3 = − f ^ 1 † f ^ 3 f ^ 2 {\displaystyle :{\hat {f}}_{1}^{\dagger }\,{\hat {f}}_{2}\,{\hat {f}}_{3}:\,={\hat {f}}_{1}^{\dagger }\,{\hat {f}}_{2}\,{\hat {f}}_{3}=-{\hat {f}}_{1}^{\dagger }\,{\hat {f}}_{3}\,{\hat {f}}_{2}} Notice that since (by the anticommutation relations) f ^ 2 f ^ 3 = − f ^ 3 f ^ 2 {\displaystyle {\hat {f}}_{2}\,{\hat {f}}_{3}=-{\hat {f}}_{3}\,{\hat {f}}_{2}} the order in which we write the operators does matter in this case. Similarly we have : f ^ 2 f ^ 1 † f ^ 3 : = − f ^ 1 † f ^ 2 f ^ 3 = f ^ 1 † f ^ 3 f ^ 2 {\displaystyle :{\hat {f}}_{2}\,{\hat {f}}_{1}^{\dagger }\,{\hat {f}}_{3}:\,=-{\hat {f}}_{1}^{\dagger }\,{\hat {f}}_{2}\,{\hat {f}}_{3}={\hat {f}}_{1}^{\dagger }\,{\hat {f}}_{3}\,{\hat {f}}_{2}} : f ^ 3 f ^ 2 f ^ 1 † : = f ^ 1 † f ^ 3 f ^ 2 = − f ^ 1 † f ^ 2 f ^ 3 {\displaystyle :{\hat {f}}_{3}{\hat {f}}_{2}\,{\hat {f}}_{1}^{\dagger }:\,={\hat {f}}_{1}^{\dagger }\,{\hat {f}}_{3}\,{\hat {f}}_{2}=-{\hat {f}}_{1}^{\dagger }\,{\hat {f}}_{2}\,{\hat {f}}_{3}} Uses in quantum field theory The vacuum expectation value of a normal ordered product of creation and annihilation operators is zero. This is because, denoting the vacuum state by | 0 ⟩ {\displaystyle |0\rangle } , the creation and annihilation operators satisfy ⟨ 0 | a ^ † = 0 and a ^ | 0 ⟩ = 0 {\displaystyle \langle 0|{\hat {a}}^{\dagger }=0\qquad {\textrm {and}}\qquad {\hat {a}}|0\rangle =0} (here a ^ † {\displaystyle {\hat {a}}^{\dagger }} and a ^ {\displaystyle {\hat {a}}} are creation and annihilation operators (either bosonic or fermionic)). Let O ^ {\displaystyle {\hat {O}}} denote a non-empty product of creation and annihilation operators. Although this may satisfy ⟨ 0 | O ^ | 0 ⟩ ≠ 0 , {\displaystyle \langle 0|{\hat {O}}|0\rangle \neq 0,} we have ⟨ 0 | : O ^ : | 0 ⟩ = 0. {\displaystyle \langle 0|:{\hat {O}}:|0\rangle =0.} Normal ordered operators are particularly useful when defining a quantum mechanical Hamiltonian. If the Hamiltonian of a theory is in normal order then the ground state energy will be zero: ⟨ 0 | H ^ | 0 ⟩ = 0 {\displaystyle \langle 0|{\hat {H}}|0\rangle =0} . Free fields With two free fields φ and χ, : ϕ ( x ) χ ( y ) : = ϕ ( x ) χ ( y ) − ⟨ 0 | ϕ ( x ) χ ( y ) | 0 ⟩ {\displaystyle :\phi (x)\chi (y):\,\,=\phi (x)\chi (y)-\langle 0|\phi (x)\chi (y)|0\rangle } where | 0 ⟩ {\displaystyle |0\rangle } is again the vacuum state. Each of the two terms on the right hand side typically blows up in the limit as y approaches x but the difference between them has a well-defined limit. This allows us to define :φ(x)χ(x):. Wick's theorem Wick's theorem states the relationship between the time ordered product of n {\displaystyle n} fields and a sum of normal ordered products. This may be expressed for n {\displaystyle n} even as T [ ϕ ( x 1 ) ⋯ ϕ ( x n ) ] = : ϕ ( x 1 ) ⋯ ϕ ( x n ) : + ∑ perm ⟨ 0 | T [ ϕ ( x 1 ) ϕ ( x 2 ) ] | 0 ⟩ : ϕ ( x 3 ) ⋯ ϕ ( x n ) : + ∑ perm ⟨ 0 | T [ ϕ ( x 1 ) ϕ ( x 2 ) ] | 0 ⟩ ⟨ 0 | T [ ϕ ( x 3 ) ϕ ( x 4 ) ] | 0 ⟩ : ϕ ( x 5 ) ⋯ ϕ ( x n ) : ⋮ + ∑ perm ⟨ 0 | T [ ϕ ( x 1 ) ϕ ( x 2 ) ] | 0 ⟩ ⋯ ⟨ 0 | T [ ϕ ( x n − 1 ) ϕ ( x n ) ] | 0 ⟩ {\displaystyle {\begin{aligned}T\left[\phi (x_{1})\cdots \phi (x_{n})\right]=&:\phi (x_{1})\cdots \phi (x_{n}):+\sum _{\textrm {perm}}\langle 0|T\left[\phi (x_{1})\phi (x_{2})\right]|0\rangle :\phi (x_{3})\cdots \phi (x_{n}):\\&+\sum _{\textrm {perm}}\langle 0|T\left[\phi (x_{1})\phi (x_{2})\right]|0\rangle \langle 0|T\left[\phi (x_{3})\phi (x_{4})\right]|0\rangle :\phi (x_{5})\cdots \phi (x_{n}):\\\vdots \\&+\sum _{\textrm {perm}}\langle 0|T\left[\phi (x_{1})\phi (x_{2})\right]|0\rangle \cdots \langle 0|T\left[\phi (x_{n-1})\phi (x_{n})\right]|0\rangle \end{aligned}}} where the summation is over all the distinct ways in which one may pair up fields. The result for n {\displaystyle n} odd looks the same except for the last line which reads ∑ perm ⟨ 0 | T [ ϕ ( x 1 ) ϕ ( x 2 ) ] | 0 ⟩ ⋯ ⟨ 0 | T [ ϕ ( x n − 2 ) ϕ ( x n − 1 ) ] | 0 ⟩ ϕ ( x n ) . {\displaystyle \sum _{\text{perm}}\langle 0|T\left[\phi (x_{1})\phi (x_{2})\right]|0\rangle \cdots \langle 0|T\left[\phi (x_{n-2})\phi (x_{n-1})\right]|0\rangle \phi (x_{n}).} This theorem provides a simple method for computing vacuum expectation values of time ordered products of operators and was the motivation behind the introduction of normal ordering. Alternative definitions The most general definition of normal ordering involves splitting all quantum fields into two parts (for example see Evans and Steer 1996) ϕ i ( x ) = ϕ i + ( x ) + ϕ i − ( x ) {\displaystyle \phi _{i}(x)=\phi _{i}^{+}(x)+\phi _{i}^{-}(x)} . In a product of fields, the fields are split into the two parts and the ϕ + ( x ) {\displaystyle \phi ^{+}(x)} parts are moved so as to be always to the left of all the ϕ − ( x ) {\displaystyle \phi ^{-}(x)} parts. In the usual case considered in the rest of the article, the ϕ + ( x ) {\displaystyle \phi ^{+}(x)} contains only creation operators, while the ϕ − ( x ) {\displaystyle \phi ^{-}(x)} contains only annihilation operators. As this is a mathematical identity, one can split fields in any way one likes. However, for this to be a useful procedure one demands that the normal ordered product of any combination of fields has zero expectation value ⟨ : ϕ 1 ( x 1 ) ϕ 2 ( x 2 ) … ϕ n ( x n ) : ⟩ = 0 {\displaystyle \langle :\phi _{1}(x_{1})\phi _{2}(x_{2})\ldots \phi _{n}(x_{n}):\rangle =0} It is also important for practical calculations that all the commutators (anti-commutator for fermionic fields) of all ϕ i + {\displaystyle \phi _{i}^{+}} and ϕ j − {\displaystyle \phi _{j}^{-}} are all c-numbers. These two properties means that we can apply Wick's theorem in the usual way, turning expectation values of time-ordered products of fields into products of c-number pairs, the contractions. In this generalised setting, the contraction is defined to be the difference between the time-ordered product and the normal ordered product of a pair of fields. The simplest example is found in the context of thermal quantum field theory (Evans and Steer 1996). In this case the expectation values of interest are statistical ensembles, traces over all states weighted by exp ⁡ ( − β H ^ ) {\displaystyle \exp(-\beta {\hat {H}})} . For instance, for a single bosonic quantum harmonic oscillator we have that the thermal expectation value of the number operator is simply the Bose–Einstein distribution ⟨ b ^ † b ^ ⟩ = T r ( e − β ω b ^ † b ^ b ^ † b ^ ) T r ( e − β ω b ^ † b ^ ) = 1 e β ω − 1 {\displaystyle \langle {\hat {b}}^{\dagger }{\hat {b}}\rangle ={\frac {\mathrm {Tr} (e^{-\beta \omega {\hat {b}}^{\dagger }{\hat {b}}}{\hat {b}}^{\dagger }{\hat {b}})}{\mathrm {Tr} (e^{-\beta \omega {\hat {b}}^{\dagger }{\hat {b}}})}}={\frac {1}{e^{\beta \omega }-1}}} So here the number operator b ^ † b ^ {\displaystyle {\hat {b}}^{\dagger }{\hat {b}}} is normal ordered in the usual sense used in the rest of the article yet its thermal expectation values are non-zero. Applying Wick's theorem and doing calculation with the usual normal ordering in this thermal context is possible but computationally impractical. The solution is to define a different ordering, such that the ϕ i + {\displaystyle \phi _{i}^{+}} and ϕ j − {\displaystyle \phi _{j}^{-}} are linear combinations of the original annihilation and creations operators. The combinations are chosen to ensure that the thermal expectation values of normal ordered products are always zero so the split chosen will depend on the temperature. References F. Mandl, G. Shaw, Quantum Field Theory, John Wiley & Sons, 1984. S. Weinberg, The Quantum Theory of Fields (Volume I) Cambridge University Press (1995) T.S. Evans, D.A. Steer, Wick's theorem at finite temperature, Nucl. Phys B 474, 481-496 (1996) arXiv:hep-ph/9601268 In quantum field theory, the anomaly matching condition by Gerard 't Hooft states that the calculation of any chiral anomaly for the flavor symmetry must not depend on what scale is chosen for the calculation if it is done by using the degrees of freedom of the theory at some energy scale. It is also known as the 't Hooft condition and the 't Hooft UV-IR anomaly matching condition. 't Hooft anomalies There are two closely related but different types of obstructions to formulating a quantum field theory that are both called anomalies: chiral, or Adler–Bell–Jackiw anomalies, and 't Hooft anomalies. If we say that the symmetry of the theory has a 't Hooft anomaly, it means that the symmetry is exact as a global symmetry of the quantum theory, but there is some impediment to using it as a gauge in the theory. As an example of a 't Hooft anomaly, we consider quantum chromodynamics with N f {\displaystyle N_{f}} massless fermions: This is the S U ( N c ) {\displaystyle SU(N_{c})} gauge theory with N f {\displaystyle N_{f}} massless Dirac fermions. This theory has the global symmetry S U ( N f ) L × S U ( N f ) R × U ( 1 ) V {\displaystyle SU(N_{f})_{L}\times SU(N_{f})_{R}\times U(1)_{V}} , which is often called the flavor symmetry, and this has a 't Hooft anomaly. Anomaly matching for continuous symmetry The anomaly matching condition by G. 't Hooft proposes that a 't Hooft anomaly of continuous symmetry can be computed both in the high-energy and low-energy degrees of freedom (“UV” and “IR”) and give the same answer. Example For example, consider the quantum chromodynamics with Nf massless quarks. This theory has the flavor symmetry S U ( N f ) L × S U ( N f ) R × U ( 1 ) V {\displaystyle SU(N_{f})_{L}\times SU(N_{f})_{R}\times U(1)_{V}} This flavor symmetry S U ( N f ) L × S U ( N f ) R × U ( 1 ) V {\displaystyle SU(N_{f})_{L}\times SU(N_{f})_{R}\times U(1)_{V}} becomes anomalous when the background gauge field is introduced. One may use either the degrees of freedom at the far low energy limit (far “IR” ) or the degrees of freedom at the far high energy limit (far “UV”) in order to calculate the anomaly. In the former case one should only consider massless fermions or Nambu–Goldstone bosons which may be composite particles, while in the latter case one should only consider the elementary fermions of the underlying short-distance theory. In both cases, the answer must be the same. Indeed, in the case of QCD, the chiral symmetry breaking occurs and the Wess–Zumino–Witten term for the Nambu–Goldstone bosons reproduces the anomaly. Proof One proves this condition by the following procedure: we may add to the theory a gauge field which couples to the current related with this symmetry, as well as chiral fermions which couple only to this gauge field, and cancel the anomaly (so that the gauge symmetry will remain non-anomalous, as needed for consistency). In the limit where the coupling constants we have added go to zero, one gets back to the original theory, plus the fermions we have added; the latter remain good degrees of freedom at every energy scale, as they are free fermions at this limit. The gauge symmetry anomaly can be computed at any energy scale, and must always be zero, so that the theory is consistent. One may now get the anomaly of the symmetry in the original theory by subtracting the free fermions we have added, and the result is independent of the energy scale. Alternative proof Another way to prove the anomaly matching for continuous symmetries is to use the anomaly inflow mechanism. To be specific, we consider four-dimensional spacetime in the following. For global continuous symmetries G {\displaystyle G} , we introduce the background gauge field A {\displaystyle A} and compute the effective action Γ [ A ] {\displaystyle \Gamma [A]} . If there is a 't Hooft anomaly for G {\displaystyle G} , the effective action Γ [ A ] {\displaystyle \Gamma [A]} is not invariant under the G {\displaystyle G} gauge transformation on the background gauge field A {\displaystyle A} and it cannot be restored by adding any four-dimensional local counter terms of A {\displaystyle A} . Wess–Zumino consistency condition shows that we can make it gauge invariant by adding the five-dimensional Chern–Simons action. With the extra dimension, we can now define the effective action Γ [ A ] {\displaystyle \Gamma [A]} by using the low-energy effective theory that only contains the massless degrees of freedom by integrating out massive fields. Since it must be again gauge invariant by adding the same five-dimensional Chern–Simons term, the 't Hooft anomaly does not change by integrating out massive degrees of freedom. See also 't Hooft–Polyakov monopole 't Hooft loop 't Hooft symbol Notes == References == The W state is an entangled quantum state of three qubits which in the bra-ket notation has the following shape | W ⟩ = 1 3 ( | 001 ⟩ + | 010 ⟩ + | 100 ⟩ ) {\displaystyle |\mathrm {W} \rangle ={\frac {1}{\sqrt {3}}}(|001\rangle +|010\rangle +|100\rangle )} and which is remarkable for representing a specific type of multipartite entanglement and for occurring in several applications in quantum information theory. Particles prepared in this state reproduce the properties of Bell's theorem, which states that no classical theory of local hidden variables can produce the predictions of quantum mechanics. The state is named after Wolfgang Dür, who first reported the state together with Guifré Vidal, and Ignacio Cirac in 2000. Properties The W state is the representative of one of the two non-biseparable classes of three-qubit states, the other being the Greenberger–Horne–Zeilinger state, | G H Z ⟩ = ( | 000 ⟩ + | 111 ⟩ ) / 2 {\displaystyle |\mathrm {GHZ} \rangle =(|000\rangle +|111\rangle )/{\sqrt {2}}} . The | W ⟩ {\displaystyle |\mathrm {W} \rangle } and | G H Z ⟩ {\displaystyle |\mathrm {GHZ} \rangle } states represent two very different kinds of tripartite entanglement, as they cannot be transformed (not even probabilistically) into each other by local quantum operations. This difference is, for example, illustrated by the following interesting property of the W state: if one of the three qubits is lost, the state of the remaining 2-qubit system is still entangled. This robustness of W-type entanglement contrasts strongly with the GHZ state, which is fully separable after loss of one qubit. The states in the W class can be distinguished from all other 3-qubit states by means of multipartite entanglement measures. In particular, W states have non-zero entanglement across any bipartition, while the 3-tangle vanishes, which is also non-zero for GHZ-type states. Generalization The notion of W state has been generalized for n {\displaystyle n} qubits and then refers to the quantum superposition with equal expansion coefficients of all possible pure states in which exactly one of the qubits is in an "excited state" | 1 ⟩ {\displaystyle |1\rangle } , while all other ones are in the "ground state" | 0 ⟩ {\displaystyle |0\rangle } : | W ⟩ = 1 n ( | 100...0 ⟩ + | 010...0 ⟩ + . . . + | 00...01 ⟩ ) . {\displaystyle |\mathrm {W} \rangle ={\frac {1}{\sqrt {n}}}(|100...0\rangle +|010...0\rangle +...+|00...01\rangle ).} Both the robustness against particle loss and the LOCC-inequivalence with the (generalized) GHZ state also hold for the n {\displaystyle n} -qubit W state. Applications In systems in which a single qubit is stored in an ensemble of many two-level systems the logical "1" is often represented by the W state, while the logical "0" is represented by the state | 00...0 ⟩ {\displaystyle |00...0\rangle } . Here the W state's robustness against particle loss is a very beneficial property ensuring good storage properties of these ensemble-based quantum memories. See also NOON state == References == In perturbation theory, the Poincaré–Lindstedt method or Lindstedt–Poincaré method is a technique for uniformly approximating periodic solutions to ordinary differential equations, when regular perturbation approaches fail. The method removes secular terms—terms growing without bound—arising in the straightforward application of perturbation theory to weakly nonlinear problems with finite oscillatory solutions. The method is named after Henri Poincaré, and Anders Lindstedt. All efforts of geometers in the second half of this century have had as main objective the elimination of secular terms.The article gives several examples. The theory can be found in Chapter 10 of Nonlinear Differential Equations and Dynamical Systems by Verhulst. Example: the Duffing equation The undamped, unforced Duffing equation is given by x ¨ + x + ε x 3 = 0 {\displaystyle {\ddot {x}}+x+\varepsilon \,x^{3}=0\,} for t > 0, with 0 < ε ≪ 1. Consider initial conditions x ( 0 ) = 1 , {\displaystyle x(0)=1,\,} x ˙ ( 0 ) = 0. {\displaystyle {\dot {x}}(0)=0.\,} A perturbation-series solution of the form x(t) = x0(t) + ε x1(t) + ... is sought. The first two terms of the series are x ( t ) = cos ⁡ ( t ) + ε [ 1 32 ( cos ⁡ ( 3 t ) − cos ⁡ ( t ) ) − 3 8 t sin ⁡ ( t ) ] + ⋯ . {\displaystyle x(t)=\cos(t)+\varepsilon \left[{\tfrac {1}{32}}\,\left(\cos(3t)-\cos(t)\right)-{\tfrac {3}{8}}\,t\,\sin(t)\right]+\cdots .\,} This approximation grows without bound in time, which is inconsistent with the physical system that the equation models. The term responsible for this unbounded growth, called the secular term, is t sin ⁡ ( t ) {\displaystyle t\sin(t)} . The Poincaré–Lindstedt method allows for the creation of an approximation that is accurate for all time, as follows. In addition to expressing the solution itself as an asymptotic series, form another series with which to scale time t: τ = ω t , {\displaystyle \tau =\omega t,\,} where ω = ω 0 + ε ω 1 + ⋯ . {\displaystyle \omega =\omega _{0}+\varepsilon \omega _{1}+\cdots .\,} We have the leading order ω 0 = 1 {\displaystyle \omega _{0}=1} , because when ε = 0 {\displaystyle \varepsilon =0} , the equation has solution x = cos ⁡ ( t ) {\displaystyle x=\cos(t)} . Then the original problem becomes ω 2 x ″ ( τ ) + x ( τ ) + ε x 3 ( τ ) = 0 {\displaystyle \omega ^{2}\,x''(\tau )+x(\tau )+\varepsilon \,x^{3}(\tau )=0\,} Now search for a solution of the form x(τ) = x0(τ) + ε x1(τ) + ... . The following solutions for the zeroth and first order problem in ε {\displaystyle \varepsilon } are obtained: x 0 = cos ⁡ ( τ ) and x 1 = 1 32 ( cos ⁡ ( 3 τ ) − cos ⁡ ( τ ) ) + ( ω 1 − 3 8 ) τ sin ⁡ ( τ ) . {\displaystyle {\begin{aligned}x_{0}&=\cos(\tau )\\{\text{and }}x_{1}&={\tfrac {1}{32}}\,\left(\cos(3\tau )-\cos(\tau )\right)+\left(\omega _{1}-{\tfrac {3}{8}}\right)\,\tau \,\sin(\tau ).\end{aligned}}} So the secular term can be removed through the choice: ω1 = ⁠3/8⁠. Higher orders of accuracy can be obtained by continuing the perturbation analysis along this way. As of now, the approximation—correct up to first order in ε—is x ( t ) ≈ cos ⁡ ( ( 1 + 3 8 ε ) t ) + 1 32 ε [ cos ⁡ ( 3 ( 1 + 3 8 ε ) t ) − cos ⁡ ( ( 1 + 3 8 ε ) t ) ] . {\displaystyle x(t)\approx \cos {\Bigl (}\left(1+{\tfrac {3}{8}}\,\varepsilon \right)\,t{\Bigr )}+{\tfrac {1}{32}}\,\varepsilon \,\left[\cos {\Bigl (}3\left(1+{\tfrac {3}{8}}\,\varepsilon \,\right)\,t{\Bigr )}-\cos {\Bigl (}\left(1+{\tfrac {3}{8}}\,\varepsilon \,\right)\,t{\Bigr )}\right].\,} Example: the van der Pol oscillator We solve the van der Pol oscillator only up to order 2. This method can be continued indefinitely in the same way, where the order-n term ϵ n x n {\displaystyle \epsilon ^{n}x_{n}} consists of a harmonic term a n cos ⁡ ( t ) + b n cos ⁡ ( t ) {\displaystyle a_{n}\cos(t)+b_{n}\cos(t)} , plus some super-harmonic terms a n , 2 cos ⁡ ( 2 t ) + b n , 2 cos ⁡ ( 2 t ) + ⋯ {\displaystyle a_{n,2}\cos(2t)+b_{n,2}\cos(2t)+\cdots } . The coefficients of the super-harmonic terms are solved directly, and the coefficients of the harmonic term are determined by expanding down to order-(n+1), and eliminating its secular term. See chapter 10 of for a derivation up to order 3, and for a computer derivation up to order 164. Consider the van der Pol oscillator with equation x ¨ + ϵ ( x 2 − 1 ) x ˙ + x = 0 {\displaystyle {\ddot {x}}+\epsilon (x^{2}-1){\dot {x}}+x=0} where ϵ {\displaystyle \epsilon } is a small positive number. Perform substitution to the second order: τ = ω t , {\displaystyle \tau =\omega t,\,} where ω = 1 + ϵ ω 1 + ϵ 2 ω 2 + O ( ϵ 3 ) {\displaystyle \omega =1+\epsilon \omega _{1}+\epsilon ^{2}\omega _{2}+O(\epsilon ^{3})} which yields the equation ω 2 x ¨ + ω ϵ ( x 2 − 1 ) x ˙ + x = 0 {\displaystyle \omega ^{2}{\ddot {x}}+\omega \epsilon (x^{2}-1){\dot {x}}+x=0} Now plug in x = x 0 + ϵ x 1 + ϵ 2 x 2 + O ( ϵ 3 ) {\displaystyle x=x_{0}+\epsilon x_{1}+\epsilon ^{2}x_{2}+O(\epsilon ^{3})} , and we have three equations, for the orders 1 , ϵ , ϵ 2 {\displaystyle 1,\epsilon ,\epsilon ^{2}} respectively: { x ¨ 0 + x 0 = 0 x ¨ 1 + x 1 + 2 ω 1 x ¨ 0 + ( x 0 2 − 1 ) x ˙ 0 = 0 x ¨ 2 + x 2 + ( ω 1 2 + 2 ω 2 ) x ¨ 0 + 2 ω 1 x ¨ 1 + 2 x 0 x 1 x ˙ 0 + ω 1 ( x 0 2 − 1 ) x ˙ 0 + x ˙ 1 ( x 0 2 − 1 ) = 0 {\displaystyle {\begin{cases}{\ddot {x}}_{0}+x_{0}=0\\{\ddot {x}}_{1}+x_{1}+2\omega _{1}{\ddot {x}}_{0}+(x_{0}^{2}-1){\dot {x}}_{0}=0\\{\ddot {x}}_{2}+x_{2}+(\omega _{1}^{2}+2\omega _{2}){\ddot {x}}_{0}+2\omega _{1}{\ddot {x}}_{1}+2x_{0}x_{1}{\dot {x}}_{0}+\omega _{1}(x_{0}^{2}-1){\dot {x}}_{0}+{\dot {x}}_{1}(x_{0}^{2}-1)=0\end{cases}}} The first equation has general solution x 0 = A cos ⁡ ( τ + ϕ ) {\displaystyle x_{0}=A\cos(\tau +\phi )} . Pick origin of time such that ϕ = 0 {\displaystyle \phi =0} . Then plug it into the second equation to obtain (after some trigonometric identities) x ¨ 1 + x 1 + ( A − A 3 / 4 ) sin ⁡ τ − 2 ω 1 A cos ⁡ τ − ( A 3 / 4 ) sin ⁡ ( 3 τ ) = 0 {\displaystyle {\ddot {x}}_{1}+x_{1}+(A-A^{3}/4)\sin \tau -2\omega _{1}A\cos \tau -(A^{3}/4)\sin(3\tau )=0} To eliminate the secular term, we must set both sin ⁡ τ , cos ⁡ τ {\displaystyle \sin \tau ,\cos \tau } coefficients to zero, thus we have { A = A 3 / 4 2 ω 1 A = 0 {\displaystyle {\begin{cases}A=A^{3}/4\\2\omega _{1}A=0\end{cases}}} yielding A = 2 , ω 1 = 0 {\displaystyle A=2,\omega _{1}=0} . In particular, we found that when ϵ {\displaystyle \epsilon } increases from zero to a small positive constant, all circular orbits in phase space are destroyed, except the one at radius 2. Now solving x ¨ 1 + x 1 = 2 sin ⁡ ( 3 τ ) {\displaystyle {\ddot {x}}_{1}+x_{1}=2\sin(3\tau )} yields x 1 = B cos ⁡ ( τ + ϕ ) − 1 4 sin ⁡ ( 3 τ ) {\displaystyle x_{1}=B\cos(\tau +\phi )-{\frac {1}{4}}\sin(3\tau )} . We can always absorb ϵ B cos ⁡ ( τ + ϕ ) {\displaystyle \epsilon B\cos(\tau +\phi )} term into x 0 {\displaystyle x_{0}} , so we can WLOG have just x 1 = − 1 4 sin ⁡ ( 3 τ ) {\displaystyle x_{1}=-{\frac {1}{4}}\sin(3\tau )} . Now plug into the second equation to obtain x ¨ 2 + x 2 − ( 4 ω 2 + 1 / 4 ) cos ⁡ τ − 3 4 cos ⁡ 3 τ − 5 4 cos ⁡ 5 τ = 0 {\displaystyle {\ddot {x}}_{2}+x_{2}-(4\omega _{2}+1/4)\cos \tau -{\frac {3}{4}}\cos 3\tau -{\frac {5}{4}}\cos 5\tau =0} To eliminate the secular term, we set ω 2 = − 1 16 {\displaystyle \omega _{2}=-{\frac {1}{16}}} . Thus we find that ω = 1 − 1 16 ϵ 2 + O ( ϵ 3 ) {\displaystyle \omega =1-{\frac {1}{16}}\epsilon ^{2}+O(\epsilon ^{3})} . Example: Mathieu equation This is an example of parametric resonance. Consider the Mathieu equation x ¨ + ( 1 + b ϵ 2 + ϵ cos ⁡ ( t ) ) x = 0 {\displaystyle {\ddot {x}}+(1+b\epsilon ^{2}+\epsilon \cos(t))x=0} , where b {\displaystyle b} is a constant, and ϵ {\displaystyle \epsilon } is small. The equation's solution would have two time-scales, one fast-varying on the order of t {\displaystyle t} , and another slow-varying on the order of T = ϵ 2 t {\displaystyle T=\epsilon ^{2}t} . So expand the solution as x ( t ) = x 0 ( t , T ) + ϵ x 1 ( t , T ) + ϵ 2 x 2 ( t , T ) + O ( ϵ 3 ) {\displaystyle x(t)=x_{0}(t,T)+\epsilon x_{1}(t,T)+\epsilon ^{2}x_{2}(t,T)+O(\epsilon ^{3})} Now plug into the Mathieu equation and expand to obtain { ∂ t 2 x 0 + x 0 = 0 ∂ t 2 x 1 + x 1 = − cos ⁡ ( t ) x 0 ∂ t 2 x 2 + x 2 = − b x 0 − 2 ∂ t T x 0 − cos ⁡ ( t ) x 1 {\displaystyle {\begin{cases}\partial _{t}^{2}x_{0}+x_{0}=0\\\partial _{t}^{2}x_{1}+x_{1}=-\cos(t)x_{0}\\\partial _{t}^{2}x_{2}+x_{2}=-bx_{0}-2\partial _{tT}x_{0}-\cos(t)x_{1}\end{cases}}} As before, we have the solutions { x 0 = A cos ⁡ ( t ) + B sin ⁡ ( t ) x 1 = − A 2 + A 6 cos ⁡ ( 2 t ) + B 6 sin ⁡ ( 2 t ) {\displaystyle {\begin{cases}x_{0}=A\cos(t)+B\sin(t)\\x_{1}=-{\frac {A}{2}}+{\frac {A}{6}}\cos(2t)+{\frac {B}{6}}\sin(2t)\end{cases}}} The secular term coefficients in the third equation are { 1 12 ( − 12 b A + 5 A − 24 B ′ ) 1 12 ( 24 A ′ − 12 b B − B ) {\displaystyle {\begin{cases}{\frac {1}{12}}\left(-12bA+5A-24B'\right)\\{\frac {1}{12}}\left(24A'-12bB-B\right)\end{cases}}} Setting them to zero, we find the equations of motion: d d T [ A B ] = [ 0 1 2 ( 1 12 + b ) 1 2 ( 5 12 − b ) 0 ] [ A B ] {\displaystyle {\frac {d}{dT}}{\begin{bmatrix}A\\B\end{bmatrix}}={\begin{bmatrix}0&{\frac {1}{2}}({\frac {1}{12}}+b)\\{\frac {1}{2}}({\frac {5}{12}}-b)&0\\\end{bmatrix}}{\begin{bmatrix}A\\B\end{bmatrix}}} Its determinant is 1 4 ( b − 5 / 12 ) ( b + 1 / 12 ) {\displaystyle {\frac {1}{4}}(b-5/12)(b+1/12)} , and so when b ∈ ( − 1 / 12 , 5 / 12 ) {\displaystyle b\in (-1/12,5/12)} , the origin is a saddle point, so the amplitude of oscillation A 2 + B 2 {\displaystyle {\sqrt {A^{2}+B^{2}}}} grows unboundedly. In other words, when the angular frequency (in this case, 1 {\displaystyle 1} ) in the parameter is sufficiently close to the angular frequency (in this case, 1 + b ϵ 2 {\displaystyle {\sqrt {1+b\epsilon ^{2}}}} ) of the original oscillator, the oscillation grows unboundedly, like a child swinging on a swing pumping all the way to the moon. Shohat expansion For the van der Pol oscillator, we have ω ∼ 1 / ϵ {\displaystyle \omega \sim 1/\epsilon } for large ϵ {\displaystyle \epsilon } , so as ϵ {\displaystyle \epsilon } becomes large, the serial expansion of ω {\displaystyle \omega } in terms of ϵ {\displaystyle \epsilon } diverges and we would need to keep more and more terms of it to keep ω {\displaystyle \omega } bounded. This suggests to us a parametrization that is bounded: r := ϵ 1 + ϵ {\displaystyle r:={\frac {\epsilon }{1+\epsilon }}} Then, using serial expansions ϵ ω = r + c 2 r 2 + c 3 r 3 + c 4 r 4 + ⋯ {\displaystyle \epsilon \omega =r+c_{2}r^{2}+c_{3}r^{3}+c_{4}r^{4}+\cdots } and x = x 0 + r x 1 + r 2 x 2 + ⋯ {\displaystyle x=x_{0}+rx_{1}+r^{2}x_{2}+\cdots } , and using the same method of eliminating the secular terms, we find c 2 = 1 , c 3 = 15 16 , c 4 = 13 16 {\displaystyle c_{2}=1,c_{3}={\frac {15}{16}},c_{4}={\frac {13}{16}}} . Because lim ϵ → ∞ r = 1 {\displaystyle \lim _{\epsilon \to \infty }r=1} , the expansion ϵ ω = r + c 2 r 2 + c 3 r 3 + c 4 r 4 + ⋯ {\displaystyle \epsilon \omega =r+c_{2}r^{2}+c_{3}r^{3}+c_{4}r^{4}+\cdots } allows us to take a finite number of terms for the series on the right, and it would converge to a finite value at ϵ → ∞ {\displaystyle \epsilon \to \infty } limit. Then we would have ω ∼ 1 / ϵ {\displaystyle \omega \sim 1/\epsilon } , which is exactly the desired asymptotic behavior. This is the idea behind Shohat expansion. The exact asymptotic constant is ϵ ω → 2 π 3 − 2 ln ⁡ 2 = 3.8936 ⋯ {\displaystyle \epsilon \omega \to {\frac {2\pi }{3-2\ln 2}}=3.8936\cdots } , which as we can see is approached by 1 + c 2 + c 3 + c 4 = 3.75 {\displaystyle 1+c_{2}+c_{3}+c_{4}=3.75} . == References and notes == In quantum field theory, a bosonic field is a quantum field whose quanta are bosons; that is, they obey Bose–Einstein statistics. Bosonic fields obey canonical commutation relations, as distinct from the canonical anticommutation relations obeyed by fermionic fields. Examples include scalar fields, describing spin-0 particles such as the Higgs boson, and gauge fields, describing spin-1 particles such as the photon. Basic properties Free (non-interacting) bosonic fields obey canonical commutation relations. Those relations also hold for interacting bosonic fields in the interaction picture, where the fields evolve in time as if free and the effects of the interaction are encoded in the evolution of the states. It is these commutation relations that imply Bose–Einstein statistics for the field quanta. Examples Examples of bosonic fields include scalar fields, gauge fields, and symmetric 2-tensor fields, which are characterized by their covariance under Lorentz transformations and have spins 0, 1 and 2, respectively. Physical examples, in the same order, are the Higgs field, the photon field, and the graviton field. Of the last two, only the photon field can be quantized using the conventional methods of canonical or path integral quantization. This has led to the theory of quantum electrodynamics, one of the most successful theories in physics. Quantization of gravity, on the other hand, is a long-standing problem that has led to development of theories such as string theory and loop quantum gravity. Spin and statistics The spin–statistics theorem implies that quantization of local, relativistic field theories in 3+1 dimensions may lead either to bosonic or fermionic quantum fields, i.e., fields obeying commutation or anti-commutation relations, according to whether they have integer or half-integer spin, respectively. Thus bosonic fields are one of the two theoretically possible types of quantum field, namely those corresponding to particles with integer spin. In a non-relativistic many-body theory, the spin and the statistical properties of the quanta are not directly related. In fact, the commutation or anti-commutation relations are assumed based on whether the theory one intends to study corresponds to particles obeying Bose–Einstein or Fermi–Dirac statistics. In this context the spin remains an internal quantum number that is only phenomenologically related to the statistical properties of the quanta. Examples of non-relativistic bosonic fields include those describing cold bosonic atoms, such as Helium-4. Such non-relativistic fields are not as fundamental as their relativistic counterparts: they provide a convenient 're-packaging' of the many-body wave function describing the state of the system, whereas the relativistic fields described above are a necessary consequence of the consistent union of relativity and quantum mechanics. See also Quantum triviality Composite field Auxiliary field References Edwards, David A. (1981). "Mathematical foundations of quantum field theory: Fermions, gauge fields, and supersymmetry part I: Lattice field theories". International Journal of Theoretical Physics. 20 (7). Springer Nature: 503–517. Bibcode:1981IJTP...20..503E. doi:10.1007/bf00669437. ISSN 0020-7748. S2CID 120108219. Hoffmann, Scott E.; Corney, Joel F.; Drummond, Peter D. (18 July 2008). "Hybrid phase-space simulation method for interacting Bose fields". Physical Review A. 78 (1). American Physical Society (APS): 013622. arXiv:0803.1887. Bibcode:2008PhRvA..78a3622H. doi:10.1103/physreva.78.013622. ISSN 1050-2947. S2CID 17652144. Peskin, M and Schroeder, D. (1995). An Introduction to Quantum Field Theory, Westview Press. Srednicki, Mark (2007). Quantum Field Theory Archived 2011-07-25 at the Wayback Machine, Cambridge University Press, ISBN 978-0-521-86449-7. Weinberg, Steven (1995). The Quantum Theory of Fields, (3 volumes) Cambridge University Press. The Octacube is a large, stainless steel sculpture displayed in the mathematics department of Pennsylvania State University in State College, PA. The sculpture represents a mathematical object called the 24-cell or "octacube". Because a real 24-cell is four-dimensional, the artwork is actually a projection into the three-dimensional world. Octacube has very high intrinsic symmetry, which matches features in chemistry (molecular symmetry) and physics (quantum field theory). The sculpture was designed by Adrian Ocneanu, a mathematics professor at Pennsylvania State University. The university's machine shop spent over a year completing the intricate metal-work. Octacube was funded by an alumna in memory of her husband, Kermit Anderson, who died in the September 11 attacks. Artwork The Octacube's metal skeleton measures about 6 feet (1.8 meters) in all three dimensions. It is a complex arrangement of unpainted, tri-cornered flanges. The base is a 3-foot (0.91-meter) high granite block, with some engraving. The artwork was designed by Adrian Ocneanu, a Penn State mathematics professor. He supplied the specifications for the sculpture's 96 triangular pieces of stainless steel and for their assembly. Fabrication was done by Penn State's machine shop, led by Jerry Anderson. The work took over a year, involving bending and welding as well as cutting. Discussing the construction, Ocneanu said: It's very hard to make 12 steel sheets meet perfectly—and conformally—at each of the 23 vertices, with no trace of welding left. The people who built it are really world-class experts and perfectionists—artists in steel. Because of the reflective metal at different angles, the appearance is pleasantly strange. In some cases, the mirror-like surfaces create an illusion of transparency by showing reflections from unexpected sides of the structure. The sculpture's mathematician creator commented: When I saw the actual sculpture, I had quite a shock. I never imagined the play of light on the surfaces. There are subtle optical effects that you can feel but can't quite put your finger on. Views of the Octacube from multiple angles Interpretation Regular shapes The Platonic solids are three-dimensional shapes with special, high, symmetry. They are the next step up in dimension from the two-dimensional regular polygons (squares, equilateral triangles, etc.). The five Platonic solids are the tetrahedron (4 faces), cube (6 faces), octahedron (8 faces), dodecahedron (12 faces), and icosahedron (20 faces). They have been known since the time of the Ancient Greeks and valued for their aesthetic appeal and philosophical, even mystical, import. (See also the Timaeus, a dialogue of Plato.) In higher dimensions, the counterparts of the Platonic solids are the regular polytopes. These shapes were first described in the mid-19th century by a Swiss mathematician, Ludwig Schläfli. In four dimensions, there are six of them: the pentachoron (5-cell), tesseract (8-cell), hexadecachoron (16-cell), octacube (24-cell), hecatonicosachoron (120-cell), and the hexacosichoron (600-cell). The 24-cell consists of 24 octahedrons, joined in 4-dimensional space. The 24-cell's vertex figure (the 3-D shape formed when a 4-D corner is cut off) is a cube. Despite its suggestive name, the octacube is not the 4-D analog of either the octahedron or the cube. In fact, it is the only one of the six 4-D regular polytopes that lacks a corresponding Platonic solid. Projections Ocneanu explains the conceptual challenge in working in the fourth dimension: "Although mathematicians can work with a fourth dimension abstractly by adding a fourth coordinate to the three that we use to describe a point in space, a fourth spatial dimension is difficult to visualize." Although it is impossible to see or make 4-dimensional objects, it is possible to map them into lower dimensions to get some impressions of them. An analogy for converting the 4-D 24-cell into its 3-D sculpture is cartographic projection, where the surface of the 3-D Earth (or a globe) is reduced to a flat 2-D plane (a portable map). This is done either with light 'casting a shadow' from the globe onto the map or with some mathematical transformation. Many different types of map projection exist: the familiar rectangular Mercator (used for navigation), the circular gnomonic (first projection invented), and several others. All of them have limitations in that they show some features in a distorted manner—'you can't flatten an orange peel without damaging it'—but they are useful visual aids and convenient references. In the same manner that the exterior of the Earth is a 2-D skin (bent into the third dimension), the exterior of a 4-dimensional shape is a 3-D space (but folded through hyperspace, the fourth dimension). However, just as the surface of Earth's globe cannot be mapped onto a plane without some distortions, neither can the exterior 3-D shape of the 24-cell 4-D hyper-shape. In the image on the right a 24-cell is shown projected into space as a 3-D object (and then the image is a 2-D rendering of it, with perspective to aid the eye). Some of the distortions: Curving edge lines: these are straight in four dimensions, but the projection into a lower dimension makes them appear to curve (similar effects occur when mapping the Earth). It is necessary to use semi-transparent faces because of the complexity of the object, so the many "boxes" (octahedral cells) are seen. Only 23 cells are clearly seen. The 24th cell is the "outside in", the whole exterior space around the object as seen in three dimensions. To map the 24-cell, Ocneanu uses a related projection which he calls windowed radial stereographic projection. As with the stereographic projection, there are curved lines shown in 3-D space. Instead of using semitransparent surfaces, "windows" are cut into the faces of the cells so that interior cells can be seen. Also, only 23 vertices are physically present. The 24th vertex "occurs at infinity" because of the projection; what one sees is the 8 legs and arms of the sculpture diverging outwards from the center of the 3-D sculpture. Symmetry The Octacube sculpture has very high symmetry. The stainless steel structure has the same amount of symmetry as a cube or an octahedron. The artwork can be visualized as related to a cube: the arms and legs of the structure extend to the corners. Imagining an octahedron is more difficult; it involves thinking of the faces of the visualized cube forming the corners of an octahedron. The cube and octahedron have the same amount and type of symmetry: octahedral symmetry, called Oh (order 48) in mathematical notation. Some, but not all, of the symmetry elements are 3 different four-fold rotation axes (one through each pair of opposing faces of the visualized cube): up/down, in/out and left/right as seen in the photograph 4 different three-fold rotation axes (one through each pair of opposing corners of the cube [along each of the opposing arm/leg pairs]) 6 different two-fold rotation axes (one through the midpoint of each opposing edge of the visualized cube) 9 mirror planes that bisect the visualized cube 3 that cut it top/bottom, left/right and front/back. These mirrors represent its reflective dihedral subsymmetry D2h, order 8 (a subordinate symmetry of any object with octahedral symmetry) 6 that go along the diagonals of opposing faces of the visualized cube (these go along double sets of arm-leg pairs). These mirrors represent its reflective tetrahedral subsymmetry Td, order 24 (a subordinate symmetry of any object with octahedral symmetry). Using the mid room points, the sculpture represents the root systems of type D4, B4=C4 and F4, that is all 4d ones other than A4. It can visualize the projection of D4 to B3 and D4 to G2. Science allusions Many molecules have the same symmetry as the Octacube sculpture. The organic molecule cubane (C8H8) is one example. The arms and legs of the sculpture are similar to the outward projecting hydrogen atoms. Sulfur hexafluoride (or any molecule with exact octahedral molecular geometry) also shares the same symmetry although the resemblance is not as similar. The Octacube also shows parallels to concepts in theoretical physics. Creator Ocneanu researches mathematical aspects of quantum field theory (QFT). The subject has been described by a Fields medal winner, Ed Witten, as the most difficult area in physics. Part of Ocneanu's work is to build theoretical, and even physical, models of the symmetry features in QFT. Ocneanu cites the relationship of the inner and outer halves of the structure as analogous to the relationship of spin 1/2 particles (e.g. electrons) and spin 1 particles (e.g. photons). Memorial Octacube was commissioned and funded by Jill Anderson, a 1965 PSU math graduate, in memory of her husband, Kermit, another 1965 math graduate, who was killed in the 9-11 terrorist attacks. Summarizing the memorial, Anderson said: I hope that the sculpture will encourage students, faculty, administrators, alumnae, and friends to ponder and appreciate the wonderful world of mathematics. I also hope that all who view the sculpture will begin to grasp the sobering fact that everyone is vulnerable to something terrible happening to them and that we all must learn to live one day at a time, making the very best of what has been given to us. It would be great if everyone who views the Octacube walks away with the feeling that being kind to others is a good way to live. Anderson also funded a math scholarship in Kermit's name at the same time the sculpture project went forward. Reception A more complete explanation of the sculpture, including how it came to be made, how its construction was funded and its role in mathematics and physics, has been made available by Penn State. In addition, Ocneanu has provided his own commentary. See also Artists: Salvador Dalí, painter of fourth-dimension allusions David Smith, a sculptor of abstract, geometric stainless steel Tony Smith, another creator of large abstract geometric sculptures Math: Group theory, the mathematical discipline that historically encompassed much research into symmetry Operator algebra and representation theory, Ocneanu's areas of math research References Notes Citations External links Video from Penn State about the Octacube User created video on imagining a four dimensional object (but a tesseract). Note discussion of projections at ~22 minutes and the discussion of the cells in the model at ~35 minutes. In quantum mechanics, separable states are multipartite quantum states that can be written as a convex combination of product states. Product states are multipartite quantum states that can be written as a tensor product of states in each space. The physical intuition behind these definitions is that product states have no correlation between the different degrees of freedom, while separable states might have correlations, but all such correlations can be explained as due to a classical random variable, as opposed to being due to entanglement. In the special case of pure states the definition simplifies: a pure state is separable if and only if it is a product state. A state is said to be entangled if it is not separable. In general, determining if a state is separable is not straightforward and the problem is classed as NP-hard. Separability of bipartite systems Consider first composite states with two degrees of freedom, referred to as bipartite states. By a postulate of quantum mechanics these can be described as vectors in the tensor product space H 1 ⊗ H 2 {\displaystyle H_{1}\otimes H_{2}} . In this discussion we will focus on the case of the Hilbert spaces H 1 {\displaystyle H_{1}} and H 2 {\displaystyle H_{2}} being finite-dimensional. Pure states Let { | a i ⟩ } i = 1 n ⊂ H 1 {\displaystyle \{|{a_{i}}\rangle \}_{i=1}^{n}\subset H_{1}} and { | b j ⟩ } j = 1 m ⊂ H 2 {\displaystyle \{|{b_{j}}\rangle \}_{j=1}^{m}\subset H_{2}} be orthonormal bases for H 1 {\displaystyle H_{1}} and H 2 {\displaystyle H_{2}} , respectively. A basis for H 1 ⊗ H 2 {\displaystyle H_{1}\otimes H_{2}} is then { | a i ⟩ ⊗ | b j ⟩ } {\displaystyle \{|{a_{i}}\rangle \otimes |{b_{j}}\rangle \}} , or in more compact notation { | a i b j ⟩ } {\displaystyle \{|a_{i}b_{j}\rangle \}} . From the very definition of the tensor product, any vector of norm 1, i.e. a pure state of the composite system, can be written as | ψ ⟩ = ∑ i , j c i , j ( | a i ⟩ ⊗ | b j ⟩ ) = ∑ i , j c i , j | a i b j ⟩ , {\displaystyle |\psi \rangle =\sum _{i,j}c_{i,j}(|a_{i}\rangle \otimes |b_{j}\rangle )=\sum _{i,j}c_{i,j}|a_{i}b_{j}\rangle ,} where c i , j {\displaystyle c_{i,j}} is a constant. If | ψ ⟩ {\displaystyle |\psi \rangle } can be written as a simple tensor, that is, in the form | ψ ⟩ = | ψ 1 ⟩ ⊗ | ψ 2 ⟩ {\displaystyle |\psi \rangle =|\psi _{1}\rangle \otimes |\psi _{2}\rangle } with | ψ i ⟩ {\displaystyle |\psi _{i}\rangle } a pure state in the i-th space, it is said to be a product state, and, in particular, separable. Otherwise it is called entangled. Note that, even though the notions of product and separable states coincide for pure states, they do not in the more general case of mixed states. Pure states are entangled if and only if their partial states are not pure. To see this, write the Schmidt decomposition of | ψ ⟩ {\displaystyle |\psi \rangle } as | ψ ⟩ = ∑ k = 1 r ψ p k ( | u k ⟩ ⊗ | v k ⟩ ) , {\displaystyle |\psi \rangle =\sum _{k=1}^{r_{\psi }}{\sqrt {p_{k}}}(|u_{k}\rangle \otimes |v_{k}\rangle ),} where p k > 0 {\displaystyle {\sqrt {p_{k}}}>0} are positive real numbers, r ψ {\displaystyle r_{\psi }} is the Schmidt rank of | ψ ⟩ {\displaystyle |\psi \rangle } , and { | u k ⟩ } k = 1 r ψ ⊂ H 1 {\displaystyle \{|u_{k}\rangle \}_{k=1}^{r_{\psi }}\subset H_{1}} and { | v k ⟩ } k = 1 r ψ ⊂ H 2 {\displaystyle \{|v_{k}\rangle \}_{k=1}^{r_{\psi }}\subset H_{2}} are sets of orthonormal states in H 1 {\displaystyle H_{1}} and H 2 {\displaystyle H_{2}} , respectively. The state | ψ ⟩ {\displaystyle |\psi \rangle } is entangled if and only if r ψ > 1 {\displaystyle r_{\psi }>1} . At the same time, the partial state has the form ρ A ≡ Tr B ⁡ ( | ψ ⟩ ⟨ ψ | ) = ∑ k = 1 r ψ p k | u k ⟩ ⟨ u k | . {\displaystyle \rho _{A}\equiv \operatorname {Tr} _{B}(|\psi \rangle \!\langle \psi |)=\sum _{k=1}^{r_{\psi }}p_{k}\,|u_{k}\rangle \!\langle u_{k}|.} It follows that ρ A {\displaystyle \rho _{A}} is pure --- that is, is projection with unit-rank --- if and only if r ψ = 1 {\displaystyle r_{\psi }=1} , which is equivalent to | ψ ⟩ {\displaystyle |\psi \rangle } being separable. Physically, this means that it is not possible to assign a definite (pure) state to the subsystems, which instead ought to be described as statistical ensembles of pure states, that is, as density matrices. A pure state ρ = | ψ ⟩ ⟨ ψ | {\displaystyle \rho =|\psi \rangle \!\langle \psi |} is thus entangled if and only if the von Neumann entropy of the partial state ρ A ≡ Tr B ⁡ ( ρ ) {\displaystyle \rho _{A}\equiv \operatorname {Tr} _{B}(\rho )} is nonzero. Formally, the embedding of a product of states into the product space is given by the Segre embedding. That is, a quantum-mechanical pure state is separable if and only if it is in the image of the Segre embedding. For example, in a two-qubit space, where H 1 = H 2 = C 2 {\displaystyle H_{1}=H_{2}=\mathbb {C} ^{2}} , the states | 0 ⟩ ⊗ | 0 ⟩ {\displaystyle |0\rangle \otimes |0\rangle } , | 0 ⟩ ⊗ | 1 ⟩ {\displaystyle |0\rangle \otimes |1\rangle } , | 1 ⟩ ⊗ | 1 ⟩ {\displaystyle |1\rangle \otimes |1\rangle } , are all product (and thus separable) pure states, as is | 0 ⟩ ⊗ | ψ ⟩ {\displaystyle |0\rangle \otimes |\psi \rangle } with | ψ ⟩ ≡ 1 / 3 | 0 ⟩ + 2 / 3 | 1 ⟩ {\displaystyle |\psi \rangle \equiv {\sqrt {1/3}}|0\rangle +{\sqrt {2/3}}|1\rangle } . On the other hand, states like 1 / 2 | 00 ⟩ + 1 / 2 | 11 ⟩ {\displaystyle {\sqrt {1/2}}|00\rangle +{\sqrt {1/2}}|11\rangle } or 1 / 3 | 01 ⟩ + 2 / 3 | 10 ⟩ {\displaystyle {\sqrt {1/3}}|01\rangle +{\sqrt {2/3}}|10\rangle } are not separable. Mixed states Consider the mixed state case. A mixed state of the composite system is described by a density matrix ρ {\displaystyle \rho } acting on H 1 ⊗ H 2 {\displaystyle H_{1}\otimes H_{2}} . Such a state ρ {\displaystyle \rho } is separable if there exist p k ≥ 0 {\displaystyle p_{k}\geq 0} , { ρ 1 k } {\displaystyle \{\rho _{1}^{k}\}} and { ρ 2 k } {\displaystyle \{\rho _{2}^{k}\}} which are mixed states of the respective subsystems such that ρ = ∑ k p k ρ 1 k ⊗ ρ 2 k {\displaystyle \rho =\sum _{k}p_{k}\rho _{1}^{k}\otimes \rho _{2}^{k}} where ∑ k p k = 1. {\displaystyle \;\sum _{k}p_{k}=1.} Otherwise ρ {\displaystyle \rho } is called an entangled state. We can assume without loss of generality in the above expression that { ρ 1 k } {\displaystyle \{\rho _{1}^{k}\}} and { ρ 2 k } {\displaystyle \{\rho _{2}^{k}\}} are all rank-1 projections, that is, they represent pure ensembles of the appropriate subsystems. It is clear from the definition that the family of separable states is a convex set. Notice that, again from the definition of the tensor product, any density matrix, indeed any matrix acting on the composite state space, can be trivially written in the desired form, if we drop the requirement that { ρ 1 k } {\displaystyle \{\rho _{1}^{k}\}} and { ρ 2 k } {\displaystyle \{\rho _{2}^{k}\}} are themselves states and ∑ k p k = 1. {\displaystyle \;\sum _{k}p_{k}=1.} If these requirements are satisfied, then we can interpret the total state as a probability distribution over uncorrelated product states. In terms of quantum channels, a separable state can be created from any other state using local actions and classical communication while an entangled state cannot. When the state spaces are infinite-dimensional, density matrices are replaced by positive trace class operators with trace 1, and a state is separable if it can be approximated, in trace norm, by states of the above form. If there is only a single non-zero p k {\displaystyle p_{k}} , then the state can be expressed just as ρ = ρ 1 ⊗ ρ 2 , {\textstyle \rho =\rho _{1}\otimes \rho _{2},} and is called simply separable or product state. One property of the product state is that in terms of entropy, S ( ρ ) = S ( ρ 1 ) + S ( ρ 2 ) . {\displaystyle S(\rho )=S(\rho _{1})+S(\rho _{2}).} Extending to the multipartite case The above discussion generalizes easily to the case of a quantum system consisting of more than two subsystems. Let a system have n subsystems and have state space H = H 1 ⊗ ⋯ ⊗ H n {\displaystyle H=H_{1}\otimes \cdots \otimes H_{n}} . A pure state | ψ ⟩ ∈ H {\displaystyle |\psi \rangle \in H} is separable if it takes the form | ψ ⟩ = | ψ 1 ⟩ ⊗ ⋯ ⊗ | ψ n ⟩ . {\displaystyle |\psi \rangle =|\psi _{1}\rangle \otimes \cdots \otimes |\psi _{n}\rangle .} Similarly, a mixed state ρ acting on H is separable if it is a convex sum ρ = ∑ k p k ρ 1 k ⊗ ⋯ ⊗ ρ n k . {\displaystyle \rho =\sum _{k}p_{k}\rho _{1}^{k}\otimes \cdots \otimes \rho _{n}^{k}.} Or, in the infinite-dimensional case, ρ is separable if it can be approximated in the trace norm by states of the above form. Separability criterion The problem of deciding whether a state is separable in general is sometimes called the separability problem in quantum information theory. It is considered to be a difficult problem. It has been shown to be NP-hard in many cases and is believed to be so in general. Some appreciation for this difficulty can be obtained if one attempts to solve the problem by employing the direct brute force approach, for a fixed dimension. The problem quickly becomes intractable, even for low dimensions. Thus more sophisticated formulations are required. The separability problem is a subject of current research. A separability criterion is a necessary condition a state must satisfy to be separable. In the low-dimensional (2 X 2 and 2 X 3) cases, the Peres-Horodecki criterion is actually a necessary and sufficient condition for separability. Other separability criteria include (but not limited to) the range criterion, reduction criterion, and those based on uncertainty relations. See Ref. for a review of separability criteria in discrete variable systems. In continuous variable systems, the Peres-Horodecki criterion also applies. Specifically, Simon formulated a particular version of the Peres-Horodecki criterion in terms of the second-order moments of canonical operators and showed that it is necessary and sufficient for 1 ⊕ 1 {\displaystyle 1\oplus 1} -mode Gaussian states (see Ref. for a seemingly different but essentially equivalent approach). It was later found that Simon's condition is also necessary and sufficient for 1 ⊕ n {\displaystyle 1\oplus n} -mode Gaussian states, but no longer sufficient for 2 ⊕ 2 {\displaystyle 2\oplus 2} -mode Gaussian states. Simon's condition can be generalized by taking into account the higher order moments of canonical operators or by using entropic measures. Characterization via algebraic geometry Quantum mechanics may be modelled on a projective Hilbert space, and the categorical product of two such spaces is the Segre embedding. In the bipartite case, a quantum state is separable if and only if it lies in the image of the Segre embedding. Jon Magne Leinaas, Jan Myrheim and Eirik Ovrum in their paper "Geometrical aspects of entanglement" describe the problem and study the geometry of the separable states as a subset of the general state matrices. This subset have some intersection with the subset of states holding Peres-Horodecki criterion. In this paper, Leinaas et al. also give a numerical approach to test for separability in the general case. Testing for separability Testing for separability in the general case is an NP-hard problem. Leinaas et al. formulated an iterative, probabilistic algorithm for testing if a given state is separable. When the algorithm is successful, it gives an explicit, random, representation of the given state as a separable state. Otherwise it gives the distance of the given state from the nearest separable state it can find. See also Entanglement witness References External links "StateSeparator" web-app Mary Jacquiline Romero is a quantum physicist in the Australian Research Council Centre of Excellence for Engineered Quantum Systems at the University of Queensland, Australia. Her research expertise and interests are in the field of quantum foundations and quantum information. In particular, Romero is an experimental quantum physicist studying the properties of single photons for the development of new quantum alphabets and the nature of quantum causality. Education Romero attended Philippine Science High School where she enjoyed physics the most out of her subjects. She completed a Bachelor of Science (Applied Physics) at the University of the Philippines in 2005. Following this she then completed her Masters in Physics in 2007 also at the University of the Philippines as a Philippine Council for Advanced Science and Technology Research and Development (PCASTRD) Scholar. Her Master's thesis was focussed on manipulating the shape of light for microscopy and microfabrication by using spatial light modulators (SLMs). Romero then left the Philippines and moved to the UK to do a PhD with Miles J. Padgett and Stephen Barnett at the University of Glasgow. Her PhD thesis explored the experimental and theoretical aspects of entanglement of spatial modes related to optical orbital angular momentum, extending the use of SLMs for the manipulation of single photons. Career and research Following her PhD, Romero worked as a postdoctoral researcher at the University of Glasgow. During this time, Romero and her colleagues made headlines with their discovery of a way to slow down photons in free space. In 2015 she moved to Brisbane, Australia as a research fellow in the School of Mathematics and Physics at the University of Queensland. In 2016, Romero was awarded a Discovery Early Career Research Award (DECRA) to continue her work on quantum foundations and single photon manipulation, specifically studying security in higher-dimensional quantum systems. Romero's work on quantum alphabets seeks to increase the amount of information encoded in a single photon, as opposed to only two options (0 and 1) for encoding in classical computing. This research has implications for cyber security and more efficient data transfer and storage. Romero is also currently developing a brain-inspired computer based on current quantum photonic capabilities to provide insight for both neuroscience and computing. Honours 2019 Westpac Research Fellow 2019 L’Oréal-UNESCO For Women in Science program International Rising Talent Prize 2018 Ruby Payne-Scott medal from the Australian Institute of Physics 2018 UQ Early Career Researcher Award for Photonic neuromorphic computing 2018 Queensland Young Tall Poppy Award 2018 Japan Society for the Promotion of Science HOPE Fellowship 2017 L’Oreal-UNESCO For Women In Science Fellowship 2016 Discovery Early Career Research Award (DECRA) Advocacy Romero has been an advocate for women in science, emphasising the importance of role models for young women interested in physics. In interviews, Romero emphasises that it is possible to have a productive research career and have children saying, “I do not feel less of a physicist because I am a mother, nor less of a mother because I am a physicist” == References == In functional analysis, a reproducing kernel Hilbert space (RKHS) is a Hilbert space of functions in which point evaluation is a continuous linear functional. Specifically, a Hilbert space H {\displaystyle H} of functions from a set X {\displaystyle X} (to R {\displaystyle \mathbb {R} } or C {\displaystyle \mathbb {C} } ) is an RKHS if the point-evaluation functional L x : H → C {\displaystyle L_{x}:H\to \mathbb {C} } , L x ( f ) = f ( x ) {\displaystyle L_{x}(f)=f(x)} , is continuous for every x ∈ X {\displaystyle x\in X} . Equivalently, H {\displaystyle H} is an RKHS if there exists a function K x ∈ H {\displaystyle K_{x}\in H} such that, for all f ∈ H {\displaystyle f\in H} , ⟨ f , K x ⟩ = f ( x ) . {\displaystyle \langle f,K_{x}\rangle =f(x).} The function K x {\displaystyle K_{x}} is then called the reproducing kernel, and it reproduces the value of f {\displaystyle f} at x {\displaystyle x} via the inner product. An immediate consequence of this property is that convergence in norm implies uniform convergence on any subset of X {\displaystyle X} on which ‖ K x ‖ {\displaystyle \|K_{x}\|} is bounded. However, the converse does not necessarily hold. Often the set X {\displaystyle X} carries a topology, and ‖ K x ‖ {\displaystyle \|K_{x}\|} depends continuously on x ∈ X {\displaystyle x\in X} , in which case: convergence in norm implies uniform convergence on compact subsets of X {\displaystyle X} . It is not entirely straightforward to construct natural examples of a Hilbert space which are not an RKHS in a non-trivial fashion. Some examples, however, have been found. While, formally, L2 spaces are defined as Hilbert spaces of equivalence classes of functions, this definition can trivially be extended to a Hilbert space of functions by choosing a (total) function as a representative for each equivalence class. However, no choice of representatives can make this space an RKHS ( K 0 {\displaystyle K_{0}} would need to be the non-existent Dirac delta function). However, there are RKHSs in which the norm is an L2-norm, such as the space of band-limited functions (see the example below). An RKHS is associated with a kernel that reproduces every function in the space in the sense that for every x {\displaystyle x} in the set on which the functions are defined, "evaluation at x {\displaystyle x} " can be performed by taking an inner product with a function determined by the kernel. Such a reproducing kernel exists if and only if every evaluation functional is continuous. The reproducing kernel was first introduced in the 1907 work of Stanisław Zaremba concerning boundary value problems for harmonic and biharmonic functions. James Mercer simultaneously examined functions which satisfy the reproducing property in the theory of integral equations. The idea of the reproducing kernel remained untouched for nearly twenty years until it appeared in the dissertations of Gábor Szegő, Stefan Bergman, and Salomon Bochner. The subject was eventually systematically developed in the early 1950s by Nachman Aronszajn and Stefan Bergman. These spaces have wide applications, including complex analysis, harmonic analysis, and quantum mechanics. Reproducing kernel Hilbert spaces are particularly important in the field of statistical learning theory because of the celebrated representer theorem which states that every function in an RKHS that minimises an empirical risk functional can be written as a linear combination of the kernel function evaluated at the training points. This is a practically useful result as it effectively simplifies the empirical risk minimization problem from an infinite dimensional to a finite dimensional optimization problem. For ease of understanding, we provide the framework for real-valued Hilbert spaces. The theory can be easily extended to spaces of complex-valued functions and hence include the many important examples of reproducing kernel Hilbert spaces that are spaces of analytic functions. Definition Let X {\displaystyle X} be an arbitrary set and H {\displaystyle H} a Hilbert space of real-valued functions on X {\displaystyle X} , equipped with pointwise addition and pointwise scalar multiplication. The evaluation functional over the Hilbert space of functions H {\displaystyle H} is a linear functional that evaluates each function at a point x {\displaystyle x} , L x : f ↦ f ( x ) ∀ f ∈ H . {\displaystyle L_{x}:f\mapsto f(x){\text{ }}\forall f\in H.} We say that H is a reproducing kernel Hilbert space if, for all x {\displaystyle x} in X {\displaystyle X} , L x {\displaystyle L_{x}} is continuous at every f {\displaystyle f} in H {\displaystyle H} or, equivalently, if L x {\displaystyle L_{x}} is a bounded operator on H {\displaystyle H} , i.e. there exists some M x > 0 {\displaystyle M_{x}>0} such that Although M x < ∞ {\displaystyle M_{x}<\infty } is assumed for all x ∈ X {\displaystyle x\in X} , it might still be the case that sup x M x = ∞ {\textstyle \sup _{x}M_{x}=\infty } . While property (1) is the weakest condition that ensures both the existence of an inner product and the evaluation of every function in H {\displaystyle H} at every point in the domain, it does not lend itself to easy application in practice. A more intuitive definition of the RKHS can be obtained by observing that this property guarantees that the evaluation functional can be represented by taking the inner product of f {\displaystyle f} with a function K x {\displaystyle K_{x}} in H {\displaystyle H} . This function is the so-called reproducing kernel for the Hilbert space H {\displaystyle H} from which the RKHS takes its name. More formally, the Riesz representation theorem implies that for all x {\displaystyle x} in X {\displaystyle X} there exists a unique element K x {\displaystyle K_{x}} of H {\displaystyle H} with the reproducing property, Since K x {\displaystyle K_{x}} is itself a function defined on X {\displaystyle X} with values in the field R {\displaystyle \mathbb {R} } (or C {\displaystyle \mathbb {C} } in the case of complex Hilbert spaces) and as K x {\displaystyle K_{x}} is in H {\displaystyle H} we have that K x ( y ) = L y ( K x ) = ⟨ K x , K y ⟩ H , {\displaystyle K_{x}(y)=L_{y}(K_{x})=\langle K_{x},\ K_{y}\rangle _{H},} where K y ∈ H {\displaystyle K_{y}\in H} is the element in H {\displaystyle H} associated to L y {\displaystyle L_{y}} . This allows us to define the reproducing kernel of H {\displaystyle H} as a function K : X × X → R {\displaystyle K:X\times X\to \mathbb {R} } (or C {\displaystyle \mathbb {C} } in the complex case) by K ( x , y ) = ⟨ K x , K y ⟩ H . {\displaystyle K(x,y)=\langle K_{x},\ K_{y}\rangle _{H}.} From this definition it is easy to see that K : X × X → R {\displaystyle K:X\times X\to \mathbb {R} } (or C {\displaystyle \mathbb {C} } in the complex case) is both symmetric (resp. conjugate symmetric) and positive definite, i.e. ∑ i , j = 1 n c i c j K ( x i , x j ) = ∑ i = 1 n c i ⟨ K x i , ∑ j = 1 n c j K x j ⟩ H = ⟨ ∑ i = 1 n c i K x i , ∑ j = 1 n c j K x j ⟩ H = ‖ ∑ i = 1 n c i K x i ‖ H 2 ≥ 0 {\displaystyle \sum _{i,j=1}^{n}c_{i}c_{j}K(x_{i},x_{j})=\sum _{i=1}^{n}c_{i}\left\langle K_{x_{i}},\sum _{j=1}^{n}c_{j}K_{x_{j}}\right\rangle _{H}=\left\langle \sum _{i=1}^{n}c_{i}K_{x_{i}},\sum _{j=1}^{n}c_{j}K_{x_{j}}\right\rangle _{H}=\left\|\sum _{i=1}^{n}c_{i}K_{x_{i}}\right\|_{H}^{2}\geq 0} for every n ∈ N , x 1 , … , x n ∈ X , and c 1 , … , c n ∈ R . {\displaystyle n\in \mathbb {N} ,x_{1},\dots ,x_{n}\in X,{\text{ and }}c_{1},\dots ,c_{n}\in \mathbb {R} .} The Moore–Aronszajn theorem (see below) is a sort of converse to this: if a function K {\displaystyle K} satisfies these conditions then there is a Hilbert space of functions on X {\displaystyle X} for which it is a reproducing kernel. Examples The simplest example of a reproducing kernel Hilbert space is the space L 2 ( X , μ ) {\displaystyle L^{2}(X,\mu )} where X {\displaystyle X} is a set and μ {\displaystyle \mu } is the counting measure on X {\displaystyle X} . For x ∈ X {\displaystyle x\in X} , the reproducing kernel K x {\displaystyle K_{x}} is the indicator function of the one point set { x } ⊂ X {\displaystyle \{x\}\subset X} . Nontrivial reproducing kernel Hilbert spaces often involve analytic functions, as we now illustrate by example. Consider the Hilbert space of bandlimited continuous functions H {\displaystyle H} . Fix some cutoff frequency 0 < a < ∞ {\displaystyle 0<a<\infty } and define the Hilbert space H = { f ∈ L 2 ( R ) ∣ supp ⁡ ( F ) ⊂ [ − a , a ] } {\displaystyle H=\{f\in L^{2}(\mathbb {R} )\mid \operatorname {supp} (F)\subset [-a,a]\}} where L 2 ( R ) {\displaystyle L^{2}(\mathbb {R} )} is the set of square integrable functions, and F ( ω ) = ∫ − ∞ ∞ f ( t ) e − i ω t d t {\textstyle F(\omega )=\int _{-\infty }^{\infty }f(t)e^{-i\omega t}\,dt} is the Fourier transform of f {\displaystyle f} . As the inner product, we use ⟨ f , g ⟩ L 2 = ∫ − ∞ ∞ f ( x ) ⋅ g ( x ) ¯ d x . {\displaystyle \langle f,g\rangle _{L^{2}}=\int _{-\infty }^{\infty }f(x)\cdot {\overline {g(x)}}\,dx.} Since this is a closed subspace of L 2 ( R ) {\displaystyle L^{2}(\mathbb {R} )} , it is a Hilbert space. Moreover, the elements of H {\displaystyle H} are smooth functions on R {\displaystyle \mathbb {R} } that tend to zero at infinity, essentially by the Riemann-Lebesgue lemma. In fact, the elements of H {\displaystyle H} are the restrictions to R {\displaystyle \mathbb {R} } of entire holomorphic functions, by the Paley–Wiener theorem. From the Fourier inversion theorem, we have f ( x ) = 1 2 π ∫ − a a F ( ω ) e i x ω d ω . {\displaystyle f(x)={\frac {1}{2\pi }}\int _{-a}^{a}F(\omega )e^{ix\omega }\,d\omega .} It then follows by the Cauchy–Schwarz inequality and Plancherel's theorem that, for all x {\displaystyle x} , | f ( x ) | ≤ 1 2 π 2 a ∫ − a a | F ( ω ) | 2 d ω = 2 a 2 π ∫ − ∞ ∞ | F ( ω ) | 2 d ω = a π ‖ f ‖ L 2 . {\displaystyle |f(x)|\leq {\frac {1}{2\pi }}{\sqrt {2a\int _{-a}^{a}|F(\omega )|^{2}\,d\omega }}={\frac {\sqrt {2a}}{2\pi }}{\sqrt {\int _{-\infty }^{\infty }|F(\omega )|^{2}\,d\omega }}={\sqrt {\frac {a}{\pi }}}\|f\|_{L^{2}}.} This inequality shows that the evaluation functional is bounded, proving that H {\displaystyle H} is indeed a RKHS. The kernel function K x {\displaystyle K_{x}} in this case is given by K x ( y ) = a π sinc ⁡ ( a π ( y − x ) ) = sin ⁡ ( a ( y − x ) ) π ( y − x ) . {\displaystyle K_{x}(y)={\frac {a}{\pi }}\operatorname {sinc} \left({\frac {a}{\pi }}(y-x)\right)={\frac {\sin(a(y-x))}{\pi (y-x)}}.} The Fourier transform of K x ( y ) {\displaystyle K_{x}(y)} defined above is given by ∫ − ∞ ∞ K x ( y ) e − i ω y d y = { e − i ω x if ω ∈ [ − a , a ] , 0 otherwise , {\displaystyle \int _{-\infty }^{\infty }K_{x}(y)e^{-i\omega y}\,dy={\begin{cases}e^{-i\omega x}&{\text{if }}\omega \in [-a,a],\\0&{\textrm {otherwise}},\end{cases}}} which is a consequence of the time-shifting property of the Fourier transform. Consequently, using Plancherel's theorem, we have ⟨ f , K x ⟩ L 2 = ∫ − ∞ ∞ f ( y ) ⋅ K x ( y ) ¯ d y = 1 2 π ∫ − a a F ( ω ) ⋅ e i ω x d ω = f ( x ) . {\displaystyle \langle f,K_{x}\rangle _{L^{2}}=\int _{-\infty }^{\infty }f(y)\cdot {\overline {K_{x}(y)}}\,dy={\frac {1}{2\pi }}\int _{-a}^{a}F(\omega )\cdot e^{i\omega x}\,d\omega =f(x).} Thus we obtain the reproducing property of the kernel. K x {\displaystyle K_{x}} in this case is the "bandlimited version" of the Dirac delta function, and that K x ( y ) {\displaystyle K_{x}(y)} converges to δ ( y − x ) {\displaystyle \delta (y-x)} in the weak sense as the cutoff frequency a {\displaystyle a} tends to infinity. Moore–Aronszajn theorem We have seen how a reproducing kernel Hilbert space defines a reproducing kernel function that is both symmetric and positive definite. The Moore–Aronszajn theorem goes in the other direction; it states that every symmetric, positive definite kernel defines a unique reproducing kernel Hilbert space. The theorem first appeared in Aronszajn's Theory of Reproducing Kernels, although he attributes it to E. H. Moore. Theorem. Suppose K is a symmetric, positive definite kernel on a set X. Then there is a unique Hilbert space of functions on X for which K is a reproducing kernel. Proof. For all x in X, define Kx = K(x, ⋅ ). Let H0 be the linear span of {Kx : x ∈ X}. Define an inner product on H0 by ⟨ ∑ j = 1 n b j K y j , ∑ i = 1 m a i K x i ⟩ H 0 = ∑ i = 1 m ∑ j = 1 n a i b j K ( y j , x i ) , {\displaystyle \left\langle \sum _{j=1}^{n}b_{j}K_{y_{j}},\sum _{i=1}^{m}a_{i}K_{x_{i}}\right\rangle _{H_{0}}=\sum _{i=1}^{m}\sum _{j=1}^{n}{a_{i}}b_{j}K(y_{j},x_{i}),} which implies K ( x , y ) = ⟨ K x , K y ⟩ H 0 {\displaystyle K(x,y)=\left\langle K_{x},K_{y}\right\rangle _{H_{0}}} . The symmetry of this inner product follows from the symmetry of K and the non-degeneracy follows from the fact that K is positive definite. Let H be the completion of H0 with respect to this inner product. Then H consists of functions of the form f ( x ) = ∑ i = 1 ∞ a i K x i ( x ) where lim n → ∞ sup p ≥ 0 ‖ ∑ i = n n + p a i K x i ‖ H 0 = 0. {\displaystyle f(x)=\sum _{i=1}^{\infty }a_{i}K_{x_{i}}(x)\quad {\text{where}}\quad \lim _{n\to \infty }\sup _{p\geq 0}\left\|\sum _{i=n}^{n+p}a_{i}K_{x_{i}}\right\|_{H_{0}}=0.} Now we can check the reproducing property (2): ⟨ f , K x ⟩ H = ∑ i = 1 ∞ a i ⟨ K x i , K x ⟩ H 0 = ∑ i = 1 ∞ a i K ( x i , x ) = f ( x ) . {\displaystyle \langle f,K_{x}\rangle _{H}=\sum _{i=1}^{\infty }a_{i}\left\langle K_{x_{i}},K_{x}\right\rangle _{H_{0}}=\sum _{i=1}^{\infty }a_{i}K(x_{i},x)=f(x).} To prove uniqueness, let G be another Hilbert space of functions for which K is a reproducing kernel. For every x and y in X, (2) implies that ⟨ K x , K y ⟩ H = K ( x , y ) = ⟨ K x , K y ⟩ G . {\displaystyle \langle K_{x},K_{y}\rangle _{H}=K(x,y)=\langle K_{x},K_{y}\rangle _{G}.} By linearity, ⟨ ⋅ , ⋅ ⟩ H = ⟨ ⋅ , ⋅ ⟩ G {\displaystyle \langle \cdot ,\cdot \rangle _{H}=\langle \cdot ,\cdot \rangle _{G}} on the span of { K x : x ∈ X } {\displaystyle \{K_{x}:x\in X\}} . Then H ⊂ G {\displaystyle H\subset G} because G is complete and contains H0 and hence contains its completion. Now we need to prove that every element of G is in H. Let f {\displaystyle f} be an element of G. Since H is a closed subspace of G, we can write f = f H + f H ⊥ {\displaystyle f=f_{H}+f_{H^{\bot }}} where f H ∈ H {\displaystyle f_{H}\in H} and f H ⊥ ∈ H ⊥ {\displaystyle f_{H^{\bot }}\in H^{\bot }} . Now if x ∈ X {\displaystyle x\in X} then, since K is a reproducing kernel of G and H: f ( x ) = ⟨ K x , f ⟩ G = ⟨ K x , f H ⟩ G + ⟨ K x , f H ⊥ ⟩ G = ⟨ K x , f H ⟩ G = ⟨ K x , f H ⟩ H = f H ( x ) , {\displaystyle f(x)=\langle K_{x},f\rangle _{G}=\langle K_{x},f_{H}\rangle _{G}+\langle K_{x},f_{H^{\bot }}\rangle _{G}=\langle K_{x},f_{H}\rangle _{G}=\langle K_{x},f_{H}\rangle _{H}=f_{H}(x),} where we have used the fact that K x {\displaystyle K_{x}} belongs to H so that its inner product with f H ⊥ {\displaystyle f_{H^{\bot }}} in G is zero. This shows that f = f H {\displaystyle f=f_{H}} in G and concludes the proof. Integral operators and Mercer's theorem We may characterize a symmetric positive definite kernel K {\displaystyle K} via the integral operator using Mercer's theorem and obtain an additional view of the RKHS. Let X {\displaystyle X} be a compact space equipped with a strictly positive finite Borel measure μ {\displaystyle \mu } and K : X × X → R {\displaystyle K:X\times X\to \mathbb {R} } a continuous, symmetric, and positive definite function. Define the integral operator T K : L 2 ( X ) → L 2 ( X ) {\displaystyle T_{K}:L_{2}(X)\to L_{2}(X)} as [ T K f ] ( ⋅ ) = ∫ X K ( ⋅ , t ) f ( t ) d μ ( t ) {\displaystyle [T_{K}f](\cdot )=\int _{X}K({}\cdot {},t)f(t)\,d\mu (t)} where L 2 ( X ) {\displaystyle L_{2}(X)} is the space of square integrable functions with respect to μ {\displaystyle \mu } . Mercer's theorem states that the spectral decomposition of the integral operator T K {\displaystyle T_{K}} of K {\displaystyle K} yields a series representation of K {\displaystyle K} in terms of the eigenvalues and eigenfunctions of T K {\displaystyle T_{K}} . This then implies that K {\displaystyle K} is a reproducing kernel so that the corresponding RKHS can be defined in terms of these eigenvalues and eigenfunctions. We provide the details below. Under these assumptions T K {\displaystyle T_{K}} is a compact, continuous, self-adjoint, and positive operator. The spectral theorem for self-adjoint operators implies that there is an at most countable decreasing sequence ( σ i ) i ≥ 0 {\displaystyle (\sigma _{i})_{i\geq 0}} such that lim i → ∞ σ i = 0 {\textstyle \lim _{i\to \infty }\sigma _{i}=0} and T K φ i ( x ) = σ i φ i ( x ) {\displaystyle T_{K}\varphi _{i}(x)=\sigma _{i}\varphi _{i}(x)} , where the { φ i } {\displaystyle \{\varphi _{i}\}} form an orthonormal basis of L 2 ( X ) {\displaystyle L_{2}(X)} . By the positivity of T K , σ i > 0 {\displaystyle T_{K},\sigma _{i}>0} for all i . {\displaystyle i.} One can also show that T K {\displaystyle T_{K}} maps continuously into the space of continuous functions C ( X ) {\displaystyle C(X)} and therefore we may choose continuous functions as the eigenvectors, that is, φ i ∈ C ( X ) {\displaystyle \varphi _{i}\in C(X)} for all i . {\displaystyle i.} Then by Mercer's theorem K {\displaystyle K} may be written in terms of the eigenvalues and continuous eigenfunctions as K ( x , y ) = ∑ j = 1 ∞ σ j φ j ( x ) φ j ( y ) {\displaystyle K(x,y)=\sum _{j=1}^{\infty }\sigma _{j}\,\varphi _{j}(x)\,\varphi _{j}(y)} for all x , y ∈ X {\displaystyle x,y\in X} such that lim n → ∞ sup u , v | K ( u , v ) − ∑ j = 1 n σ j φ j ( u ) φ j ( v ) | = 0. {\displaystyle \lim _{n\to \infty }\sup _{u,v}\left|K(u,v)-\sum _{j=1}^{n}\sigma _{j}\,\varphi _{j}(u)\,\varphi _{j}(v)\right|=0.} This above series representation is referred to as a Mercer kernel or Mercer representation of K {\displaystyle K} . Furthermore, it can be shown that the RKHS H {\displaystyle H} of K {\displaystyle K} is given by H = { f ∈ L 2 ( X ) | ∑ i = 1 ∞ ⟨ f , φ i ⟩ L 2 2 σ i < ∞ } {\displaystyle H=\left\{f\in L_{2}(X)\,{\Bigg \vert }\,\sum _{i=1}^{\infty }{\frac {\left\langle f,\varphi _{i}\right\rangle _{L_{2}}^{2}}{\sigma _{i}}}<\infty \right\}} where the inner product of H {\displaystyle H} given by ⟨ f , g ⟩ H = ∑ i = 1 ∞ ⟨ f , φ i ⟩ L 2 ⟨ g , φ i ⟩ L 2 σ i . {\displaystyle \left\langle f,g\right\rangle _{H}=\sum _{i=1}^{\infty }{\frac {\left\langle f,\varphi _{i}\right\rangle _{L_{2}}\left\langle g,\varphi _{i}\right\rangle _{L_{2}}}{\sigma _{i}}}.} This representation of the RKHS has application in probability and statistics, for example to the Karhunen–Loève representation for stochastic processes and kernel PCA. Feature maps A feature map is a map φ : X → F {\displaystyle \varphi \colon X\rightarrow F} , where F {\displaystyle F} is a Hilbert space which we will call the feature space. The first sections presented the connection between bounded/continuous evaluation functions, positive definite functions, and integral operators and in this section we provide another representation of the RKHS in terms of feature maps. Every feature map defines a kernel via Clearly K {\displaystyle K} is symmetric and positive definiteness follows from the properties of inner product in F {\displaystyle F} . Conversely, every positive definite function and corresponding reproducing kernel Hilbert space has infinitely many associated feature maps such that (3) holds. For example, we can trivially take F = H {\displaystyle F=H} and φ ( x ) = K x {\displaystyle \varphi (x)=K_{x}} for all x ∈ X {\displaystyle x\in X} . Then (3) is satisfied by the reproducing property. Another classical example of a feature map relates to the previous section regarding integral operators by taking F = ℓ 2 {\displaystyle F=\ell ^{2}} and φ ( x ) = ( σ i φ i ( x ) ) i {\displaystyle \varphi (x)=({\sqrt {\sigma _{i}}}\varphi _{i}(x))_{i}} . This connection between kernels and feature maps provides us with a new way to understand positive definite functions and hence reproducing kernels as inner products in H {\displaystyle H} . Moreover, every feature map can naturally define a RKHS by means of the definition of a positive definite function. Lastly, feature maps allow us to construct function spaces that reveal another perspective on the RKHS. Consider the linear space H φ = { f : X → R ∣ ∃ w ∈ F , f ( x ) = ⟨ w , φ ( x ) ⟩ F , ∀ x ∈ X } . {\displaystyle H_{\varphi }=\{f:X\to \mathbb {R} \mid \exists w\in F,f(x)=\langle w,\varphi (x)\rangle _{F},\forall {\text{ }}x\in X\}.} We can define a norm on H φ {\displaystyle H_{\varphi }} by ‖ f ‖ φ = inf { ‖ w ‖ F : w ∈ F , f ( x ) = ⟨ w , φ ( x ) ⟩ F , ∀ x ∈ X } . {\displaystyle \|f\|_{\varphi }=\inf\{\|w\|_{F}:w\in F,f(x)=\langle w,\varphi (x)\rangle _{F},\forall {\text{ }}x\in X\}.} It can be shown that H φ {\displaystyle H_{\varphi }} is a RKHS with kernel defined by K ( x , y ) = ⟨ φ ( x ) , φ ( y ) ⟩ F {\displaystyle K(x,y)=\langle \varphi (x),\varphi (y)\rangle _{F}} . This representation implies that the elements of the RKHS are inner products of elements in the feature space and can accordingly be seen as hyperplanes. This view of the RKHS is related to the kernel trick in machine learning. Properties Useful properties of RKHSs: Let ( X i ) i = 1 p {\displaystyle (X_{i})_{i=1}^{p}} be a sequence of sets and ( K i ) i = 1 p {\displaystyle (K_{i})_{i=1}^{p}} be a collection of corresponding positive definite functions on ( X i ) i = 1 p . {\displaystyle (X_{i})_{i=1}^{p}.} It then follows that K ( ( x 1 , … , x p ) , ( y 1 , … , y p ) ) = K 1 ( x 1 , y 1 ) ⋯ K p ( x p , y p ) {\displaystyle K((x_{1},\ldots ,x_{p}),(y_{1},\ldots ,y_{p}))=K_{1}(x_{1},y_{1})\cdots K_{p}(x_{p},y_{p})} is a kernel on X = X 1 × ⋯ × X p . {\displaystyle X=X_{1}\times \dots \times X_{p}.} Let X 0 ⊂ X , {\displaystyle X_{0}\subset X,} then the restriction of K {\displaystyle K} to X 0 × X 0 {\displaystyle X_{0}\times X_{0}} is also a reproducing kernel. Consider a normalized kernel K {\displaystyle K} such that K ( x , x ) = 1 {\displaystyle K(x,x)=1} for all x ∈ X {\displaystyle x\in X} . Define a pseudo-metric on X as d K ( x , y ) = ‖ K x − K y ‖ H 2 = 2 ( 1 − K ( x , y ) ) ∀ x ∈ X . {\displaystyle d_{K}(x,y)=\|K_{x}-K_{y}\|_{H}^{2}=2(1-K(x,y))\qquad \forall x\in X.} By the Cauchy–Schwarz inequality, K ( x , y ) 2 ≤ K ( x , x ) K ( y , y ) = 1 ∀ x , y ∈ X . {\displaystyle K(x,y)^{2}\leq K(x,x)K(y,y)=1\qquad \forall x,y\in X.} This inequality allows us to view K {\displaystyle K} as a measure of similarity between inputs. If x , y ∈ X {\displaystyle x,y\in X} are similar then K ( x , y ) {\displaystyle K(x,y)} will be closer to 1 while if x , y ∈ X {\displaystyle x,y\in X} are dissimilar then K ( x , y ) {\displaystyle K(x,y)} will be closer to 0. The closure of the span of { K x ∣ x ∈ X } {\displaystyle \{K_{x}\mid x\in X\}} coincides with H {\displaystyle H} . Common examples Bilinear kernels K ( x , y ) = ⟨ x , y ⟩ {\displaystyle K(x,y)=\langle x,y\rangle } The RKHS H {\displaystyle H} corresponding to this kernel is the dual space, consisting of functions f ( x ) = ⟨ x , β ⟩ {\displaystyle f(x)=\langle x,\beta \rangle } satisfying ‖ f ‖ H 2 = ‖ β ‖ 2 {\displaystyle \|f\|_{H}^{2}=\|\beta \|^{2}} . Polynomial kernels K ( x , y ) = ( α ⟨ x , y ⟩ + 1 ) d , α ∈ R , d ∈ N {\displaystyle K(x,y)=(\alpha \langle x,y\rangle +1)^{d},\qquad \alpha \in \mathbb {R} ,d\in \mathbb {N} } Radial basis function kernels These are another common class of kernels which satisfy K ( x , y ) = K ( ‖ x − y ‖ ) {\displaystyle K(x,y)=K(\|x-y\|)} . Some examples include: Gaussian or squared exponential kernel: K ( x , y ) = e − ‖ x − y ‖ 2 2 σ 2 , σ > 0 {\displaystyle K(x,y)=e^{-{\frac {\|x-y\|^{2}}{2\sigma ^{2}}}},\qquad \sigma >0} Laplacian kernel: K ( x , y ) = e − ‖ x − y ‖ σ , σ > 0 {\displaystyle K(x,y)=e^{-{\frac {\|x-y\|}{\sigma }}},\qquad \sigma >0} The squared norm of a function f {\displaystyle f} in the RKHS H {\displaystyle H} with this kernel is: ‖ f ‖ H 2 = ∫ R ( 1 σ f ( x ) 2 + σ f ′ ( x ) 2 ) d x . {\displaystyle \|f\|_{H}^{2}=\int _{\mathbb {R} }{\Big (}{\frac {1}{\sigma }}f(x)^{2}+\sigma f'(x)^{2}{\Big )}\mathrm {d} x.} Bergman kernels We also provide examples of Bergman kernels. Let X be finite and let H consist of all complex-valued functions on X. Then an element of H can be represented as an array of complex numbers. If the usual inner product is used, then Kx is the function whose value is 1 at x and 0 everywhere else, and K ( x , y ) {\displaystyle K(x,y)} can be thought of as an identity matrix since K ( x , y ) = { 1 x = y 0 x ≠ y {\displaystyle K(x,y)={\begin{cases}1&x=y\\0&x\neq y\end{cases}}} In this case, H is isomorphic to C n {\displaystyle \mathbb {C} ^{n}} . The case of X = D {\displaystyle X=\mathbb {D} } (where D {\displaystyle \mathbb {D} } denotes the unit disc) is more sophisticated. Here the Bergman space A 2 ( D ) {\displaystyle A^{2}(\mathbb {D} )} is the space of square-integrable holomorphic functions on D {\displaystyle \mathbb {D} } . It can be shown that the reproducing kernel for A 2 ( D ) {\displaystyle A^{2}(\mathbb {D} )} is K ( x , y ) = 1 π 1 ( 1 − x y ¯ ) 2 . {\displaystyle K(x,y)={\frac {1}{\pi }}{\frac {1}{(1-x{\overline {y}})^{2}}}.} Lastly, the space of band limited functions in L 2 ( R ) {\displaystyle L^{2}(\mathbb {R} )} with bandwidth 2 a {\displaystyle 2a} is a RKHS with reproducing kernel K ( x , y ) = sin ⁡ a ( x − y ) π ( x − y ) . {\displaystyle K(x,y)={\frac {\sin a(x-y)}{\pi (x-y)}}.} Extension to vector-valued functions In this section we extend the definition of the RKHS to spaces of vector-valued functions as this extension is particularly important in multi-task learning and manifold regularization. The main difference is that the reproducing kernel Γ {\displaystyle \Gamma } is a symmetric function that is now a positive semi-definite matrix for every x , y {\displaystyle x,y} in X {\displaystyle X} . More formally, we define a vector-valued RKHS (vvRKHS) as a Hilbert space of functions f : X → R T {\displaystyle f:X\to \mathbb {R} ^{T}} such that for all c ∈ R T {\displaystyle c\in \mathbb {R} ^{T}} and x ∈ X {\displaystyle x\in X} Γ x c ( y ) = Γ ( x , y ) c ∈ H for y ∈ X {\displaystyle \Gamma _{x}c(y)=\Gamma (x,y)c\in H{\text{ for }}y\in X} and ⟨ f , Γ x c ⟩ H = f ( x ) ⊺ c . {\displaystyle \langle f,\Gamma _{x}c\rangle _{H}=f(x)^{\intercal }c.} This second property parallels the reproducing property for the scalar-valued case. This definition can also be connected to integral operators, bounded evaluation functions, and feature maps as we saw for the scalar-valued RKHS. We can equivalently define the vvRKHS as a vector-valued Hilbert space with a bounded evaluation functional and show that this implies the existence of a unique reproducing kernel by the Riesz Representation theorem. Mercer's theorem can also be extended to address the vector-valued setting and we can therefore obtain a feature map view of the vvRKHS. Lastly, it can also be shown that the closure of the span of { Γ x c : x ∈ X , c ∈ R T } {\displaystyle \{\Gamma _{x}c:x\in X,c\in \mathbb {R} ^{T}\}} coincides with H {\displaystyle H} , another property similar to the scalar-valued case. We can gain intuition for the vvRKHS by taking a component-wise perspective on these spaces. In particular, we find that every vvRKHS is isometrically isomorphic to a scalar-valued RKHS on a particular input space. Let Λ = { 1 , … , T } {\displaystyle \Lambda =\{1,\dots ,T\}} . Consider the space X × Λ {\displaystyle X\times \Lambda } and the corresponding reproducing kernel As noted above, the RKHS associated to this reproducing kernel is given by the closure of the span of { γ ( x , t ) : x ∈ X , t ∈ Λ } {\displaystyle \{\gamma _{(x,t)}:x\in X,t\in \Lambda \}} where γ ( x , t ) ( y , s ) = γ ( ( x , t ) , ( y , s ) ) {\displaystyle \gamma _{(x,t)}(y,s)=\gamma ((x,t),(y,s))} for every set of pairs ( x , t ) , ( y , s ) ∈ X × Λ {\displaystyle (x,t),(y,s)\in X\times \Lambda } . The connection to the scalar-valued RKHS can then be made by the fact that every matrix-valued kernel can be identified with a kernel of the form of (4) via Γ ( x , y ) ( t , s ) = γ ( ( x , t ) , ( y , s ) ) . {\displaystyle \Gamma (x,y)_{(t,s)}=\gamma ((x,t),(y,s)).} Moreover, every kernel with the form of (4) defines a matrix-valued kernel with the above expression. Now letting the map D : H Γ → H γ {\displaystyle D:H_{\Gamma }\to H_{\gamma }} be defined as ( D f ) ( x , t ) = ⟨ f ( x ) , e t ⟩ R T {\displaystyle (Df)(x,t)=\langle f(x),e_{t}\rangle _{\mathbb {R} ^{T}}} where e t {\displaystyle e_{t}} is the t th {\displaystyle t^{\text{th}}} component of the canonical basis for R T {\displaystyle \mathbb {R} ^{T}} , one can show that D {\displaystyle D} is bijective and an isometry between H Γ {\displaystyle H_{\Gamma }} and H γ {\displaystyle H_{\gamma }} . While this view of the vvRKHS can be useful in multi-task learning, this isometry does not reduce the study of the vector-valued case to that of the scalar-valued case. In fact, this isometry procedure can make both the scalar-valued kernel and the input space too difficult to work with in practice as properties of the original kernels are often lost. An important class of matrix-valued reproducing kernels are separable kernels which can factorized as the product of a scalar valued kernel and a T {\displaystyle T} -dimensional symmetric positive semi-definite matrix. In light of our previous discussion these kernels are of the form γ ( ( x , t ) , ( y , s ) ) = K ( x , y ) K T ( t , s ) {\displaystyle \gamma ((x,t),(y,s))=K(x,y)K_{T}(t,s)} for all x , y {\displaystyle x,y} in X {\displaystyle X} and t , s {\displaystyle t,s} in T {\displaystyle T} . As the scalar-valued kernel encodes dependencies between the inputs, we can observe that the matrix-valued kernel encodes dependencies among both the inputs and the outputs. We lastly remark that the above theory can be further extended to spaces of functions with values in function spaces but obtaining kernels for these spaces is a more difficult task. Connection between RKHSs and the ReLU function The ReLU function is commonly defined as f ( x ) = max { 0 , x } {\displaystyle f(x)=\max\{0,x\}} and is a mainstay in the architecture of neural networks where it is used as an activation function. One can construct a ReLU-like nonlinear function using the theory of reproducing kernel Hilbert spaces. Below, we derive this construction and show how it implies the representation power of neural networks with ReLU activations. We will work with the Hilbert space H = L 2 1 ( 0 ) [ 0 , ∞ ) {\displaystyle {\mathcal {H}}=L_{2}^{1}(0)[0,\infty )} of absolutely continuous functions with f ( 0 ) = 0 {\displaystyle f(0)=0} and square integrable (i.e. L 2 {\displaystyle L_{2}} ) derivative. It has the inner product ⟨ f , g ⟩ H = ∫ 0 ∞ f ′ ( x ) g ′ ( x ) d x . {\displaystyle \langle f,g\rangle _{\mathcal {H}}=\int _{0}^{\infty }f'(x)g'(x)\,dx.} To construct the reproducing kernel it suffices to consider a dense subspace, so let f ∈ C 1 [ 0 , ∞ ) {\displaystyle f\in C^{1}[0,\infty )} and f ( 0 ) = 0 {\displaystyle f(0)=0} . The Fundamental Theorem of Calculus then gives f ( y ) = ∫ 0 y f ′ ( x ) d x = ∫ 0 ∞ G ( x , y ) f ′ ( x ) d x = ⟨ K y , f ⟩ {\displaystyle f(y)=\int _{0}^{y}f'(x)\,dx=\int _{0}^{\infty }G(x,y)f'(x)\,dx=\langle K_{y},f\rangle } where G ( x , y ) = { 1 , x < y 0 , otherwise {\displaystyle G(x,y)={\begin{cases}1,&x<y\\0,&{\text{otherwise}}\end{cases}}} and K y ′ ( x ) = G ( x , y ) , K y ( 0 ) = 0 {\displaystyle K_{y}'(x)=G(x,y),\ K_{y}(0)=0} i.e. K ( x , y ) = K y ( x ) = ∫ 0 x G ( z , y ) d z = { x , 0 ≤ x < y y , otherwise. = min ( x , y ) {\displaystyle K(x,y)=K_{y}(x)=\int _{0}^{x}G(z,y)\,dz={\begin{cases}x,&0\leq x<y\\y,&{\text{otherwise.}}\end{cases}}=\min(x,y)} This implies K y = K ( ⋅ , y ) {\displaystyle K_{y}=K(\cdot ,y)} reproduces f {\displaystyle f} . Moreover the minimum function on X × X = [ 0 , ∞ ) × [ 0 , ∞ ) {\displaystyle X\times X=[0,\infty )\times [0,\infty )} has the following representations with the ReLu function: min ( x , y ) = x − ReLU ⁡ ( x − y ) = y − ReLU ⁡ ( y − x ) . {\displaystyle \min(x,y)=x-\operatorname {ReLU} (x-y)=y-\operatorname {ReLU} (y-x).} Using this formulation, we can apply the representer theorem to the RKHS, letting one prove the optimality of using ReLU activations in neural network settings. See also Positive definite kernel Mercer's theorem Kernel trick Kernel embedding of distributions Representer theorem Notes References Alvarez, Mauricio, Rosasco, Lorenzo and Lawrence, Neil, “Kernels for Vector-Valued Functions: a Review,” https://arxiv.org/abs/1106.6251, June 2011. Aronszajn, Nachman (1950). "Theory of Reproducing Kernels". Transactions of the American Mathematical Society. 68 (3): 337–404. doi:10.1090/S0002-9947-1950-0051437-7. JSTOR 1990404. MR 0051437. Berlinet, Alain and Thomas, Christine. Reproducing kernel Hilbert spaces in Probability and Statistics, Kluwer Academic Publishers, 2004. Cucker, Felipe; Smale, Steve (2002). "On the Mathematical Foundations of Learning". Bulletin of the American Mathematical Society. 39 (1): 1–49. doi:10.1090/S0273-0979-01-00923-5. MR 1864085. De Vito, Ernest, Umanita, Veronica, and Villa, Silvia. "An extension of Mercer theorem to vector-valued measurable kernels," arXiv:1110.4017 , June 2013. Durrett, Greg. 9.520 Course Notes, Massachusetts Institute of Technology, https://www.mit.edu/~9.520/scribe-notes/class03_gdurett.pdf, February 2010. Kimeldorf, George; Wahba, Grace (1971). "Some results on Tchebycheffian Spline Functions" (PDF). Journal of Mathematical Analysis and Applications. 33 (1): 82–95. doi:10.1016/0022-247X(71)90184-3. MR 0290013. Okutmustur, Baver. “Reproducing Kernel Hilbert Spaces,” M.S. dissertation, Bilkent University, https://users.metu.edu.tr/baver/MS.Thesis.pdf, August 2005. Paulsen, Vern. “An introduction to the theory of reproducing kernel Hilbert spaces,” https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=440218056738e05b5ab43679f932a9f33fccee87. Steinwart, Ingo; Scovel, Clint (2012). "Mercer's theorem on general domains: On the interaction between measures, kernels, and RKHSs". Constr. Approx. 35 (3): 363–417. doi:10.1007/s00365-012-9153-3. MR 2914365. S2CID 253885172. Rosasco, Lorenzo and Poggio, Thomas. "A Regularization Tour of Machine Learning – MIT 9.520 Lecture Notes" Manuscript, Dec. 2014. Wahba, Grace, Spline Models for Observational Data, SIAM, 1990. Zhang, Haizhang; Xu, Yuesheng; Zhang, Qinghui (2012). "Refinement of Operator-valued Reproducing Kernels" (PDF). Journal of Machine Learning Research. 13: 91–136. In theoretical physics, one often analyzes theories with supersymmetry in which F-terms play an important role. In four dimensions, the minimal N=1 supersymmetry may be written using a superspace. This superspace involves four extra fermionic coordinates θ 1 , θ 2 , θ ¯ 1 , θ ¯ 2 {\displaystyle \theta ^{1},\theta ^{2},{\bar {\theta }}^{1},{\bar {\theta }}^{2}} , transforming as a two-component spinor and its conjugate. Every superfield—i.e. a field that depends on all coordinates of the superspace—may be expanded with respect to the new fermionic coordinates. There exists a special kind of superfields, the so-called chiral superfields, that only depend on the variables θ {\displaystyle \theta } but not their conjugates. The last term in the corresponding expansion, namely F θ 1 θ 2 {\displaystyle F\theta ^{1}\theta ^{2}} , is called the F-term. Applying an infinitesimal supersymmetry transformation to a chiral superfield results in yet another chiral superfield whose F-term, in particular, changes by a total derivative. This is significant because then ∫ d 4 x F ( x ) {\displaystyle \int {d^{4}x\,F(x)}} is invariant under SUSY transformations as long as boundary terms vanish. Thus F-terms may be used in constructing supersymmetric actions. Manifestly-supersymmetric Lagrangians may also be written as integrals over the whole superspace. Some special terms, such as the superpotential, may be written as integrals over θ {\displaystyle \theta } s only. They are also referred to as F-terms, much like the terms in the ordinary potential that arise from these terms of the supersymmetric Lagrangian. See also D-term Supersymmetric gauge theory == References == The Nuclear Ensemble Approach (NEA) is a general method for simulations of diverse types of molecular spectra. It works by sampling an ensemble of molecular conformations (nuclear geometries) in the source state, computing the transition probabilities to the target states for each of these geometries, and performing a sum over all these transitions convoluted with shape function. The result is an incoherent spectrum containing absolute band shapes through inhomogeneous broadening. Motivation Spectrum simulation is one of the most fundamental tasks in quantum chemistry. It allows comparing the theoretical results to experimental measurements. There are many theoretical methods for simulating spectra. Some are simple approximations (like stick spectra); others are high-level, accurate approximations (like those based on Fourier-transform of wavepacket propagations). The NEA lies in between. On the one hand, it is intuitive and straightforward to apply, providing much improved results compared to the stick spectrum. On the other hand, it does not recover all spectral effects and delivers a limited spectral resolution. Historical The NEA is a multidimensional extension of the reflection principle, an approach often used for estimating spectra in photodissociative systems. With popularization molecular mechanics, ensembles of geometries started to be also used to estimate the spectra through incoherent sums. Thus, different from the reflection principle, which is usually done via direct integration of analytical functions, the NEA is a numerical approach. In 2012, a formal account of NEA showed that it corresponded to an approximation to the time-dependent spectrum simulation approach, employing a Monte Carlo integration of the wavepacket overlap time evolution. NEA for absorption spectrum Consider an ensemble of molecules absorbing radiation in the UV/vis. Initially, all molecules are in the ground electronic state Because of the molecular zero-point energy and temperature, the molecular geometry has a distribution around the equilibrium geometry. From a classical point of view, supposing that the photon absorption is an instantaneous process, each time a molecule is excited, it does so from a different geometry. As a consequence, the transition energy has not always the same value, but is a function of the nuclear coordinates. The NEA captures this effect by creating an ensemble of geometries reflecting the zero-point energy, the temperature, or both. In the NEA, the absorption spectrum (or absorption cross section) σ(E) at excitation energy E is calculated as σ ( E ) = π e 2 ℏ 2 m c ϵ 0 E ∑ n N f s 1 N p ∑ i N p Δ E 0 n ( x i ) f 0 n ( x i ) g ( E − Δ E 0 n ( x i ) , δ ) , {\displaystyle \sigma \left(E\right)={\frac {\pi {{e}^{2}}\hbar }{2mc{{\epsilon }_{0}}E}}\sum \limits _{n}^{{N}_{fs}}{{\frac {1}{{N}_{p}}}\sum \limits _{i}^{{N}_{p}}{\Delta {{E}_{0n}}\left({{\mathbf {x} }_{i}}\right){{f}_{0n}}\left({{\mathbf {x} }_{i}}\right)g\left(E-\Delta {{E}_{0n}}\left({{\mathbf {x} }_{i}}\right),\delta \right)}},} where e and m are the electron charge and mass, c is the speed of light, ε0 the vacuum permittivity, and ћ the reduced Planck constant. The sums run over Nfs excited states and Np nuclear geometries xi. For each of such geometries in the ensemble, transition energies ΔE0n(xi) and oscillator strengths f0n(xi) between the ground (0) and the excited (n) states are computed. Each transition in the ensemble is convoluted with a normalized line shape function centered at ΔE0n(xi) and with width δ. Each xi is a vector collecting the cartesian components of the geometries of each atom. The line shape function may be, for instance, a normalized Gaussian function given by g ( E − Δ E 0 n , δ ) = 1 2 π ( δ / 2 ) 2 exp ⁡ ( − ( E − Δ E 0 n ) 2 2 ( δ / 2 ) 2 ) . {\displaystyle g\left(E-\Delta {{E}_{0n}},\delta \right)={\frac {1}{\sqrt {2\pi {{\left(\delta /2\right)}^{2}}}}}\exp \left(-{\frac {{\left(E-\Delta {{E}_{0n}}\right)}^{2}}{2{{\left(\delta /2\right)}^{2}}}}\right).} Although δ is an arbitrary parameter, it must be much narrower than the band width, not to interfere in its description. As the average value of band widths is around 0.3 eV, it is a good practice to adopt δ ≤ 0.05 eV. The geometries xi can be generated by any method able to describe the ground state distribution. Two of the most employed are dynamics and Wigner distribution nuclear normal modes. Molar extinction coefficient ε can be obtained from absorption cross section through σ = ln ⁡ ( 10 ) 10 3 N A ε ≈ 3.82353216 × 10 − 21 ε . {\displaystyle \sigma =\ln(10){\frac {10^{3}}{N_{\text{A}}}}\varepsilon \approx 3.82353216\times 10^{-21}\,\varepsilon .} Because of the dependence of f0n on xi, NEA is a post-Condon approximation, and it can predict dark vibronic bands. NEA for emission spectrum In the case of fluorescence, the differential emission rate is given by Γ ( E ) = e 2 2 π ℏ m c 3 ϵ 0 1 N p ∑ i N p Δ E 1 , 0 ( x i ) 2 | f 1 , 0 ( x i ) | g ( E − Δ E 1 , 0 ( x i ) , δ ) {\displaystyle \Gamma (E)={\frac {e^{2}}{2\pi \hbar mc^{3}\epsilon _{0}}}{\frac {1}{N_{p}}}\sum _{i}^{N_{p}}\Delta E_{1,0}(\mathbf {x} _{i})^{2}\left|f_{1,0}(\mathbf {x} _{i})\right|g\left(E-\Delta E_{1,0}(\mathbf {x} _{i}),\delta \right)} . This expression assumes the validity of the Kasha's rule, with emission from the first excited state. NEA for other types of spectrum NEA can be used for many types of steady-state and time-resolved spectrum simulations. Some examples beyond absorption and emission spectra are: two-dimensional differential transmission photoelectron ultrafast Auger X-ray photo-scattering Limitations of NEA By construction, NEA does not include information about the target (final) states. For this reason, any spectral information that depends on these states cannot be described in the framework of NEA. For example, vibronically resolved peaks in the absorption spectrum will not appear in the simulations, only the band envelope around them, because these peaks depend on the wavefunction overlap between the ground and excited state. NEA can be, however, coupled to excited-state dynamics to recover these effects. NEA may be too computationally expensive for large molecules. The spectrum simulation requires the calculation of transition probabilities for hundreds of different nuclear geometries, which may become prohibitive due to the high computational costs. Machine learning methods coupled to NEA have been proposed to reduce these costs. == References == The postulate that consciousness causes collapse is an interpretation of quantum mechanics in which consciousness is postulated to be the main mechanism behind the process of measurement in quantum mechanics. It is a historical interpretation of quantum mechanics that is largely discarded by modern physicists. The idea is attributed to Eugene Wigner who wrote about it in the 1960s, but traces of the idea appear as early as the 1930s. Wigner later rejected this interpretation in the 1970s and 1980s. History Earlier work According to Werner Heisenberg recollections in Physics and Beyond, Niels Bohr is said to have rejected the necessity of a conscious observer in quantum mechanics as early as 1927. In his 1932 book Mathematical Foundations of Quantum Mechanics, John von Neumann argued that the mathematics of quantum mechanics allows the collapse of the wave function to be placed at any position in the causal chain from the measurement device to the "subjective perception" of the human observer. However von Neumann did not explicitly relate measurement with consciousness. In 1939, Fritz London and Edmond Bauer argued that the consciousness of the observer played an important role in measurement. However London wrote about consciousness in terms of philosophical phenomenology and not necessarily as a physical process. Wigner's work The idea of "consciousness causes collapse" is attributed to Eugene Wigner who first wrote about it in his 1961 article "Remarks on the mind-body question" and developed it further during the 1960s. Wigner reformulated the Schrödinger's cat thought experiment as the Wigner's friend and proposed that the consciousness of an observer is the demarcation line that precipitates collapse of the wave function, independent of any realist interpretation. The mind is postulated to be non-physical and the only true measurement apparatus. The idea was criticized early by Abner Shimony in 1963 and by Hilary Putnam a year later. Wigner discarded the conscious collapse interpretation in the later 1970s. In a lecture 1982, Wigner says that he early view of quantum mechanics should be criticized as solipsism. In 1984, he writes that he was convinced out of it by the 1970 work of H. Dieter Zeh on quantum decoherence and macroscopic quantum phenomena. After Wigner The idea of consciousness causing collapse has been promoted and developed by Henry Stapp since 1993. Description Measurement in standard quantum mechanics In the orthodox Copenhagen interpretation, quantum mechanics predicts only the probabilities for different observed experimental outcomes. What constitutes an observer or a measurement is not directly specified by the theory, and the behavior of a system under measurement and observation is completely different from its usual behavior: the wavefunction that describes a system spreads out into an ever-larger superposition of different possible situations. However, during observation, the wavefunction describing the system collapses to one of several options. If there is no observation, this collapse does not occur, and none of the options ever become less likely. It can be predicted using quantum mechanics, absent a collapse postulate, that an observer observing a quantum superposition will turn into a superposition of different observers seeing different things. The observer will have a wavefunction which describes all the possible outcomes. Still, in actual experience, an observer never senses a superposition, but always senses that one of the outcomes has occurred with certainty. This apparent conflict between a wavefunction description and classical experience is called the problem of observation (see Measurement problem). Consciousness-causes-collapse interpretation This consciousness causes collapse interpretation has been summarized thus: The rules of quantum mechanics are correct but there is only one system which may be treated with quantum mechanics, namely the entire material world. There exist external observers which cannot be treated within quantum mechanics, namely human (and perhaps animal) minds, which perform measurements on the brain causing wave function collapse. Stapp has argued for the concept as follows: From the point of view of the mathematics of quantum theory it makes no sense to treat a measuring device as intrinsically different from the collection of atomic constituents that make it up. A device is just another part of the physical universe... Moreover, the conscious thoughts of a human observer ought to be causally connected most directly and immediately to what is happening in his brain, not to what is happening out at some measuring device... Our bodies and brains thus become ... parts of the quantum mechanically described physical universe. Treating the entire physical universe in this unified way provides a conceptually simple and logically coherent theoretical foundation... Objections to the interpretation Wigner shifted away from "consciousness causes collapse" in his later years. This was partly because he was embarrassed that "consciousness causes collapse" can lead to a kind of solipsism, but also because he decided that he had been wrong to try to apply quantum physics at the scale of everyday life (specifically, he rejected his initial idea of treating macroscopic objects as isolated systems). Bohr said circa 1927 that it "still makes no difference whether the observer is a man, an animal, or a piece of apparatus." This interpretation relies upon an interactionist form of dualism that is inconsistent with the materialism that is commonly used to understand the brain, and accepted by most scientists. (Materialism assumes that consciousness has no special role in relation to quantum mechanics.) The measurement problem notwithstanding, they point to a causal closure of physics, suggesting a problem with how consciousness and matter might interact, reminiscent of objections to Descartes' substance dualism. The only form of interactionist dualism that has seemed even remotely tenable in the contemporary picture is one that exploits certain properties of quantum mechanics. There are two ways this might go. First, some [e.g., Eccles 1986] have appealed to the existence of quantum indeterminacy, and have suggested that a nonphysical consciousness might be responsible for filling the resultant causal gaps, determining which values some physical magnitudes might take within an apparently "probabilistic" distribution... This is an audacious and interesting suggestion, but it has a number of problems... A second way in which quantum mechanics bears on the issue of causal closure lies with the fact that in some interpretations of the quantum formalism, consciousness itself plays a vital causal role, being required to bring about the so-called "collapse of the wave-function." This collapse is supposed to occur upon any act of measurement; and in one interpretation, the only way to distinguish a measurement from a nonmeasurement is via the presence of consciousness. This theory is certainly not universally accepted (for a start, it presupposes that consciousness is not itself physical, surely contrary to the views of most physicists), and I do not accept it myself, but in any case it seems that the kind of causal work consciousness performs here is quite different from the kind required for consciousness to play a role in directing behavior... In any case, all versions of interactionist dualism have a conceptual problem that suggests that they are less successful in avoiding epiphenomenalism than they might seem; or at least they are no better off than naturalistic dualism. Even on these views, there is a sense in which the phenomenal is irrelevant. We can always subtract the phenomenal component from any explanatory account, yielding a purely causal component. The interpretation has also been criticized for not explaining which things have sufficient consciousness to collapse the wave function. Also, it posits an important role for the conscious mind, and it has been questioned how this could be the case for the earlier universe, before consciousness had evolved or emerged. It has been argued that "[consciousness causes collapse] does not allow sensible discussion of Big Bang cosmology or biological evolution". For example, Roger Penrose remarked: "[T]he evolution of conscious life on this planet is due to appropriate mutations having taken place at various times. These, presumably, are quantum events, so they would exist only in linearly superposed form until they finally led to the evolution of a conscious being—whose very existence depends on all the right mutations having 'actually' taken place!" Others further suppose a universal mind (see also panpsychism and panexperientialism). Other researchers have expressed similar objections to the introduction of any subjective element in the collapse of the wavefunction. Testability It has been argued that the results of delayed-choice quantum eraser experiments empirically falsify this interpretation. However, the argument was shown to be invalid because an interference pattern would only be visible after post-measurement detections were correlated through use of a coincidence counter; if that was not true, the experiment would allow signaling into the past. The delayed-choice quantum eraser experiment has also been used to argue for support of this interpretation, but, as with other arguments, none of the cited references prove or falsify this interpretation. The central role played by consciousness in this interpretation naturally calls for use of psychological experiments to verify or falsify it. One such approach relies on explaining the empirical presentiment effect quantum mechanically. Another approach makes use of the psychological priming effect to design an appropriate test. Both methods claim verification success. Reception A poll was conducted at a quantum mechanics conference in 2011 using 33 participants (including physicists, mathematicians, and philosophers). Researchers found that 6% of participants (2 of the 33) indicated that they believed the observer "plays a distinguished physical role (e.g., wave-function collapse by consciousness)". This poll also states that 55% (18 of the 33) indicated that they believed the observer "plays a fundamental role in the application of the formalism but plays no distinguished physical role". They also mention that "Popular accounts have sometimes suggested that the Copenhagen interpretation attributes such a role to consciousness. In our view, this is to misunderstand the Copenhagen interpretation." Quantum mysticism This interpretation has been tied to the origin of pseudoscientific currents and New Age movements, specifically quantum mysticism. See also Interpretations of quantum mechanics Measurement in quantum mechanics Quantum mind Quantum Zeno effect Wigner's friend References External links Don N. Page, Mindful Sensationalism: A Quantum Framework for Consciousness (2001), arXiv Sergei Vladimirovich Tyablikov (Russian: Серге́й Влади́мирович Тя́бликов; 7 September 1921 – 17 March 1968) was a Soviet theoretical physicist known for his significant contributions to statistical mechanics, solid-state physics, and for the development of the double-time Green function's formalism. Biography Tyablikov was born in Klin, Russia. In 1944 he graduated from the Faculty of Physics at the Moscow State University (MSU) and started his postgraduate study with Anatoly Vlasov and later with Nikolay Bogoliubov at the Department of Theoretical Physics. In 1947 he obtained PhD degree (Candidate of Sciences) with PhD Thesis on the subject of crystallization theory and was appointed to the Steklov Institute of Mathematics, where he continued to work for the rest of his life. In 1954 he defended at the MSU his doctoral dissertation "Studies of the Polaron Theory" and obtained the degree of Doktor nauk (Doctor of Science, similar to Habilitation). Since 1962 he was the Head of the Division of Statistical Mechanics in the Steklov Institute of Mathematics. In the period 1966–1968, Sergei Tyablikov also worked at the Joint Institute for Nuclear Research, where he was the first Head of the Statistical Mechanics and Theory of Condensed Matter Group at the Laboratory of Theoretical Physics. Research work During postgraduate study in 1944–1947 he worked on theory of crystallization, where he applied such methods as diagonalization of bilinear forms in Bose or Fermi operators, etc., which later became a common tool for theoretical physicists. After finishing PhD he started to work on the problem of a particle interacting with a quantum field. This problem is directly related to polaron theory, the effect of impurities on the energy spectrum of superfluids, and other problems in condensed matter physics. He was involved in the development of operator form of perturbation theory, approximate second quantization, adiabatic approximation for systems with translational invariance, and other theoretical physics methods which play an important role in the theory of many-particle systems. Since 1948 in collaboration with Nikolay Bogoliubov he started to work on quantum theory of ferromagnetism and antiferromagnetism. In 1948 they developed a consistent theoretical polar model of metals. Later Tyablikov developed the first consistent quantum theory of magnetic anisotropy. His particularly important contribution to antiferromagnetism was in the development of the method of quantum temperature Green's functions. In 1959, Sergei Tyablikov and Nikolay Bogoliubov published the paper which strongly influenced the development of the many-body physics and specifically the quantum theory of magnetism. He also co-authored with V.L. Bonch-Bruevich the book The Green Function Method in Statistical Mechanics, the first book with a consistent exposition of the method of Green's functions. Publications Books Bonch-Bruevich V. L., Tyablikov S. V. (1962): The Green Function Method in Statistical Mechanics. North Holland Publishing Co. Tyablikov S. V. (1995): Methods in the Quantum Theory of Magnetism. (Translated to English) Springer; 1st edition. ISBN 0306302632. ISBN 9780306302633. Selected papers References Sergei Vladimirovich Tyablikov Soviet Physics Uspekhi 11(4), 606—607 (January–February 1969). Biography of S. V. Tyablikov (1921-1968) at the Joint Institute for Nuclear Research. In theoretical physics, soft SUSY breaking is type of supersymmetry breaking that does not cause ultraviolet divergences to appear in scalar masses. Overview These terms are relevant operators—i.e. operators whose coefficients have a positive dimension of mass—though there are some exceptions. A model with soft SUSY breaking was proposed in 1981 by Howard Georgi and Savas Dimopoulos. Before this, dynamical models of supersymmetry breaking were being used that suffered from giving rise to color and charge breaking vacua. Soft SUSY breaking decouples the origin of supersymmetry breaking from its phenomenological consequences. In effect, soft SUSY breaking adds explicit symmetry breaking to the supersymmetric Standard Model Lagrangian. The source of SUSY breaking results from a different sector where supersymmetry is broken spontaneously. Divorcing the spontaneous supersymmetry breaking from the supersymmetric Standard Model leads to the notion of mediated supersymmetry breaking. Example operators Gaugino mass Scalar masses Scalar trilinear interactions ("A-terms") Nonholomorphic soft supersymmetry breaking interactions In low energy supersymmetry based models, the soft supersymmetry breaking interactions excepting the mass terms are usually considered to be holomorphic functions of fields. While a superpotential such as that of MSSM needs to be holomorphic, there is no reason why soft supersymmetry breaking interactions are required to be holomorphic functions of fields. Of course, an arbitrary nonholomorphic interaction may invite an appearance of quadratic divergence (or hard supersymmetry breaking); however, there are scenarios with no gauge singlet fields where nonholomorphic interactions can as well be of soft supersymmetry breaking type. One may consider a hidden sector based supersymmetry breaking, with X {\displaystyle X} and Φ {\displaystyle \Phi } to be chiral superfields. Then, there exist nonholomorphic D {\displaystyle D} -term contributions of the forms 1 M 3 [ X X ∗ Φ 2 Φ ∗ ] D {\displaystyle {\frac {1}{M^{3}}}[XX^{*}\Phi ^{2}\Phi ^{*}]_{D}} and 1 M 3 [ X X ∗ D α Φ D α Φ ] D {\displaystyle {\frac {1}{M^{3}}}[XX^{*}D^{\alpha }\Phi D_{\alpha }\Phi ]_{D}} that are soft supersymmetry breaking in nature. The above lead to nonholomorphic trilinear soft terms like ϕ 2 ϕ ∗ {\displaystyle \phi ^{2}\phi ^{*}} and an explicit Higgsino soft mass term like ψ ψ {\displaystyle \psi \psi } in the Lagrangian. The coefficients of both ϕ 2 ϕ ∗ {\displaystyle \phi ^{2}\phi ^{*}} and ψ ψ {\displaystyle \psi \psi } terms are proportional to | F | 2 M 3 {\displaystyle {\frac {|F|^{2}}{M^{3}}}} , where | F | {\displaystyle |F|} is the vacuum expectation value of the auxiliary field components of X {\displaystyle X} and M {\displaystyle M} is the scale of mediation of supersymmetry breaking. Away from MSSM, there can be higgsino-gaugino interactions like ψ λ {\displaystyle \psi \lambda } that are also nonholomorphic in nature. == References == Lagrangian field theory is a formalism in classical field theory. It is the field-theoretic analogue of Lagrangian mechanics. Lagrangian mechanics is used to analyze the motion of a system of discrete particles each with a finite number of degrees of freedom. Lagrangian field theory applies to continua and fields, which have an infinite number of degrees of freedom. One motivation for the development of the Lagrangian formalism on fields, and more generally, for classical field theory, is to provide a clear mathematical foundation for quantum field theory, which is infamously beset by formal difficulties that make it unacceptable as a mathematical theory. The Lagrangians presented here are identical to their quantum equivalents, but, in treating the fields as classical fields, instead of being quantized, one can provide definitions and obtain solutions with properties compatible with the conventional formal approach to the mathematics of partial differential equations. This enables the formulation of solutions on spaces with well-characterized properties, such as Sobolev spaces. It enables various theorems to be provided, ranging from proofs of existence to the uniform convergence of formal series to the general settings of potential theory. In addition, insight and clarity is obtained by generalizations to Riemannian manifolds and fiber bundles, allowing the geometric structure to be clearly discerned and disentangled from the corresponding equations of motion. A clearer view of the geometric structure has in turn allowed highly abstract theorems from geometry to be used to gain insight, ranging from the Chern–Gauss–Bonnet theorem and the Riemann–Roch theorem to the Atiyah–Singer index theorem and Chern–Simons theory. Overview In field theory, the independent variable is replaced by an event in spacetime (x, y, z, t), or more generally still by a point s on a Riemannian manifold. The dependent variables are replaced by the value of a field at that point in spacetime φ ( x , y , z , t ) {\displaystyle \varphi (x,y,z,t)} so that the equations of motion are obtained by means of an action principle, written as: δ S δ φ i = 0 , {\displaystyle {\frac {\delta {\mathcal {S}}}{\delta \varphi _{i}}}=0,} where the action, S {\displaystyle {\mathcal {S}}} , is a functional of the dependent variables φ i ( s ) {\displaystyle \varphi _{i}(s)} , their derivatives and s itself S [ φ i ] = ∫ L ( φ i ( s ) , { ∂ φ i ( s ) ∂ s α } , { s α } ) d n s , {\displaystyle {\mathcal {S}}\left[\varphi _{i}\right]=\int {{\mathcal {L}}\left(\varphi _{i}(s),\left\{{\frac {\partial \varphi _{i}(s)}{\partial s^{\alpha }}}\right\},\{s^{\alpha }\}\right)\,\mathrm {d} ^{n}s},} where the brackets denote { ⋅ ∀ α } {\displaystyle \{\cdot ~\forall \alpha \}} ; and s = {sα} denotes the set of n independent variables of the system, including the time variable, and is indexed by α = 1, 2, 3, ..., n. The calligraphic typeface, L {\displaystyle {\mathcal {L}}} , is used to denote the density, and d n s {\displaystyle \mathrm {d} ^{n}s} is the volume form of the field function, i.e., the measure of the domain of the field function. In mathematical formulations, it is common to express the Lagrangian as a function on a fiber bundle, wherein the Euler–Lagrange equations can be interpreted as specifying the geodesics on the fiber bundle. Abraham and Marsden's textbook provided the first comprehensive description of classical mechanics in terms of modern geometrical ideas, i.e., in terms of tangent manifolds, symplectic manifolds and contact geometry. Bleecker's textbook provided a comprehensive presentation of field theories in physics in terms of gauge invariant fiber bundles. Such formulations were known or suspected long before. Jost continues with a geometric presentation, clarifying the relation between Hamiltonian and Lagrangian forms, describing spin manifolds from first principles, etc. Current research focuses on non-rigid affine structures, (sometimes called "quantum structures") wherein one replaces occurrences of vector spaces by tensor algebras. This research is motivated by the breakthrough understanding of quantum groups as affine Lie algebras (Lie groups are, in a sense "rigid", as they are determined by their Lie algebra. When reformulated on a tensor algebra, they become "floppy", having infinite degrees of freedom; see e.g., Virasoro algebra.) Definitions In Lagrangian field theory, the Lagrangian as a function of generalized coordinates is replaced by a Lagrangian density, a function of the fields in the system and their derivatives, and possibly the space and time coordinates themselves. In field theory, the independent variable t is replaced by an event in spacetime (x, y, z, t) or still more generally by a point s on a manifold. Often, a "Lagrangian density" is simply referred to as a "Lagrangian". Scalar fields For one scalar field φ {\displaystyle \varphi } , the Lagrangian density will take the form: L ( φ , ∇ φ , ∂ φ / ∂ t , x , t ) {\displaystyle {\mathcal {L}}(\varphi ,{\boldsymbol {\nabla }}\varphi ,\partial \varphi /\partial t,\mathbf {x} ,t)} For many scalar fields L ( φ 1 , ∇ φ 1 , ∂ φ 1 / ∂ t , … , φ n , ∇ φ n , ∂ φ n / ∂ t , … , x , t ) {\displaystyle {\mathcal {L}}(\varphi _{1},{\boldsymbol {\nabla }}\varphi _{1},\partial \varphi _{1}/\partial t,\ldots ,\varphi _{n},{\boldsymbol {\nabla }}\varphi _{n},\partial \varphi _{n}/\partial t,\ldots ,\mathbf {x} ,t)} In mathematical formulations, the scalar fields are understood to be coordinates on a fiber bundle, and the derivatives of the field are understood to be sections of the jet bundle. Vector fields, tensor fields, spinor fields The above can be generalized for vector fields, tensor fields, and spinor fields. In physics, fermions are described by spinor fields. Bosons are described by tensor fields, which include scalar and vector fields as special cases. For example, if there are m {\displaystyle m} real-valued scalar fields, φ 1 , … , φ m {\displaystyle \varphi _{1},\dots ,\varphi _{m}} , then the field manifold is R m {\displaystyle \mathbb {R} ^{m}} . If the field is a real vector field, then the field manifold is isomorphic to R n {\displaystyle \mathbb {R} ^{n}} . Action The time integral of the Lagrangian is called the action denoted by S. In field theory, a distinction is occasionally made between the Lagrangian L, of which the time integral is the action S = ∫ L d t , {\displaystyle {\mathcal {S}}=\int L\,\mathrm {d} t\,,} and the Lagrangian density L {\displaystyle {\mathcal {L}}} , which one integrates over all spacetime to get the action: S [ φ ] = ∫ L ( φ , ∇ φ , ∂ φ / ∂ t , x , t ) d 3 x d t . {\displaystyle {\mathcal {S}}[\varphi ]=\int {\mathcal {L}}(\varphi ,{\boldsymbol {\nabla }}\varphi ,\partial \varphi /\partial t,\mathbf {x} ,t)\,\mathrm {d} ^{3}\mathbf {x} \,\mathrm {d} t.} The spatial volume integral of the Lagrangian density is the Lagrangian; in 3D, L = ∫ L d 3 x . {\displaystyle L=\int {\mathcal {L}}\,\mathrm {d} ^{3}\mathbf {x} \,.} The action is often referred to as the "action functional", in that it is a function of the fields (and their derivatives). Volume form In the presence of gravity or when using general curvilinear coordinates, the Lagrangian density L {\displaystyle {\mathcal {L}}} will include a factor of g {\textstyle {\sqrt {g}}} . This ensures that the action is invariant under general coordinate transformations. In mathematical literature, spacetime is taken to be a Riemannian manifold M {\displaystyle M} and the integral then becomes the volume form S = ∫ M | g | d x 1 ∧ ⋯ ∧ d x m L {\displaystyle {\mathcal {S}}=\int _{M}{\sqrt {|g|}}dx^{1}\wedge \cdots \wedge dx^{m}{\mathcal {L}}} Here, the ∧ {\displaystyle \wedge } is the wedge product and | g | {\textstyle {\sqrt {|g|}}} is the square root of the determinant | g | {\displaystyle |g|} of the metric tensor g {\displaystyle g} on M {\displaystyle M} . For flat spacetime (e.g., Minkowski spacetime), the unit volume is one, i.e. | g | = 1 {\textstyle {\sqrt {|g|}}=1} and so it is commonly omitted, when discussing field theory in flat spacetime. Likewise, the use of the wedge-product symbols offers no additional insight over the ordinary concept of a volume in multivariate calculus, and so these are likewise dropped. Some older textbooks, e.g., Landau and Lifschitz write − g {\textstyle {\sqrt {-g}}} for the volume form, since the minus sign is appropriate for metric tensors with signature (+−−−) or (−+++) (since the determinant is negative, in either case). When discussing field theory on general Riemannian manifolds, the volume form is usually written in the abbreviated notation ∗ ( 1 ) {\displaystyle *(1)} where ∗ {\displaystyle *} is the Hodge star. That is, ∗ ( 1 ) = | g | d x 1 ∧ ⋯ ∧ d x m {\displaystyle *(1)={\sqrt {|g|}}dx^{1}\wedge \cdots \wedge dx^{m}} and so S = ∫ M ∗ ( 1 ) L {\displaystyle {\mathcal {S}}=\int _{M}*(1){\mathcal {L}}} Not infrequently, the notation above is considered to be entirely superfluous, and S = ∫ M L {\displaystyle {\mathcal {S}}=\int _{M}{\mathcal {L}}} is frequently seen. Do not be misled: the volume form is implicitly present in the integral above, even if it is not explicitly written. Euler–Lagrange equations The Euler–Lagrange equations describe the geodesic flow of the field φ {\displaystyle \varphi } as a function of time. Taking the variation with respect to φ {\displaystyle \varphi } , one obtains 0 = δ S δ φ = ∫ M ∗ ( 1 ) ( − ∂ μ ( ∂ L ∂ ( ∂ μ φ ) ) + ∂ L ∂ φ ) . {\displaystyle 0={\frac {\delta {\mathcal {S}}}{\delta \varphi }}=\int _{M}*(1)\left(-\partial _{\mu }\left({\frac {\partial {\mathcal {L}}}{\partial (\partial _{\mu }\varphi )}}\right)+{\frac {\partial {\mathcal {L}}}{\partial \varphi }}\right).} Solving, with respect to the boundary conditions, one obtains the Euler–Lagrange equations: ∂ L ∂ φ = ∂ μ ( ∂ L ∂ ( ∂ μ φ ) ) . {\displaystyle {\frac {\partial {\mathcal {L}}}{\partial \varphi }}=\partial _{\mu }\left({\frac {\partial {\mathcal {L}}}{\partial (\partial _{\mu }\varphi )}}\right).} Lagrangian terms Often the Lagrangian consists of a sum of polynomial terms, with the symmetries of the theory and the fields involved dictating the types of terms that are allowed. For example, in relativistic theories, each term must be Lorentz invariant while in a theory with a gauge field, they must be gauge invariant. Terms that contain two fields and no derivatives are known as mass terms, with these giving mass to the fields. For example, a single real scalar field ϕ ( x ) {\displaystyle \phi (x)} of mass m {\displaystyle m} has a mass term given by L m = − 1 2 m 2 ϕ 2 ( x ) . {\displaystyle {\mathcal {L}}_{m}=-{\frac {1}{2}}m^{2}\phi ^{2}(x).} The other terms that have two fields, those with at least one derivative, are known as kinetic terms. They make fields dynamical, with most theories requiring a restriction of at most two derivatives in kinetic terms to preserve probabililties in a quantum theory. They are also usually positive-definite to ensure positive energies. For example, the kinetic term for a relativistic real scalar field is given by L k = 1 2 ∂ μ ϕ ∂ μ ϕ . {\displaystyle {\mathcal {L}}_{k}={\frac {1}{2}}\partial _{\mu }\phi \partial ^{\mu }\phi .} Fields with no kinetic terms can also be found, playing the role of auxiliary fields, background fields, or currents. Theories with only kinetic and mass terms, form free field theories. Any term with more than two fields per term is known as an interaction term. The presence of these gives rise to interacting theories where particles can scatter off each other. The coefficients in front of these terms are known as coupling constants and they dictate the strength of the interaction. For example, a quartic interaction in a real scalar field theory is given by L i = − g 4 ! ϕ 4 , {\displaystyle {\mathcal {L}}_{i}=-{\frac {g}{4!}}\phi ^{4},} where g {\displaystyle g} is its coupling constant. This term gives rise to scattering processes whereby two scalar fields can scatter off each other. Interacting terms can have any number of derivatives, with each derivative providing a momentum dependence to the scattering term as can be seen by going into momentum space. Terms with only one field are known as tadpole terms since they give rise to tadpole Feynman diagrams.: 415 In theories with translational symmetries, such terms can usually be eliminated by redefining some of the fields though a shift. Constant terms, those with no fields, have no physical consequences in non-gravitational theories. In classical field theories, the equations of motion only depend on variations of the Lagrangian, so constant terms play no role. In quantum field theories they only provide an irrelevant overall multiplicative term to the partition function, so again play no role. Physically this is because in these theories there is no absolute energy scale as the potential energy can always be shifted by an arbitrary constant without altering the physics. However, in gravitational systems the constant terms are multiplied by the metric determinant, coupling them to the spacetime. They play the role of the cosmological constant, directly affecting the dynamics of the theory at both a classical and quantum level. Polynomial terms are often expressed with certain canonical normalizations, used to simplify the Feynman rules that are derived from them. Usually one divides by the product of the factorial of the multipicity of the fields. For example, in a theory with two real scalar fields, a term of the form g ϕ n φ m {\displaystyle g\phi ^{n}\varphi ^{m}} term would be divided by n ! m ! {\displaystyle n!m!} . Particles and antiparticles are distinguished in this counting, so that a complex scalar field term of the form g ′ ϕ ¯ p ϕ p {\displaystyle g'{\bar {\phi }}^{p}\phi ^{p}} is divided by p ! p ! {\displaystyle p!p!} rather than ( 2 p ) ! {\displaystyle (2p)!} . Examples A large variety of physical systems have been formulated in terms of Lagrangians over fields. Below is a sampling of some of the most common ones found in physics textbooks on field theory. Newtonian gravity The Lagrangian density for Newtonian gravity is: L ( x , t ) = − 1 8 π G ( ∇ Φ ( x , t ) ) 2 − ρ ( x , t ) Φ ( x , t ) {\displaystyle {\mathcal {L}}(\mathbf {x} ,t)=-{1 \over 8\pi G}(\nabla \Phi (\mathbf {x} ,t))^{2}-\rho (\mathbf {x} ,t)\Phi (\mathbf {x} ,t)} where Φ is the gravitational potential, ρ is the mass density, and G in m3·kg−1·s−2 is the gravitational constant. The density L {\displaystyle {\mathcal {L}}} has units of J·m−3. Here the interaction term involves a continuous mass density ρ in kg·m−3. This is necessary because using a point source for a field would result in mathematical difficulties. This Lagrangian can be written in the form of L = T − V {\displaystyle {\mathcal {L}}=T-V} , with the T = − ( ∇ Φ ) 2 / 8 π G {\displaystyle T=-(\nabla \Phi )^{2}/8\pi G} providing a kinetic term, and the interaction V = ρ Φ {\displaystyle V=\rho \Phi } the potential term. See also Nordström's theory of gravitation for how this could be modified to deal with changes over time. This form is reprised in the next example of a scalar field theory. The variation of the integral with respect to Φ is: δ L ( x , t ) = − ρ ( x , t ) δ Φ ( x , t ) − 2 8 π G ( ∇ Φ ( x , t ) ) ⋅ ( ∇ δ Φ ( x , t ) ) . {\displaystyle \delta {\mathcal {L}}(\mathbf {x} ,t)=-\rho (\mathbf {x} ,t)\delta \Phi (\mathbf {x} ,t)-{2 \over 8\pi G}(\nabla \Phi (\mathbf {x} ,t))\cdot (\nabla \delta \Phi (\mathbf {x} ,t)).} After integrating by parts, discarding the total integral, and dividing out by δΦ the formula becomes: 0 = − ρ ( x , t ) + 1 4 π G ∇ ⋅ ∇ Φ ( x , t ) {\displaystyle 0=-\rho (\mathbf {x} ,t)+{\frac {1}{4\pi G}}\nabla \cdot \nabla \Phi (\mathbf {x} ,t)} which is equivalent to: 4 π G ρ ( x , t ) = ∇ 2 Φ ( x , t ) {\displaystyle 4\pi G\rho (\mathbf {x} ,t)=\nabla ^{2}\Phi (\mathbf {x} ,t)} which yields Gauss's law for gravity. Scalar field theory The Lagrangian for a scalar field moving in a potential V ( ϕ ) {\displaystyle V(\phi )} can be written as L = 1 2 ∂ μ ϕ ∂ μ ϕ − V ( ϕ ) = 1 2 ∂ μ ϕ ∂ μ ϕ − 1 2 m 2 ϕ 2 − ∑ n = 3 ∞ 1 n ! g n ϕ n {\displaystyle {\mathcal {L}}={\frac {1}{2}}\partial ^{\mu }\phi \partial _{\mu }\phi -V(\phi )={\frac {1}{2}}\partial ^{\mu }\phi \partial _{\mu }\phi -{\frac {1}{2}}m^{2}\phi ^{2}-\sum _{n=3}^{\infty }{\frac {1}{n!}}g_{n}\phi ^{n}} It is not at all an accident that the scalar theory resembles the undergraduate textbook Lagrangian L = T − V {\displaystyle L=T-V} for the kinetic term of a free point particle written as T = m v 2 / 2 {\displaystyle T=mv^{2}/2} . The scalar theory is the field-theory generalization of a particle moving in a potential. When the V ( ϕ ) {\displaystyle V(\phi )} is the Mexican hat potential, the resulting fields are termed the Higgs fields. Sigma model Lagrangian The sigma model describes the motion of a scalar point particle constrained to move on a Riemannian manifold, such as a circle or a sphere. It generalizes the case of scalar and vector fields, that is, fields constrained to move on a flat manifold. The Lagrangian is commonly written in one of three equivalent forms: L = 1 2 d ϕ ∧ ∗ d ϕ {\displaystyle {\mathcal {L}}={\frac {1}{2}}\mathrm {d} \phi \wedge {*\mathrm {d} \phi }} where the d {\displaystyle \mathrm {d} } is the differential. An equivalent expression is L = 1 2 ∑ i = 1 n ∑ j = 1 n g i j ( ϕ ) ∂ μ ϕ i ∂ μ ϕ j {\displaystyle {\mathcal {L}}={\frac {1}{2}}\sum _{i=1}^{n}\sum _{j=1}^{n}g_{ij}(\phi )\;\partial ^{\mu }\phi _{i}\partial _{\mu }\phi _{j}} with g i j {\displaystyle g_{ij}} the Riemannian metric on the manifold of the field; i.e. the fields ϕ i {\displaystyle \phi _{i}} are just local coordinates on the coordinate chart of the manifold. A third common form is L = 1 2 t r ( L μ L μ ) {\displaystyle {\mathcal {L}}={\frac {1}{2}}\mathrm {tr} \left(L_{\mu }L^{\mu }\right)} with L μ = U − 1 ∂ μ U {\displaystyle L_{\mu }=U^{-1}\partial _{\mu }U} and U ∈ S U ( N ) {\displaystyle U\in \mathrm {SU} (N)} , the Lie group SU(N). This group can be replaced by any Lie group, or, more generally, by a symmetric space. The trace is just the Killing form in hiding; the Killing form provides a quadratic form on the field manifold, the lagrangian is then just the pullback of this form. Alternately, the Lagrangian can also be seen as the pullback of the Maurer–Cartan form to the base spacetime. In general, sigma models exhibit topological soliton solutions. The most famous and well-studied of these is the Skyrmion, which serves as a model of the nucleon that has withstood the test of time. Electromagnetism in special relativity Consider a point particle, a charged particle, interacting with the electromagnetic field. The interaction terms − q ϕ ( x ( t ) , t ) + q x ˙ ( t ) ⋅ A ( x ( t ) , t ) {\displaystyle -q\phi (\mathbf {x} (t),t)+q{\dot {\mathbf {x} }}(t)\cdot \mathbf {A} (\mathbf {x} (t),t)} are replaced by terms involving a continuous charge density ρ in A·s·m−3 and current density j {\displaystyle \mathbf {j} } in A·m−2. The resulting Lagrangian density for the electromagnetic field is: L ( x , t ) = − ρ ( x , t ) ϕ ( x , t ) + j ( x , t ) ⋅ A ( x , t ) + ϵ 0 2 E 2 ( x , t ) − 1 2 μ 0 B 2 ( x , t ) . {\displaystyle {\mathcal {L}}(\mathbf {x} ,t)=-\rho (\mathbf {x} ,t)\phi (\mathbf {x} ,t)+\mathbf {j} (\mathbf {x} ,t)\cdot \mathbf {A} (\mathbf {x} ,t)+{\epsilon _{0} \over 2}{E}^{2}(\mathbf {x} ,t)-{1 \over {2\mu _{0}}}{B}^{2}(\mathbf {x} ,t).} Varying this with respect to ϕ, we get 0 = − ρ ( x , t ) + ϵ 0 ∇ ⋅ E ( x , t ) {\displaystyle 0=-\rho (\mathbf {x} ,t)+\epsilon _{0}\nabla \cdot \mathbf {E} (\mathbf {x} ,t)} which yields Gauss' law. Varying instead with respect to A {\displaystyle \mathbf {A} } , we get 0 = j ( x , t ) + ϵ 0 E ˙ ( x , t ) − 1 μ 0 ∇ × B ( x , t ) {\displaystyle 0=\mathbf {j} (\mathbf {x} ,t)+\epsilon _{0}{\dot {\mathbf {E} }}(\mathbf {x} ,t)-{1 \over \mu _{0}}\nabla \times \mathbf {B} (\mathbf {x} ,t)} which yields Ampère's law. Using tensor notation, we can write all this more compactly. The term − ρ ϕ ( x , t ) + j ⋅ A {\displaystyle -\rho \phi (\mathbf {x} ,t)+\mathbf {j} \cdot \mathbf {A} } is actually the inner product of two four-vectors. We package the charge density into the current 4-vector and the potential into the potential 4-vector. These two new vectors are j μ = ( ρ , j ) and A μ = ( − ϕ , A ) {\displaystyle j^{\mu }=(\rho ,\mathbf {j} )\quad {\text{and}}\quad A_{\mu }=(-\phi ,\mathbf {A} )} We can then write the interaction term as − ρ ϕ + j ⋅ A = j μ A μ {\displaystyle -\rho \phi +\mathbf {j} \cdot \mathbf {A} =j^{\mu }A_{\mu }} Additionally, we can package the E and B fields into what is known as the electromagnetic tensor F μ ν {\displaystyle F_{\mu \nu }} . We define this tensor as F μ ν = ∂ μ A ν − ∂ ν A μ {\displaystyle F_{\mu \nu }=\partial _{\mu }A_{\nu }-\partial _{\nu }A_{\mu }} The term we are looking out for turns out to be ϵ 0 2 E 2 − 1 2 μ 0 B 2 = − 1 4 μ 0 F μ ν F μ ν = − 1 4 μ 0 F μ ν F ρ σ η μ ρ η ν σ {\displaystyle {\epsilon _{0} \over 2}{E}^{2}-{1 \over {2\mu _{0}}}{B}^{2}=-{\frac {1}{4\mu _{0}}}F_{\mu \nu }F^{\mu \nu }=-{\frac {1}{4\mu _{0}}}F_{\mu \nu }F_{\rho \sigma }\eta ^{\mu \rho }\eta ^{\nu \sigma }} We have made use of the Minkowski metric to raise the indices on the EMF tensor. In this notation, Maxwell's equations are ∂ μ F μ ν = − μ 0 j ν and ϵ μ ν λ σ ∂ ν F λ σ = 0 {\displaystyle \partial _{\mu }F^{\mu \nu }=-\mu _{0}j^{\nu }\quad {\text{and}}\quad \epsilon ^{\mu \nu \lambda \sigma }\partial _{\nu }F_{\lambda \sigma }=0} where ε is the Levi-Civita tensor. So the Lagrange density for electromagnetism in special relativity written in terms of Lorentz vectors and tensors is L ( x ) = j μ ( x ) A μ ( x ) − 1 4 μ 0 F μ ν ( x ) F μ ν ( x ) {\displaystyle {\mathcal {L}}(x)=j^{\mu }(x)A_{\mu }(x)-{\frac {1}{4\mu _{0}}}F_{\mu \nu }(x)F^{\mu \nu }(x)} In this notation it is apparent that classical electromagnetism is a Lorentz-invariant theory. By the equivalence principle, it becomes simple to extend the notion of electromagnetism to curved spacetime. Electromagnetism and the Yang–Mills equations Using differential forms, the electromagnetic action S in vacuum on a (pseudo-) Riemannian manifold M {\displaystyle {\mathcal {M}}} can be written (using natural units, c = ε0 = 1) as S [ A ] = − ∫ M ( 1 2 F ∧ ∗ F − A ∧ ∗ J ) . {\displaystyle {\mathcal {S}}[\mathbf {A} ]=-\int _{\mathcal {M}}\left({\frac {1}{2}}\,\mathbf {F} \wedge \ast \mathbf {F} -\mathbf {A} \wedge \ast \mathbf {J} \right).} Here, A stands for the electromagnetic potential 1-form, J is the current 1-form, F is the field strength 2-form and the star denotes the Hodge star operator. This is exactly the same Lagrangian as in the section above, except that the treatment here is coordinate-free; expanding the integrand into a basis yields the identical, lengthy expression. Note that with forms, an additional integration measure is not necessary because forms have coordinate differentials built in. Variation of the action leads to d ∗ F = ∗ J . {\displaystyle \mathrm {d} {\ast }\mathbf {F} ={\ast }\mathbf {J} .} These are Maxwell's equations for the electromagnetic potential. Substituting F = dA immediately yields the equation for the fields, d F = 0 {\displaystyle \mathrm {d} \mathbf {F} =0} because F is an exact form. The A field can be understood to be the affine connection on a U(1)-fiber bundle. That is, classical electrodynamics, all of its effects and equations, can be completely understood in terms of a circle bundle over Minkowski spacetime. The Yang–Mills equations can be written in exactly the same form as above, by replacing the Lie group U(1) of electromagnetism by an arbitrary Lie group. In the Standard model, it is conventionally taken to be S U ( 3 ) × S U ( 2 ) × U ( 1 ) {\displaystyle \mathrm {SU} (3)\times \mathrm {SU} (2)\times \mathrm {U} (1)} although the general case is of general interest. In all cases, there is no need for any quantization to be performed. Although the Yang–Mills equations are historically rooted in quantum field theory, the above equations are purely classical. Chern–Simons functional In the same vein as the above, one can consider the action in one dimension less, i.e. in a contact geometry setting. This gives the Chern–Simons functional. It is written as S [ A ] = ∫ M t r ( A ∧ d A + 2 3 A ∧ A ∧ A ) . {\displaystyle {\mathcal {S}}[\mathbf {A} ]=\int _{\mathcal {M}}\mathrm {tr} \left(\mathbf {A} \wedge d\mathbf {A} +{\frac {2}{3}}\mathbf {A} \wedge \mathbf {A} \wedge \mathbf {A} \right).} Chern–Simons theory was deeply explored in physics, as a toy model for a broad range of geometric phenomena that one might expect to find in a grand unified theory. Ginzburg–Landau Lagrangian The Lagrangian density for Ginzburg–Landau theory combines the Lagrangian for the scalar field theory with the Lagrangian for the Yang–Mills action. It may be written as: L ( ψ , A ) = | F | 2 + | D ψ | 2 + 1 4 ( σ − | ψ | 2 ) 2 {\displaystyle {\mathcal {L}}(\psi ,A)=\vert F\vert ^{2}+\vert D\psi \vert ^{2}+{\frac {1}{4}}\left(\sigma -\vert \psi \vert ^{2}\right)^{2}} where ψ {\displaystyle \psi } is a section of a vector bundle with fiber C n {\displaystyle \mathbb {C} ^{n}} . The ψ {\displaystyle \psi } corresponds to the order parameter in a superconductor; equivalently, it corresponds to the Higgs field, after noting that the second term is the famous "Sombrero hat" potential. The field A {\displaystyle A} is the (non-Abelian) gauge field, i.e. the Yang–Mills field and F {\displaystyle F} is its field-strength. The Euler–Lagrange equations for the Ginzburg–Landau functional are the Yang–Mills equations D ⋆ D ψ = 1 2 ( σ − | ψ | 2 ) ψ {\displaystyle D{\star }D\psi ={\frac {1}{2}}\left(\sigma -\vert \psi \vert ^{2}\right)\psi } and D ⋆ F = − Re ⁡ ⟨ D ψ , ψ ⟩ {\displaystyle D{\star }F=-\operatorname {Re} \langle D\psi ,\psi \rangle } where ⋆ {\displaystyle {\star }} is the Hodge star operator, i.e. the fully antisymmetric tensor. These equations are closely related to the Yang–Mills–Higgs equations. Another closely related Lagrangian is found in Seiberg–Witten theory. Dirac Lagrangian The Lagrangian density for a Dirac field is: L = ψ ¯ ( i ℏ c ∂ / − m c 2 ) ψ {\displaystyle {\mathcal {L}}={\bar {\psi }}(i\hbar c{\partial }\!\!\!/\ -mc^{2})\psi } where ψ {\displaystyle \psi } is a Dirac spinor, ψ ¯ = ψ † γ 0 {\displaystyle {\bar {\psi }}=\psi ^{\dagger }\gamma ^{0}} is its Dirac adjoint, and ∂ / {\displaystyle {\partial }\!\!\!/} is Feynman slash notation for γ σ ∂ σ {\displaystyle \gamma ^{\sigma }\partial _{\sigma }} . There is no particular need to focus on Dirac spinors in the classical theory. The Weyl spinors provide a more general foundation; they can be constructed directly from the Clifford algebra of spacetime; the construction works in any number of dimensions, and the Dirac spinors appear as a special case. Weyl spinors have the additional advantage that they can be used in a vielbein for the metric on a Riemannian manifold; this enables the concept of a spin structure, which, roughly speaking, is a way of formulating spinors consistently in a curved spacetime. Quantum electrodynamic Lagrangian The Lagrangian density for QED combines the Lagrangian for the Dirac field together with the Lagrangian for electrodynamics in a gauge-invariant way. It is: L Q E D = ψ ¯ ( i ℏ c D / − m c 2 ) ψ − 1 4 μ 0 F μ ν F μ ν {\displaystyle {\mathcal {L}}_{\mathrm {QED} }={\bar {\psi }}(i\hbar c{D}\!\!\!\!/\ -mc^{2})\psi -{1 \over 4\mu _{0}}F_{\mu \nu }F^{\mu \nu }} where F μ ν {\displaystyle F^{\mu \nu }} is the electromagnetic tensor, D is the gauge covariant derivative, and D / {\displaystyle {D}\!\!\!\!/} is Feynman notation for γ σ D σ {\displaystyle \gamma ^{\sigma }D_{\sigma }} with D σ = ∂ σ − i e A σ {\displaystyle D_{\sigma }=\partial _{\sigma }-ieA_{\sigma }} where A σ {\displaystyle A_{\sigma }} is the electromagnetic four-potential. Although the word "quantum" appears in the above, this is a historical artifact. The definition of the Dirac field requires no quantization whatsoever, it can be written as a purely classical field of anti-commuting Weyl spinors constructed from first principles from a Clifford algebra. The full gauge-invariant classical formulation is given in Bleecker. Quantum chromodynamic Lagrangian The Lagrangian density for quantum chromodynamics combines the Lagrangian for one or more massive Dirac spinors with the Lagrangian for the Yang–Mills action, which describes the dynamics of a gauge field; the combined Lagrangian is gauge invariant. It may be written as: L Q C D = ∑ n ψ ¯ n ( i ℏ c D / − m n c 2 ) ψ n − 1 4 G α μ ν G α μ ν {\displaystyle {\mathcal {L}}_{\mathrm {QCD} }=\sum _{n}{\bar {\psi }}_{n}\left(i\hbar c{D}\!\!\!\!/\ -m_{n}c^{2}\right)\psi _{n}-{1 \over 4}G^{\alpha }{}_{\mu \nu }G_{\alpha }{}^{\mu \nu }} where D is the QCD gauge covariant derivative, n = 1, 2, ...6 counts the quark types, and G α μ ν {\displaystyle G^{\alpha }{}_{\mu \nu }\!} is the gluon field strength tensor. As for the electrodynamics case above, the appearance of the word "quantum" above only acknowledges its historical development. The Lagrangian and its gauge invariance can be formulated and treated in a purely classical fashion. Einstein gravity The Lagrange density for general relativity in the presence of matter fields is L GR = L EH + L matter = c 4 16 π G ( R − 2 Λ ) + L matter {\displaystyle {\mathcal {L}}_{\text{GR}}={\mathcal {L}}_{\text{EH}}+{\mathcal {L}}_{\text{matter}}={\frac {c^{4}}{16\pi G}}\left(R-2\Lambda \right)+{\mathcal {L}}_{\text{matter}}} where Λ {\displaystyle \Lambda } is the cosmological constant, R {\displaystyle R} is the curvature scalar, which is the Ricci tensor contracted with the metric tensor, and the Ricci tensor is the Riemann tensor contracted with a Kronecker delta. The integral of L EH {\displaystyle {\mathcal {L}}_{\text{EH}}} is known as the Einstein–Hilbert action. The Riemann tensor is the tidal force tensor, and is constructed out of Christoffel symbols and derivatives of Christoffel symbols, which define the metric connection on spacetime. The gravitational field itself was historically ascribed to the metric tensor; the modern view is that the connection is "more fundamental". This is due to the understanding that one can write connections with non-zero torsion. These alter the metric without altering the geometry one bit. As to the actual "direction in which gravity points" (e.g. on the surface of the Earth, it points down), this comes from the Riemann tensor: it is the thing that describes the "gravitational force field" that moving bodies feel and react to. (This last statement must be qualified: there is no "force field" per se; moving bodies follow geodesics on the manifold described by the connection. They move in a "straight line".) The Lagrangian for general relativity can also be written in a form that makes it manifestly similar to the Yang–Mills equations. This is called the Einstein–Yang–Mills action principle. This is done by noting that most of differential geometry works "just fine" on bundles with an affine connection and arbitrary Lie group. Then, plugging in SO(3,1) for that symmetry group, i.e. for the frame fields, one obtains the equations above. Substituting this Lagrangian into the Euler–Lagrange equation and taking the metric tensor g μ ν {\displaystyle g_{\mu \nu }} as the field, we obtain the Einstein field equations R μ ν − 1 2 R g μ ν + g μ ν Λ = 8 π G c 4 T μ ν . {\displaystyle R_{\mu \nu }-{\frac {1}{2}}Rg_{\mu \nu }+g_{\mu \nu }\Lambda ={\frac {8\pi G}{c^{4}}}T_{\mu \nu }\,.} T μ ν {\displaystyle T_{\mu \nu }} is the energy momentum tensor and is defined by T μ ν ≡ − 2 − g δ ( L m a t t e r − g ) δ g μ ν = − 2 δ L m a t t e r δ g μ ν + g μ ν L m a t t e r . {\displaystyle T_{\mu \nu }\equiv {\frac {-2}{\sqrt {-g}}}{\frac {\delta ({\mathcal {L}}_{\mathrm {matter} }{\sqrt {-g}})}{\delta g^{\mu \nu }}}=-2{\frac {\delta {\mathcal {L}}_{\mathrm {matter} }}{\delta g^{\mu \nu }}}+g_{\mu \nu }{\mathcal {L}}_{\mathrm {matter} }\,.} where g {\displaystyle g} is the determinant of the metric tensor when regarded as a matrix. Generally, in general relativity, the integration measure of the action of Lagrange density is − g d 4 x {\textstyle {\sqrt {-g}}\,d^{4}x} . This makes the integral coordinate independent, as the root of the metric determinant is equivalent to the Jacobian determinant. The minus sign is a consequence of the metric signature (the determinant by itself is negative). This is an example of the volume form, previously discussed, becoming manifest in non-flat spacetime. Electromagnetism in general relativity The Lagrange density of electromagnetism in general relativity also contains the Einstein–Hilbert action from above. The pure electromagnetic Lagrangian is precisely a matter Lagrangian L matter {\displaystyle {\mathcal {L}}_{\text{matter}}} . The Lagrangian is L ( x ) = j μ ( x ) A μ ( x ) − 1 4 μ 0 F μ ν ( x ) F ρ σ ( x ) g μ ρ ( x ) g ν σ ( x ) + c 4 16 π G R ( x ) = L Maxwell + L Einstein–Hilbert . {\displaystyle {\begin{aligned}{\mathcal {L}}(x)&=j^{\mu }(x)A_{\mu }(x)-{1 \over 4\mu _{0}}F_{\mu \nu }(x)F_{\rho \sigma }(x)g^{\mu \rho }(x)g^{\nu \sigma }(x)+{\frac {c^{4}}{16\pi G}}R(x)\\&={\mathcal {L}}_{\text{Maxwell}}+{\mathcal {L}}_{\text{Einstein–Hilbert}}.\end{aligned}}} This Lagrangian is obtained by simply replacing the Minkowski metric in the above flat Lagrangian with a more general (possibly curved) metric g μ ν ( x ) {\displaystyle g_{\mu \nu }(x)} . We can generate the Einstein Field Equations in the presence of an EM field using this lagrangian. The energy-momentum tensor is T μ ν ( x ) = 2 − g ( x ) δ δ g μ ν ( x ) S Maxwell = 1 μ 0 ( F λ μ ( x ) F ν λ ( x ) − 1 4 g μ ν ( x ) F ρ σ ( x ) F ρ σ ( x ) ) {\displaystyle T^{\mu \nu }(x)={\frac {2}{\sqrt {-g(x)}}}{\frac {\delta }{\delta g_{\mu \nu }(x)}}{\mathcal {S}}_{\text{Maxwell}}={\frac {1}{\mu _{0}}}\left(F_{{\text{ }}\lambda }^{\mu }(x)F^{\nu \lambda }(x)-{\frac {1}{4}}g^{\mu \nu }(x)F_{\rho \sigma }(x)F^{\rho \sigma }(x)\right)} It can be shown that this energy momentum tensor is traceless, i.e. that T = g μ ν T μ ν = 0 {\displaystyle T=g_{\mu \nu }T^{\mu \nu }=0} If we take the trace of both sides of the Einstein Field Equations, we obtain R = − 8 π G c 4 T {\displaystyle R=-{\frac {8\pi G}{c^{4}}}T} So the tracelessness of the energy momentum tensor implies that the curvature scalar in an electromagnetic field vanishes. The Einstein equations are then R μ ν = 8 π G c 4 1 μ 0 ( F μ λ ( x ) F ν λ ( x ) − 1 4 g μ ν ( x ) F ρ σ ( x ) F ρ σ ( x ) ) {\displaystyle R^{\mu \nu }={\frac {8\pi G}{c^{4}}}{\frac {1}{\mu _{0}}}\left({F^{\mu }}_{\lambda }(x)F^{\nu \lambda }(x)-{\frac {1}{4}}g^{\mu \nu }(x)F_{\rho \sigma }(x)F^{\rho \sigma }(x)\right)} Additionally, Maxwell's equations are D μ F μ ν = − μ 0 j ν {\displaystyle D_{\mu }F^{\mu \nu }=-\mu _{0}j^{\nu }} where D μ {\displaystyle D_{\mu }} is the covariant derivative. For free space, we can set the current tensor equal to zero, j μ = 0 {\displaystyle j^{\mu }=0} . Solving both Einstein and Maxwell's equations around a spherically symmetric mass distribution in free space leads to the Reissner–Nordström charged black hole, with the defining line element (written in natural units and with charge Q): d s 2 = ( 1 − 2 M r + Q 2 r 2 ) d t 2 − ( 1 − 2 M r + Q 2 r 2 ) − 1 d r 2 − r 2 d Ω 2 {\displaystyle \mathrm {d} s^{2}=\left(1-{\frac {2M}{r}}+{\frac {Q^{2}}{r^{2}}}\right)\mathrm {d} t^{2}-\left(1-{\frac {2M}{r}}+{\frac {Q^{2}}{r^{2}}}\right)^{-1}\mathrm {d} r^{2}-r^{2}\mathrm {d} \Omega ^{2}} One possible way of unifying the electromagnetic and gravitational Lagrangians (by using a fifth dimension) is given by Kaluza–Klein theory. Effectively, one constructs an affine bundle, just as for the Yang–Mills equations given earlier, and then considers the action separately on the 4-dimensional and the 1-dimensional parts. Such factorizations, such as the fact that the 7-sphere can be written as a product of the 4-sphere and the 3-sphere, or that the 11-sphere is a product of the 4-sphere and the 7-sphere, accounted for much of the early excitement that a theory of everything had been found. Unfortunately, the 7-sphere proved not large enough to enclose all of the Standard model, dashing these hopes. Additional examples The BF model Lagrangian, short for "Background Field", describes a system with trivial dynamics, when written on a flat spacetime manifold. On a topologically non-trivial spacetime, the system will have non-trivial classical solutions, which may be interpreted as solitons or instantons. A variety of extensions exist, forming the foundations for topological field theories. See also Notes == Citations == Mathematical Foundations of Quantum Mechanics (German: Mathematische Grundlagen der Quantenmechanik) is a quantum mechanics book written by John von Neumann in 1932. It is an important early work in the development of the mathematical formulation of quantum mechanics. The book mainly summarizes results that von Neumann had published in earlier papers. Von Neumman formalized quantum mechanics using the concept of Hilbert spaces and linear operators. He acknowledged the previous work by Paul Dirac on the mathematical formalization of quantum mechanics, but was skeptical of Dirac's use of delta functions. He wrote the book in an attempt to be even more mathematically rigorous than Dirac. It was von Neumann's last book in German, afterwards he started publishing in English. Publication history The book was originally published in German in 1932 by Springer. It was translated into French by Alexandru Proca in 1946, and into Spanish in 1949. An English translation by Robert T. Beyer was published in 1955 by Princeton University Press. A Russian translation, edited by Nikolay Bogolyubov, was published by Nauka in 1964. A new English edition, edited by Nicholas A. Wheeler, was published in 2018 by Princeton University Press. Table of contents According to the 2018 version, the main chapters are: Introductory considerations Abstract Hilbert space The quantum statistics Deductive development of the theory General considerations The measuring process Measurement process In chapter 6, von Neumann develops the theory of quantum measurement. Von Neumann addresses measurement by outlining two kind of processes: Process I: during measurement a quantum state of a system evolves into a mixed state of eigenstates of the measured observable. This process is non-causal (the outcome of a single measurement does not depend only on the initial state) and irreversible. Process II: when the system is unobserved, the state evolves according to Schrödinger equation. This process is causal and reversible. Von Neumann was concerned that having two incompatible processes violated what he called the principle of psycho-physical parallelism, indicating the need that every mental process can be described as a physical process. Von Neumann argues that this issue does not appear in quantum mechanics as it set the border between observed and observer arbitrarily along a sequence of subsystems. The sequence begins with a quantum system whose observable is to be measured. When the system interacts with a measuring device, they become entangled. As a result, the system does not end up in a definite eigenstate of the observable, and the measuring device does not display a specific value. When the observer is added to the picture, the description implies that their body (including the brain) are also entangled with the measuring apparatus and the system. This sequence is known as the von Neumann chain. The problem then becomes understanding how collapse to one of the eigenstates emerges from this chain. Von Neumann demonstrated that, when it comes to the final outcomes, the chain can be interrupted at any and a wave function collapse can be introduced at any point to explain the results. Interpretations Von Neumann measurement scheme is part of the orthodox Copenhagen interpretation which postulates a collapse, however alternative interpretations of quantum mechanics have come out of this idea. Eugene Wigner considered that the von Neumann chain implied that consciousness causes collapse of the wave function. However Wigner rejected this idea after the formalism of quantum decoherence was developed. Hugh Everett III developed the many-worlds interpretation based on von Neumann's processes, by keeping only process II. No hidden variables proof One significant passage is its mathematical argument against the idea of hidden variables. Von Neumann's claim rested on the assumption that any linear combination of Hermitian operators represents an observable and the expectation value of such combined operator follows the combination of the expectation values of the operators themselves. Von Neumann's makes the following assumptions: For an observable R {\displaystyle R} , a function f {\displaystyle f} of that observable is represented by f ( R ) {\displaystyle f(R)} . For the sum of observables R {\displaystyle R} and S {\displaystyle S} is represented by the operation R + S {\displaystyle R+S} , independently of the mutual commutation relations. The correspondence between observables and Hermitian operators is one to one. If the observable R {\displaystyle R} is a non-negative operator, then its expected value ⟨ R ⟩ ≥ 0 {\displaystyle \langle R\rangle \geq 0} . Additivity postulate: For arbitrary observables R {\displaystyle R} and S {\displaystyle S} , and real numbers a {\displaystyle a} and b {\displaystyle b} , we have ⟨ a R + b S ⟩ = a ⟨ R ⟩ + b ⟨ S ⟩ {\displaystyle \langle aR+bS\rangle =a\langle R\rangle +b\langle S\rangle } for all possible ensembles. Von Neumann then shows that one can write ⟨ R ⟩ = ∑ m , n ρ n m R m n = T r ( ρ R ) {\displaystyle \langle R\rangle =\sum _{m,n}\rho _{nm}R_{mn}=\mathrm {Tr} (\rho R)} for some ρ {\displaystyle \rho } , where R m n {\displaystyle R_{mn}} and ρ n m {\displaystyle \rho _{nm}} are the matrix elements in some basis. The proof concludes by noting that ρ {\displaystyle \rho } must be Hermitian and non-negative definite ( ⟨ ρ ⟩ ≥ 0 {\displaystyle \langle \rho \rangle \geq 0} ) by construction. For von Neumann, this meant that the statistical operator representation of states could be deduced from the postulates. Consequently, there are no "dispersion-free" states: it is impossible to prepare a system in such a way that all measurements have predictable results. But if hidden variables existed, then knowing the values of the hidden variables would make the results of all measurements predictable, and hence there can be no hidden variables. Von Neumann's argues that if dispersion-free states were found, assumptions 1 to 3 should be modified. Von Neumann's concludes: if there existed other, as yet undiscovered, physical quantities, in addition to those represented by the operators in quantum mechanics, because the relations assumed by quantum mechanics would have to fail already for the by now known quantities, those that we discussed above. It is therefore not, as is often assumed, a question of a re-interpretation of quantum mechanics, the present system of quantum mechanics would have to be objectively false, in order that another description of the elementary processes than the statistical one be possible. Rejection This proof was rejected as early as 1935 by Grete Hermann who found a flaw in the proof. The additive postulate above holds for quantum states, but it does not need to apply for measurements of dispersion-free states, specifically when considering non-commuting observables. Dispersion-free states only require to recover additivity when averaging over the hidden parameters. For example, for a spin-1/2 system, measurements of ( σ x + σ y ) {\displaystyle (\sigma _{x}+\sigma _{y})} can take values ± 2 {\displaystyle \pm {\sqrt {2}}} for a dispersion-free state, but independent measurements of σ x {\displaystyle \sigma _{x}} and σ y {\displaystyle \sigma _{y}} can only take values of ± 1 {\displaystyle \pm 1} (their sum can be ± 2 {\displaystyle \pm 2} or ⁠ 0 {\displaystyle 0} ⁠). Thus there still the possibility that a hidden variable theory could reproduce quantum mechanics statistically. However, Hermann's critique remained relatively unknown until 1974 when it was rediscovered by Max Jammer. In 1952, David Bohm constructed the Bohmian interpretation of quantum mechanics in terms of statistical argument, suggesting a limit to the validity of von Neumann's proof. The problem was brought back to wider attention by John Stewart Bell in 1966. Bell showed that the consequences of that assumption are at odds with results of incompatible measurements, which are not explicitly taken into von Neumann's considerations. Reception It was considered the most complete book written in quantum mechanics at the time of release. It was praised for its axiomatic approach. A review by Jacob Tamarkin compared von Neumann's book to what the works on Niels Henrik Abel or Augustin-Louis Cauchy did for mathematical analysis in the 19th century, but for quantum mechanics. Freeman Dyson said that he learned quantum mechanics from the book. Dyson remarks that in the 1940s, von Neumann's work was not very well cited in the English world, as the book was not translated into English until 1955, but also because the worlds of mathematics and physics were significantly distant at the time. Works adapted in the book von Neumann, J. (1927). "Mathematische Begründung der Quantenmechanik [Mathematical Foundation of Quantum Mechanics]". Nachrichten von der Gesellschaft der Wissenschaften zu Göttingen, Mathematisch-Physikalische Klasse: 1–57. von Neumann, J. (1927). "Wahrscheinlichkeitstheoretischer Aufbau der Quantenmechanik [Probabilistic Theory of Quantum Mechanics]". Nachrichten von der Gesellschaft der Wissenschaften zu Göttingen, Mathematisch-Physikalische Klasse: 245–272. von Neumann, J. (1927). "Thermodynamik quantenmechanischer Gesamtheiten [Thermodynamics of Quantum Mechanical Quantities]". Nachrichten von der Gesellschaft der Wissenschaften zu Göttingen, Mathematisch-Physikalische Klasse. 102: 273–291. von Neumann, J. (1929). "Allgemeine Eigenwerttheorie Hermitescher Funktionaloperatoren [General Eigenvalue Theory of Hermitian Functional Operators]". Mathematische Annalen: 49–131. doi:10.1007/BF01782338. von Neumann, J. (1931). "Die Eindeutigkeit der Schrödingerschen Operatoren [The uniqueness of Schrödinger operators]". Mathematische Annalen. 104: 570–578. doi:10.1007/bf01457956. S2CID 120528257. See also Dirac–von Neumann axioms The Principles of Quantum Mechanics by Paul Dirac Heisenberg cut Notes References External links Full online text of the 1932 German edition (facsimile) at the University of Göttingen. In theoretical physics and applied mathematics, a field equation is a partial differential equation which determines the dynamics of a physical field, specifically the time evolution and spatial distribution of the field. The solutions to the equation are mathematical functions which correspond directly to the field, as functions of time and space. Since the field equation is a partial differential equation, there are families of solutions which represent a variety of physical possibilities. Usually, there is not just a single equation, but a set of coupled equations which must be solved simultaneously. Field equations are not ordinary differential equations since a field depends on space and time, which requires at least two variables. Whereas the "wave equation", the "diffusion equation", and the "continuity equation" all have standard forms (and various special cases or generalizations), there is no single, special equation referred to as "the field equation". The topic broadly splits into equations of classical field theory and quantum field theory. Classical field equations describe many physical properties like temperature of a substance, velocity of a fluid, stresses in an elastic material, electric and magnetic fields from a current, etc. They also describe the fundamental forces of nature, like electromagnetism and gravity. In quantum field theory, particles or systems of "particles" like electrons and photons are associated with fields, allowing for infinite degrees of freedom (unlike finite degrees of freedom in particle mechanics) and variable particle numbers which can be created or annihilated. Generalities Origin Usually, field equations are postulated (like the Einstein field equations and the Schrödinger equation, which underlies all quantum field equations) or obtained from the results of experiments (like Maxwell's equations). The extent of their validity is their ability to correctly predict and agree with experimental results. From a theoretical viewpoint, field equations can be formulated in the frameworks of Lagrangian field theory, Hamiltonian field theory, and field theoretic formulations of the principle of stationary action. Given a suitable Lagrangian or Hamiltonian density, a function of the fields in a given system, as well as their derivatives, the principle of stationary action will obtain the field equation. Symmetry In both classical and quantum theories, field equations will satisfy the symmetry of the background physical theory. Most of the time Galilean symmetry is enough, for speeds (of propagating fields) much less than light. When particles and fields propagate at speeds close to light, Lorentz symmetry is one of the most common settings because the equation and its solutions are then consistent with special relativity. Another symmetry arises from gauge freedom, which is intrinsic to the field equations. Fields which correspond to interactions may be gauge fields, which means they can be derived from a potential, and certain values of potentials correspond to the same value of the field. Classification Field equations can be classified in many ways: classical or quantum, nonrelativistic or relativistic, according to the spin or mass of the field, and the number of components the field has and how they change under coordinate transformations (e.g. scalar fields, vector fields, tensor fields, spinor fields, twistor fields etc.). They can also inherit the classification of differential equations, as linear or nonlinear, the order of the highest derivative, or even as fractional differential equations. Gauge fields may be classified as in group theory, as abelian or nonabelian. Waves Field equations underlie wave equations, because periodically changing fields generate waves. Wave equations can be thought of as field equations, in the sense they can often be derived from field equations. Alternatively, given suitable Lagrangian or Hamiltonian densities and using the principle of stationary action, the wave equations can be obtained also. For example, Maxwell's equations can be used to derive inhomogeneous electromagnetic wave equations, and from the Einstein field equations one can derive equations for gravitational waves. Supplementary equations to field equations Not every partial differential equation (PDE) in physics is automatically called a "field equation", even if fields are involved. They are extra equations to provide additional constraints for a given physical system. "Continuity equations" and "diffusion equations" describe transport phenomena, even though they may involve fields which influence the transport processes. If a "constitutive equation" takes the form of a PDE and involves fields, it is not usually called a field equation because it does not govern the dynamical behaviour of the fields. They relate one field to another, in a given material. Constitutive equations are used along with field equations when the effects of matter need to be taken into account. Classical field equation Classical field equations arise in continuum mechanics (including elastodynamics and fluid mechanics), heat transfer, electromagnetism, and gravitation. Fundamental classical field equations include Newton's Law of Universal Gravitation for nonrelativistic gravitation. Einstein field equations for relativistic gravitation Maxwell's equations for electromagnetism. Important equations derived from fundamental laws include: Navier–Stokes equations for fluid flow. As part of real-life mathematical modelling processes, classical field equations are accompanied by other equations of motion, equations of state, constitutive equations, and continuity equations. Quantum field equation In quantum field theory, particles are described by quantum fields which satisfy the Schrödinger equation. They are also creation and annihilation operators which satisfy commutation relations and are subject to the spin–statistics theorem. Particular cases of relativistic quantum field equations include the Klein–Gordon equation for spin-0 particles the Dirac equation for spin-1/2 particles the Bargmann–Wigner equations for particles of any spin In quantum field equations, it is common to use momentum components of the particle instead of position coordinates of the particle's location, the fields are in momentum space and Fourier transforms relate them to the position representation. See also Field strength Wave function Fundamental interaction Field coupling Field decoupling Coupling parameter Vacuum solution References General G. Woan (2010). The Cambridge Handbook of Physics Formulas. Cambridge University Press. ISBN 978-0-521-57507-2. Classical field theory Misner, Charles W.; Thorne, Kip. S.; Wheeler, John A. (1973), Gravitation, W. H. Freeman, ISBN 0-7167-0344-0 Chadwick, P. (1976), Continuum mechanics: Concise theory and problems, Dover (originally George Allen & Unwin Ltd.), Bibcode:1976nyhp.book.....C, ISBN 0-486-40180-4 Quantum field theory Weinberg, S. (1995). The Quantum Theory of Fields. Vol. 1. Cambridge University Press. ISBN 0-521-55001-7. V.B. Berestetskii, E.M. Lifshitz, L.P. Pitaevskii (1982). Quantum Electrodynamics. Course of Theoretical Physics. Vol. 4 (2nd ed.). Butterworth-Heinemann. ISBN 978-0-7506-3371-0.{{cite book}}: CS1 maint: multiple names: authors list (link) Greiner, W.; Reinhardt, J. (1996), Field Quantization, Springer, ISBN 3-540-59179-6 Aitchison, I.J.R.; Hey, A.J.G. (2003). Gauge Theories in Particle Physics: From Relativistic Quantum Mechanics to QED. Vol. 1 (3rd ed.). IoP. ISBN 0-7503-0864-8. Aitchison, I.J.R.; Hey, A.J.G. (2004). Gauge Theories in Particle Physics: Non-Abelian Gauge Theories: QCD and electroweak theory. Vol. 2 (3rd ed.). IoP. ISBN 0-7503-0950-4. Classical and quantum field theory Sexl, R. U.; Urbantke, H. K. (2001) [1992]. Relativity, Groups Particles. Special Relativity and Relativistic Symmetry in Field and Particle Physics. Springer. ISBN 978-3211834435. External links J.C.A. Wevers (1999). "Physics formulary" (PDF). Archived from the original (PDF) on 27 December 2016. Retrieved 27 December 2016. Glenn Elert (1998). "Frequently Used Equations". Retrieved 27 December 2016. Quantum chemistry, also called molecular quantum mechanics, is a branch of physical chemistry focused on the application of quantum mechanics to chemical systems, particularly towards the quantum-mechanical calculation of electronic contributions to physical and chemical properties of molecules, materials, and solutions at the atomic level. These calculations include systematically applied approximations intended to make calculations computationally feasible while still capturing as much information about important contributions to the computed wave functions as well as to observable properties such as structures, spectra, and thermodynamic properties. Quantum chemistry is also concerned with the computation of quantum effects on molecular dynamics and chemical kinetics. Chemists rely heavily on spectroscopy through which information regarding the quantization of energy on a molecular scale can be obtained. Common methods are infra-red (IR) spectroscopy, nuclear magnetic resonance (NMR) spectroscopy, and scanning probe microscopy. Quantum chemistry may be applied to the prediction and verification of spectroscopic data as well as other experimental data. Many quantum chemistry studies are focused on the electronic ground state and excited states of individual atoms and molecules as well as the study of reaction pathways and transition states that occur during chemical reactions. Spectroscopic properties may also be predicted. Typically, such studies assume the electronic wave function is adiabatically parameterized by the nuclear positions (i.e., the Born–Oppenheimer approximation). A wide variety of approaches are used, including semi-empirical methods, density functional theory, Hartree–Fock calculations, quantum Monte Carlo methods, and coupled cluster methods. Understanding electronic structure and molecular dynamics through the development of computational solutions to the Schrödinger equation is a central goal of quantum chemistry. Progress in the field depends on overcoming several challenges, including the need to increase the accuracy of the results for small molecular systems, and to also increase the size of large molecules that can be realistically subjected to computation, which is limited by scaling considerations — the computation time increases as a power of the number of atoms. History Some view the birth of quantum chemistry as starting with the discovery of the Schrödinger equation and its application to the hydrogen atom. However, a 1927 article of Walter Heitler (1904–1981) and Fritz London is often recognized as the first milestone in the history of quantum chemistry. This was the first application of quantum mechanics to the diatomic hydrogen molecule, and thus to the phenomenon of the chemical bond. However, prior to this a critical conceptual framework was provided by Gilbert N. Lewis in his 1916 paper The Atom and the Molecule, wherein Lewis developed the first working model of valence electrons. Important contributions were also made by Yoshikatsu Sugiura and S.C. Wang. A series of articles by Linus Pauling, written throughout the 1930s, integrated the work of Heitler, London, Sugiura, Wang, Lewis, and John C. Slater on the concept of valence and its quantum-mechanical basis into a new theoretical framework. Many chemists were introduced to the field of quantum chemistry by Pauling's 1939 text The Nature of the Chemical Bond and the Structure of Molecules and Crystals: An Introduction to Modern Structural Chemistry, wherein he summarized this work (referred to widely now as valence bond theory) and explained quantum mechanics in a way which could be followed by chemists. The text soon became a standard text at many universities. In 1937, Hans Hellmann appears to have been the first to publish a book on quantum chemistry, in the Russian and German languages. In the years to follow, this theoretical basis slowly began to be applied to chemical structure, reactivity, and bonding. In addition to the investigators mentioned above, important progress and critical contributions were made in the early years of this field by Irving Langmuir, Robert S. Mulliken, Max Born, J. Robert Oppenheimer, Hans Hellmann, Maria Goeppert Mayer, Erich Hückel, Douglas Hartree, John Lennard-Jones, and Vladimir Fock. Electronic structure The electronic structure of an atom or molecule is the quantum state of its electrons. The first step in solving a quantum chemical problem is usually solving the Schrödinger equation (or Dirac equation in relativistic quantum chemistry) with the electronic molecular Hamiltonian, usually making use of the Born–Oppenheimer (B–O) approximation. This is called determining the electronic structure of the molecule. An exact solution for the non-relativistic Schrödinger equation can only be obtained for the hydrogen atom (though exact solutions for the bound state energies of the hydrogen molecular ion within the B-O approximation have been identified in terms of the generalized Lambert W function). Since all other atomic and molecular systems involve the motions of three or more "particles", their Schrödinger equations cannot be solved analytically and so approximate and/or computational solutions must be sought. The process of seeking computational solutions to these problems is part of the field known as computational chemistry. Valence bond theory As mentioned above, Heitler and London's method was extended by Slater and Pauling to become the valence-bond (VB) method. In this method, attention is primarily devoted to the pairwise interactions between atoms, and this method therefore correlates closely with classical chemists' drawings of bonds. It focuses on how the atomic orbitals of an atom combine to give individual chemical bonds when a molecule is formed, incorporating the two key concepts of orbital hybridization and resonance. Molecular orbital theory An alternative approach to valence bond theory was developed in 1929 by Friedrich Hund and Robert S. Mulliken, in which electrons are described by mathematical functions delocalized over an entire molecule. The Hund–Mulliken approach or molecular orbital (MO) method is less intuitive to chemists, but has turned out capable of predicting spectroscopic properties better than the VB method. This approach is the conceptual basis of the Hartree–Fock method and further post-Hartree–Fock methods. Density functional theory The Thomas–Fermi model was developed independently by Thomas and Fermi in 1927. This was the first attempt to describe many-electron systems on the basis of electronic density instead of wave functions, although it was not very successful in the treatment of entire molecules. The method did provide the basis for what is now known as density functional theory (DFT). Modern day DFT uses the Kohn–Sham method, where the density functional is split into four terms; the Kohn–Sham kinetic energy, an external potential, exchange and correlation energies. A large part of the focus on developing DFT is on improving the exchange and correlation terms. Though this method is less developed than post Hartree–Fock methods, its significantly lower computational requirements (scaling typically no worse than n3 with respect to n basis functions, for the pure functionals) allow it to tackle larger polyatomic molecules and even macromolecules. This computational affordability and often comparable accuracy to MP2 and CCSD(T) (post-Hartree–Fock methods) has made it one of the most popular methods in computational chemistry. Chemical dynamics A further step can consist of solving the Schrödinger equation with the total molecular Hamiltonian in order to study the motion of molecules. Direct solution of the Schrödinger equation is called quantum dynamics, whereas its solution within the semiclassical approximation is called semiclassical dynamics. Purely classical simulations of molecular motion are referred to as molecular dynamics (MD). Another approach to dynamics is a hybrid framework known as mixed quantum-classical dynamics; yet another hybrid framework uses the Feynman path integral formulation to add quantum corrections to molecular dynamics, which is called path integral molecular dynamics. Statistical approaches, using for example classical and quantum Monte Carlo methods, are also possible and are particularly useful for describing equilibrium distributions of states. Adiabatic chemical dynamics In adiabatic dynamics, interatomic interactions are represented by single scalar potentials called potential energy surfaces. This is the Born–Oppenheimer approximation introduced by Born and Oppenheimer in 1927. Pioneering applications of this in chemistry were performed by Rice and Ramsperger in 1927 and Kassel in 1928, and generalized into the RRKM theory in 1952 by Marcus who took the transition state theory developed by Eyring in 1935 into account. These methods enable simple estimates of unimolecular reaction rates from a few characteristics of the potential surface. Non-adiabatic chemical dynamics Non-adiabatic dynamics consists of taking the interaction between several coupled potential energy surfaces (corresponding to different electronic quantum states of the molecule). The coupling terms are called vibronic couplings. The pioneering work in this field was done by Stueckelberg, Landau, and Zener in the 1930s, in their work on what is now known as the Landau–Zener transition. Their formula allows the transition probability between two adiabatic potential curves in the neighborhood of an avoided crossing to be calculated. Spin-forbidden reactions are one type of non-adiabatic reactions where at least one change in spin state occurs when progressing from reactant to product. See also References Sources External links Early ideas in the history of quantum chemistry The I. I. Rabi Prize in Atomic, Molecular, and Optical Physics is given by the American Physical Society to recognize outstanding work by mid-career researchers in the field of atomic, molecular, and optical physics. The award was endowed in 1989 in honor of the physicist I. I. Rabi and has been awarded biannually since 1991. The prize citation reads: "To recognize and encourage outstanding research in Atomic, Molecular and Optical Physics by investigators who have held a Ph.D. for no more than 10 years prior to the nomination deadline. The prize consists of $10,000 and a certificate citing the contributions made by the recipient. An allowance will be provided for travel expenses of the recipient to the Society meeting at which the prize is presented. It is awarded in odd-numbered years." Recipients Source: 1991 Chris H. Greene: "For his many contributions to atomic and molecular theory including studies of resonance vibronic processes, multiple electron excitations, photo-absorption in external fields, and threshold effects of long range forces." 1993 Timothy E. Chupp: "For his contributions to the development of high density polarized noble gases by spin exchange with optically pumped alkali atoms and in particular for his leadership and use of polarized 3He as a target for fundamental experiments in nuclear physics." 1995 Randall G. Hulet: "For his contributions to a broad range of important problems in atomic and optical physics including cavity quantum electrodynamics, quantum jumps, ion storage, and laser cooling of atoms. In the latter field, in particular for his demonstration of multiphoton cooling involving Doppleron resonances in neutral Lithium and his collision experiments with cooled Lithium vapor." 1997 Eric Allin Cornell and Wolfgang Ketterle: "For achieving Bose-Einstein condensation of an atomic gas, for creating techniques for studying the Bose condensate, and for measuring the physical properties of the weakly interacting atomic Bose gas." 1999 Mark G. Raizen: "For his pioneering advances in the experimental study of atom optics, and especially for the insightful connections he has developed between this discipline and studies of chaotic dynamics, condensed matter physics, and dissipative quantum systems." 2001 Christopher Monroe: "For his pivotal experiments that implemented quantum logic using trapped atomic ions, and for his fundamental studies of coherence and decoherence in entangled quantum systems." 2003 Mark A. Kasevich: "For developing atom interferometer inertial sensors with unprecedented precision, and for pioneering studies of Bose-Einstein condensates, especially the achievement of non-classical spin states and the demonstration of a mode-locked atom laser." 2005 Deborah Jin: 2007 Jun Ye: "For advances in precision measurement, including techniques for stabilizing and measuring optical frequencies, controlling the phase of femtosecond laser pulses, and measuring molecular transitions." 2009 Mikhail Lukin: "For pioneering theoretical and experimental work at the interface between quantum optics, quantum information processing, and the quantum many body problem." 2011 Cheng Chin: "For pioneering work in strongly interacting Fermi gas and few body physics including the discovery of the Effimov effect." 2013 Markus Greiner: "For seminal contributions to the field of ultracold atoms, including the observation of the superfluid-to-Mott-insulator transition, the study of the BEC-BCS cross over for fermions, and the development of imaging techniques for atoms in optical lattices with single-atom resolution" 2015 Ian Spielman: "For the development of quantum simulations using ultra-cold atoms, creation of synthetic electromagnetic fields, demonstration of synthetic spinorbit coupling, and applications to studying new physical systems." 2017 Martin Zwierlein: "For seminal studies of ultracold Fermi gases, including precision measurements of the equation of state, the observation of superfluidity, solitons, vortices, and polarons, the realization of a microscope for fermions in a lattice; and the production of chemically stable polar molecules." 2019 Kang-Kuen Ni: "For seminal work on ultracold molecules, including original contributions to the understanding of chemical reactions in the quantum regime, deterministic creation of individual molecules with optical tweezers, and development of novel, high-precision techniques to interrogate and control the complete set of internal molecular resources." 2021 Monika Schleier-Smith: "For seminal work in quantum optics and for discoveries at the intersection of AMO, condensed matter, and quantum information, including original contributions to spin squeezing in optical cavities, engineering long-range interactions for quantum simulations, and metrology, and for theoretical development of a measurement protocol related to the scrambling of quantum information." 2023 Adam M. Kaufman: "For seminal developments in optical tweezer arrays and clocks based on alkaline earth atoms, with applications to metrology and quantum information processing." See also List of physics awards Arthur L. Schawlow Prize in Laser Science Norman F. Ramsey Prize in Atomic, Molecular and Optical Physics, and in Precision Tests of Fundamental Laws and Symmetries References External links I.I. Rabi Prize in Atomic, Molecular, and Optical Physics (official site) Mictomagnetism is a spin system in which various exchange interactions are mixed. It is observed in several kinds of alloys, including Cu–Mn, Fe–Al and Ni–Mn alloys. Cooled in zero magnetic field, these materials have low remanence and coercivity. Cooled in a magnetic field, they have much larger remanence, and the hysteresis loop is shifted in the direction opposite to the field (an effect similar to exchange bias). References == Further reading == Within the Schwinger-Dyson equation approach to calculate structure of bound states under quantum field theory dynamics, one applies truncation schemes such that the finite tower of integral equations for Green's functions becomes manageable. For hadrons (mesons and baryons) as relativistic bound states of quarks and gluons interacting via the strong nuclear force, a well-adopted scheme is the rainbow-ladder truncation. Particularly the bound state amplitude (Bethe-Salpeter amplitude) of mesons is determined from the homogeneous Bethe-Salpeter equation. While the amplitude for baryons is solved from the Faddeev equation. Information on the structure of hadrons is contained within these amplitudes. The established quantum field theory of the strong interaction is quantum chromodynamics (QCD). The Maris–Tandy model is a practical case of the rainbow-ladder truncation that yields reasonable description for hadrons with up quarks, down quarks, and strange quarks as their valence quarks. Description of the model Within the Maris–Tandy model of QCD interactions for quarks and gluons, the quark-gluon proper vertex Γ μ ( k , p ) {\displaystyle \Gamma _{\mu }(k,p)} in combination with the dressed gluon propagator G μ ν ( q ) {\displaystyle G^{\mu \nu }(q)} in the Landau gauge is replaced by the bare vertex multiplied by a scalar dressing function: Γ μ ( k , p ) G μ ν ( q ) = γ μ ( q μ ν − q μ q ν / q 2 ) G ( q 2 ) , {\displaystyle \Gamma _{\mu }(k,p)\,G^{\mu \nu }(q)=\gamma _{\mu }\left(q^{\mu \nu }-q^{\mu }q^{\nu }/q^{2}\right)\,{\mathcal {G}}(q^{2}),} where k μ {\displaystyle k^{\mu }} , p μ {\displaystyle p^{\mu }} , and q μ {\displaystyle q^{\mu }} are the momenta of the quarks and the gluon in Euclidean space. The matrix γ μ {\displaystyle \gamma ^{\mu }} is the Dirac matrix. And the scalar function G ( q 2 ) {\displaystyle {\mathcal {G}}(q^{2})} is given by g 2 G ( q 2 ) = 4 π 2 ω 6 d I R q 2 e − q 2 / ω 2 + 4 π 2 γ m ln ⁡ [ e 2 − 1 + ( 1 + q 2 / Λ Q C D ) 2 ] 1 − e − q 2 / ( 4 m t 2 ) q 2 . {\displaystyle g^{2}\,{\mathcal {G}}(q^{2})={\dfrac {4\pi ^{2}}{\omega ^{6}}}d_{\mathrm {IR} }q^{2}e^{-q^{2}/\omega ^{2}}+{\dfrac {4\pi ^{2}\gamma _{m}}{\ln \left[e^{2}-1+\left(1+q^{2}/\Lambda _{\mathrm {QCD} }\right)^{2}\right]}}{\dfrac {1-e^{-q^{2}/(4m_{\mathrm {t} }^{2})}}{q^{2}}}.} The parameters d I R {\displaystyle d_{\mathrm {IR} }} and ω {\displaystyle \omega } specify the strength and the scale of the infrared term, respectively. The elementary color charge is given by g {\displaystyle g} . The second term on the right-hand side is the ultraviolet (UV) term constructed in agreement with perturbative QCD, within which the parameter Λ Q C D {\displaystyle \Lambda _{\mathrm {QCD} }} is the characteristic scale of QCD, Other parameters in the UV term are explained in Ref. Applications The Maris–Tandy model can be applied to solve for the structure of pions, kaons, and a selection of vector mesons from the homogeneous Bethe-Salpeter equation. It can also be used to solve for the quark-photon vertex from the inhomogeneous Bethe-Salpeter equation, for the elastic form factors of pseudoscalar mesons, and for the radiative transitions of mesons. Meanwhile the mass spectrum and structure of nucleons can be solved within this model from the Faddeev equation. == References == The quantum Hall effect (or integer quantum Hall effect) is a quantized version of the Hall effect which is observed in two-dimensional electron systems subjected to low temperatures and strong magnetic fields, in which the Hall resistance Rxy exhibits steps that take on the quantized values R x y = V Hall I channel = h e 2 ν , {\displaystyle R_{xy}={\frac {V_{\text{Hall}}}{I_{\text{channel}}}}={\frac {h}{e^{2}\nu }},} where VHall is the Hall voltage, Ichannel is the channel current, e is the elementary charge and h is the Planck constant. The divisor ν can take on either integer (ν = 1, 2, 3,...) or fractional (ν = ⁠1/3⁠, ⁠2/5⁠, ⁠3/7⁠, ⁠2/3⁠, ⁠3/5⁠, ⁠1/5⁠, ⁠2/9⁠, ⁠3/13⁠, ⁠5/2⁠, ⁠12/5⁠,...) values. Here, ν is roughly but not exactly equal to the filling factor of Landau levels. The quantum Hall effect is referred to as the integer or fractional quantum Hall effect depending on whether ν is an integer or fraction, respectively. The striking feature of the integer quantum Hall effect is the persistence of the quantization (i.e. the Hall plateau) as the electron density is varied. Since the electron density remains constant when the Fermi level is in a clean spectral gap, this situation corresponds to one where the Fermi level is an energy with a finite density of states, though these states are localized (see Anderson localization). The fractional quantum Hall effect is more complicated and still considered an open research problem. Its existence relies fundamentally on electron–electron interactions. In 1988, it was proposed that there was a quantum Hall effect without Landau levels. This quantum Hall effect is referred to as the quantum anomalous Hall (QAH) effect. There is also a new concept of the quantum spin Hall effect which is an analogue of the quantum Hall effect, where spin currents flow instead of charge currents. Applications Electrical resistance standards The quantization of the Hall conductance ( G x y = 1 / R x y {\displaystyle G_{xy}=1/R_{xy}} ) has the important property of being exceedingly precise. Actual measurements of the Hall conductance have been found to be integer or fractional multiples of ⁠e2/h⁠ to better than one part in a billion. It has allowed for the definition of a new practical standard for electrical resistance, based on the resistance quantum given by the von Klitzing constant RK. This is named after Klaus von Klitzing, the discoverer of exact quantization. The quantum Hall effect also provides an extremely precise independent determination of the fine-structure constant, a quantity of fundamental importance in quantum electrodynamics. In 1990, a fixed conventional value RK-90 = 25812.807 Ω was defined for use in resistance calibrations worldwide. Later, the 2019 revision of the SI fixed exact values of h and e, resulting in an exact RK = ⁠h/e2⁠ = 25812.80745... Ω. Research status The fractional quantum Hall effect is considered part of exact quantization. Exact quantization in full generality is not completely understood but it has been explained as a very subtle manifestation of the combination of the principle of gauge invariance together with another symmetry (see Anomalies). The integer quantum Hall effect instead is considered a solved research problem and understood in the scope of TKNN formula and Chern–Simons Lagrangians. The fractional quantum Hall effect is still considered an open research problem. The fractional quantum Hall effect can be also understood as an integer quantum Hall effect, although not of electrons but of charge–flux composites known as composite fermions. Other models to explain the fractional quantum Hall effect also exists. Currently it is considered an open research problem because no single, confirmed and agreed list of fractional quantum numbers exists, neither a single agreed model to explain all of them, although there are such claims in the scope of composite fermions and Non Abelian Chern–Simons Lagrangians. History In 1957, Carl Frosch and Lincoln Derick were able to manufacture the first silicon dioxide field effect transistors at Bell Labs, the first transistors in which drain and source were adjacent at the surface. Subsequently, a team demonstrated a working MOSFET at Bell Labs 1960. This enabled physicists to study electron behavior in a nearly ideal two-dimensional gas. In a MOSFET, conduction electrons travel in a thin surface layer, and a "gate" voltage controls the number of charge carriers in this layer. This allows researchers to explore quantum effects by operating high-purity MOSFETs at liquid helium temperatures. The integer quantization of the Hall conductance was originally predicted by University of Tokyo researchers Tsuneya Ando, Yukio Matsumoto and Yasutada Uemura in 1975, on the basis of an approximate calculation which they themselves did not believe to be true. In 1978, the Gakushuin University researchers Jun-ichi Wakabayashi and Shinji Kawaji subsequently observed the effect in experiments carried out on the inversion layer of MOSFETs. In 1980, Klaus von Klitzing, working at the high magnetic field laboratory in Grenoble with silicon-based MOSFET samples developed by Michael Pepper and Gerhard Dorda, made the unexpected discovery that the Hall resistance was exactly quantized. For this finding, von Klitzing was awarded the 1985 Nobel Prize in Physics. A link between exact quantization and gauge invariance was subsequently proposed by Robert Laughlin, who connected the quantized conductivity to the quantized charge transport in a Thouless charge pump. Most integer quantum Hall experiments are now performed on gallium arsenide heterostructures, although many other semiconductor materials can be used. In 2007, the integer quantum Hall effect was reported in graphene at temperatures as high as room temperature, and in the magnesium zinc oxide ZnO–MgxZn1−xO. Integer quantum Hall effect Landau levels In two dimensions, when classical electrons are subjected to a magnetic field they follow circular cyclotron orbits. When the system is treated quantum mechanically, these orbits are quantized. To determine the values of the energy levels the Schrödinger equation must be solved. Since the system is subjected to a magnetic field, it has to be introduced as an electromagnetic vector potential in the Schrödinger equation. The system considered is an electron gas that is free to move in the x and y directions, but is tightly confined in the z direction. Then, a magnetic field is applied in the z direction and according to the Landau gauge the electromagnetic vector potential is A = ( 0 , B x , 0 ) {\displaystyle \mathbf {A} =(0,Bx,0)} and the scalar potential is ϕ = 0 {\displaystyle \phi =0} . Thus the Schrödinger equation for a particle of charge q {\displaystyle q} and effective mass m ∗ {\displaystyle m^{*}} in this system is: { 1 2 m ∗ [ p − q A ] 2 + V ( z ) } ψ ( x , y , z ) = ε ψ ( x , y , z ) {\displaystyle \left\{{\frac {1}{2m^{*}}}\left[\mathbf {p} -q\mathbf {A} \right]^{2}+V(z)\right\}\psi (x,y,z)=\varepsilon \psi (x,y,z)} where p {\displaystyle \mathbf {p} } is the canonical momentum, which is replaced by the operator − i ℏ ∇ {\displaystyle -i\hbar \nabla } and ε {\displaystyle \varepsilon } is the total energy. To solve this equation it is possible to separate it into two equations since the magnetic field just affects the movement along x and y axes. The total energy becomes then, the sum of two contributions ε = ε z + ε x y {\displaystyle \varepsilon =\varepsilon _{z}+\varepsilon _{xy}} . The corresponding equations in z axis is: [ − ℏ 2 2 m ∗ ∂ 2 ∂ z 2 + V ( z ) ] u ( z ) = ε z u ( z ) {\displaystyle \left[-{\frac {\hbar ^{2}}{2m^{*}}}{\partial ^{2} \over \partial z^{2}}+V(z)\right]u(z)=\varepsilon _{z}u(z)} To simplify things, the solution V ( z ) {\displaystyle V(z)} is considered as an infinite well. Thus the solutions for the z direction are the energies ε z = n z 2 π 2 ℏ 2 2 m ∗ L 2 {\textstyle \varepsilon _{z}={\frac {n_{z}^{2}\pi ^{2}\hbar ^{2}}{2m^{*}L^{2}}}} , n z = 1 , 2 , 3... {\displaystyle n_{z}=1,2,3...} and the wavefunctions are sinusoidal. For the x {\displaystyle x} and y {\displaystyle y} directions, the solution of the Schrödinger equation can be chosen to be the product of a plane wave in y {\displaystyle y} -direction with some unknown function of x {\displaystyle x} , i.e., ψ x y = u ( x ) e i k y y {\displaystyle \psi _{xy}=u(x)e^{ik_{y}y}} . This is because the vector potential does not depend on y {\displaystyle y} and the momentum operator p ^ y {\displaystyle {\hat {p}}_{y}} therefore commutes with the Hamiltonian. By substituting this Ansatz into the Schrödinger equation one gets the one-dimensional harmonic oscillator equation centered at x k y = ℏ k y e B {\textstyle x_{k_{y}}={\frac {\hbar k_{y}}{eB}}} . [ − ℏ 2 2 m ∗ ∂ 2 ∂ x 2 + 1 2 m ∗ ω c 2 ( x − l B 2 k y ) 2 ] u ( x ) = ε x y u ( x ) {\displaystyle \left[-{\frac {\hbar ^{2}}{2m^{*}}}{\partial ^{2} \over \partial x^{2}}+{\frac {1}{2}}m^{*}\omega _{\rm {c}}^{2}(x-l_{B}^{2}k_{y})^{2}\right]u(x)=\varepsilon _{xy}u(x)} where ω c = e B m ∗ {\textstyle \omega _{\rm {c}}={\frac {eB}{m^{*}}}} is defined as the cyclotron frequency and l B 2 = ℏ e B {\textstyle l_{B}^{2}={\frac {\hbar }{eB}}} the magnetic length. The energies are: ε x y ≡ ε n x = ℏ ω c ( n x + 1 2 ) {\displaystyle \varepsilon _{xy}\equiv \varepsilon _{n_{x}}=\hbar \omega _{\rm {c}}\left(n_{x}+{\frac {1}{2}}\right)} , n x = 1 , 2 , 3... {\displaystyle n_{x}=1,2,3...} And the wavefunctions for the motion in the x y {\displaystyle xy} plane are given by the product of a plane wave in y {\displaystyle y} and Hermite polynomials attenuated by the gaussian function in x {\displaystyle x} , which are the wavefunctions of a harmonic oscillator. From the expression for the Landau levels one notices that the energy depends only on n x {\displaystyle n_{x}} , not on k y {\displaystyle k_{y}} . States with the same n x {\displaystyle n_{x}} but different k y {\displaystyle k_{y}} are degenerate. Density of states At zero field, the density of states per unit surface for the two-dimensional electron gas taking into account degeneration due to spin is independent of the energy n 2 D = m ∗ π ℏ 2 {\displaystyle n_{\rm {2D}}={\frac {m^{*}}{\pi \hbar ^{2}}}} . As the field is turned on, the density of states collapses from the constant to a Dirac comb, a series of Dirac δ {\displaystyle \delta } functions, corresponding to the Landau levels separated Δ ε x y = ℏ ω c {\displaystyle \Delta \varepsilon _{xy}=\hbar \omega _{\rm {c}}} . At finite temperature, however, the Landau levels acquire a width Γ = ℏ τ i {\textstyle \Gamma ={\frac {\hbar }{\tau _{i}}}} being τ i {\displaystyle \tau _{i}} the time between scattering events. Commonly it is assumed that the precise shape of Landau levels is a Gaussian or Lorentzian profile. Another feature is that the wave functions form parallel strips in the y {\displaystyle y} -direction spaced equally along the x {\displaystyle x} -axis, along the lines of A {\displaystyle \mathbf {A} } . Since there is nothing special about any direction in the x y {\displaystyle xy} -plane if the vector potential was differently chosen one should find circular symmetry. Given a sample of dimensions L x × L y {\displaystyle L_{x}\times L_{y}} and applying the periodic boundary conditions in the y {\displaystyle y} -direction k = 2 π L y j {\textstyle k={\frac {2\pi }{L_{y}}}j} being j {\displaystyle j} an integer, one gets that each parabolic potential is placed at a value x k = l B 2 k {\displaystyle x_{k}=l_{B}^{2}k} . The number of states for each Landau Level and k {\displaystyle k} can be calculated from the ratio between the total magnetic flux that passes through the sample and the magnetic flux corresponding to a state. N B = ϕ ϕ 0 = B A B L y Δ x k = A 2 π l B 2 l B = A e B 2 π ℏ ω c = m ∗ ω c A 2 π ℏ {\displaystyle N_{B}={\frac {\phi }{\phi _{0}}}={\frac {BA}{BL_{y}\Delta x_{k}}}={\frac {A}{2\pi l_{B}^{2}}}{\begin{array}{lcr}&l_{B}&\\&=&\\&&\end{array}}{\frac {AeB}{2\pi \hbar }}{\begin{array}{lcr}&\omega _{\rm {c}}&\\&=&\\&&\end{array}}{\frac {m^{*}\omega _{\rm {c}}A}{2\pi \hbar }}} Thus the density of states per unit surface is n B = m ∗ ω c 2 π ℏ {\displaystyle n_{B}={\frac {m^{*}\omega _{\rm {c}}}{2\pi \hbar }}} . Note the dependency of the density of states with the magnetic field. The larger the magnetic field is, the more states are in each Landau level. As a consequence, there is more confinement in the system since fewer energy levels are occupied. Rewriting the last expression as n B = ℏ ω c 2 m ∗ π ℏ 2 {\textstyle n_{B}={\frac {\hbar \omega _{\rm {c}}}{2}}{\frac {m^{*}}{\pi \hbar ^{2}}}} it is clear that each Landau level contains as many states as in a 2DEG in a Δ ε = ℏ ω c {\displaystyle \Delta \varepsilon =\hbar \omega _{\rm {c}}} . Given the fact that electrons are fermions, for each state available in the Landau levels it corresponds to two electrons, one electron with each value for the spin s = ± 1 2 {\textstyle s=\pm {\frac {1}{2}}} . However, if a large magnetic field is applied, the energies split into two levels due to the magnetic moment associated with the alignment of the spin with the magnetic field. The difference in the energies is Δ E = ± 1 2 g μ B B {\textstyle \Delta E=\pm {\frac {1}{2}}g\mu _{\rm {B}}B} being g {\displaystyle g} a factor which depends on the material ( g = 2 {\displaystyle g=2} for free electrons) and μ B {\displaystyle \mu _{\rm {B}}} the Bohr magneton. The sign + {\displaystyle +} is taken when the spin is parallel to the field and − {\displaystyle -} when it is antiparallel. This fact called spin splitting implies that the density of states for each level is reduced by a half. Note that Δ E {\displaystyle \Delta E} is proportional to the magnetic field so, the larger the magnetic field is, the more relevant is the split. In order to get the number of occupied Landau levels, one defines the so-called filling factor ν {\displaystyle \nu } as the ratio between the density of states in a 2DEG and the density of states in the Landau levels. ν = n 2 D n B = h n 2 D e B {\displaystyle \nu ={\frac {n_{\rm {2D}}}{n_{B}}}={\frac {hn_{\rm {2D}}}{eB}}} In general the filling factor ν {\displaystyle \nu } is not an integer. It happens to be an integer when there is an exact number of filled Landau levels. Instead, it becomes a non-integer when the top level is not fully occupied. In actual experiments, one varies the magnetic field and fixes electron density (and not the Fermi energy!) or varies the electron density and fixes the magnetic field. Both cases correspond to a continuous variation of the filling factor ν {\displaystyle \nu } and one cannot expect ν {\displaystyle \nu } to be an integer. Since n B ∝ B {\displaystyle n_{B}\propto B} , by increasing the magnetic field, the Landau levels move up in energy and the number of states in each level grow, so fewer electrons occupy the top level until it becomes empty. If the magnetic field keeps increasing, eventually, all electrons will be in the lowest Landau level ( ν < 1 {\displaystyle \nu <1} ) and this is called the magnetic quantum limit. Longitudinal resistivity It is possible to relate the filling factor to the resistivity and hence, to the conductivity of the system. When ν {\displaystyle \nu } is an integer, the Fermi energy lies in between Landau levels where there are no states available for carriers, so the conductivity becomes zero (it is considered that the magnetic field is big enough so that there is no overlap between Landau levels, otherwise there would be few electrons and the conductivity would be approximately 0 {\displaystyle 0} ). Consequently, the resistivity becomes zero too (At very high magnetic fields it is proven that longitudinal conductivity and resistivity are proportional). With the conductivity σ = ρ − 1 {\displaystyle \sigma =\rho ^{-1}} one finds σ = 1 det ρ ( ρ y y − ρ x y − ρ y x ρ x x ) . {\displaystyle \sigma ={\frac {1}{\det \rho }}{\begin{pmatrix}\rho _{yy}&-\rho _{xy}\\-\rho _{yx}&\rho _{xx}\end{pmatrix}}\;.} If the longitudinal resistivity is zero and transversal is finite, then det ρ ≠ 0 {\displaystyle \det \rho \neq 0} . Thus both the longitudinal conductivity and resistivity become zero. Instead, when ν {\displaystyle \nu } is a half-integer, the Fermi energy is located at the peak of the density distribution of some Landau Level. This means that the resistivity will have a maximum due to increased scattering. This distribution of minimums and maximums corresponds to ¨quantum oscillations¨ called Shubnikov–de Haas oscillations which become more relevant as the magnetic field increases. Obviously, the height of the peaks are larger as the magnetic field increases since the density of states increases with the field, so there are more carriers which contribute to the resistivity. It is interesting to notice that if the magnetic field is very small, the longitudinal resistivity is a constant which means that the classical result is reached. Transverse resistivity From the classical relation of the transverse resistivity ρ x y = B e n 2 D {\textstyle \rho _{xy}={\frac {B}{en_{\rm {2D}}}}} and substituting n 2 D = ν e B h {\textstyle n_{\rm {2D}}=\nu {\frac {eB}{h}}} one finds out the quantization of the transverse resistivity and conductivity: ρ x y = h ν e 2 ⇒ σ = ν e 2 h {\displaystyle \rho _{xy}={\frac {h}{\nu e^{2}}}\Rightarrow \sigma =\nu {\frac {e^{2}}{h}}} One concludes then, that the transverse resistivity is a multiple of the inverse of the so-called conductance quantum e 2 / h {\displaystyle e^{2}/h} if the filling factor is an integer. In experiments, however, plateaus are observed for whole plateaus of filling values ν {\displaystyle \nu } , which indicates that there are in fact electron states between the Landau levels. These states are localized in, for example, impurities of the material where they are trapped in orbits so they can not contribute to the conductivity. That is why the resistivity remains constant in between Landau levels. Again if the magnetic field decreases, one gets the classical result in which the resistivity is proportional to the magnetic field. Photonic quantum Hall effect The quantum Hall effect, in addition to being observed in two-dimensional electron systems, can be observed in photons. Photons do not possess inherent electric charge, but through the manipulation of discrete optical resonators and coupling phases or on-site phases, an artificial magnetic field can be created. This process can be expressed through a metaphor of photons bouncing between multiple mirrors. By shooting the light across multiple mirrors, the photons are routed and gain additional phase proportional to their angular momentum. This creates an effect like they are in a magnetic field. Topological classification The integers that appear in the Hall effect are examples of topological quantum numbers. They are known in mathematics as the first Chern numbers and are closely related to Berry's phase. A striking model of much interest in this context is the Azbel–Harper–Hofstadter model whose quantum phase diagram is the Hofstadter butterfly shown in the figure. The vertical axis is the strength of the magnetic field and the horizontal axis is the chemical potential, which fixes the electron density. The colors represent the integer Hall conductances. Warm colors represent positive integers and cold colors negative integers. Note, however, that the density of states in these regions of quantized Hall conductance is zero; hence, they cannot produce the plateaus observed in the experiments. The phase diagram is fractal and has structure on all scales. In the figure there is an obvious self-similarity. In the presence of disorder, which is the source of the plateaus seen in the experiments, this diagram is very different and the fractal structure is mostly washed away. Also, the experiments control the filling factor and not the Fermi energy. If this diagram is plotted as a function of filling factor, all the features are completely washed away, hence, it has very little to do with the actual Hall physics. Concerning physical mechanisms, impurities and/or particular states (e.g., edge currents) are important for both the 'integer' and 'fractional' effects. In addition, Coulomb interaction is also essential in the fractional quantum Hall effect. The observed strong similarity between integer and fractional quantum Hall effects is explained by the tendency of electrons to form bound states with an even number of magnetic flux quanta, called composite fermions. Bohr atom interpretation of the von Klitzing constant The value of the von Klitzing constant may be obtained already on the level of a single atom within the Bohr model while looking at it as a single-electron Hall effect. While during the cyclotron motion on a circular orbit the centrifugal force is balanced by the Lorentz force responsible for the transverse induced voltage and the Hall effect, one may look at the Coulomb potential difference in the Bohr atom as the induced single atom Hall voltage and the periodic electron motion on a circle as a Hall current. Defining the single atom Hall current as a rate a single electron charge e {\displaystyle e} is making Kepler revolutions with angular frequency ω {\displaystyle \omega } I = ω e 2 π , {\displaystyle I={\frac {\omega e}{2\pi }},} and the induced Hall voltage as a difference between the hydrogen nucleus Coulomb potential at the electron orbital point and at infinity: U = V C ( ∞ ) − V C ( r ) = 0 − V C ( r ) = e 4 π ϵ 0 r {\displaystyle U=V_{\text{C}}(\infty )-V_{\text{C}}(r)=0-V_{\text{C}}(r)={\frac {e}{4\pi \epsilon _{0}r}}} One obtains the quantization of the defined Bohr orbit Hall resistance in steps of the von Klitzing constant as R Bohr ( n ) = U I = n h e 2 {\displaystyle R_{\text{Bohr}}(n)={\frac {U}{I}}=n{\frac {h}{e^{2}}}} which for the Bohr atom is linear but not inverse in the integer n. Relativistic analogs Relativistic examples of the integer quantum Hall effect and quantum spin Hall effect arise in the context of lattice gauge theory. See also References Further reading D. R. Yennie (1987). "Integral quantum Hall effect for nonspecialists". Rev. Mod. Phys. 59 (3): 781–824. Bibcode:1987RvMP...59..781Y. doi:10.1103/RevModPhys.59.781. D. Hsieh; D. Qian; L. Wray; Y. Xia; Y. S. Hor; R. J. Cava; M. Z. Hasan (2008). "A topological Dirac insulator in a quantum spin Hall phase". Nature. 452 (7190): 970–974. arXiv:0902.1356. Bibcode:2008Natur.452..970H. doi:10.1038/nature06843. PMID 18432240. S2CID 4402113. 25 years of Quantum Hall Effect, K. von Klitzing, Poincaré Seminar (Paris-2004). Postscript. Pdf. Magnet Lab Press Release Quantum Hall Effect Observed at Room Temperature Avron, Joseph E.; Osadchy, Daniel; Seiler, Ruedi (2003). "A Topological Look at the Quantum Hall Effect". Physics Today. 56 (8): 38. Bibcode:2003PhT....56h..38A. doi:10.1063/1.1611351. Zyun F. Ezawa: Quantum Hall Effects - Field Theoretical Approach and Related Topics. World Scientific, Singapore 2008, ISBN 978-981-270-032-2 Sankar D. Sarma, Aron Pinczuk: Perspectives in Quantum Hall Effects. Wiley-VCH, Weinheim 2004, ISBN 978-0-471-11216-7 A. Baumgartner; T. Ihn; K. Ensslin; K. Maranowski; A. Gossard (2007). "Quantum Hall effect transition in scanning gate experiments". Phys. Rev. B. 76 (8): 085316. Bibcode:2007PhRvB..76h5316B. doi:10.1103/PhysRevB.76.085316. E. I. Rashba and V. B. Timofeev, Quantum Hall Effect, Sov. Phys. – Semiconductors v. 20, pp. 617–647 (1986). In quantum mechanics, a Fock state or number state is a quantum state that is an element of a Fock space with a well-defined number of particles (or quanta). These states are named after the Soviet physicist Vladimir Fock. Fock states play an important role in the second quantization formulation of quantum mechanics. The particle representation was first treated in detail by Paul Dirac for bosons and by Pascual Jordan and Eugene Wigner for fermions.: 35 The Fock states of bosons and fermions obey useful relations with respect to the Fock space creation and annihilation operators. Definition One specifies a multiparticle state of N non-interacting identical particles by writing the state as a sum of tensor products of N one-particle states. Additionally, depending on the integrality of the particles' spin, the tensor products must be alternating (anti-symmetric) or symmetric products of the underlying one-particle Hilbert spaces. Specifically: Fermions, having half-integer spin and obeying the Pauli exclusion principle, correspond to antisymmetric tensor products. Bosons, possessing integer spin (and not governed by the exclusion principle) correspond to symmetric tensor products. If the number of particles is variable, one constructs the Fock space as the direct sum of the tensor product Hilbert spaces for each particle number. In the Fock space, it is possible to specify the same state in a new notation, the occupancy number notation, by specifying the number of particles in each possible one-particle state. Let { k i } i ∈ I {\textstyle \left\{\mathbf {k} _{i}\right\}_{i\in I}} be an orthonormal basis of states in the underlying one-particle Hilbert space. This induces a corresponding basis of the Fock space called the "occupancy number basis". A quantum state in the Fock space is called a Fock state if it is an element of the occupancy number basis. A Fock state satisfies an important criterion: for each i, the state is an eigenstate of the particle number operator N k i ^ {\displaystyle {\widehat {N_{{\mathbf {k} }_{i}}}}} corresponding to the i-th elementary state ki. The corresponding eigenvalue gives the number of particles in the state. This criterion nearly defines the Fock states (one must in addition select a phase factor). A given Fock state is denoted by | n k 1 , n k 2 , . . n k i . . . ⟩ {\displaystyle |n_{{\mathbf {k} }_{1}},n_{{\mathbf {k} }_{2}},..n_{{\mathbf {k} }_{i}}...\rangle } . In this expression, n k i {\displaystyle n_{{\mathbf {k} }_{i}}} denotes the number of particles in the i-th state ki, and the particle number operator for the i-th state, N k i ^ {\displaystyle {\widehat {N_{{\mathbf {k} }_{i}}}}} , acts on the Fock state in the following way: N k i ^ | n k 1 , n k 2 , . . n k i . . . ⟩ = n k i | n k 1 , n k 2 , . . n k i . . . ⟩ {\displaystyle {\widehat {N_{{\mathbf {k} }_{i}}}}|n_{{\mathbf {k} }_{1}},n_{{\mathbf {k} }_{2}},..n_{{\mathbf {k} }_{i}}...\rangle =n_{{\mathbf {k} }_{i}}|n_{{\mathbf {k} }_{1}},n_{{\mathbf {k} }_{2}},..n_{{\mathbf {k} }_{i}}...\rangle } Hence the Fock state is an eigenstate of the number operator with eigenvalue n k i {\displaystyle n_{{\mathbf {k} }_{i}}} .: 478 Fock states often form the most convenient basis of a Fock space. Elements of a Fock space that are superpositions of states of differing particle number (and thus not eigenstates of the number operator) are not Fock states. For this reason, not all elements of a Fock space are referred to as "Fock states". If we define the aggregate particle number operator N ^ {\textstyle {\widehat {N}}} as N ^ = ∑ i N k i ^ , {\displaystyle {\widehat {N}}=\sum _{i}{\widehat {N_{{\mathbf {k} }_{i}}}},} the definition of Fock state ensures that the variance of measurement Var ⁡ ( N ^ ) = 0 {\displaystyle \operatorname {Var} \left({\widehat {N}}\right)=0} , i.e., measuring the number of particles in a Fock state always returns a definite value with no fluctuation. Example using two particles For any final state | f ⟩ {\displaystyle |f\rangle } , any Fock state of two identical particles given by | 1 k 1 , 1 k 2 ⟩ {\displaystyle |1_{\mathbf {k} _{1}},1_{\mathbf {k} _{2}}\rangle } , and any operator O ^ {\displaystyle {\widehat {\mathbb {O} }}} , we have the following condition for indistinguishability:: 191 | ⟨ f | O ^ | 1 k 1 , 1 k 2 ⟩ | 2 = | ⟨ f | O ^ | 1 k 2 , 1 k 1 ⟩ | 2 {\displaystyle \left|\left\langle f\left|{\widehat {\mathbb {O} }}\right|1_{\mathbf {k} _{1}},1_{\mathbf {k} _{2}}\right\rangle \right|^{2}=\left|\left\langle f\left|{\widehat {\mathbb {O} }}\right|1_{\mathbf {k} _{2}},1_{\mathbf {k} _{1}}\right\rangle \right|^{2}} . So, we must have ⟨ f | O ^ | 1 k 1 , 1 k 2 ⟩ = e i δ ⟨ f | O ^ | 1 k 2 , 1 k 1 ⟩ {\displaystyle \left\langle f\left|{\widehat {\mathbb {O} }}\right|1_{\mathbf {k} _{1}},1_{\mathbf {k} _{2}}\right\rangle =e^{i\delta }\left\langle f\left|{\widehat {\mathbb {O} }}\right|1_{\mathbf {k} _{2}},1_{\mathbf {k} _{1}}\right\rangle } where e i δ = + 1 {\displaystyle e^{i\delta }=+1} for bosons and − 1 {\displaystyle -1} for fermions. Since ⟨ f | {\displaystyle \langle f|} and O ^ {\displaystyle {\widehat {\mathbb {O} }}} are arbitrary, we can say, | 1 k 1 , 1 k 2 ⟩ = + | 1 k 2 , 1 k 1 ⟩ {\displaystyle \left|1_{\mathbf {k} _{1}},1_{\mathbf {k} _{2}}\right\rangle =+\left|1_{\mathbf {k} _{2}},1_{\mathbf {k} _{1}}\right\rangle } for bosons and | 1 k 1 , 1 k 2 ⟩ = − | 1 k 2 , 1 k 1 ⟩ {\displaystyle \left|1_{\mathbf {k} _{1}},1_{\mathbf {k} _{2}}\right\rangle =-\left|1_{\mathbf {k} _{2}},1_{\mathbf {k} _{1}}\right\rangle } for fermions.: 191 Note that the number operator does not distinguish bosons from fermions; indeed, it just counts particles without regard to their symmetry type. To perceive any difference between them, we need other operators, namely the creation and annihilation operators. Bosonic Fock state Bosons, which are particles with integer spin, follow a simple rule: their composite eigenstate is symmetric under operation by an exchange operator. For example, in a two particle system in the tensor product representation we have P ^ | x 1 , x 2 ⟩ = | x 2 , x 1 ⟩ {\displaystyle {\hat {P}}\left|x_{1},x_{2}\right\rangle =\left|x_{2},x_{1}\right\rangle } . Boson creation and annihilation operators We should be able to express the same symmetric property in this new Fock space representation. For this we introduce non-Hermitian bosonic creation and annihilation operators, denoted by b † {\displaystyle b^{\dagger }} and b {\displaystyle b} respectively. The action of these operators on a Fock state are given by the following two equations: Creation operator b k l † {\textstyle b_{{\mathbf {k} }_{l}}^{\dagger }} : b k l † | n k 1 , n k 2 , n k 3 . . . n k l , . . . ⟩ = n k l + 1 | n k 1 , n k 2 , n k 3 . . . n k l + 1 , . . . ⟩ {\displaystyle b_{{\mathbf {k} }_{l}}^{\dagger }|n_{{\mathbf {k} }_{1}},n_{{\mathbf {k} }_{2}},n_{{\mathbf {k} }_{3}}...n_{{\mathbf {k} }_{l}},...\rangle ={\sqrt {n_{{\mathbf {k} }_{l}}+1}}|n_{{\mathbf {k} }_{1}},n_{{\mathbf {k} }_{2}},n_{{\mathbf {k} }_{3}}...n_{{\mathbf {k} }_{l}}+1,...\rangle } Annihilation operator b k l {\textstyle b_{{\mathbf {k} }_{l}}} : b k l | n k 1 , n k 2 , n k 3 . . . n k l , . . . ⟩ = n k l | n k 1 , n k 2 , n k 3 . . . n k l − 1 , . . . ⟩ {\displaystyle b_{{\mathbf {k} }_{l}}|n_{{\mathbf {k} }_{1}},n_{{\mathbf {k} }_{2}},n_{{\mathbf {k} }_{3}}...n_{{\mathbf {k} }_{l}},...\rangle ={\sqrt {n_{{\mathbf {k} }_{l}}}}|n_{{\mathbf {k} }_{1}},n_{{\mathbf {k} }_{2}},n_{{\mathbf {k} }_{3}}...n_{{\mathbf {k} }_{l}}-1,...\rangle } Non-Hermiticity of creation and annihilation operators The bosonic Fock state creation and annihilation operators are not Hermitian operators. Operator identities The commutation relations of creation and annihilation operators in a bosonic system are [ b i , b j † ] ≡ b i b j † − b j † b i = δ i j , {\displaystyle \left[b_{i}^{\,},b_{j}^{\dagger }\right]\equiv b_{i}^{\,}b_{j}^{\dagger }-b_{j}^{\dagger }b_{i}^{\,}=\delta _{ij},} [ b i † , b j † ] = [ b i , b j ] = 0 , {\displaystyle \left[b_{i}^{\dagger },b_{j}^{\dagger }\right]=\left[b_{i}^{\,},b_{j}^{\,}\right]=0,} where [ , ] {\displaystyle [\ \ ,\ \ ]} is the commutator and δ i j {\displaystyle \delta _{ij}} is the Kronecker delta. N bosonic basis states Action on some specific Fock states Action of number operators The number operators N k l ^ {\textstyle {\widehat {N_{{\mathbf {k} }_{l}}}}} for a bosonic system are given by N k l ^ = b k l † b k l {\displaystyle {\widehat {N_{{\mathbf {k} }_{l}}}}=b_{{\mathbf {k} }_{l}}^{\dagger }b_{{\mathbf {k} }_{l}}} , where N k l ^ | n k 1 , n k 2 , n k 3 . . . n k l . . . ⟩ = n k l | n k 1 , n k 2 , n k 3 . . . n k l . . . ⟩ {\displaystyle {\widehat {N_{{\mathbf {k} }_{l}}}}|n_{{\mathbf {k} }_{1}},n_{{\mathbf {k} }_{2}},n_{{\mathbf {k} }_{3}}...n_{{\mathbf {k} }_{l}}...\rangle =n_{{\mathbf {k} }_{l}}|n_{{\mathbf {k} }_{1}},n_{{\mathbf {k} }_{2}},n_{{\mathbf {k} }_{3}}...n_{{\mathbf {k} }_{l}}...\rangle } Number operators are Hermitian operators. Symmetric behaviour of bosonic Fock states The commutation relations of the creation and annihilation operators ensure that the bosonic Fock states have the appropriate symmetric behaviour under particle exchange. Here, exchange of particles between two states (say, l and m) is done by annihilating a particle in state l and creating one in state m. If we start with a Fock state | ψ ⟩ = | n k 1 , n k 2 , . . . . n k m . . . n k l . . . ⟩ {\displaystyle |\psi \rangle =\left|n_{\mathbf {k} _{1}},n_{\mathbf {k} _{2}},....n_{\mathbf {k} _{m}}...n_{\mathbf {k} _{l}}...\right\rangle } , and want to shift a particle from state k l {\displaystyle k_{l}} to state k m {\displaystyle k_{m}} , then we operate the Fock state by b k m † b k l {\displaystyle b_{\mathbf {k} _{m}}^{\dagger }b_{\mathbf {k} _{l}}} in the following way: Using the commutation relation we have, b k m † . b k l = b k l . b k m † {\displaystyle b_{\mathbf {k} _{m}}^{\dagger }.b_{\mathbf {k} _{l}}=b_{\mathbf {k} _{l}}.b_{\mathbf {k} _{m}}^{\dagger }} b k m † . b k l | n k 1 , n k 2 , . . . . n k m . . . n k l . . . ⟩ = b k l . b k m † | n k 1 , n k 2 , . . . . n k m . . . n k l . . . ⟩ = n k m + 1 n k l | n k 1 , n k 2 , . . . . n k m + 1... n k l − 1... ⟩ {\displaystyle {\begin{aligned}b_{\mathbf {k} _{m}}^{\dagger }.b_{\mathbf {k} _{l}}\left|n_{\mathbf {k} _{1}},n_{\mathbf {k} _{2}},....n_{\mathbf {k} _{m}}...n_{\mathbf {k} _{l}}...\right\rangle &=b_{\mathbf {k} _{l}}.b_{\mathbf {k} _{m}}^{\dagger }\left|n_{\mathbf {k} _{1}},n_{\mathbf {k} _{2}},....n_{\mathbf {k} _{m}}...n_{\mathbf {k} _{l}}...\right\rangle \\&={\sqrt {n_{\mathbf {k} _{m}}+1}}{\sqrt {n_{\mathbf {k} _{l}}}}\left|n_{\mathbf {k} _{1}},n_{\mathbf {k} _{2}},....n_{\mathbf {k} _{m}}+1...n_{\mathbf {k} _{l}}-1...\right\rangle \end{aligned}}} So, the Bosonic Fock state behaves to be symmetric under operation by Exchange operator. Fermionic Fock state Fermion creation and annihilation operators To be able to retain the antisymmetric behaviour of fermions, for Fermionic Fock states we introduce non-Hermitian fermion creation and annihilation operators, defined for a Fermionic Fock state | ψ ⟩ = | n k 1 , n k 2 , n k 3 . . . n k l , . . . ⟩ {\displaystyle |\psi \rangle =|n_{{\mathbf {k} }_{1}},n_{{\mathbf {k} }_{2}},n_{{\mathbf {k} }_{3}}...n_{{\mathbf {k} }_{l}},...\rangle } as: The creation operator c k l † {\displaystyle c_{{\mathbf {k} }_{l}}^{\dagger }} acts as: c k l † | n k 1 , n k 2 , n k 3 . . . n k l , . . . ⟩ = n k l + 1 | n k 1 , n k 2 , n k 3 . . . n k l + 1 , . . . ⟩ {\displaystyle c_{{\mathbf {k} }_{l}}^{\dagger }|n_{{\mathbf {k} }_{1}},n_{{\mathbf {k} }_{2}},n_{{\mathbf {k} }_{3}}...n_{{\mathbf {k} }_{l}},...\rangle ={\sqrt {n_{{\mathbf {k} }_{l}}+1}}|n_{{\mathbf {k} }_{1}},n_{{\mathbf {k} }_{2}},n_{{\mathbf {k} }_{3}}...n_{{\mathbf {k} }_{l}}+1,...\rangle } The annihilation operator c k l {\textstyle c_{{\mathbf {k} }_{l}}} acts as: c k l | n k 1 , n k 2 , n k 3 . . . n k l , . . . ⟩ = n k l | n k 1 , n k 2 , n k 3 . . . n k l − 1 , . . . ⟩ {\displaystyle c_{{\mathbf {k} }_{l}}|n_{{\mathbf {k} }_{1}},n_{{\mathbf {k} }_{2}},n_{{\mathbf {k} }_{3}}...n_{{\mathbf {k} }_{l}},...\rangle ={\sqrt {n_{{\mathbf {k} }_{l}}}}|n_{{\mathbf {k} }_{1}},n_{{\mathbf {k} }_{2}},n_{{\mathbf {k} }_{3}}...n_{{\mathbf {k} }_{l}}-1,...\rangle } These two actions are done antisymmetrically, which we shall discuss later. Operator identities The anticommutation relations of creation and annihilation operators in a fermionic system are, { c i , c j † } ≡ c i c j † + c j † c i = δ i j , { c i † , c j † } = { c i , c j } = 0 , {\displaystyle {\begin{aligned}\left\{c_{i}^{\,},c_{j}^{\dagger }\right\}\equiv c_{i}^{\,}c_{j}^{\dagger }+c_{j}^{\dagger }c_{i}^{\,}&=\delta _{ij},\\\left\{c_{i}^{\dagger },c_{j}^{\dagger }\right\}=\left\{c_{i}^{\,},c_{j}^{\,}\right\}&=0,\end{aligned}}} where { , } {\displaystyle {\{\ ,\ \}}} is the anticommutator and δ i j {\displaystyle \delta _{ij}} is the Kronecker delta. These anticommutation relations can be used to show antisymmetric behaviour of Fermionic Fock states. Action of number operators Number operators N k l ^ {\textstyle {\widehat {N_{{\mathbf {k} }_{l}}}}} for Fermions are given by N k l ^ = c k l † . c k l {\displaystyle {\widehat {N_{{\mathbf {k} }_{l}}}}=c_{{\mathbf {k} }_{l}}^{\dagger }.c_{{\mathbf {k} }_{l}}} . N k l ^ | n k 1 , n k 2 , n k 3 . . . n k l . . . ⟩ = n k l | n k 1 , n k 2 , n k 3 . . . n k l . . . ⟩ {\displaystyle {\widehat {N_{{\mathbf {k} }_{l}}}}|n_{{\mathbf {k} }_{1}},n_{{\mathbf {k} }_{2}},n_{{\mathbf {k} }_{3}}...n_{{\mathbf {k} }_{l}}...\rangle =n_{{\mathbf {k} }_{l}}|n_{{\mathbf {k} }_{1}},n_{{\mathbf {k} }_{2}},n_{{\mathbf {k} }_{3}}...n_{{\mathbf {k} }_{l}}...\rangle } Maximum occupation number The action of the number operator as well as the creation and annihilation operators might seem same as the bosonic ones, but the real twist comes from the maximum occupation number of each state in the fermionic Fock state. Extending the 2-particle fermionic example above, we first must convince ourselves that a fermionic Fock state | ψ ⟩ = | n k 1 , n k 2 , n k 3 . . . n k l . . . ⟩ {\displaystyle |\psi \rangle =\left|n_{\mathbf {k} _{1}},n_{\mathbf {k} _{2}},n_{\mathbf {k} _{3}}...n_{\mathbf {k} _{l}}...\right\rangle } is obtained by applying a certain sum of permutation operators to the tensor product of eigenkets as follows: | n k 1 , n k 2 , n k 3 . . . n k l . . . ⟩ = S − | i 1 , i 2 , i 3 . . . i l . . . ⟩ = 1 N ! | | i 1 ⟩ 1 ⋯ | i 1 ⟩ N ⋮ ⋱ ⋮ | i N ⟩ 1 ⋯ | i N ⟩ N | {\displaystyle \left|n_{\mathbf {k} _{1}},n_{\mathbf {k} _{2}},n_{\mathbf {k} _{3}}...n_{\mathbf {k} _{l}}...\right\rangle =S_{-}\left|i_{1},i_{2},i_{3}...i_{l}...\right\rangle ={\frac {1}{\sqrt {N!}}}{\begin{vmatrix}\left|i_{1}\right\rangle _{1}&\cdots &\left|i_{1}\right\rangle _{N}\\\vdots &\ddots &\vdots \\\left|i_{N}\right\rangle _{1}&\cdots &\left|i_{N}\right\rangle _{N}\end{vmatrix}}} : 16 This determinant is called the Slater determinant. If any of the single particle states are the same, two rows of the Slater determinant would be the same and hence the determinant would be zero. Hence, two identical fermions must not occupy the same state (a statement of the Pauli exclusion principle). Therefore, the occupation number of any single state is either 0 or 1. The eigenvalue associated to the fermionic Fock state N k l ^ {\displaystyle {\widehat {N_{{\mathbf {k} }_{l}}}}} must be either 0 or 1. N fermionic basis states | n k 1 , n k 2 , n k 3 . . . n k l , . . . ⟩ {\displaystyle \left|n_{\mathbf {k} _{1}},n_{\mathbf {k} _{2}},n_{\mathbf {k} _{3}}...n_{\mathbf {k} _{l}},...\right\rangle } Action on some specific Fock states Antisymmetric behaviour of Fermionic Fock state Antisymmetric behaviour of Fermionic states under Exchange operator is taken care of by the anticommutation relations. Here, exchange of particles between two states is done by annihilating one particle in one state and creating one in other. If we start with a Fock state | ψ ⟩ = | n k 1 , n k 2 , . . . n k m . . . n k l . . . ⟩ {\displaystyle |\psi \rangle =\left|n_{\mathbf {k} _{1}},n_{\mathbf {k} _{2}},...n_{\mathbf {k} _{m}}...n_{\mathbf {k} _{l}}...\right\rangle } and want to shift a particle from state k l {\displaystyle k_{l}} to state k m {\displaystyle k_{m}} , then we operate the Fock state by c k m † . c k l {\displaystyle c_{\mathbf {k} _{m}}^{\dagger }.c_{\mathbf {k} _{l}}} in the following way: Using the anticommutation relation we have c k m † . c k l = − c k l . c k m † {\displaystyle c_{\mathbf {k} _{m}}^{\dagger }.c_{\mathbf {k} _{l}}=-c_{\mathbf {k} _{l}}.c_{\mathbf {k} _{m}}^{\dagger }} c k m † . c k l | n k 1 , n k 2 , . . . . n k m . . . n k l . . . ⟩ = n k m + 1 n k l | n k 1 , n k 2 , . . . . n k m + 1... n k l − 1... ⟩ {\displaystyle c_{\mathbf {k} _{m}}^{\dagger }.c_{\mathbf {k} _{l}}\left|n_{\mathbf {k} _{1}},n_{\mathbf {k} _{2}},....n_{\mathbf {k} _{m}}...n_{\mathbf {k} _{l}}...\right\rangle ={\sqrt {n_{\mathbf {k} _{m}}+1}}{\sqrt {n_{\mathbf {k} _{l}}}}\left|n_{\mathbf {k} _{1}},n_{\mathbf {k} _{2}},....n_{\mathbf {k} _{m}}+1...n_{\mathbf {k} _{l}}-1...\right\rangle } but, c k l . c k m † | n k 1 , n k 2 , . . . . n k m . . . n k l . . . ⟩ = − c k m † . c k l | n k 1 , n k 2 , . . . . n k m . . . n k l . . . ⟩ = − n k m + 1 n k l | n k 1 , n k 2 , . . . . n k m + 1... n k l − 1... ⟩ {\displaystyle {\begin{aligned}&c_{{\mathbf {k} }_{l}}.c_{{\mathbf {k} }_{m}}^{\dagger }|n_{{\mathbf {k} }_{1}},n_{{\mathbf {k} }_{2}},....n_{{\mathbf {k} }_{m}}...n_{{\mathbf {k} }_{l}}...\rangle \\={}-&c_{{\mathbf {k} }_{m}}^{\dagger }.c_{{\mathbf {k} }_{l}}|n_{{\mathbf {k} }_{1}},n_{{\mathbf {k} }_{2}},....n_{{\mathbf {k} }_{m}}...n_{{\mathbf {k} }_{l}}...\rangle \\={}-&{\sqrt {n_{{\mathbf {k} }_{m}}+1}}{\sqrt {n_{{\mathbf {k} }_{l}}}}|n_{{\mathbf {k} }_{1}},n_{{\mathbf {k} }_{2}},....n_{{\mathbf {k} }_{m}}+1...n_{{\mathbf {k} }_{l}}-1...\rangle \end{aligned}}} Thus, fermionic Fock states are antisymmetric under operation by particle exchange operators. Fock states are not energy eigenstates in general In second quantization theory, the Hamiltonian density function is given by H = 1 2 m ∇ i ψ ∗ ( x ) ∇ i ψ ( x ) {\displaystyle {\mathfrak {H}}={\frac {1}{2m}}\nabla _{i}\psi ^{*}(x)\,\nabla _{i}\psi (x)} : 189 The total Hamiltonian is given by H = ∫ d 3 x H = ∫ d 3 x ψ ∗ ( x ) ( − ∇ 2 2 m ) ψ ( x ) ∴ H = − ∇ 2 2 m {\displaystyle {\begin{aligned}{\mathcal {H}}&=\int d^{3}x\,{\mathfrak {H}}=\int d^{3}x\psi ^{*}(x)\left(-{\frac {\nabla ^{2}}{2m}}\right)\psi (x)\\\therefore {\mathfrak {H}}&=-{\frac {\nabla ^{2}}{2m}}\end{aligned}}} In free Schrödinger theory,: 189 H ψ n ( + ) ( x ) = − ∇ 2 2 m ψ n ( + ) ( x ) = E n 0 ψ n ( + ) ( x ) {\displaystyle {\mathfrak {H}}\psi _{n}^{(+)}(x)=-{\frac {\nabla ^{2}}{2m}}\psi _{n}^{(+)}(x)=E_{n}^{0}\psi _{n}^{(+)}(x)} and ∫ d 3 x ψ n ( + ) ∗ ( x ) ψ n ′ ( + ) ( x ) = δ n n ′ {\displaystyle \int d^{3}x\,\psi _{n}^{(+)^{*}}(x)\,\psi _{n'}^{(+)}(x)=\delta _{nn'}} and ψ ( x ) = ∑ n a n ψ n ( + ) ( x ) {\displaystyle \psi (x)=\sum _{n}a_{n}\psi _{n}^{(+)}(x)} , where a n {\displaystyle a_{n}} is the annihilation operator. ∴ H = ∑ n , n ′ ∫ d 3 x a n ′ † ψ n ′ ( + ) ∗ ( x ) H a n ψ n ( + ) ( x ) {\displaystyle \therefore {\mathcal {H}}=\sum _{n,n'}\int d^{3}x\,a_{n'}^{\dagger }\psi _{n'}^{(+)^{*}}(x)\,{\mathfrak {H}}a_{n}\psi _{n}^{(+)}(x)} Only for non-interacting particles do H {\displaystyle {\mathfrak {H}}} and a n {\displaystyle a_{n}} commute; in general they do not commute. For non-interacting particles, H = ∑ n , n ′ ∫ d 3 x a n ′ † ψ n ′ ( + ) ∗ ( x ) E n 0 ψ n ( + ) ( x ) a n = ∑ n , n ′ E n 0 a n ′ † a n δ n n ′ = ∑ n E n 0 a n † a n = ∑ n E n 0 N ^ {\displaystyle {\mathcal {H}}=\sum _{n,n'}\int d^{3}x\,a_{n'}^{\dagger }\psi _{n'}^{(+)^{*}}(x)\,E_{n}^{0}\psi _{n}^{(+)}(x)a_{n}=\sum _{n,n'}E_{n}^{0}a_{n'}^{\dagger }a_{n}\delta _{nn'}=\sum _{n}E_{n}^{0}a_{n}^{\dagger }a_{n}=\sum _{n}E_{n}^{0}{\widehat {N}}} If they do not commute, the Hamiltonian will not have the above expression. Therefore, in general, Fock states are not energy eigenstates of a system. Vacuum fluctuations The vacuum state or | 0 ⟩ {\displaystyle |0\rangle } is the state of the lowest energy and the expectation values of a {\displaystyle a} and a † {\displaystyle a^{\dagger }} vanish in this state: ⟨ 0 | a | 0 ⟩ = ⟨ 0 | a † | 0 ⟩ = 0 {\displaystyle \langle 0|a|0\rangle =\langle 0|a^{\dagger }|0\rangle =0} The electric and magnetic fields and the vector potential have the mode expansion of the same general form: F ( r → , t ) = ε a e i k → ⋅ r → − ω t + ε a † e − i k → ⋅ r → − ω t {\displaystyle F\left({\vec {r}},t\right)=\varepsilon ae^{i{\vec {k}}\cdot {\vec {r}}-\omega t}+\varepsilon a^{\dagger }e^{-i{\vec {k}}\cdot {\vec {r}}-\omega t}} The expectation values of these field operators vanish in the vacuum state: ⟨ 0 | F | 0 ⟩ = 0 {\displaystyle \langle 0|F|0\rangle =0} However, the expectation values of the square of these field operators are non-zero: there are field fluctuations in the vacuum state. These vacuum fluctuations are responsible for many interesting phenomena including the Lamb shift in quantum optics. Multi-mode Fock states In a multi-mode field each creation and annihilation operator operates on its own mode. So a k l {\displaystyle a_{\mathbf {k} _{l}}} and a k l † {\displaystyle a_{\mathbf {k} _{l}}^{\dagger }} will operate only on | n k l ⟩ {\displaystyle \left|n_{\mathbf {k} _{l}}\right\rangle } . Since operators corresponding to different modes operate in different sub-spaces of the Hilbert space, the entire field is a direct product of | n k l ⟩ {\displaystyle |n_{\mathbf {k} _{l}}\rangle } over all the modes: | n k 1 ⟩ | n k 2 ⟩ | n k 3 ⟩ … ≡ | n k 1 , n k 2 , n k 3 . . . n k l . . . ⟩ ≡ | { n k } ⟩ {\displaystyle \left|n_{\mathbf {k} _{1}}\right\rangle \left|n_{\mathbf {k} _{2}}\right\rangle \left|n_{\mathbf {k} _{3}}\right\rangle \ldots \equiv \left|n_{\mathbf {k} _{1}},n_{\mathbf {k} _{2}},n_{\mathbf {k} _{3}}...n_{\mathbf {k} _{l}}...\right\rangle \equiv \left|\{n_{\mathbf {k} }\}\right\rangle } The creation and annihilation operators operate on the multi-mode state by only raising or lowering the number state of their own mode: a k l | n k 1 , n k 2 , n k 3 . . . n k l , . . . ⟩ = n k l | n k 1 , n k 2 , n k 3 . . . n k l − 1 , . . . ⟩ a k l † | n k 1 , n k 2 , n k 3 . . . n k l , . . . ⟩ = n k l + 1 | n k 1 , n k 2 , n k 3 . . . n k l + 1 , . . . ⟩ {\displaystyle {\begin{aligned}a_{{\mathbf {k} }_{l}}|n_{{\mathbf {k} }_{1}},n_{{\mathbf {k} }_{2}},n_{{\mathbf {k} }_{3}}...n_{{\mathbf {k} }_{l}},...\rangle &={\sqrt {n_{{\mathbf {k} }_{l}}}}|n_{{\mathbf {k} }_{1}},n_{{\mathbf {k} }_{2}},n_{{\mathbf {k} }_{3}}...n_{{\mathbf {k} }_{l}}-1,...\rangle \\a_{{\mathbf {k} }_{l}}^{\dagger }|n_{{\mathbf {k} }_{1}},n_{{\mathbf {k} }_{2}},n_{{\mathbf {k} }_{3}}...n_{{\mathbf {k} }_{l}},...\rangle &={\sqrt {n_{{\mathbf {k} }_{l}}+1}}|n_{{\mathbf {k} }_{1}},n_{{\mathbf {k} }_{2}},n_{{\mathbf {k} }_{3}}...n_{{\mathbf {k} }_{l}}+1,...\rangle \end{aligned}}} We also define the total number operator for the field which is a sum of number operators of each mode: n ^ k = ∑ n ^ k l {\displaystyle {\hat {n}}_{\mathbf {k} }=\sum {\hat {n}}_{\mathbf {k} _{l}}} The multi-mode Fock state is an eigenvector of the total number operator whose eigenvalue is the total occupation number of all the modes n ^ k | { n k } ⟩ = ( ∑ n k l ) | { n k } ⟩ {\displaystyle {\hat {n}}_{\mathbf {k} }|\{n_{\mathbf {k} }\}\rangle =\left(\sum n_{\mathbf {k} _{l}}\right)|\{n_{\mathbf {k} }\}\rangle } In case of non-interacting particles, number operator and Hamiltonian commute with each other and hence multi-mode Fock states become eigenstates of the multi-mode Hamiltonian H ^ | { n k } ⟩ = ( ∑ ℏ ω ( n k l + 1 2 ) ) | { n k } ⟩ {\displaystyle {\hat {H}}\left|\{n_{\mathbf {k} }\}\right\rangle =\left(\sum \hbar \omega \left(n_{\mathbf {k} _{l}}+{\frac {1}{2}}\right)\right)\left|\{n_{\mathbf {k} }\}\right\rangle } Source of single photon state Single photons are routinely generated using single emitters (atoms, ions, molecules, Nitrogen-vacancy center, Quantum dot). However, these sources are not always very efficient, often presenting a low probability of actually getting a single photon on demand; and often complex and unsuitable out of a laboratory environment. Other sources are commonly used that overcome these issues at the expense of a nondeterministic behavior. Heralded single photon sources are probabilistic two-photon sources from whom the pair is split and the detection of one photon heralds the presence of the remaining one. These sources usually rely on the optical non-linearity of some materials like periodically poled Lithium niobate (Spontaneous parametric down-conversion), or silicon (spontaneous Four-wave mixing) for example. Non-classical behaviour The Glauber–Sudarshan P-representation of Fock states shows that these states are purely quantum mechanical and have no classical counterpart. The φ ( α ) {\displaystyle \scriptstyle \varphi (\alpha )\,} of these states in the representation is a 2 n {\displaystyle 2n} 'th derivative of the Dirac delta function and therefore not a classical probability distribution. See also Coherent states Heisenberg limit Nonclassical light References External links Vladan Vuletic of MIT has used an ensemble of atoms to produce a Fock state (a.k.a. single photon) source (PDF) Produce and measure a single photon state (Fock state) with an interactive experiment QuantumLab Uri Sivan (Hebrew: אורי סיון; born 1955) is an Israeli physicist who is the 17th president of the Technion – Israel Institute of Technology. He is also the holder of the Bertoldo Badler Chair in the Technion's Faculty of Physics. Biography Uri Sivan's parents immigrated to Mandatory Palestine from Poland in 1936. They studied at the Technion – Israel Institute of Technology after being banned from European universities because they were Jewish. Sivan served as a pilot in the Israeli Air Force. Sivan has a BSc in Physics and Mathematics, and an MSc and PhD in Physics from Tel Aviv University. Sivan lives in Haifa, Israel. He is married and has three children. Academic career In 1991, after three years at IBM’s T. J. Watson Research Center in New York State, Sivan joined the Faculty of Physics at the Technion – Israel Institute of Technology, and became the holder of the Bertoldo Badler Chair. Sivan set up and led the Russell Berrie Nanotechnology Research Institute at Technion from 2005 to 2010, and in 2017 he set up the National Advisory Committee for Quantum Science and Technology of the Council for Higher Education's Planning and Budgeting Committee. Israel's second astronaut carried the nano-bible, a 0.5 square-millimeter silicon nanochip with 1.2 million letters, created by Uri Sivan into space in 2022. In September 2019, Sivan became the 17th President of the Technion – Israel Institute of Technology, replacing Peretz Lavie. Awards and recognition Sivan was awarded the Israel Academy of Sciences Bergmann Prize, the Mifal Hapais Landau Prize for the Sciences and Research, the Rothschild Foundation Bruno Prize, the Technion's Hershel Rich Innovation Award, and the Taub Award for Excellence in Research. == References == In theoretical physics, the Coleman–Mandula theorem is a no-go theorem stating that spacetime and internal symmetries can only combine in a trivial way. This means that the charges associated with internal symmetries must always transform as Lorentz scalars. Some notable exceptions to the no-go theorem are conformal symmetry and supersymmetry. It is named after Sidney Coleman and Jeffrey Mandula who proved it in 1967 as the culmination of a series of increasingly generalized no-go theorems investigating how internal symmetries can be combined with spacetime symmetries. The supersymmetric generalization is known as the Haag–Łopuszański–Sohnius theorem. History In the early 1960s, the global SU ( 3 ) {\displaystyle {\text{SU}}(3)} flavor symmetry associated with the eightfold way was shown to successfully describe the hadron spectrum for hadrons of the same spin. This led to efforts to expand the global SU ( 3 ) {\displaystyle {\text{SU}}(3)} symmetry to a larger SU ( 6 ) {\displaystyle {\text{SU}}(6)} symmetry mixing both flavour and spin, an idea similar to that previously considered in nuclear physics by Eugene Wigner in 1937 for an SU ( 4 ) {\displaystyle {\text{SU}}(4)} symmetry. This non-relativistic SU ( 6 ) {\displaystyle {\text{SU}}(6)} model united vector and pseudoscalar mesons of different spin into a 35-dimensional multiplet and it also united the two baryon decuplets into a 56-dimensional multiplet. While this was reasonably successful in describing various aspects of the hadron spectrum, from the perspective of quantum chromodynamics this success is merely a consequence of the flavour and spin independence of the force between quarks. There were many attempts to generalize this non-relativistic SU ( 6 ) {\displaystyle {\text{SU}}(6)} model into a fully relativistic one, but these all failed. At the time it was also an open question whether there existed a symmetry for which particles of different masses could belong to the same multiplet. Such a symmetry could then account for the mass splitting found in mesons and baryons. It was only later understood that this is instead a consequence of the differing up-, down-, and strange-quark masses which leads to a breakdown of the SU ( 3 ) {\displaystyle {\text{SU}}(3)} internal flavor symmetry. These two motivations led to a series of no-go theorems to show that spacetime symmetries and internal symmetries could not be combined in any but a trivial way. The first notable theorem was proved by William McGlinn in 1964, with a subsequent generalization by Lochlainn O'Raifeartaigh in 1965. These efforts culminated with the most general theorem by Sidney Coleman and Jeffrey Mandula in 1967. Little notice was given to this theorem in subsequent years. As a result, the theorem played no role in the early development of supersymmetry, which instead emerged in the early 1970s from the study of dual resonance models, which are the precursor to string theory, rather than from any attempts to overcome the no-go theorem. Similarly, the Haag–Łopuszański–Sohnius theorem, a supersymmetric generalization of the Coleman–Mandula theorem, was proved in 1975 after the study of supersymmetry was already underway. Theorem Consider a theory that can be described by an S-matrix and that satisfies the following conditions The symmetry group is a Lie group which includes the Poincaré group as a subgroup, Below any mass, there are only a finite number of particle types, Any two-particle state undergoes some reaction at almost all energies, The amplitudes for elastic two-body scattering are analytic functions of the scattering angle at almost all energies and angles, A technical assumption that the group generators are distributions in momentum space. The Coleman–Mandula theorem states that the symmetry group of this theory is necessarily a direct product of the Poincaré group and an internal symmetry group. The last technical assumption is unnecessary if the theory is described by a quantum field theory and is only needed to apply the theorem in a wider context. A kinematic argument for why the theorem should hold was provided by Edward Witten. The argument is that Poincaré symmetry acts as a very strong constraint on elastic scattering, leaving only the scattering angle unknown. Any additional spacetime dependent symmetry would overdetermine the amplitudes, making them nonzero only at discrete scattering angles. Since this conflicts with the assumption of the analyticity of the scattering angles, such additional spacetime dependent symmetries are ruled out. Limitations Conformal symmetry The theorem does not apply to a theory of massless particles, with these allowing for conformal symmetry as an additional spacetime dependent symmetry. In particular, the algebra of this group is the conformal algebra, which consists of the Poincaré algebra together with the commutation relations for the dilaton generator and the special conformal transformations generator. Supersymmetry The Coleman–Mandula theorem assumes that the only symmetry algebras are Lie algebras, but the theorem can be generalized by instead considering Lie superalgebras. Doing this allows for additional anticommutating generators known as supercharges which transform as spinors under Lorentz transformations. This extension gives rise to the super-Poincaré algebra, with the associated symmetry known as supersymmetry. The Haag–Łopuszański–Sohnius theorem is the generalization of the Coleman–Mandula theorem to Lie superalgebras, with it stating that supersymmetry is the only new spacetime dependent symmetry that is allowed. For a theory with massless particles, the theorem is again evaded by conformal symmetry which can be present in addition to supersymmetry giving a superconformal algebra. Low dimensions In a one or two dimensional theory the only possible scattering is forwards and backwards scattering so analyticity of the scattering angles is no longer possible and the theorem no longer holds. Spacetime dependent internal symmetries are then possible, such as in the massive Thirring model which can admit an infinite tower of conserved charges of ever higher tensorial rank. Quantum groups Models with nonlocal symmetries whose charges do not act on multiparticle states as if they were a tensor product of one-particle states, evade the theorem. Such an evasion is found more generally for quantum group symmetries which avoid the theorem because the corresponding algebra is no longer a Lie algebra. Other limitations For other spacetime symmetries besides the Poincaré group, such as theories with a de Sitter background or non-relativistic field theories with Galilean invariance, the theorem no longer applies. It also does not hold for discrete symmetries, since these are not Lie groups, or for spontaneously broken symmetries since these do not act on the S-matrix level and thus do not commute with the S-matrix. See also Extended supersymmetry Supergroup Supersymmetry algebra Notes Further reading Coleman–Mandula theorem on Scholarpedia Sascha Leonhardt on the Coleman–Mandula theorem In addition to the model of the atom, Niels Bohr also proposed a model of the chemical bond. He proposed this model first in the article "Systems containing several nuclei" - the third and last of the classic series of articles by Bohr, published in November 1913 in Philosophical Magazine. According to his model for a diatomic molecule, the electrons of the atoms of the molecule form a rotating ring whose plane is perpendicular to the axis of the molecule and equidistant from the atomic nuclei. The dynamic equilibrium of the molecular system is achieved through the balance of forces between the forces of attraction of nuclei to the plane of the ring of electrons and the forces of mutual repulsion of the nuclei. The Bohr model of the chemical bond took into account the Coulomb repulsion - the electrons in the ring are at the maximum distance from each other. Thus, according to this model, the methane molecule is a regular tetrahedron, in which center the carbon nucleus locates, and in the corners - the nucleus of hydrogen. The chemical bond between them forms four two-electron rings, rotating around the lines connecting the center with the corners. The Bohr model of the chemical bond could not explain the properties of the molecules. Attempts to improve it have been undertaken many times, but have not led to success. A working theory of chemical bonding was formulated only by quantum mechanics on the basis of the principle of uncertainty and the Pauli exclusion principle. In contrast to the Bohr model of chemical bonding, it turned out that the electron cloud mainly concentrates on the line between the nuclei, providing a Coulomb attraction between them. For many-electron atoms, the valence bond theory, laid down in 1927 by Walter Heitler and Fritz London, was a successful approximation. References Bibliography Kragh, Helge (2012). "Ch. 6. Molecules and Other Failures". Niels Bohr and the Quantum Atom: The Bohr Model of Atomic Structure 1913-1925. OUP Oxford. pp. 226–270. ISBN 9780199654987. Mehra, Jagdish; Rechenberg, Helmut (2001). "IV.2. The Helium Atom and Other Few-Body Problems". The Historical Development of Quantum Theory. Vol. 1, часть 2. The Quantum Theory of Planck, Einstein, Bohr and Sommerfeld: Its Foundation and the Rise of Its Difficulties 1900—1925. Springer. pp. 391–398, 422. ISBN 9780387951751. In quantum field theory, and especially in quantum electrodynamics, the interacting theory leads to infinite quantities that have to be absorbed in a renormalization procedure, in order to be able to predict measurable quantities. The renormalization scheme can depend on the type of particles that are being considered. For particles that can travel asymptotically large distances, or for low energy processes, the on-shell scheme, also known as the physical scheme, is appropriate. If these conditions are not fulfilled, one can turn to other schemes, like the minimal subtraction scheme (MS scheme). Fermion propagator in the interacting theory Knowing the different propagators is the basis for being able to calculate Feynman diagrams which are useful tools to predict, for example, the result of scattering experiments. In a theory where the only field is the Dirac field, the Feynman propagator reads ⟨ 0 | T ( ψ ( x ) ψ ¯ ( 0 ) ) | 0 ⟩ = i S F ( x ) = ∫ d 4 p ( 2 π ) 4 i e − i p ⋅ x p / − m + i ϵ {\displaystyle \langle 0|T(\psi (x){\bar {\psi }}(0))|0\rangle =iS_{F}(x)=\int {\frac {d^{4}p}{(2\pi )^{4}}}{\frac {ie^{-ip\cdot x}}{p\!\!\!/-m+i\epsilon }}} where T {\displaystyle T} is the time-ordering operator, | 0 ⟩ {\displaystyle |0\rangle } the vacuum in the non interacting theory, ψ ( x ) {\displaystyle \psi (x)} and ψ ¯ ( x ) {\displaystyle {\bar {\psi }}(x)} the Dirac field and its Dirac adjoint, and where the left-hand side of the equation is the two-point correlation function of the Dirac field. In a new theory, the Dirac field can interact with another field, for example with the electromagnetic field in quantum electrodynamics, and the strength of the interaction is measured by a parameter, in the case of QED it is the bare electron charge, e {\displaystyle e} . The general form of the propagator should remain unchanged, meaning that if | Ω ⟩ {\displaystyle |\Omega \rangle } now represents the vacuum in the interacting theory, the two-point correlation function would now read ⟨ Ω | T ( ψ ( x ) ψ ¯ ( 0 ) ) | Ω ⟩ = ∫ d 4 p ( 2 π ) 4 i Z 2 e − i p ⋅ x p / − m r + i ϵ {\displaystyle \langle \Omega |T(\psi (x){\bar {\psi }}(0))|\Omega \rangle =\int {\frac {d^{4}p}{(2\pi )^{4}}}{\frac {iZ_{2}e^{-ip\cdot x}}{p\!\!\!/-m_{r}+i\epsilon }}} Two new quantities have been introduced. First the renormalized mass m r {\displaystyle m_{r}} has been defined as the pole in the Fourier transform of the Feynman propagator. This is the main prescription of the on-shell renormalization scheme (there is then no need to introduce other mass scales like in the minimal subtraction scheme). The quantity Z 2 {\displaystyle Z_{2}} represents the new strength of the Dirac field. As the interaction is turned down to zero by letting e → 0 {\displaystyle e\rightarrow 0} , these new parameters should tend to a value so as to recover the propagator of the free fermion, namely m r → m {\displaystyle m_{r}\rightarrow m} and Z 2 → 1 {\displaystyle Z_{2}\rightarrow 1} . This means that m r {\displaystyle m_{r}} and Z 2 {\displaystyle Z_{2}} can be defined as a series in e {\displaystyle e} if this parameter is small enough (in the unit system where ℏ = c = 1 {\displaystyle \hbar =c=1} , e = 4 π α ≃ 0.3 {\displaystyle e={\sqrt {4\pi \alpha }}\simeq 0.3} , where α {\displaystyle \alpha } is the fine-structure constant). Thus these parameters can be expressed as Z 2 = 1 + δ 2 {\displaystyle Z_{2}=1+\delta _{2}} m r = m + δ m {\displaystyle m_{r}=m+\delta m} On the other hand, the modification to the propagator can be calculated up to a certain order in e {\displaystyle e} using Feynman diagrams. These modifications are summed up in the fermion self energy Σ ( p ) {\displaystyle \Sigma (p)} ⟨ Ω | T ( ψ ( x ) ψ ¯ ( 0 ) ) | Ω ⟩ = ∫ d 4 p ( 2 π ) 4 i e − i p ⋅ x p / − m − Σ ( p ) + i ϵ {\displaystyle \langle \Omega |T(\psi (x){\bar {\psi }}(0))|\Omega \rangle =\int {\frac {d^{4}p}{(2\pi )^{4}}}{\frac {ie^{-ip\cdot x}}{p\!\!\!/-m-\Sigma (p)+i\epsilon }}} These corrections are often divergent because they contain loops. By identifying the two expressions of the correlation function up to a certain order in e {\displaystyle e} , the counterterms can be defined, and they are going to absorb the divergent contributions of the corrections to the fermion propagator. Thus, the renormalized quantities, such as m r {\displaystyle m_{r}} , will remain finite, and will be the quantities measured in experiments. Photon propagator Just like what has been done with the fermion propagator, the form of the photon propagator inspired by the free photon field will be compared to the photon propagator calculated up to a certain order in e {\displaystyle e} in the interacting theory. The photon self energy is noted Π ( q 2 ) {\displaystyle \Pi (q^{2})} and the metric tensor η μ ν {\displaystyle \eta ^{\mu \nu }} (here taking the +--- convention) ⟨ Ω | T ( A μ ( x ) A ν ( 0 ) ) | Ω ⟩ = ∫ d 4 q ( 2 π ) 4 − i η μ ν e − i p ⋅ x q 2 ( 1 − Π ( q 2 ) ) + i ϵ = ∫ d 4 q ( 2 π ) 4 − i Z 3 η μ ν e − i p ⋅ x q 2 + i ϵ {\displaystyle \langle \Omega |T(A^{\mu }(x)A^{\nu }(0))|\Omega \rangle =\int {\frac {d^{4}q}{(2\pi )^{4}}}{\frac {-i\eta ^{\mu \nu }e^{-ip\cdot x}}{q^{2}(1-\Pi (q^{2}))+i\epsilon }}=\int {\frac {d^{4}q}{(2\pi )^{4}}}{\frac {-iZ_{3}\eta ^{\mu \nu }e^{-ip\cdot x}}{q^{2}+i\epsilon }}} The behaviour of the counterterm δ 3 = Z 3 − 1 {\displaystyle \delta _{3}=Z_{3}-1} is independent of the momentum of the incoming photon q {\displaystyle q} . To fix it, the behaviour of QED at large distances (which should help recover classical electrodynamics), i.e. when q 2 → 0 {\displaystyle q^{2}\rightarrow 0} , is used : − i η μ ν e − i p ⋅ x q 2 ( 1 − Π ( q 2 ) ) + i ϵ ∼ − i η μ ν e − i p ⋅ x q 2 {\displaystyle {\frac {-i\eta ^{\mu \nu }e^{-ip\cdot x}}{q^{2}(1-\Pi (q^{2}))+i\epsilon }}\sim {\frac {-i\eta ^{\mu \nu }e^{-ip\cdot x}}{q^{2}}}} Thus the counterterm δ 3 {\displaystyle \delta _{3}} is fixed with the value of Π ( 0 ) {\displaystyle \Pi (0)} . Vertex function A similar reasoning using the vertex function leads to the renormalization of the electric charge e r {\displaystyle e_{r}} . This renormalization, and the fixing of renormalization terms is done using what is known from classical electrodynamics at large space scales. This leads to the value of the counterterm δ 1 {\displaystyle \delta _{1}} , which is, in fact, equal to δ 2 {\displaystyle \delta _{2}} because of the Ward–Takahashi identity. It is this calculation that accounts for the anomalous magnetic dipole moment of fermions. Rescaling of the QED Lagrangian We have considered some proportionality factors (like the Z i {\displaystyle Z_{i}} ) that have been defined from the form of the propagator. However they can also be defined from the QED Lagrangian, which will be done in this section, and these definitions are equivalent. The Lagrangian that describes the physics of quantum electrodynamics is L = − 1 4 F μ ν F μ ν + ψ ¯ ( i ∂ / − m ) ψ + e ψ ¯ γ μ ψ A μ {\displaystyle {\mathcal {L}}=-{\frac {1}{4}}F_{\mu \nu }F^{\mu \nu }+{\bar {\psi }}(i\partial \!\!\!/-m)\psi +e{\bar {\psi }}\gamma ^{\mu }\psi A_{\mu }} where F μ ν {\displaystyle F_{\mu \nu }} is the field strength tensor, ψ {\displaystyle \psi } is the Dirac spinor (the relativistic equivalent of the wavefunction), and A {\displaystyle A} the electromagnetic four-potential. The parameters of the theory are ψ {\displaystyle \psi } , A {\displaystyle A} , m {\displaystyle m} and e {\displaystyle e} . These quantities happen to be infinite due to loop corrections (see below). One can define the renormalized quantities (which will be finite and observable): ψ = Z 2 ψ r A = Z 3 A r m = m r + δ m e = Z 1 Z 2 Z 3 e r with Z i = 1 + δ i {\displaystyle \psi ={\sqrt {Z_{2}}}\psi _{r}\;\;\;\;\;A={\sqrt {Z_{3}}}A_{r}\;\;\;\;\;m=m_{r}+\delta m\;\;\;\;\;e={\frac {Z_{1}}{Z_{2}{\sqrt {Z_{3}}}}}e_{r}\;\;\;\;\;{\text{with}}\;\;\;\;\;Z_{i}=1+\delta _{i}} The δ i {\displaystyle \delta _{i}} are called counterterms (some other definitions of them are possible). They are supposed to be small in the parameter e {\displaystyle e} . The Lagrangian now reads in terms of renormalized quantities (to first order in the counterterms): L = − 1 4 Z 3 F μ ν , r F r μ ν + Z 2 ψ ¯ r ( i ∂ / − m r ) ψ r − ψ ¯ r δ m ψ r + Z 1 e r ψ ¯ r γ μ ψ r A μ , r {\displaystyle {\mathcal {L}}=-{\frac {1}{4}}Z_{3}F_{\mu \nu ,r}F_{r}^{\mu \nu }+Z_{2}{\bar {\psi }}_{r}(i\partial \!\!\!/-m_{r})\psi _{r}-{\bar {\psi }}_{r}\delta m\psi _{r}+Z_{1}e_{r}{\bar {\psi }}_{r}\gamma ^{\mu }\psi _{r}A_{\mu ,r}} A renormalization prescription is a set of rules that describes what part of the divergences should be in the renormalized quantities and what parts should be in the counterterms. The prescription is often based on the theory of free fields, that is of the behaviour of ψ {\displaystyle \psi } and A {\displaystyle A} when they do not interact (which corresponds to removing the term e ψ ¯ γ μ ψ A μ {\displaystyle e{\bar {\psi }}\gamma ^{\mu }\psi A_{\mu }} in the Lagrangian). References M. Peskin; D. Schroeder (1995). An Introduction to Quantum Field Theory. Reading: Addison-Weasley. A spin model is a mathematical model used in physics primarily to explain magnetism. Spin models may either be classical or quantum mechanical in nature. Spin models have been studied in quantum field theory as examples of integrable models. Spin models are also used in quantum information theory and computability theory in theoretical computer science. The theory of spin models is a far reaching and unifying topic that cuts across many fields. Introduction In ordinary materials, the magnetic dipole moments of individual atoms produce magnetic fields that cancel one another, because each dipole points in a random direction. Ferromagnetic materials below their Curie temperature, however, exhibit magnetic domains in which the atomic dipole moments are locally aligned, producing a macroscopic, non-zero magnetic field from the domain. These are the ordinary "magnets" with which we are all familiar. The study of the behavior of such "spin models" is a thriving area of research in condensed matter physics. For instance, the Ising model describes spins (dipoles) that have only two possible states, up and down, whereas in the Heisenberg model the spin vector is allowed to point in any direction. In certain magnets, the magnetic dipoles are only free to rotate in a 2D plane, a system which can be adequately described by the so-called xy-model. The lack of a unified theory of magnetism forces scientist to model magnetic systems theoretically with one, or a combination of these spin models in order to understand the intricate behavior of atomic magnetic interactions. Numerical implementation of these models has led to several interesting results, such as quantitative research in the theory of phase transitions. Quantum A quantum spin model is a quantum Hamiltonian model that describes a system which consists of spins either interacting or not and are an active area of research in the fields of strongly correlated electron systems, quantum information theory, and quantum computing. The physical observables in these quantum models are actually operators in a Hilbert space acting on state vectors as opposed to the physical observables in the corresponding classical spin models - like the Ising model - which are commutative variables. See also References Bibliography Bethe, H. (March 1931). "Zur Theorie der Metalle". Zeitschrift für Physik. 71 (3–4): 205–226. Bibcode:1931ZPhy...71..205B. doi:10.1007/BF01341708. S2CID 124225487. R.J. Baxter, Exactly solved models in statistical mechanics, London, Academic Press, 1982 [1] Archived 2011-04-10 at the Wayback Machine Affleck, Ian; Marston, J. Brad (1 March 1988). "Large-n limit of the Heisenberg-Hubbard model: Implications for high-Tc superconductors". Physical Review B. 37 (7): 3774–3777. Bibcode:1988PhRvB..37.3774A. doi:10.1103/PhysRevB.37.3774. PMID 9944997. External links Introduction to classical and Ising Spin Models Quantum Field Theory of Many-Body Systems Institute of Quantum Information Caltech Florian Neukart is an Austrian business executive, computer scientist, physicist, and scientific author known for his work in quantum computing and artificial intelligence. He has primarily been working on utilizing quantum computers, artificial intelligence, and related technologies for solving industry problems. In his work on artificial intelligence, he describes methods for interpreting signals in the human brain in combination with paradigms from artificial intelligence to create artificial conscious entities. Biography Neukart holds a Ph.D. in computer science from the Transilvania University of Brasov and master's degrees in physics, information technology, and computer science from the Liverpool John Moores University, CAMPUS02 University of Applied Sciences and the Joanneum University of Applied Sciences. Work He is a member of the Board of Management at Terra Quantum AG, and previously worked as Director, Advanced Technologies and IT Innovation at Volkswagen Group of America, where he was concerned with research in the fields of quantum computing, quantum machine learning, artificial intelligence, and materials science. Neukart, born in Bruck/Mur, was also a member of the World Economic Forum's global future council on quantum computing, and an assistant professor for quantum computing at Leiden University. He is the author of the books "Reverse Engineering the Mind Consciously Acting Machines and Accelerated Evolution", in which he elaborates on establishing a symbiotic relationship between a biological brain, sensors, AI, and quantum hard- and software, resulting in solutions for the continuous consciousness problem as well as other state-of-the-art problems, and "Humankind's Hunger for Energy: The journey of a million years, from using flints to harvesting galaxies", in which he describes the evolution of humankind in terms of its energy consumption. He is the co-editor of the book "Chancen und Risiken der Quantentechnologien", in which the potential and the risks of quantum technologies for society and industry are discussed. His work has been featured broadly in the media. He was one of the first researchers to propose and implement quantum neural networks. At Volkswagen, he pioneered applied quantum computing and was among the first ones to solve real-world problems of society and environment employing quantum computers. Neukart was awarded by the Science Park Austria for his work in biologically-inspired artificial intelligence software. == References == In probability theory, for a probability measure P on a Hilbert space H with inner product ⟨ ⋅ , ⋅ ⟩ {\displaystyle \langle \cdot ,\cdot \rangle } , the covariance of P is the bilinear form Cov: H × H → R given by C o v ( x , y ) = ∫ H ⟨ x , z ⟩ ⟨ y , z ⟩ d P ( z ) {\displaystyle \mathrm {Cov} (x,y)=\int _{H}\langle x,z\rangle \langle y,z\rangle \,\mathrm {d} \mathbf {P} (z)} for all x and y in H. The covariance operator C is then defined by C o v ( x , y ) = ⟨ C x , y ⟩ {\displaystyle \mathrm {Cov} (x,y)=\langle Cx,y\rangle } (from the Riesz representation theorem, such operator exists if Cov is bounded). Since Cov is symmetric in its arguments, the covariance operator is self-adjoint. Even more generally, for a probability measure P on a Banach space B, the covariance of P is the bilinear form on the algebraic dual B#, defined by C o v ( x , y ) = ∫ B ⟨ x , z ⟩ ⟨ y , z ⟩ d P ( z ) {\displaystyle \mathrm {Cov} (x,y)=\int _{B}\langle x,z\rangle \langle y,z\rangle \,\mathrm {d} \mathbf {P} (z)} where ⟨ x , z ⟩ {\displaystyle \langle x,z\rangle } is now the value of the linear functional x on the element z. Quite similarly, the covariance function of a function-valued random element (in special cases is called random process or random field) z is C o v ( x , y ) = ∫ z ( x ) z ( y ) d P ( z ) = E ( z ( x ) z ( y ) ) {\displaystyle \mathrm {Cov} (x,y)=\int z(x)z(y)\,\mathrm {d} \mathbf {P} (z)=E(z(x)z(y))} where z(x) is now the value of the function z at the point x, i.e., the value of the linear functional u ↦ u ( x ) {\displaystyle u\mapsto u(x)} evaluated at z. See also Abstract Wiener space – Mathematical construction relating to infinite-dimensional spaces Cameron–Martin theorem – Theorem defining translation of Gaussian measures (Wiener measures) on Hilbert spaces. Feldman–Hájek theorem – Theory in probability theory Structure theorem for Gaussian measures – Mathematical theorem Further reading Baker, C. R. (September 1970). On Covariance Operators. Mimeo Series. Vol. 712. University of North Carolina at Chapel Hill. Baker, C. R. (December 1973). "Joint Measures and Cross-Covariance Operators" (PDF). Transactions of the American Mathematical Society. 186: 273–289. Vakhania, N. N.; Tarieladze, V. I.; Chobanyan, S. A. (1987). "Covariance Operators". Probability Distributions on Banach Spaces. Dordrecht: Springer Netherlands. pp. 144–183. doi:10.1007/978-94-009-3873-1_3. ISBN 978-94-010-8222-8. Retrieved 2024-04-11. == References == In theoretical physics, Rajesh Gopakumar and Cumrun Vafa introduced in a series of papers numerical invariants of Calabi-Yau threefolds, later referred to as the Gopakumar–Vafa invariants. These physically defined invariants represent the number of BPS states on a Calabi–Yau threefold. In the same papers, the authors also derived the following formula which relates the Gromov–Witten invariants and the Gopakumar-Vafa invariants. ∑ g = 0 ∞ ∑ β ∈ H 2 ( M , Z ) GW ( g , β ) q β λ 2 g − 2 = ∑ g = 0 ∞ ∑ k = 1 ∞ ∑ β ∈ H 2 ( M , Z ) GV ( g , β ) 1 k ( 2 sin ⁡ ( k λ 2 ) ) 2 g − 2 q k β {\displaystyle \sum _{g=0}^{\infty }~\sum _{\beta \in H_{2}(M,\mathbb {Z} )}{\text{GW}}(g,\beta )q^{\beta }\lambda ^{2g-2}=\sum _{g=0}^{\infty }~\sum _{k=1}^{\infty }~\sum _{\beta \in H_{2}(M,\mathbb {Z} )}{\text{GV}}(g,\beta ){\frac {1}{k}}\left(2\sin \left({\frac {k\lambda }{2}}\right)\right)^{2g-2}q^{k\beta }} , where β {\displaystyle \beta } is the class of holomorphic curves with genus g, λ {\displaystyle \lambda } is the topological string coupling, mathematically a formal variable, q β = exp ⁡ ( 2 π i t β ) {\displaystyle q^{\beta }=\exp(2\pi it_{\beta })} with t β {\displaystyle t_{\beta }} the Kähler parameter of the curve class β {\displaystyle \beta } , GW ( g , β ) {\displaystyle {\text{GW}}(g,\beta )} are the Gromov–Witten invariants of curve class β {\displaystyle \beta } at genus g {\displaystyle g} , GV ( g , β ) {\displaystyle {\text{GV}}(g,\beta )} are the Gopakumar–Vafa invariants of curve class β {\displaystyle \beta } at genus g {\displaystyle g} . Notably, Gromov-Witten invariants are generally rational numbers while Gopakumar-Vafa invariants are always integers. As a partition function in topological quantum field theory Gopakumar–Vafa invariants can be viewed as a partition function in topological quantum field theory. They are proposed to be the partition function in Gopakumar–Vafa form: Z t o p = exp ⁡ [ ∑ g = 0 ∞ ∑ k = 1 ∞ ∑ β ∈ H 2 ( M , Z ) GV ( g , β ) 1 k ( 2 sin ⁡ ( k λ 2 ) ) 2 g − 2 q k β ] . {\displaystyle Z_{top}=\exp \left[\sum _{g=0}^{\infty }~\sum _{k=1}^{\infty }~\sum _{\beta \in H_{2}(M,\mathbb {Z} )}{\text{GV}}(g,\beta ){\frac {1}{k}}\left(2\sin \left({\frac {k\lambda }{2}}\right)\right)^{2g-2}q^{k\beta }\right]\ .} Mathematical approaches While Gromov-Witten invariants have rigorous mathematical definitions (both in symplectic and algebraic geometry), there is no mathematically rigorous definition of the Gopakumar-Vafa invariants, except for very special cases. On the other hand, Gopakumar-Vafa's formula implies that Gromov-Witten invariants and Gopakumar-Vafa invariants determine each other. One can solve Gopakumar-Vafa invariants from Gromov-Witten invariants, while the solutions are a priori rational numbers. Ionel-Parker proved that these expressions are indeed integers. See also Gopakumar–Vafa duality Notes References Gopakumar, Rajesh; Vafa, Cumrun (1998a), M-Theory and Topological strings-I, arXiv:hep-th/9809187, Bibcode:1998hep.th....9187G Gopakumar, Rajesh; Vafa, Cumrun (1998b), M-Theory and Topological strings-II, arXiv:hep-th/9812127, Bibcode:1998hep.th...12127G Gopakumar, Rajesh; Vafa, Cumrun (1999), "On the Gauge Theory/Geometry Correspondence", Adv. Theor. Math. Phys., 3 (5): 1415–1443, arXiv:hep-th/9811131, Bibcode:1998hep.th...11131G, doi:10.4310/ATMP.1999.v3.n5.a5, S2CID 13824856 Gopakumar, Rajesh; Vafa, Cumrun (1998d), "Topological Gravity as Large N Topological Gauge Theory", Adv. Theor. Math. Phys., 2 (2): 413–442, arXiv:hep-th/9802016, Bibcode:1998hep.th....2016G, doi:10.4310/ATMP.1998.v2.n2.a8, S2CID 16676561 Ionel, Eleny-Nicoleta; Parker, Thomas H. (2018), "The Gopakumar–Vafa formula for symplectic manifolds", Annals of Mathematics, Second Series, 187 (1): 1–64, arXiv:1306.1516, doi:10.4007/annals.2018.187.1.1, MR 3739228, S2CID 7070264 In physics, a Feshbach resonance can occur upon collision of two slow atoms when they temporarily stick together forming an unstable compound with short lifetime (so-called resonance). It is a feature of many-body systems in which a bound state is achieved if the coupling(s) between at least one internal degree of freedom and the reaction coordinates, which lead to dissociation, vanish. The opposite situation, when a bound state is not formed, is a shape resonance. It is named after Herman Feshbach, a physicist at MIT. Feshbach resonances have become important in the study of cold atoms systems, including Fermi gases and Bose–Einstein condensates (BECs). In the context of scattering processes in many-body systems, the Feshbach resonance occurs when the energy of a bound state of an interatomic potential is equal to the kinetic energy of a colliding pair of atoms. In experimental settings, the Feshbach resonances provide a way to vary interaction strength between atoms in the cloud by changing scattering length, asc, of elastic collisions. For atomic species that possess these resonances (like K39 and K40), it is possible to vary the interaction strength by applying a uniform magnetic field. Among many uses, this tool has served to explore the transition from a BEC of fermionic molecules to weakly interacting fermion-pairs the BCS in Fermi clouds. For the BECs, Feshbach resonances have been used to study a spectrum of systems from the non-interacting ideal Bose gases to the unitary regime of interactions. Introduction Consider a general quantum scattering event between two particles. In this reaction, there are two reactant particles denoted by A and B, and two product particles denoted by A' and B' . For the case of a reaction (such as a nuclear reaction), we may denote this scattering event by A + B → A ′ + B ′ {\displaystyle A+B\rightarrow A'+B'} or A ( B , B ′ ) A ′ {\displaystyle A(B,B')A'} . The combination of the species and quantum states of the two reactant particles before or after the scattering event is referred to as a reaction channel. Specifically, the species and states of A and B constitute the entrance channel, while the types and states of A' and B' constitute the exit channel. An energetically accessible reaction channel is referred to as an open channel, whereas a reaction channel forbidden by energy conservation is referred to as a closed channel. Consider the interaction of two particles A and B in an entrance channel C. The positions of these two particles are given by r → A {\displaystyle {\vec {r}}_{A}} and r → B {\displaystyle {\vec {r}}_{B}} , respectively. The interaction energy of the two particles will usually depend only on the magnitude of the separation R ≡ | r → A − r → B | {\displaystyle R\equiv |{\vec {r}}_{A}-{\vec {r}}_{B}|} , and this function, sometimes referred to as a potential energy curve, is denoted by V c ( R ) {\displaystyle V_{c}(R)} . Often, this potential will have a pronounced minimum and thus admit bound states. The total energy of the two particles in the entrance channel is E = T + V c ( R ) + Δ ( P → ) {\displaystyle E=T+V_{c}(R)+\Delta ({\vec {P}})} , where T {\displaystyle T} denotes the total kinetic energy of the relative motion (center-of-mass motion plays no role in the two-body interaction), Δ {\displaystyle \Delta } is the contribution to the energy from couplings to external fields, and P → {\displaystyle {\vec {P}}} represents a vector of one or more parameters such as magnetic field or electric field. We consider now a second reaction channel, denoted by D, which is closed for large values of R. Let this potential curve V D ( R ) {\displaystyle V_{D}(R)} admit a bound state with energy E D {\displaystyle E_{D}} . A Feshbach resonance occurs when E D ≈ T + V c ( R ) + Δ ( P → 0 ) {\displaystyle E_{D}\approx T+V_{c}(R)+\Delta ({\vec {P}}_{0})} for some range of parameter vectors { P → 0 } {\displaystyle \lbrace {\vec {P}}_{0}\rbrace } . When this condition is met, then any coupling between channel C and channel D can give rise to significant mixing between the two channels; this manifests itself as a drastic dependence of the outcome of the scattering event on the parameter or parameters that control the energy of the entrance channel. These couplings can arise from spin-exchange interactions or relativistic spin-dependent interactions. Magnetic Feshbach resonance In ultracold atomic experiments, the resonance is controlled via the magnetic field and we assume that the kinetic energy T {\displaystyle T} is approximately 0. Since the channels differ in internal degrees of freedom such as spin and angular momentum, their difference in energy is dependent on B → {\displaystyle {\vec {B}}} by the Zeeman effect. The scattering length is modified as a = a b g ( 1 − Δ B − B 0 ) {\displaystyle a=a_{bg}\left(1-{\frac {\Delta }{B-B_{0}}}\right)} where a b g {\displaystyle a_{bg}} is the background scattering length, B 0 {\displaystyle B_{0}} is the magnetic field strength where resonance occurs, and Δ {\displaystyle \Delta } is the resonance width. This allows for manipulation of the scattering length to 0 or arbitrarily high values. As the magnetic field is swept through the resonance, the states in the open and closed channel can also mix and a large number of atoms, sometimes near 100% efficiency, convert to Feshbach molecules. These molecules have high vibrational states, so they then need to be transitioned to lower, more stable states to prevent dissociation. This can be done through stimulated emissions or other optical techniques such as STIRAP. Other methods include inducing stimulated emission through an oscillating magnetic field and atom-molecule thermalization. Feshbach resonances in avoided crossings In molecules, the nonadiabatic couplings between two adiabatic potentials build the avoided crossing (AC) region. The rovibronic resonances in the AC region of two-coupled potentials are very special, since they are not in the bound state region of the adiabatic potentials, and they usually do not play important roles on the scatterings and are less discussed. Yu Kun Yang et al studied this problem in the New J. Phys. 22 (2020). Exemplified in particle scattering, resonances in the AC region are comprehensively investigated. The effects of resonances in the AC region on the scattering cross sections strongly depend on the nonadiabatic couplings of the system, it can be very significant as sharp peaks, or inconspicuous buried in the background. More importantly, it shows a simple quantity proposed by Zhu and Nakamura to classify the coupling strength of nonadiabatic interactions, can be well applied to quantitatively estimate the importance of resonances in the AC region. Unstable state A virtual state, or unstable state is a bound or transient state which can decay into a free state or relax at some finite rate. This state may be the metastable state of a certain class of Feshbach resonance, "A special case of a Feshbach-type resonance occurs when the energy level lies near the very top of the potential well. Such a state is called 'virtual'" and may be further contrasted to a shape resonance depending on the angular momentum. Because of their transient existence, they can require special techniques for analysis and measurement, for example. See also Fano resonance Feshbach–Fano partitioning References R.J. Fletcher; A.L. Gaunt; N. Navon; R. Smith; Z. Hadzibabic (2013). "Stability of a Unitary Bose Gas". Phys. Rev. Lett. 111 (12): 125303. arXiv:1307.3193. Bibcode:2013PhRvL.111l5303F. doi:10.1103/PhysRevLett.111.125303. PMID 24093273. S2CID 7983994. Pethick; Smith (2002). Bose–Einstein Condensation in Dilute Gases. Cambridge. ISBN 0-521-66580-9. Herman Feshbach (1958). "Unified theory of nuclear reactions". Annals of Physics. 5 (4): 357. Bibcode:1958AnPhy...5..357F. doi:10.1016/0003-4916(58)90007-1. Fano, Ugo (1935). "Sullo spettro di assorbimento dei gas nobili presso il limite dello spettro d'arco". Il Nuovo Cimento (in Italian). 12 (3). Springer Science and Business Media LLC: 154–161. Bibcode:1935NCim...12..154F. doi:10.1007/bf02958288. ISSN 0029-6341. S2CID 119640917. Fano, U. (1961-12-15). "Effects of Configuration Interaction on Intensities and Phase Shifts". Physical Review. 124 (6). American Physical Society (APS): 1866–1878. Bibcode:1961PhRv..124.1866F. doi:10.1103/physrev.124.1866. ISSN 0031-899X. Per-Olov Löwdin (1962). "Studies in Perturbation Theory. IV. Solution of Eigenvalue Problem by Projection Operator Formalism". J. Math. Phys. 3 (5): 969–982. Bibcode:1962JMP.....3..969L. doi:10.1063/1.1724312. Claude Bloch (1958). "Sur la théorie des perturbations des états liés". Nucl. Phys. 6: 329. Bibcode:1958NucPh...6..329B. doi:10.1016/0029-5582(58)90116-0. The Roothaan equations are a representation of the Hartree–Fock equation in a non orthonormal basis set which can be of Gaussian-type or Slater-type. It applies to closed-shell molecules or atoms where all molecular orbitals or atomic orbitals, respectively, are doubly occupied. This is generally called Restricted Hartree–Fock theory. The method was developed independently by Clemens C. J. Roothaan and George G. Hall in 1951, and is thus sometimes called the Roothaan-Hall equations. The Roothaan equations can be written in a form resembling generalized eigenvalue problem, although they are not a standard eigenvalue problem because they are nonlinear: F C = S C ϵ {\displaystyle \mathbf {F} \mathbf {C} =\mathbf {S} \mathbf {C} \mathbf {\epsilon } } where F is the Fock matrix (which depends on the coefficients C due to electron-electron interactions), C is a matrix of coefficients, S is the overlap matrix of the basis functions, and ϵ {\displaystyle \epsilon } is the (diagonal, by convention) matrix of orbital energies. In the case of an orthonormalised basis set the overlap matrix, S, reduces to the identity matrix. These equations are essentially a special case of a Galerkin method applied to the Hartree–Fock equation using a particular basis set. In contrast to the Hartree–Fock equations - which are integro-differential equations - the Roothaan–Hall equations have a matrix-form. Therefore, they can be solved using standard techniques. See also Hartree–Fock method == References == In mathematics and physical science, spherical harmonics are special functions defined on the surface of a sphere. They are often employed in solving partial differential equations in many scientific fields. The table of spherical harmonics contains a list of common spherical harmonics. Since the spherical harmonics form a complete set of orthogonal functions and thus an orthonormal basis, every function defined on the surface of a sphere can be written as a sum of these spherical harmonics. This is similar to periodic functions defined on a circle that can be expressed as a sum of circular functions (sines and cosines) via Fourier series. Like the sines and cosines in Fourier series, the spherical harmonics may be organized by (spatial) angular frequency, as seen in the rows of functions in the illustration on the right. Further, spherical harmonics are basis functions for irreducible representations of SO(3), the group of rotations in three dimensions, and thus play a central role in the group theoretic discussion of SO(3). Spherical harmonics originate from solving Laplace's equation in the spherical domains. Functions that are solutions to Laplace's equation are called harmonics. Despite their name, spherical harmonics take their simplest form in Cartesian coordinates, where they can be defined as homogeneous polynomials of degree ℓ {\displaystyle \ell } in ( x , y , z ) {\displaystyle (x,y,z)} that obey Laplace's equation. The connection with spherical coordinates arises immediately if one uses the homogeneity to extract a factor of radial dependence r ℓ {\displaystyle r^{\ell }} from the above-mentioned polynomial of degree ℓ {\displaystyle \ell } ; the remaining factor can be regarded as a function of the spherical angular coordinates θ {\displaystyle \theta } and φ {\displaystyle \varphi } only, or equivalently of the orientational unit vector r {\displaystyle \mathbf {r} } specified by these angles. In this setting, they may be viewed as the angular portion of a set of solutions to Laplace's equation in three dimensions, and this viewpoint is often taken as an alternative definition. Notice, however, that spherical harmonics are not functions on the sphere which are harmonic with respect to the Laplace-Beltrami operator for the standard round metric on the sphere: the only harmonic functions in this sense on the sphere are the constants, since harmonic functions satisfy the Maximum principle. Spherical harmonics, as functions on the sphere, are eigenfunctions of the Laplace-Beltrami operator (see Higher dimensions). A specific set of spherical harmonics, denoted Y ℓ m ( θ , φ ) {\displaystyle Y_{\ell }^{m}(\theta ,\varphi )} or Y ℓ m ( r ) {\displaystyle Y_{\ell }^{m}({\mathbf {r} })} , are known as Laplace's spherical harmonics, as they were first introduced by Pierre Simon de Laplace in 1782. These functions form an orthogonal system, and are thus basic to the expansion of a general function on the sphere as alluded to above. Spherical harmonics are important in many theoretical and practical applications, including the representation of multipole electrostatic and electromagnetic fields, electron configurations, gravitational fields, geoids, the magnetic fields of planetary bodies and stars, and the cosmic microwave background radiation. In 3D computer graphics, spherical harmonics play a role in a wide variety of topics including indirect lighting (ambient occlusion, global illumination, precomputed radiance transfer, etc.) and modelling of 3D shapes. History Spherical harmonics were first investigated in connection with the Newtonian potential of Newton's law of universal gravitation in three dimensions. In 1782, Pierre-Simon de Laplace had, in his Mécanique Céleste, determined that the gravitational potential R 3 → R {\displaystyle \mathbb {R} ^{3}\to \mathbb {R} } at a point x associated with a set of point masses mi located at points xi was given by V ( x ) = ∑ i m i | x i − x | . {\displaystyle V(\mathbf {x} )=\sum _{i}{\frac {m_{i}}{|\mathbf {x} _{i}-\mathbf {x} |}}.} Each term in the above summation is an individual Newtonian potential for a point mass. Just prior to that time, Adrien-Marie Legendre had investigated the expansion of the Newtonian potential in powers of r = |x| and r1 = |x1|. He discovered that if r ≤ r1 then 1 | x 1 − x | = P 0 ( cos ⁡ γ ) 1 r 1 + P 1 ( cos ⁡ γ ) r r 1 2 + P 2 ( cos ⁡ γ ) r 2 r 1 3 + ⋯ {\displaystyle {\frac {1}{|\mathbf {x} _{1}-\mathbf {x} |}}=P_{0}(\cos \gamma ){\frac {1}{r_{1}}}+P_{1}(\cos \gamma ){\frac {r}{r_{1}^{2}}}+P_{2}(\cos \gamma ){\frac {r^{2}}{r_{1}^{3}}}+\cdots } where γ is the angle between the vectors x and x1. The functions P i : [ − 1 , 1 ] → R {\displaystyle P_{i}:[-1,1]\to \mathbb {R} } are the Legendre polynomials, and they can be derived as a special case of spherical harmonics. Subsequently, in his 1782 memoir, Laplace investigated these coefficients using spherical coordinates to represent the angle γ between x1 and x. (See Legendre polynomials § Applications for more detail.) In 1867, William Thomson (Lord Kelvin) and Peter Guthrie Tait introduced the solid spherical harmonics in their Treatise on Natural Philosophy, and also first introduced the name of "spherical harmonics" for these functions. The solid harmonics were homogeneous polynomial solutions R 3 → R {\displaystyle \mathbb {R} ^{3}\to \mathbb {R} } of Laplace's equation ∂ 2 u ∂ x 2 + ∂ 2 u ∂ y 2 + ∂ 2 u ∂ z 2 = 0. {\displaystyle {\frac {\partial ^{2}u}{\partial x^{2}}}+{\frac {\partial ^{2}u}{\partial y^{2}}}+{\frac {\partial ^{2}u}{\partial z^{2}}}=0.} By examining Laplace's equation in spherical coordinates, Thomson and Tait recovered Laplace's spherical harmonics. (See Harmonic polynomial representation.) The term "Laplace's coefficients" was employed by William Whewell to describe the particular system of solutions introduced along these lines, whereas others reserved this designation for the zonal spherical harmonics that had properly been introduced by Laplace and Legendre. The 19th century development of Fourier series made possible the solution of a wide variety of physical problems in rectangular domains, such as the solution of the heat equation and wave equation. This could be achieved by expansion of functions in series of trigonometric functions. Whereas the trigonometric functions in a Fourier series represent the fundamental modes of vibration in a string, the spherical harmonics represent the fundamental modes of vibration of a sphere in much the same way. Many aspects of the theory of Fourier series could be generalized by taking expansions in spherical harmonics rather than trigonometric functions. Moreover, analogous to how trigonometric functions can equivalently be written as complex exponentials, spherical harmonics also possessed an equivalent form as complex-valued functions. This was a boon for problems possessing spherical symmetry, such as those of celestial mechanics originally studied by Laplace and Legendre. The prevalence of spherical harmonics already in physics set the stage for their later importance in the 20th century birth of quantum mechanics. The (complex-valued) spherical harmonics S 2 → C {\displaystyle S^{2}\to \mathbb {C} } are eigenfunctions of the square of the orbital angular momentum operator − i ℏ r × ∇ , {\displaystyle -i\hbar \mathbf {r} \times \nabla ,} and therefore they represent the different quantized configurations of atomic orbitals. Laplace's spherical harmonics Laplace's equation imposes that the Laplacian of a scalar field f is zero. (Here the scalar field is understood to be complex, i.e. to correspond to a (smooth) function f : R 3 → C {\displaystyle f:\mathbb {R} ^{3}\to \mathbb {C} } .) In spherical coordinates this is: ∇ 2 f = 1 r 2 ∂ ∂ r ( r 2 ∂ f ∂ r ) + 1 r 2 sin ⁡ θ ∂ ∂ θ ( sin ⁡ θ ∂ f ∂ θ ) + 1 r 2 sin 2 ⁡ θ ∂ 2 f ∂ φ 2 = 0. {\displaystyle \nabla ^{2}f={\frac {1}{r^{2}}}{\frac {\partial }{\partial r}}\left(r^{2}{\frac {\partial f}{\partial r}}\right)+{\frac {1}{r^{2}\sin \theta }}{\frac {\partial }{\partial \theta }}\left(\sin \theta {\frac {\partial f}{\partial \theta }}\right)+{\frac {1}{r^{2}\sin ^{2}\theta }}{\frac {\partial ^{2}f}{\partial \varphi ^{2}}}=0.} Consider the problem of finding solutions of the form f(r, θ, φ) = R(r) Y(θ, φ). By separation of variables, two differential equations result by imposing Laplace's equation: 1 R d d r ( r 2 d R d r ) = λ , 1 Y 1 sin ⁡ θ ∂ ∂ θ ( sin ⁡ θ ∂ Y ∂ θ ) + 1 Y 1 sin 2 ⁡ θ ∂ 2 Y ∂ φ 2 = − λ . {\displaystyle {\frac {1}{R}}{\frac {d}{dr}}\left(r^{2}{\frac {dR}{dr}}\right)=\lambda ,\qquad {\frac {1}{Y}}{\frac {1}{\sin \theta }}{\frac {\partial }{\partial \theta }}\left(\sin \theta {\frac {\partial Y}{\partial \theta }}\right)+{\frac {1}{Y}}{\frac {1}{\sin ^{2}\theta }}{\frac {\partial ^{2}Y}{\partial \varphi ^{2}}}=-\lambda .} The second equation can be simplified under the assumption that Y has the form Y(θ, φ) = Θ(θ) Φ(φ). Applying separation of variables again to the second equation gives way to the pair of differential equations 1 Φ d 2 Φ d φ 2 = − m 2 {\displaystyle {\frac {1}{\Phi }}{\frac {d^{2}\Phi }{d\varphi ^{2}}}=-m^{2}} λ sin 2 ⁡ θ + sin ⁡ θ Θ d d θ ( sin ⁡ θ d Θ d θ ) = m 2 {\displaystyle \lambda \sin ^{2}\theta +{\frac {\sin \theta }{\Theta }}{\frac {d}{d\theta }}\left(\sin \theta {\frac {d\Theta }{d\theta }}\right)=m^{2}} for some number m. A priori, m is a complex constant, but because Φ must be a periodic function whose period evenly divides 2π, m is necessarily an integer and Φ is a linear combination of the complex exponentials e± imφ. The solution function Y(θ, φ) is regular at the poles of the sphere, where θ = 0, π. Imposing this regularity in the solution Θ of the second equation at the boundary points of the domain is a Sturm–Liouville problem that forces the parameter λ to be of the form λ = ℓ (ℓ + 1) for some non-negative integer with ℓ ≥ |m|; this is also explained below in terms of the orbital angular momentum. Furthermore, a change of variables t = cos θ transforms this equation into the Legendre equation, whose solution is a multiple of the associated Legendre polynomial Pmℓ(cos θ) . Finally, the equation for R has solutions of the form R(r) = A rℓ + B r−ℓ − 1; requiring the solution to be regular throughout R3 forces B = 0. Here the solution was assumed to have the special form Y(θ, φ) = Θ(θ) Φ(φ). For a given value of ℓ, there are 2ℓ + 1 independent solutions of this form, one for each integer m with −ℓ ≤ m ≤ ℓ. These angular solutions Y ℓ m : S 2 → C {\displaystyle Y_{\ell }^{m}:S^{2}\to \mathbb {C} } are a product of trigonometric functions, here represented as a complex exponential, and associated Legendre polynomials: Y ℓ m ( θ , φ ) = N e i m φ P ℓ m ( cos ⁡ θ ) {\displaystyle Y_{\ell }^{m}(\theta ,\varphi )=Ne^{im\varphi }P_{\ell }^{m}(\cos {\theta })} which fulfill r 2 ∇ 2 Y ℓ m ( θ , φ ) = − ℓ ( ℓ + 1 ) Y ℓ m ( θ , φ ) . {\displaystyle r^{2}\nabla ^{2}Y_{\ell }^{m}(\theta ,\varphi )=-\ell (\ell +1)Y_{\ell }^{m}(\theta ,\varphi ).} Here Y ℓ m : S 2 → C {\displaystyle Y_{\ell }^{m}:S^{2}\to \mathbb {C} } is called a spherical harmonic function of degree ℓ and order m, P ℓ m : [ − 1 , 1 ] → R {\displaystyle P_{\ell }^{m}:[-1,1]\to \mathbb {R} } is an associated Legendre polynomial, N is a normalization constant, and θ and φ represent colatitude and longitude, respectively. In particular, the colatitude θ, or polar angle, ranges from 0 at the North Pole, to π/2 at the Equator, to π at the South Pole, and the longitude φ, or azimuth, may assume all values with 0 ≤ φ < 2π. For a fixed integer ℓ, every solution Y(θ, φ), Y : S 2 → C {\displaystyle Y:S^{2}\to \mathbb {C} } , of the eigenvalue problem r 2 ∇ 2 Y = − ℓ ( ℓ + 1 ) Y {\displaystyle r^{2}\nabla ^{2}Y=-\ell (\ell +1)Y} is a linear combination of Y ℓ m : S 2 → C {\displaystyle Y_{\ell }^{m}:S^{2}\to \mathbb {C} } . In fact, for any such solution, rℓ Y(θ, φ) is the expression in spherical coordinates of a homogeneous polynomial R 3 → C {\displaystyle \mathbb {R} ^{3}\to \mathbb {C} } that is harmonic (see below), and so counting dimensions shows that there are 2ℓ + 1 linearly independent such polynomials. The general solution f : R 3 → C {\displaystyle f:\mathbb {R} ^{3}\to \mathbb {C} } to Laplace's equation Δ f = 0 {\displaystyle \Delta f=0} in a ball centered at the origin is a linear combination of the spherical harmonic functions multiplied by the appropriate scale factor rℓ, f ( r , θ , φ ) = ∑ ℓ = 0 ∞ ∑ m = − ℓ ℓ f ℓ m r ℓ Y ℓ m ( θ , φ ) , {\displaystyle f(r,\theta ,\varphi )=\sum _{\ell =0}^{\infty }\sum _{m=-\ell }^{\ell }f_{\ell }^{m}r^{\ell }Y_{\ell }^{m}(\theta ,\varphi ),} where the f ℓ m ∈ C {\displaystyle f_{\ell }^{m}\in \mathbb {C} } are constants and the factors rℓ Yℓm are known as (regular) solid harmonics R 3 → C {\displaystyle \mathbb {R} ^{3}\to \mathbb {C} } . Such an expansion is valid in the ball r < R = 1 lim sup ℓ → ∞ | f ℓ m | 1 / ℓ . {\displaystyle r<R={\frac {1}{\limsup _{\ell \to \infty }|f_{\ell }^{m}|^{{1}/{\ell }}}}.} For r > R {\displaystyle r>R} , the solid harmonics with negative powers of r {\displaystyle r} (the irregular solid harmonics R 3 ∖ { 0 } → C {\displaystyle \mathbb {R} ^{3}\setminus \{\mathbf {0} \}\to \mathbb {C} } ) are chosen instead. In that case, one needs to expand the solution of known regions in Laurent series (about r = ∞ {\displaystyle r=\infty } ), instead of the Taylor series (about r = 0 {\displaystyle r=0} ) used above, to match the terms and find series expansion coefficients f ℓ m ∈ C {\displaystyle f_{\ell }^{m}\in \mathbb {C} } . Orbital angular momentum In quantum mechanics, Laplace's spherical harmonics are understood in terms of the orbital angular momentum L = − i ℏ ( x × ∇ ) = L x i + L y j + L z k . {\displaystyle \mathbf {L} =-i\hbar (\mathbf {x} \times \mathbf {\nabla } )=L_{x}\mathbf {i} +L_{y}\mathbf {j} +L_{z}\mathbf {k} .} The ħ is conventional in quantum mechanics; it is convenient to work in units in which ħ = 1. The spherical harmonics are eigenfunctions of the square of the orbital angular momentum L 2 = − r 2 ∇ 2 + ( r ∂ ∂ r + 1 ) r ∂ ∂ r = − 1 sin ⁡ θ ∂ ∂ θ sin ⁡ θ ∂ ∂ θ − 1 sin 2 ⁡ θ ∂ 2 ∂ φ 2 . {\displaystyle {\begin{aligned}\mathbf {L} ^{2}&=-r^{2}\nabla ^{2}+\left(r{\frac {\partial }{\partial r}}+1\right)r{\frac {\partial }{\partial r}}\\&=-{\frac {1}{\sin \theta }}{\frac {\partial }{\partial \theta }}\sin \theta {\frac {\partial }{\partial \theta }}-{\frac {1}{\sin ^{2}\theta }}{\frac {\partial ^{2}}{\partial \varphi ^{2}}}.\end{aligned}}} Laplace's spherical harmonics are the joint eigenfunctions of the square of the orbital angular momentum and the generator of rotations about the azimuthal axis: L z = − i ( x ∂ ∂ y − y ∂ ∂ x ) = − i ∂ ∂ φ . {\displaystyle {\begin{aligned}L_{z}&=-i\left(x{\frac {\partial }{\partial y}}-y{\frac {\partial }{\partial x}}\right)\\&=-i{\frac {\partial }{\partial \varphi }}.\end{aligned}}} These operators commute, and are densely defined self-adjoint operators on the weighted Hilbert space of functions f square-integrable with respect to the normal distribution as the weight function on R3: 1 ( 2 π ) 3 / 2 ∫ R 3 | f ( x ) | 2 e − | x | 2 / 2 d x < ∞ . {\displaystyle {\frac {1}{(2\pi )^{3/2}}}\int _{\mathbb {R} ^{3}}|f(x)|^{2}e^{-|x|^{2}/2}\,dx<\infty .} Furthermore, L2 is a positive operator. If Y is a joint eigenfunction of L2 and Lz, then by definition L 2 Y = λ Y L z Y = m Y {\displaystyle {\begin{aligned}\mathbf {L} ^{2}Y&=\lambda Y\\L_{z}Y&=mY\end{aligned}}} for some real numbers m and λ. Here m must in fact be an integer, for Y must be periodic in the coordinate φ with period a number that evenly divides 2π. Furthermore, since L 2 = L x 2 + L y 2 + L z 2 {\displaystyle \mathbf {L} ^{2}=L_{x}^{2}+L_{y}^{2}+L_{z}^{2}} and each of Lx, Ly, Lz are self-adjoint, it follows that λ ≥ m2. Denote this joint eigenspace by Eλ,m, and define the raising and lowering operators by L + = L x + i L y L − = L x − i L y {\displaystyle {\begin{aligned}L_{+}&=L_{x}+iL_{y}\\L_{-}&=L_{x}-iL_{y}\end{aligned}}} Then L+ and L− commute with L2, and the Lie algebra generated by L+, L−, Lz is the special linear Lie algebra of order 2, s l 2 ( C ) {\displaystyle {\mathfrak {sl}}_{2}(\mathbb {C} )} , with commutation relations [ L z , L + ] = L + , [ L z , L − ] = − L − , [ L + , L − ] = 2 L z . {\displaystyle [L_{z},L_{+}]=L_{+},\quad [L_{z},L_{-}]=-L_{-},\quad [L_{+},L_{-}]=2L_{z}.} Thus L+ : Eλ,m → Eλ,m+1 (it is a "raising operator") and L− : Eλ,m → Eλ,m−1 (it is a "lowering operator"). In particular, Lk+ : Eλ,m → Eλ,m+k must be zero for k sufficiently large, because the inequality λ ≥ m2 must hold in each of the nontrivial joint eigenspaces. Let Y ∈ Eλ,m be a nonzero joint eigenfunction, and let k be the least integer such that L + k Y = 0. {\displaystyle L_{+}^{k}Y=0.} Then, since L − L + = L 2 − L z 2 − L z {\displaystyle L_{-}L_{+}=\mathbf {L} ^{2}-L_{z}^{2}-L_{z}} it follows that 0 = L − L + k Y = ( λ − ( m + k ) 2 − ( m + k ) ) Y . {\displaystyle 0=L_{-}L_{+}^{k}Y=(\lambda -(m+k)^{2}-(m+k))Y.} Thus λ = ℓ(ℓ + 1) for the positive integer ℓ = m + k. The foregoing has been all worked out in the spherical coordinate representation, ⟨ θ , φ | l m ⟩ = Y l m ( θ , φ ) {\displaystyle \langle \theta ,\varphi |lm\rangle =Y_{l}^{m}(\theta ,\varphi )} but may be expressed more abstractly in the complete, orthonormal spherical ket basis. Harmonic polynomial representation The spherical harmonics can be expressed as the restriction to the unit sphere of certain polynomial functions R 3 → C {\displaystyle \mathbb {R} ^{3}\to \mathbb {C} } . Specifically, we say that a (complex-valued) polynomial function p : R 3 → C {\displaystyle p:\mathbb {R} ^{3}\to \mathbb {C} } is homogeneous of degree ℓ {\displaystyle \ell } if p ( λ x ) = λ ℓ p ( x ) {\displaystyle p(\lambda \mathbf {x} )=\lambda ^{\ell }p(\mathbf {x} )} for all real numbers λ ∈ R {\displaystyle \lambda \in \mathbb {R} } and all x ∈ R 3 {\displaystyle \mathbf {x} \in \mathbb {R} ^{3}} . We say that p {\displaystyle p} is harmonic if Δ p = 0 , {\displaystyle \Delta p=0,} where Δ {\displaystyle \Delta } is the Laplacian. Then for each ℓ {\displaystyle \ell } , we define A ℓ = { harmonic polynomials R 3 → C that are homogeneous of degree ℓ } . {\displaystyle \mathbf {A} _{\ell }=\left\{{\text{harmonic polynomials }}\mathbb {R} ^{3}\to \mathbb {C} {\text{ that are homogeneous of degree }}\ell \right\}.} For example, when ℓ = 1 {\displaystyle \ell =1} , A 1 {\displaystyle \mathbf {A} _{1}} is just the 3-dimensional space of all linear functions R 3 → C {\displaystyle \mathbb {R} ^{3}\to \mathbb {C} } , since any such function is automatically harmonic. Meanwhile, when ℓ = 2 {\displaystyle \ell =2} , we have a 6-dimensional space: A 2 = span C ⁡ ( x 1 x 2 , x 1 x 3 , x 2 x 3 , x 1 2 , x 2 2 , x 3 2 ) . {\displaystyle \mathbf {A} _{2}=\operatorname {span} _{\mathbb {C} }(x_{1}x_{2},\,x_{1}x_{3},\,x_{2}x_{3},\,x_{1}^{2},\,x_{2}^{2},\,x_{3}^{2}).} A general formula for the dimension, d l {\displaystyle d_{l}} , of the set of homogenous polynomials of degree ℓ {\displaystyle \ell } in R n {\displaystyle \mathbb {R} ^{n}} is d l = ( n + l − 1 ) ! ( n − 1 ) ! l ! {\displaystyle d_{l}={\frac {(n+l-1)!}{(n-1)!\,l!}}} For any ℓ {\displaystyle \ell } , the space H ℓ {\displaystyle \mathbf {H} _{\ell }} of spherical harmonics of degree ℓ {\displaystyle \ell } is just the space of restrictions to the sphere S 2 {\displaystyle S^{2}} of the elements of A ℓ {\displaystyle \mathbf {A} _{\ell }} . As suggested in the introduction, this perspective is presumably the origin of the term “spherical harmonic” (i.e., the restriction to the sphere of a harmonic function). For example, for any c ∈ C {\displaystyle c\in \mathbb {C} } the formula p ( x 1 , x 2 , x 3 ) = c ( x 1 + i x 2 ) ℓ {\displaystyle p(x_{1},x_{2},x_{3})=c(x_{1}+ix_{2})^{\ell }} defines a homogeneous polynomial of degree ℓ {\displaystyle \ell } with domain and codomain R 3 → C {\displaystyle \mathbb {R} ^{3}\to \mathbb {C} } , which happens to be independent of x 3 {\displaystyle x_{3}} . This polynomial is easily seen to be harmonic. If we write p {\displaystyle p} in spherical coordinates ( r , θ , φ ) {\displaystyle (r,\theta ,\varphi )} and then restrict to r = 1 {\displaystyle r=1} , we obtain p ( θ , φ ) = c sin ⁡ ( θ ) ℓ ( cos ⁡ ( φ ) + i sin ⁡ ( φ ) ) ℓ , {\displaystyle p(\theta ,\varphi )=c\sin(\theta )^{\ell }(\cos(\varphi )+i\sin(\varphi ))^{\ell },} which can be rewritten as p ( θ , φ ) = c ( 1 − cos 2 ⁡ ( θ ) ) ℓ e i ℓ φ . {\displaystyle p(\theta ,\varphi )=c\left({\sqrt {1-\cos ^{2}(\theta )}}\right)^{\ell }e^{i\ell \varphi }.} After using the formula for the associated Legendre polynomial P ℓ ℓ {\displaystyle P_{\ell }^{\ell }} , we may recognize this as the formula for the spherical harmonic Y ℓ ℓ ( θ , φ ) . {\displaystyle Y_{\ell }^{\ell }(\theta ,\varphi ).} (See Special cases.) Conventions Orthogonality and normalization Several different normalizations are in common use for the Laplace spherical harmonic functions S 2 → C {\displaystyle S^{2}\to \mathbb {C} } . Throughout the section, we use the standard convention that for m > 0 {\displaystyle m>0} (see associated Legendre polynomials) P ℓ − m = ( − 1 ) m ( ℓ − m ) ! ( ℓ + m ) ! P ℓ m {\displaystyle P_{\ell }^{-m}=(-1)^{m}{\frac {(\ell -m)!}{(\ell +m)!}}P_{\ell }^{m}} which is the natural normalization given by Rodrigues' formula. In acoustics, the Laplace spherical harmonics are generally defined as (this is the convention used in this article) Y ℓ m ( θ , φ ) = ( 2 ℓ + 1 ) 4 π ( ℓ − m ) ! ( ℓ + m ) ! P ℓ m ( cos ⁡ θ ) e i m φ {\displaystyle Y_{\ell }^{m}(\theta ,\varphi )={\sqrt {{\frac {(2\ell +1)}{4\pi }}{\frac {(\ell -m)!}{(\ell +m)!}}}}\,P_{\ell }^{m}(\cos {\theta })\,e^{im\varphi }} while in quantum mechanics: Y ℓ m ( θ , φ ) = ( − 1 ) m ( 2 ℓ + 1 ) 4 π ( ℓ − m ) ! ( ℓ + m ) ! P ℓ m ( cos ⁡ θ ) e i m φ {\displaystyle Y_{\ell }^{m}(\theta ,\varphi )=(-1)^{m}{\sqrt {{\frac {(2\ell +1)}{4\pi }}{\frac {(\ell -m)!}{(\ell +m)!}}}}\,P_{\ell }^{m}(\cos {\theta })\,e^{im\varphi }} where P ℓ m {\displaystyle P_{\ell }^{m}} are associated Legendre polynomials without the Condon–Shortley phase (to avoid counting the phase twice). In both definitions, the spherical harmonics are orthonormal ∫ θ = 0 π ∫ φ = 0 2 π Y ℓ m Y ℓ ′ m ′ ∗ d Ω = δ ℓ ℓ ′ δ m m ′ , {\displaystyle \int _{\theta =0}^{\pi }\int _{\varphi =0}^{2\pi }Y_{\ell }^{m}\,Y_{\ell '}^{m'}{}^{*}\,d\Omega =\delta _{\ell \ell '}\,\delta _{mm'},} where δij is the Kronecker delta and dΩ = sin(θ) dφ dθ. This normalization is used in quantum mechanics because it ensures that probability is normalized, i.e., ∫ | Y ℓ m | 2 d Ω = 1. {\displaystyle \int {|Y_{\ell }^{m}|^{2}d\Omega }=1.} The disciplines of geodesy and spectral analysis use Y ℓ m ( θ , φ ) = ( 2 ℓ + 1 ) ( ℓ − m ) ! ( ℓ + m ) ! P ℓ m ( cos ⁡ θ ) e i m φ {\displaystyle Y_{\ell }^{m}(\theta ,\varphi )={\sqrt {{(2\ell +1)}{\frac {(\ell -m)!}{(\ell +m)!}}}}\,P_{\ell }^{m}(\cos {\theta })\,e^{im\varphi }} which possess unit power 1 4 π ∫ θ = 0 π ∫ φ = 0 2 π Y ℓ m Y ℓ ′ m ′ ∗ d Ω = δ ℓ ℓ ′ δ m m ′ . {\displaystyle {\frac {1}{4\pi }}\int _{\theta =0}^{\pi }\int _{\varphi =0}^{2\pi }Y_{\ell }^{m}\,Y_{\ell '}^{m'}{}^{*}d\Omega =\delta _{\ell \ell '}\,\delta _{mm'}.} The magnetics community, in contrast, uses Schmidt semi-normalized harmonics Y ℓ m ( θ , φ ) = ( ℓ − m ) ! ( ℓ + m ) ! P ℓ m ( cos ⁡ θ ) e i m φ {\displaystyle Y_{\ell }^{m}(\theta ,\varphi )={\sqrt {\frac {(\ell -m)!}{(\ell +m)!}}}\,P_{\ell }^{m}(\cos {\theta })\,e^{im\varphi }} which have the normalization ∫ θ = 0 π ∫ φ = 0 2 π Y ℓ m Y ℓ ′ m ′ ∗ d Ω = 4 π ( 2 ℓ + 1 ) δ ℓ ℓ ′ δ m m ′ . {\displaystyle \int _{\theta =0}^{\pi }\int _{\varphi =0}^{2\pi }Y_{\ell }^{m}\,Y_{\ell '}^{m'}{}^{*}d\Omega ={\frac {4\pi }{(2\ell +1)}}\delta _{\ell \ell '}\,\delta _{mm'}.} In quantum mechanics this normalization is sometimes used as well, and is named Racah's normalization after Giulio Racah. It can be shown that all of the above normalized spherical harmonic functions satisfy Y ℓ m ∗ ( θ , φ ) = ( − 1 ) − m Y ℓ − m ( θ , φ ) , {\displaystyle Y_{\ell }^{m}{}^{*}(\theta ,\varphi )=(-1)^{-m}Y_{\ell }^{-m}(\theta ,\varphi ),} where the superscript * denotes complex conjugation. Alternatively, this equation follows from the relation of the spherical harmonic functions with the Wigner D-matrix. Condon–Shortley phase One source of confusion with the definition of the spherical harmonic functions concerns a phase factor of ( − 1 ) m {\displaystyle (-1)^{m}} , commonly referred to as the Condon–Shortley phase in the quantum mechanical literature. In the quantum mechanics community, it is common practice to either include this phase factor in the definition of the associated Legendre polynomials, or to append it to the definition of the spherical harmonic functions. There is no requirement to use the Condon–Shortley phase in the definition of the spherical harmonic functions, but including it can simplify some quantum mechanical operations, especially the application of raising and lowering operators. The geodesy and magnetics communities never include the Condon–Shortley phase factor in their definitions of the spherical harmonic functions nor in the ones of the associated Legendre polynomials. Real form A real basis of spherical harmonics Y ℓ m : S 2 → R {\displaystyle Y_{\ell m}:S^{2}\to \mathbb {R} } can be defined in terms of their complex analogues Y ℓ m : S 2 → C {\displaystyle Y_{\ell }^{m}:S^{2}\to \mathbb {C} } by setting Y ℓ m = { i 2 ( Y ℓ m − ( − 1 ) m Y ℓ − m ) if m < 0 Y ℓ 0 if m = 0 1 2 ( Y ℓ − m + ( − 1 ) m Y ℓ m ) if m > 0. = { i 2 ( Y ℓ − | m | − ( − 1 ) m Y ℓ | m | ) if m < 0 Y ℓ 0 if m = 0 1 2 ( Y ℓ − | m | + ( − 1 ) m Y ℓ | m | ) if m > 0. = { 2 ( − 1 ) m ℑ [ Y ℓ | m | ] if m < 0 Y ℓ 0 if m = 0 2 ( − 1 ) m ℜ [ Y ℓ m ] if m > 0. {\displaystyle {\begin{aligned}Y_{\ell m}&={\begin{cases}{\dfrac {i}{\sqrt {2}}}\left(Y_{\ell }^{m}-(-1)^{m}\,Y_{\ell }^{-m}\right)&{\text{if}}\ m<0\\Y_{\ell }^{0}&{\text{if}}\ m=0\\{\dfrac {1}{\sqrt {2}}}\left(Y_{\ell }^{-m}+(-1)^{m}\,Y_{\ell }^{m}\right)&{\text{if}}\ m>0.\end{cases}}\\&={\begin{cases}{\dfrac {i}{\sqrt {2}}}\left(Y_{\ell }^{-|m|}-(-1)^{m}\,Y_{\ell }^{|m|}\right)&{\text{if}}\ m<0\\Y_{\ell }^{0}&{\text{if}}\ m=0\\{\dfrac {1}{\sqrt {2}}}\left(Y_{\ell }^{-|m|}+(-1)^{m}\,Y_{\ell }^{|m|}\right)&{\text{if}}\ m>0.\end{cases}}\\&={\begin{cases}{\sqrt {2}}\,(-1)^{m}\,\Im [{Y_{\ell }^{|m|}}]&{\text{if}}\ m<0\\Y_{\ell }^{0}&{\text{if}}\ m=0\\{\sqrt {2}}\,(-1)^{m}\,\Re [{Y_{\ell }^{m}}]&{\text{if}}\ m>0.\end{cases}}\end{aligned}}} The Condon–Shortley phase convention is used here for consistency. The corresponding inverse equations defining the complex spherical harmonics Y ℓ m : S 2 → C {\displaystyle Y_{\ell }^{m}:S^{2}\to \mathbb {C} } in terms of the real spherical harmonics Y ℓ m : S 2 → R {\displaystyle Y_{\ell m}:S^{2}\to \mathbb {R} } are Y ℓ m = { 1 2 ( Y ℓ | m | − i Y ℓ , − | m | ) if m < 0 Y ℓ 0 if m = 0 ( − 1 ) m 2 ( Y ℓ | m | + i Y ℓ , − | m | ) if m > 0. {\displaystyle Y_{\ell }^{m}={\begin{cases}{\dfrac {1}{\sqrt {2}}}\left(Y_{\ell |m|}-iY_{\ell ,-|m|}\right)&{\text{if}}\ m<0\\[4pt]Y_{\ell 0}&{\text{if}}\ m=0\\[4pt]{\dfrac {(-1)^{m}}{\sqrt {2}}}\left(Y_{\ell |m|}+iY_{\ell ,-|m|}\right)&{\text{if}}\ m>0.\end{cases}}} The real spherical harmonics Y ℓ m : S 2 → R {\displaystyle Y_{\ell m}:S^{2}\to \mathbb {R} } are sometimes known as tesseral spherical harmonics. These functions have the same orthonormality properties as the complex ones Y ℓ m : S 2 → C {\displaystyle Y_{\ell }^{m}:S^{2}\to \mathbb {C} } above. The real spherical harmonics Y ℓ m {\displaystyle Y_{\ell m}} with m > 0 are said to be of cosine type, and those with m < 0 of sine type. The reason for this can be seen by writing the functions in terms of the Legendre polynomials as Y ℓ m = { ( − 1 ) m 2 2 ℓ + 1 4 π ( ℓ − | m | ) ! ( ℓ + | m | ) ! P ℓ | m | ( cos ⁡ θ ) sin ⁡ ( | m | φ ) if m < 0 2 ℓ + 1 4 π P ℓ m ( cos ⁡ θ ) if m = 0 ( − 1 ) m 2 2 ℓ + 1 4 π ( ℓ − m ) ! ( ℓ + m ) ! P ℓ m ( cos ⁡ θ ) cos ⁡ ( m φ ) if m > 0 . {\displaystyle Y_{\ell m}={\begin{cases}\left(-1\right)^{m}{\sqrt {2}}{\sqrt {{\dfrac {2\ell +1}{4\pi }}{\dfrac {(\ell -|m|)!}{(\ell +|m|)!}}}}\;P_{\ell }^{|m|}(\cos \theta )\ \sin(|m|\varphi )&{\text{if }}m<0\\[4pt]{\sqrt {\dfrac {2\ell +1}{4\pi }}}\ P_{\ell }^{m}(\cos \theta )&{\text{if }}m=0\\[4pt]\left(-1\right)^{m}{\sqrt {2}}{\sqrt {{\dfrac {2\ell +1}{4\pi }}{\dfrac {(\ell -m)!}{(\ell +m)!}}}}\;P_{\ell }^{m}(\cos \theta )\ \cos(m\varphi )&{\text{if }}m>0\,.\end{cases}}} The same sine and cosine factors can be also seen in the following subsection that deals with the Cartesian representation. See here for a list of real spherical harmonics up to and including ℓ = 4 {\displaystyle \ell =4} , which can be seen to be consistent with the output of the equations above. Use in quantum chemistry As is known from the analytic solutions for the hydrogen atom, the eigenfunctions of the angular part of the wave function are spherical harmonics. However, the solutions of the non-relativistic Schrödinger equation without magnetic terms can be made real. This is why the real forms are extensively used in basis functions for quantum chemistry, as the programs don't then need to use complex algebra. Here, the real functions span the same space as the complex ones would. For example, as can be seen from the table of spherical harmonics, the usual p functions ( ℓ = 1 {\displaystyle \ell =1} ) are complex and mix axis directions, but the real versions are essentially just x, y, and z. Spherical harmonics in Cartesian form The complex spherical harmonics Y ℓ m {\displaystyle Y_{\ell }^{m}} give rise to the solid harmonics by extending from S 2 {\displaystyle S^{2}} to all of R 3 {\displaystyle \mathbb {R} ^{3}} as a homogeneous function of degree ℓ {\displaystyle \ell } , i.e. setting R ℓ m ( v ) := ‖ v ‖ ℓ Y ℓ m ( v ‖ v ‖ ) {\displaystyle R_{\ell }^{m}(v):=\|v\|^{\ell }Y_{\ell }^{m}\left({\frac {v}{\|v\|}}\right)} It turns out that R ℓ m {\displaystyle R_{\ell }^{m}} is basis of the space of harmonic and homogeneous polynomials of degree ℓ {\displaystyle \ell } . More specifically, it is the (unique up to normalization) Gelfand-Tsetlin-basis of this representation of the rotational group S O ( 3 ) {\displaystyle SO(3)} and an explicit formula for R ℓ m {\displaystyle R_{\ell }^{m}} in cartesian coordinates can be derived from that fact. The Herglotz generating function If the quantum mechanical convention is adopted for the Y ℓ m : S 2 → C {\displaystyle Y_{\ell }^{m}:S^{2}\to \mathbb {C} } , then e v a ⋅ r = ∑ ℓ = 0 ∞ ∑ m = − ℓ ℓ 4 π 2 ℓ + 1 r ℓ v ℓ λ m ( ℓ + m ) ! ( ℓ − m ) ! Y ℓ m ( r / r ) . {\displaystyle e^{v{\mathbf {a} }\cdot {\mathbf {r} }}=\sum _{\ell =0}^{\infty }\sum _{m=-\ell }^{\ell }{\sqrt {\frac {4\pi }{2\ell +1}}}{\frac {r^{\ell }v^{\ell }{\lambda ^{m}}}{\sqrt {(\ell +m)!(\ell -m)!}}}Y_{\ell }^{m}(\mathbf {r} /r).} Here, r {\displaystyle \mathbf {r} } is the vector with components ( x , y , z ) ∈ R 3 {\displaystyle (x,y,z)\in \mathbb {R} ^{3}} , r = | r | {\displaystyle r=|\mathbf {r} |} , and a = z ^ − λ 2 ( x ^ + i y ^ ) + 1 2 λ ( x ^ − i y ^ ) . {\displaystyle {\mathbf {a} }={\mathbf {\hat {z}} }-{\frac {\lambda }{2}}\left({\mathbf {\hat {x}} }+i{\mathbf {\hat {y}} }\right)+{\frac {1}{2\lambda }}\left({\mathbf {\hat {x}} }-i{\mathbf {\hat {y}} }\right).} a {\displaystyle \mathbf {a} } is a vector with complex coordinates: a = [ 1 2 ( 1 λ − λ ) , − i 2 ( 1 λ + λ ) , 1 ] . {\displaystyle \mathbf {a} =[{\frac {1}{2}}({\frac {1}{\lambda }}-\lambda ),-{\frac {i}{2}}({\frac {1}{\lambda }}+\lambda ),1].} The essential property of a {\displaystyle \mathbf {a} } is that it is null: a ⋅ a = 0. {\displaystyle \mathbf {a} \cdot \mathbf {a} =0.} It suffices to take v {\displaystyle v} and λ {\displaystyle \lambda } as real parameters. In naming this generating function after Herglotz, we follow Courant & Hilbert 1962, §VII.7, who credit unpublished notes by him for its discovery. Essentially all the properties of the spherical harmonics can be derived from this generating function. An immediate benefit of this definition is that if the vector r {\displaystyle \mathbf {r} } is replaced by the quantum mechanical spin vector operator J {\displaystyle \mathbf {J} } , such that Y ℓ m ( J ) {\displaystyle {\mathcal {Y}}_{\ell }^{m}({\mathbf {J} })} is the operator analogue of the solid harmonic r ℓ Y ℓ m ( r / r ) {\displaystyle r^{\ell }Y_{\ell }^{m}(\mathbf {r} /r)} , one obtains a generating function for a standardized set of spherical tensor operators, Y ℓ m ( J ) {\displaystyle {\mathcal {Y}}_{\ell }^{m}({\mathbf {J} })} : e v a ⋅ J = ∑ ℓ = 0 ∞ ∑ m = − ℓ ℓ 4 π 2 ℓ + 1 v ℓ λ m ( ℓ + m ) ! ( ℓ − m ) ! Y ℓ m ( J ) . {\displaystyle e^{v{\mathbf {a} }\cdot {\mathbf {J} }}=\sum _{\ell =0}^{\infty }\sum _{m=-\ell }^{\ell }{\sqrt {\frac {4\pi }{2\ell +1}}}{\frac {v^{\ell }{\lambda ^{m}}}{\sqrt {(\ell +m)!(\ell -m)!}}}{\mathcal {Y}}_{\ell }^{m}({\mathbf {J} }).} The parallelism of the two definitions ensures that the Y ℓ m {\displaystyle {\mathcal {Y}}_{\ell }^{m}} 's transform under rotations (see below) in the same way as the Y ℓ m {\displaystyle Y_{\ell }^{m}} 's, which in turn guarantees that they are spherical tensor operators, T q ( k ) {\displaystyle T_{q}^{(k)}} , with k = ℓ {\displaystyle k={\ell }} and q = m {\displaystyle q=m} , obeying all the properties of such operators, such as the Clebsch-Gordan composition theorem, and the Wigner-Eckart theorem. They are, moreover, a standardized set with a fixed scale or normalization. Separated Cartesian form The Herglotzian definition yields polynomials which may, if one wishes, be further factorized into a polynomial of z {\displaystyle z} and another of x {\displaystyle x} and y {\displaystyle y} , as follows (Condon–Shortley phase): r ℓ ( Y ℓ m Y ℓ − m ) = [ 2 ℓ + 1 4 π ] 1 / 2 Π ¯ ℓ m ( z ) ( ( − 1 ) m ( A m + i B m ) ( A m − i B m ) ) , m > 0. {\displaystyle r^{\ell }\,{\begin{pmatrix}Y_{\ell }^{m}\\Y_{\ell }^{-m}\end{pmatrix}}=\left[{\frac {2\ell +1}{4\pi }}\right]^{1/2}{\bar {\Pi }}_{\ell }^{m}(z){\begin{pmatrix}\left(-1\right)^{m}(A_{m}+iB_{m})\\(A_{m}-iB_{m})\end{pmatrix}},\qquad m>0.} and for m = 0: r ℓ Y ℓ 0 ≡ 2 ℓ + 1 4 π Π ¯ ℓ 0 . {\displaystyle r^{\ell }\,Y_{\ell }^{0}\equiv {\sqrt {\frac {2\ell +1}{4\pi }}}{\bar {\Pi }}_{\ell }^{0}.} Here A m ( x , y ) = ∑ p = 0 m ( m p ) x p y m − p cos ⁡ ( ( m − p ) π 2 ) , {\displaystyle A_{m}(x,y)=\sum _{p=0}^{m}{\binom {m}{p}}x^{p}y^{m-p}\cos \left((m-p){\frac {\pi }{2}}\right),} B m ( x , y ) = ∑ p = 0 m ( m p ) x p y m − p sin ⁡ ( ( m − p ) π 2 ) , {\displaystyle B_{m}(x,y)=\sum _{p=0}^{m}{\binom {m}{p}}x^{p}y^{m-p}\sin \left((m-p){\frac {\pi }{2}}\right),} and Π ¯ ℓ m ( z ) = [ ( ℓ − m ) ! ( ℓ + m ) ! ] 1 / 2 ∑ k = 0 ⌊ ( ℓ − m ) / 2 ⌋ ( − 1 ) k 2 − ℓ ( ℓ k ) ( 2 ℓ − 2 k ℓ ) ( ℓ − 2 k ) ! ( ℓ − 2 k − m ) ! r 2 k z ℓ − 2 k − m . {\displaystyle {\bar {\Pi }}_{\ell }^{m}(z)=\left[{\frac {(\ell -m)!}{(\ell +m)!}}\right]^{1/2}\sum _{k=0}^{\left\lfloor (\ell -m)/2\right\rfloor }(-1)^{k}2^{-\ell }{\binom {\ell }{k}}{\binom {2\ell -2k}{\ell }}{\frac {(\ell -2k)!}{(\ell -2k-m)!}}\;r^{2k}\;z^{\ell -2k-m}.} For m = 0 {\displaystyle m=0} this reduces to Π ¯ ℓ 0 ( z ) = ∑ k = 0 ⌊ ℓ / 2 ⌋ ( − 1 ) k 2 − ℓ ( ℓ k ) ( 2 ℓ − 2 k ℓ ) r 2 k z ℓ − 2 k . {\displaystyle {\bar {\Pi }}_{\ell }^{0}(z)=\sum _{k=0}^{\left\lfloor \ell /2\right\rfloor }(-1)^{k}2^{-\ell }{\binom {\ell }{k}}{\binom {2\ell -2k}{\ell }}\;r^{2k}\;z^{\ell -2k}.} The factor Π ¯ ℓ m ( z ) {\displaystyle {\bar {\Pi }}_{\ell }^{m}(z)} is essentially the associated Legendre polynomial P ℓ m ( cos ⁡ θ ) {\displaystyle P_{\ell }^{m}(\cos \theta )} , and the factors ( A m ± i B m ) {\displaystyle (A_{m}\pm iB_{m})} are essentially e ± i m φ {\displaystyle e^{\pm im\varphi }} . Examples Using the expressions for Π ¯ ℓ m ( z ) {\displaystyle {\bar {\Pi }}_{\ell }^{m}(z)} , A m ( x , y ) {\displaystyle A_{m}(x,y)} , and B m ( x , y ) {\displaystyle B_{m}(x,y)} listed explicitly above we obtain: Y 3 1 = − 1 r 3 [ 7 4 π ⋅ 3 16 ] 1 / 2 ( 5 z 2 − r 2 ) ( x + i y ) = − [ 7 4 π ⋅ 3 16 ] 1 / 2 ( 5 cos 2 ⁡ θ − 1 ) ( sin ⁡ θ e i φ ) {\displaystyle Y_{3}^{1}=-{\frac {1}{r^{3}}}\left[{\tfrac {7}{4\pi }}\cdot {\tfrac {3}{16}}\right]^{1/2}\left(5z^{2}-r^{2}\right)\left(x+iy\right)=-\left[{\tfrac {7}{4\pi }}\cdot {\tfrac {3}{16}}\right]^{1/2}\left(5\cos ^{2}\theta -1\right)\left(\sin \theta e^{i\varphi }\right)} Y 4 − 2 = 1 r 4 [ 9 4 π ⋅ 5 32 ] 1 / 2 ( 7 z 2 − r 2 ) ( x − i y ) 2 = [ 9 4 π ⋅ 5 32 ] 1 / 2 ( 7 cos 2 ⁡ θ − 1 ) ( sin 2 ⁡ θ e − 2 i φ ) {\displaystyle Y_{4}^{-2}={\frac {1}{r^{4}}}\left[{\tfrac {9}{4\pi }}\cdot {\tfrac {5}{32}}\right]^{1/2}\left(7z^{2}-r^{2}\right)\left(x-iy\right)^{2}=\left[{\tfrac {9}{4\pi }}\cdot {\tfrac {5}{32}}\right]^{1/2}\left(7\cos ^{2}\theta -1\right)\left(\sin ^{2}\theta e^{-2i\varphi }\right)} It may be verified that this agrees with the function listed here and here. Real forms Using the equations above to form the real spherical harmonics, it is seen that for m > 0 {\displaystyle m>0} only the A m {\displaystyle A_{m}} terms (cosines) are included, and for m < 0 {\displaystyle m<0} only the B m {\displaystyle B_{m}} terms (sines) are included: r ℓ ( Y ℓ m Y ℓ − m ) = 2 ℓ + 1 2 π Π ¯ ℓ m ( z ) ( A m B m ) , m > 0. {\displaystyle r^{\ell }\,{\begin{pmatrix}Y_{\ell m}\\Y_{\ell -m}\end{pmatrix}}={\sqrt {\frac {2\ell +1}{2\pi }}}{\bar {\Pi }}_{\ell }^{m}(z){\begin{pmatrix}A_{m}\\B_{m}\end{pmatrix}},\qquad m>0.} and for m = 0: r ℓ Y ℓ 0 ≡ 2 ℓ + 1 4 π Π ¯ ℓ 0 . {\displaystyle r^{\ell }\,Y_{\ell 0}\equiv {\sqrt {\frac {2\ell +1}{4\pi }}}{\bar {\Pi }}_{\ell }^{0}.} Special cases and values When m = 0 {\displaystyle m=0} , the spherical harmonics Y ℓ m : S 2 → C {\displaystyle Y_{\ell }^{m}:S^{2}\to \mathbb {C} } reduce to the ordinary Legendre polynomials: Y ℓ 0 ( θ , φ ) = 2 ℓ + 1 4 π P ℓ ( cos ⁡ θ ) . {\displaystyle Y_{\ell }^{0}(\theta ,\varphi )={\sqrt {\frac {2\ell +1}{4\pi }}}P_{\ell }(\cos \theta ).} When m = ± ℓ {\displaystyle m=\pm \ell } , Y ℓ ± ℓ ( θ , φ ) = ( ∓ 1 ) ℓ 2 ℓ ℓ ! ( 2 ℓ + 1 ) ! 4 π sin ℓ ⁡ θ e ± i ℓ φ , {\displaystyle Y_{\ell }^{\pm \ell }(\theta ,\varphi )={\frac {(\mp 1)^{\ell }}{2^{\ell }\ell !}}{\sqrt {\frac {(2\ell +1)!}{4\pi }}}\sin ^{\ell }\theta \,e^{\pm i\ell \varphi },} or more simply in Cartesian coordinates, r ℓ Y ℓ ± ℓ ( r ) = ( ∓ 1 ) ℓ 2 ℓ ℓ ! ( 2 ℓ + 1 ) ! 4 π ( x ± i y ) ℓ . {\displaystyle r^{\ell }Y_{\ell }^{\pm \ell }({\mathbf {r} })={\frac {(\mp 1)^{\ell }}{2^{\ell }\ell !}}{\sqrt {\frac {(2\ell +1)!}{4\pi }}}(x\pm iy)^{\ell }.} At the north pole, where θ = 0 {\displaystyle \theta =0} , and φ {\displaystyle \varphi } is undefined, all spherical harmonics except those with m = 0 {\displaystyle m=0} vanish: Y ℓ m ( 0 , φ ) = Y ℓ m ( z ) = 2 ℓ + 1 4 π δ m 0 . {\displaystyle Y_{\ell }^{m}(0,\varphi )=Y_{\ell }^{m}({\mathbf {z} })={\sqrt {\frac {2\ell +1}{4\pi }}}\delta _{m0}.} Symmetry properties The spherical harmonics have deep and consequential properties under the operations of spatial inversion (parity) and rotation. Parity The spherical harmonics have definite parity. That is, they are either even or odd with respect to inversion about the origin. Inversion is represented by the operator P Ψ ( r ) = Ψ ( − r ) {\displaystyle P\Psi (\mathbf {r} )=\Psi (-\mathbf {r} )} . Then, as can be seen in many ways (perhaps most simply from the Herglotz generating function), with r {\displaystyle \mathbf {r} } being a unit vector, Y ℓ m ( − r ) = ( − 1 ) ℓ Y ℓ m ( r ) . {\displaystyle Y_{\ell }^{m}(-\mathbf {r} )=(-1)^{\ell }Y_{\ell }^{m}(\mathbf {r} ).} In terms of the spherical angles, parity transforms a point with coordinates { θ , φ } {\displaystyle \{\theta ,\varphi \}} to { π − θ , π + φ } {\displaystyle \{\pi -\theta ,\pi +\varphi \}} . The statement of the parity of spherical harmonics is then Y ℓ m ( θ , φ ) → Y ℓ m ( π − θ , π + φ ) = ( − 1 ) ℓ Y ℓ m ( θ , φ ) {\displaystyle Y_{\ell }^{m}(\theta ,\varphi )\to Y_{\ell }^{m}(\pi -\theta ,\pi +\varphi )=(-1)^{\ell }Y_{\ell }^{m}(\theta ,\varphi )} (This can be seen as follows: The associated Legendre polynomials gives (−1)ℓ+m and from the exponential function we have (−1)m, giving together for the spherical harmonics a parity of (−1)ℓ.) Parity continues to hold for real spherical harmonics, and for spherical harmonics in higher dimensions: applying a point reflection to a spherical harmonic of degree ℓ changes the sign by a factor of (−1)ℓ. Rotations Consider a rotation R {\displaystyle {\mathcal {R}}} about the origin that sends the unit vector r {\displaystyle \mathbf {r} } to r ′ {\displaystyle \mathbf {r} '} . Under this operation, a spherical harmonic of degree ℓ {\displaystyle \ell } and order m {\displaystyle m} transforms into a linear combination of spherical harmonics of the same degree. That is, Y ℓ m ( r ′ ) = ∑ m ′ = − ℓ ℓ A m m ′ Y ℓ m ′ ( r ) , {\displaystyle Y_{\ell }^{m}({\mathbf {r} }')=\sum _{m'=-\ell }^{\ell }A_{mm'}Y_{\ell }^{m'}({\mathbf {r} }),} where A m m ′ {\displaystyle A_{mm'}} is a matrix of order ( 2 ℓ + 1 ) {\displaystyle (2\ell +1)} that depends on the rotation R {\displaystyle {\mathcal {R}}} . However, this is not the standard way of expressing this property. In the standard way one writes, Y ℓ m ( r ′ ) = ∑ m ′ = − ℓ ℓ [ D m m ′ ( ℓ ) ( R ) ] ∗ Y ℓ m ′ ( r ) , {\displaystyle Y_{\ell }^{m}({\mathbf {r} }')=\sum _{m'=-\ell }^{\ell }[D_{mm'}^{(\ell )}({\mathcal {R}})]^{*}Y_{\ell }^{m'}({\mathbf {r} }),} where D m m ′ ( ℓ ) ( R ) ∗ {\displaystyle D_{mm'}^{(\ell )}({\mathcal {R}})^{*}} is the complex conjugate of an element of the Wigner D-matrix. In particular when r ′ {\displaystyle \mathbf {r} '} is a ϕ 0 {\displaystyle \phi _{0}} rotation of the azimuth we get the identity, Y ℓ m ( r ′ ) = Y ℓ m ( r ) e i m ϕ 0 . {\displaystyle Y_{\ell }^{m}({\mathbf {r} }')=Y_{\ell }^{m}({\mathbf {r} })e^{im\phi _{0}}.} The rotational behavior of the spherical harmonics is perhaps their quintessential feature from the viewpoint of group theory. The Y ℓ m {\displaystyle Y_{\ell }^{m}} 's of degree ℓ {\displaystyle \ell } provide a basis set of functions for the irreducible representation of the group SO(3) of dimension ( 2 ℓ + 1 ) {\displaystyle (2\ell +1)} . Many facts about spherical harmonics (such as the addition theorem) that are proved laboriously using the methods of analysis acquire simpler proofs and deeper significance using the methods of symmetry. Spherical harmonics expansion The Laplace spherical harmonics Y ℓ m : S 2 → C {\displaystyle Y_{\ell }^{m}:S^{2}\to \mathbb {C} } form a complete set of orthonormal functions and thus form an orthonormal basis of the Hilbert space of square-integrable functions L C 2 ( S 2 ) {\displaystyle L_{\mathbb {C} }^{2}(S^{2})} . On the unit sphere S 2 {\displaystyle S^{2}} , any square-integrable function f : S 2 → C {\displaystyle f:S^{2}\to \mathbb {C} } can thus be expanded as a linear combination of these: f ( θ , φ ) = ∑ ℓ = 0 ∞ ∑ m = − ℓ ℓ f ℓ m Y ℓ m ( θ , φ ) . {\displaystyle f(\theta ,\varphi )=\sum _{\ell =0}^{\infty }\sum _{m=-\ell }^{\ell }f_{\ell }^{m}\,Y_{\ell }^{m}(\theta ,\varphi ).} This expansion holds in the sense of mean-square convergence — convergence in L2 of the sphere — which is to say that lim N → ∞ ∫ 0 2 π ∫ 0 π | f ( θ , φ ) − ∑ ℓ = 0 N ∑ m = − ℓ ℓ f ℓ m Y ℓ m ( θ , φ ) | 2 sin ⁡ θ d θ d φ = 0. {\displaystyle \lim _{N\to \infty }\int _{0}^{2\pi }\int _{0}^{\pi }\left|f(\theta ,\varphi )-\sum _{\ell =0}^{N}\sum _{m=-\ell }^{\ell }f_{\ell }^{m}Y_{\ell }^{m}(\theta ,\varphi )\right|^{2}\sin \theta \,d\theta \,d\varphi =0.} The expansion coefficients are the analogs of Fourier coefficients, and can be obtained by multiplying the above equation by the complex conjugate of a spherical harmonic, integrating over the solid angle Ω, and utilizing the above orthogonality relationships. This is justified rigorously by basic Hilbert space theory. For the case of orthonormalized harmonics, this gives: f ℓ m = ∫ Ω f ( θ , φ ) Y ℓ m ∗ ( θ , φ ) d Ω = ∫ 0 2 π d φ ∫ 0 π d θ sin ⁡ θ f ( θ , φ ) Y ℓ m ∗ ( θ , φ ) . {\displaystyle f_{\ell }^{m}=\int _{\Omega }f(\theta ,\varphi )\,Y_{\ell }^{m*}(\theta ,\varphi )\,d\Omega =\int _{0}^{2\pi }d\varphi \int _{0}^{\pi }\,d\theta \,\sin \theta f(\theta ,\varphi )Y_{\ell }^{m*}(\theta ,\varphi ).} If the coefficients decay in ℓ sufficiently rapidly — for instance, exponentially — then the series also converges uniformly to f. A square-integrable function f : S 2 → R {\displaystyle f:S^{2}\to \mathbb {R} } can also be expanded in terms of the real harmonics Y ℓ m : S 2 → R {\displaystyle Y_{\ell m}:S^{2}\to \mathbb {R} } above as a sum f ( θ , φ ) = ∑ ℓ = 0 ∞ ∑ m = − ℓ ℓ f ℓ m Y ℓ m ( θ , φ ) . {\displaystyle f(\theta ,\varphi )=\sum _{\ell =0}^{\infty }\sum _{m=-\ell }^{\ell }f_{\ell m}\,Y_{\ell m}(\theta ,\varphi ).} The convergence of the series holds again in the same sense, namely the real spherical harmonics Y ℓ m : S 2 → R {\displaystyle Y_{\ell m}:S^{2}\to \mathbb {R} } form a complete set of orthonormal functions and thus form an orthonormal basis of the Hilbert space of square-integrable functions L R 2 ( S 2 ) {\displaystyle L_{\mathbb {R} }^{2}(S^{2})} . The benefit of the expansion in terms of the real harmonic functions Y ℓ m {\displaystyle Y_{\ell m}} is that for real functions f : S 2 → R {\displaystyle f:S^{2}\to \mathbb {R} } the expansion coefficients f ℓ m {\displaystyle f_{\ell m}} are guaranteed to be real, whereas their coefficients f ℓ m {\displaystyle f_{\ell }^{m}} in their expansion in terms of the Y ℓ m {\displaystyle Y_{\ell }^{m}} (considering them as functions f : S 2 → C ⊃ R {\displaystyle f:S^{2}\to \mathbb {C} \supset \mathbb {R} } ) do not have that property. Spectrum analysis Power spectrum in signal processing The total power of a function f is defined in the signal processing literature as the integral of the function squared, divided by the area of its domain. Using the orthonormality properties of the real unit-power spherical harmonic functions, it is straightforward to verify that the total power of a function defined on the unit sphere is related to its spectral coefficients by a generalization of Parseval's theorem (here, the theorem is stated for Schmidt semi-normalized harmonics, the relationship is slightly different for orthonormal harmonics): 1 4 π ∫ Ω | f ( Ω ) | 2 d Ω = ∑ ℓ = 0 ∞ S f f ( ℓ ) , {\displaystyle {\frac {1}{4\,\pi }}\int _{\Omega }|f(\Omega )|^{2}\,d\Omega =\sum _{\ell =0}^{\infty }S_{f\!f}(\ell ),} where S f f ( ℓ ) = 1 2 ℓ + 1 ∑ m = − ℓ ℓ | f ℓ m | 2 {\displaystyle S_{f\!f}(\ell )={\frac {1}{2\ell +1}}\sum _{m=-\ell }^{\ell }|f_{\ell m}|^{2}} is defined as the angular power spectrum (for Schmidt semi-normalized harmonics). In a similar manner, one can define the cross-power of two functions as 1 4 π ∫ Ω f ( Ω ) g ∗ ( Ω ) d Ω = ∑ ℓ = 0 ∞ S f g ( ℓ ) , {\displaystyle {\frac {1}{4\,\pi }}\int _{\Omega }f(\Omega )\,g^{\ast }(\Omega )\,d\Omega =\sum _{\ell =0}^{\infty }S_{fg}(\ell ),} where S f g ( ℓ ) = 1 2 ℓ + 1 ∑ m = − ℓ ℓ f ℓ m g ℓ m ∗ {\displaystyle S_{fg}(\ell )={\frac {1}{2\ell +1}}\sum _{m=-\ell }^{\ell }f_{\ell m}g_{\ell m}^{\ast }} is defined as the cross-power spectrum. If the functions f and g have a zero mean (i.e., the spectral coefficients f00 and g00 are zero), then Sff(ℓ) and Sfg(ℓ) represent the contributions to the function's variance and covariance for degree ℓ, respectively. It is common that the (cross-)power spectrum is well approximated by a power law of the form S f f ( ℓ ) = C ℓ β . {\displaystyle S_{f\!f}(\ell )=C\,\ell ^{\beta }.} When β = 0, the spectrum is "white" as each degree possesses equal power. When β < 0, the spectrum is termed "red" as there is more power at the low degrees with long wavelengths than higher degrees. Finally, when β > 0, the spectrum is termed "blue". The condition on the order of growth of Sff(ℓ) is related to the order of differentiability of f in the next section. Differentiability properties One can also understand the differentiability properties of the original function f in terms of the asymptotics of Sff(ℓ). In particular, if Sff(ℓ) decays faster than any rational function of ℓ as ℓ → ∞, then f is infinitely differentiable. If, furthermore, Sff(ℓ) decays exponentially, then f is actually real analytic on the sphere. The general technique is to use the theory of Sobolev spaces. Statements relating the growth of the Sff(ℓ) to differentiability are then similar to analogous results on the growth of the coefficients of Fourier series. Specifically, if ∑ ℓ = 0 ∞ ( 1 + ℓ 2 ) s S f f ( ℓ ) < ∞ , {\displaystyle \sum _{\ell =0}^{\infty }(1+\ell ^{2})^{s}S_{ff}(\ell )<\infty ,} then f is in the Sobolev space Hs(S2). In particular, the Sobolev embedding theorem implies that f is infinitely differentiable provided that S f f ( ℓ ) = O ( ℓ − s ) a s ℓ → ∞ {\displaystyle S_{ff}(\ell )=O(\ell ^{-s})\quad {\rm {{as\ }\ell \to \infty }}} for all s. Algebraic properties Addition theorem A mathematical result of considerable interest and use is called the addition theorem for spherical harmonics. Given two vectors r and r′, with spherical coordinates ( r , θ , φ ) {\displaystyle (r,\theta ,\varphi )} and ( r , θ ′ , φ ′ ) {\displaystyle (r,\theta ',\varphi ')} , respectively, the angle γ {\displaystyle \gamma } between them is given by the relation cos ⁡ γ = cos ⁡ θ ′ cos ⁡ θ + sin ⁡ θ sin ⁡ θ ′ cos ⁡ ( φ − φ ′ ) {\displaystyle \cos \gamma =\cos \theta '\cos \theta +\sin \theta \sin \theta '\cos(\varphi -\varphi ')} in which the role of the trigonometric functions appearing on the right-hand side is played by the spherical harmonics and that of the left-hand side is played by the Legendre polynomials. The addition theorem states where Pℓ is the Legendre polynomial of degree ℓ. This expression is valid for both real and complex harmonics. The result can be proven analytically, using the properties of the Poisson kernel in the unit ball, or geometrically by applying a rotation to the vector y so that it points along the z-axis, and then directly calculating the right-hand side. In particular, when x = y, this gives Unsöld's theorem ∑ m = − ℓ ℓ Y ℓ m ∗ ( x ) Y ℓ m ( x ) = 2 ℓ + 1 4 π {\displaystyle \sum _{m=-\ell }^{\ell }Y_{\ell }^{m}{}^{*}(\mathbf {x} )\,Y_{\ell }^{m}(\mathbf {x} )={\frac {2\ell +1}{4\pi }}} which generalizes the identity cos2θ + sin2θ = 1 to two dimensions. In the expansion (1), the left-hand side P ℓ ( x ⋅ y ) {\displaystyle P_{\ell }(\mathbf {x} \cdot \mathbf {y} )} is a constant multiple of the degree ℓ zonal spherical harmonic. From this perspective, one has the following generalization to higher dimensions. Let Yj be an arbitrary orthonormal basis of the space Hℓ of degree ℓ spherical harmonics on the n-sphere. Then Z x ( ℓ ) {\displaystyle Z_{\mathbf {x} }^{(\ell )}} , the degree ℓ zonal harmonic corresponding to the unit vector x, decomposes as Furthermore, the zonal harmonic Z x ( ℓ ) ( y ) {\displaystyle Z_{\mathbf {x} }^{(\ell )}({\mathbf {y} })} is given as a constant multiple of the appropriate Gegenbauer polynomial: Combining (2) and (3) gives (1) in dimension n = 2 when x and y are represented in spherical coordinates. Finally, evaluating at x = y gives the functional identity dim ⁡ H ℓ ω n − 1 = ∑ j = 1 dim ⁡ ( H ℓ ) | Y j ( x ) | 2 {\displaystyle {\frac {\dim \mathbf {H} _{\ell }}{\omega _{n-1}}}=\sum _{j=1}^{\dim(\mathbf {H} _{\ell })}|Y_{j}({\mathbf {x} })|^{2}} where ωn−1 is the volume of the (n−1)-sphere. Contraction rule Another useful identity expresses the product of two spherical harmonics as a sum over spherical harmonics Y a α ( θ , φ ) Y b β ( θ , φ ) = ( 2 a + 1 ) ( 2 b + 1 ) 4 π ∑ c = 0 ∞ ∑ γ = − c c ( − 1 ) γ 2 c + 1 ( a b c α β − γ ) ( a b c 0 0 0 ) Y c γ ( θ , φ ) . {\displaystyle Y_{a}^{\alpha }\left(\theta ,\varphi \right)Y_{b}^{\beta }\left(\theta ,\varphi \right)={\sqrt {\frac {\left(2a+1\right)\left(2b+1\right)}{4\pi }}}\sum _{c=0}^{\infty }\sum _{\gamma =-c}^{c}\left(-1\right)^{\gamma }{\sqrt {2c+1}}{\begin{pmatrix}a&b&c\\\alpha &\beta &-\gamma \end{pmatrix}}{\begin{pmatrix}a&b&c\\0&0&0\end{pmatrix}}Y_{c}^{\gamma }\left(\theta ,\varphi \right).} Many of the terms in this sum are trivially zero. The values of c {\displaystyle c} and γ {\displaystyle \gamma } that result in non-zero terms in this sum are determined by the selection rules for the 3j-symbols. Clebsch–Gordan coefficients The Clebsch–Gordan coefficients are the coefficients appearing in the expansion of the product of two spherical harmonics in terms of spherical harmonics themselves. A variety of techniques are available for doing essentially the same calculation, including the Wigner 3-jm symbol, the Racah coefficients, and the Slater integrals. Abstractly, the Clebsch–Gordan coefficients express the tensor product of two irreducible representations of the rotation group as a sum of irreducible representations: suitably normalized, the coefficients are then the multiplicities. Visualization of the spherical harmonics The Laplace spherical harmonics Y ℓ m {\displaystyle Y_{\ell }^{m}} can be visualized by considering their "nodal lines", that is, the set of points on the sphere where ℜ [ Y ℓ m ] = 0 {\displaystyle \Re [Y_{\ell }^{m}]=0} , or alternatively where ℑ [ Y ℓ m ] = 0 {\displaystyle \Im [Y_{\ell }^{m}]=0} . Nodal lines of Y ℓ m {\displaystyle Y_{\ell }^{m}} are composed of ℓ circles: there are |m| circles along longitudes and ℓ−|m| circles along latitudes. One can determine the number of nodal lines of each type by counting the number of zeros of Y ℓ m {\displaystyle Y_{\ell }^{m}} in the θ {\displaystyle \theta } and φ {\displaystyle \varphi } directions respectively. Considering Y ℓ m {\displaystyle Y_{\ell }^{m}} as a function of θ {\displaystyle \theta } , the real and imaginary components of the associated Legendre polynomials each possess ℓ−|m| zeros, each giving rise to a nodal 'line of latitude'. On the other hand, considering Y ℓ m {\displaystyle Y_{\ell }^{m}} as a function of φ {\displaystyle \varphi } , the trigonometric sin and cos functions possess 2|m| zeros, each of which gives rise to a nodal 'line of longitude'. When the spherical harmonic order m is zero (upper-left in the figure), the spherical harmonic functions do not depend upon longitude, and are referred to as zonal. Such spherical harmonics are a special case of zonal spherical functions. When ℓ = |m| (bottom-right in the figure), there are no zero crossings in latitude, and the functions are referred to as sectoral. For the other cases, the functions checker the sphere, and they are referred to as tesseral. More general spherical harmonics of degree ℓ are not necessarily those of the Laplace basis Y ℓ m {\displaystyle Y_{\ell }^{m}} , and their nodal sets can be of a fairly general kind. List of spherical harmonics Analytic expressions for the first few orthonormalized Laplace spherical harmonics Y ℓ m : S 2 → C {\displaystyle Y_{\ell }^{m}:S^{2}\to \mathbb {C} } that use the Condon–Shortley phase convention: Y 0 0 ( θ , φ ) = 1 2 1 π {\displaystyle Y_{0}^{0}(\theta ,\varphi )={\frac {1}{2}}{\sqrt {\frac {1}{\pi }}}} Y 1 − 1 ( θ , φ ) = 1 2 3 2 π sin ⁡ θ e − i φ Y 1 0 ( θ , φ ) = 1 2 3 π cos ⁡ θ Y 1 1 ( θ , φ ) = − 1 2 3 2 π sin ⁡ θ e i φ {\displaystyle {\begin{aligned}Y_{1}^{-1}(\theta ,\varphi )&={\frac {1}{2}}{\sqrt {\frac {3}{2\pi }}}\,\sin \theta \,e^{-i\varphi }\\Y_{1}^{0}(\theta ,\varphi )&={\frac {1}{2}}{\sqrt {\frac {3}{\pi }}}\,\cos \theta \\Y_{1}^{1}(\theta ,\varphi )&={\frac {-1}{2}}{\sqrt {\frac {3}{2\pi }}}\,\sin \theta \,e^{i\varphi }\end{aligned}}} Y 2 − 2 ( θ , φ ) = 1 4 15 2 π sin 2 ⁡ θ e − 2 i φ Y 2 − 1 ( θ , φ ) = 1 2 15 2 π sin ⁡ θ cos ⁡ θ e − i φ Y 2 0 ( θ , φ ) = 1 4 5 π ( 3 cos 2 ⁡ θ − 1 ) Y 2 1 ( θ , φ ) = − 1 2 15 2 π sin ⁡ θ cos ⁡ θ e i φ Y 2 2 ( θ , φ ) = 1 4 15 2 π sin 2 ⁡ θ e 2 i φ {\displaystyle {\begin{aligned}Y_{2}^{-2}(\theta ,\varphi )&={\frac {1}{4}}{\sqrt {\frac {15}{2\pi }}}\,\sin ^{2}\theta \,e^{-2i\varphi }\\Y_{2}^{-1}(\theta ,\varphi )&={\frac {1}{2}}{\sqrt {\frac {15}{2\pi }}}\,\sin \theta \,\cos \theta \,e^{-i\varphi }\\Y_{2}^{0}(\theta ,\varphi )&={\frac {1}{4}}{\sqrt {\frac {5}{\pi }}}\,(3\cos ^{2}\theta -1)\\Y_{2}^{1}(\theta ,\varphi )&={\frac {-1}{2}}{\sqrt {\frac {15}{2\pi }}}\,\sin \theta \,\cos \theta \,e^{i\varphi }\\Y_{2}^{2}(\theta ,\varphi )&={\frac {1}{4}}{\sqrt {\frac {15}{2\pi }}}\,\sin ^{2}\theta \,e^{2i\varphi }\end{aligned}}} Higher dimensions The classical spherical harmonics are defined as complex-valued functions on the unit sphere S 2 {\displaystyle S^{2}} inside three-dimensional Euclidean space R 3 {\displaystyle \mathbb {R} ^{3}} . Spherical harmonics can be generalized to higher-dimensional Euclidean space R n {\displaystyle \mathbb {R} ^{n}} as follows, leading to functions S n − 1 → C {\displaystyle S^{n-1}\to \mathbb {C} } . Let Pℓ denote the space of complex-valued homogeneous polynomials of degree ℓ in n real variables, here considered as functions R n → C {\displaystyle \mathbb {R} ^{n}\to \mathbb {C} } . That is, a polynomial p is in Pℓ provided that for any real λ ∈ R {\displaystyle \lambda \in \mathbb {R} } , one has p ( λ x ) = λ ℓ p ( x ) . {\displaystyle p(\lambda \mathbf {x} )=\lambda ^{\ell }p(\mathbf {x} ).} Let Aℓ denote the subspace of Pℓ consisting of all harmonic polynomials: A ℓ := { p ∈ P ℓ ∣ Δ p = 0 } . {\displaystyle \mathbf {A} _{\ell }:=\{p\in \mathbf {P} _{\ell }\,\mid \,\Delta p=0\}\,.} These are the (regular) solid spherical harmonics. Let Hℓ denote the space of functions on the unit sphere S n − 1 := { x ∈ R n ∣ | x | = 1 } {\displaystyle S^{n-1}:=\{\mathbf {x} \in \mathbb {R} ^{n}\,\mid \,\left|x\right|=1\}} obtained by restriction from Aℓ H ℓ := { f : S n − 1 → C ∣ for some p ∈ A ℓ , f ( x ) = p ( x ) for all x ∈ S n − 1 } . {\displaystyle \mathbf {H} _{\ell }:=\left\{f:S^{n-1}\to \mathbb {C} \,\mid \,{\text{ for some }}p\in \mathbf {A} _{\ell },\,f(\mathbf {x} )=p(\mathbf {x} ){\text{ for all }}\mathbf {x} \in S^{n-1}\right\}.} The following properties hold: The sum of the spaces Hℓ is dense in the set C ( S n − 1 ) {\displaystyle C(S^{n-1})} of continuous functions on S n − 1 {\displaystyle S^{n-1}} with respect to the uniform topology, by the Stone–Weierstrass theorem. As a result, the sum of these spaces is also dense in the space L2(Sn−1) of square-integrable functions on the sphere. Thus every square-integrable function on the sphere decomposes uniquely into a series of spherical harmonics, where the series converges in the L2 sense. For all f ∈ Hℓ, one has Δ S n − 1 f = − ℓ ( ℓ + n − 2 ) f . {\displaystyle \Delta _{S^{n-1}}f=-\ell (\ell +n-2)f.} where ΔSn−1 is the Laplace–Beltrami operator on Sn−1. This operator is the analog of the angular part of the Laplacian in three dimensions; to wit, the Laplacian in n dimensions decomposes as ∇ 2 = r 1 − n ∂ ∂ r r n − 1 ∂ ∂ r + r − 2 Δ S n − 1 = ∂ 2 ∂ r 2 + n − 1 r ∂ ∂ r + r − 2 Δ S n − 1 {\displaystyle \nabla ^{2}=r^{1-n}{\frac {\partial }{\partial r}}r^{n-1}{\frac {\partial }{\partial r}}+r^{-2}\Delta _{S^{n-1}}={\frac {\partial ^{2}}{\partial r^{2}}}+{\frac {n-1}{r}}{\frac {\partial }{\partial r}}+r^{-2}\Delta _{S^{n-1}}} It follows from the Stokes theorem and the preceding property that the spaces Hℓ are orthogonal with respect to the inner product from L2(Sn−1). That is to say, ∫ S n − 1 f g ¯ d Ω = 0 {\displaystyle \int _{S^{n-1}}f{\bar {g}}\,\mathrm {d} \Omega =0} for f ∈ Hℓ and g ∈ Hk for k ≠ ℓ. Conversely, the spaces Hℓ are precisely the eigenspaces of ΔSn−1. In particular, an application of the spectral theorem to the Riesz potential Δ S n − 1 − 1 {\displaystyle \Delta _{S^{n-1}}^{-1}} gives another proof that the spaces Hℓ are pairwise orthogonal and complete in L2(Sn−1). Every homogeneous polynomial p ∈ Pℓ can be uniquely written in the form p ( x ) = p ℓ ( x ) + | x | 2 p ℓ − 2 + ⋯ + { | x | ℓ p 0 ℓ e v e n | x | ℓ − 1 p 1 ( x ) ℓ o d d {\displaystyle p(x)=p_{\ell }(x)+|x|^{2}p_{\ell -2}+\cdots +{\begin{cases}|x|^{\ell }p_{0}&\ell {\rm {\ even}}\\|x|^{\ell -1}p_{1}(x)&\ell {\rm {\ odd}}\end{cases}}} where pj ∈ Aj. In particular, dim ⁡ H ℓ = ( n + ℓ − 1 n − 1 ) − ( n + ℓ − 3 n − 1 ) = ( n + ℓ − 2 n − 2 ) + ( n + ℓ − 3 n − 2 ) . {\displaystyle \dim \mathbf {H} _{\ell }={\binom {n+\ell -1}{n-1}}-{\binom {n+\ell -3}{n-1}}={\binom {n+\ell -2}{n-2}}+{\binom {n+\ell -3}{n-2}}.} An orthogonal basis of spherical harmonics in higher dimensions can be constructed inductively by the method of separation of variables, by solving the Sturm-Liouville problem for the spherical Laplacian Δ S n − 1 = sin 2 − n ⁡ φ ∂ ∂ φ sin n − 2 ⁡ φ ∂ ∂ φ + sin − 2 ⁡ φ Δ S n − 2 {\displaystyle \Delta _{S^{n-1}}=\sin ^{2-n}\varphi {\frac {\partial }{\partial \varphi }}\sin ^{n-2}\varphi {\frac {\partial }{\partial \varphi }}+\sin ^{-2}\varphi \Delta _{S^{n-2}}} where φ is the axial coordinate in a spherical coordinate system on Sn−1. The end result of such a procedure is Y ℓ 1 , … ℓ n − 1 ( θ 1 , … θ n − 1 ) = 1 2 π e i ℓ 1 θ 1 ∏ j = 2 n − 1 j P ¯ ℓ j ℓ j − 1 ( θ j ) {\displaystyle Y_{\ell _{1},\dots \ell _{n-1}}(\theta _{1},\dots \theta _{n-1})={\frac {1}{\sqrt {2\pi }}}e^{i\ell _{1}\theta _{1}}\prod _{j=2}^{n-1}{}_{j}{\bar {P}}_{\ell _{j}}^{\ell _{j-1}}(\theta _{j})} where the indices satisfy |ℓ1| ≤ ℓ2 ≤ ⋯ ≤ ℓn−1 and the eigenvalue is −ℓn−1(ℓn−1 + n−2). The functions in the product are defined in terms of the Legendre function j P ¯ L ℓ ( θ ) = 2 L + j − 1 2 ( L + ℓ + j − 2 ) ! ( L − ℓ ) ! sin 2 − j 2 ⁡ ( θ ) P L + j − 2 2 − ( ℓ + j − 2 2 ) ( cos ⁡ θ ) . {\displaystyle {}_{j}{\bar {P}}_{L}^{\ell }(\theta )={\sqrt {{\frac {2L+j-1}{2}}{\frac {(L+\ell +j-2)!}{(L-\ell )!}}}}\sin ^{\frac {2-j}{2}}(\theta )P_{L+{\frac {j-2}{2}}}^{-\left(\ell +{\frac {j-2}{2}}\right)}(\cos \theta )\,.} Connection with representation theory The space Hℓ of spherical harmonics of degree ℓ is a representation of the symmetry group of rotations around a point (SO(3)) and its double-cover SU(2). Indeed, rotations act on the two-dimensional sphere, and thus also on Hℓ by function composition ψ ↦ ψ ∘ ρ − 1 {\displaystyle \psi \mapsto \psi \circ \rho ^{-1}} for ψ a spherical harmonic and ρ a rotation. The representation Hℓ is an irreducible representation of SO(3). The elements of Hℓ arise as the restrictions to the sphere of elements of Aℓ: harmonic polynomials homogeneous of degree ℓ on three-dimensional Euclidean space R3. By polarization of ψ ∈ Aℓ, there are coefficients ψ i 1 … i ℓ {\displaystyle \psi _{i_{1}\dots i_{\ell }}} symmetric on the indices, uniquely determined by the requirement ψ ( x 1 , … , x n ) = ∑ i 1 … i ℓ ψ i 1 … i ℓ x i 1 ⋯ x i ℓ . {\displaystyle \psi (x_{1},\dots ,x_{n})=\sum _{i_{1}\dots i_{\ell }}\psi _{i_{1}\dots i_{\ell }}x_{i_{1}}\cdots x_{i_{\ell }}.} The condition that ψ be harmonic is equivalent to the assertion that the tensor ψ i 1 … i ℓ {\displaystyle \psi _{i_{1}\dots i_{\ell }}} must be trace free on every pair of indices. Thus as an irreducible representation of SO(3), Hℓ is isomorphic to the space of traceless symmetric tensors of degree ℓ. More generally, the analogous statements hold in higher dimensions: the space Hℓ of spherical harmonics on the n-sphere is the irreducible representation of SO(n+1) corresponding to the traceless symmetric ℓ-tensors. However, whereas every irreducible tensor representation of SO(2) and SO(3) is of this kind, the special orthogonal groups in higher dimensions have additional irreducible representations that do not arise in this manner. The special orthogonal groups have additional spin representations that are not tensor representations, and are typically not spherical harmonics. An exception are the spin representation of SO(3): strictly speaking these are representations of the double cover SU(2) of SO(3). In turn, SU(2) is identified with the group of unit quaternions, and so coincides with the 3-sphere. The spaces of spherical harmonics on the 3-sphere are certain spin representations of SO(3), with respect to the action by quaternionic multiplication. Connection with hemispherical harmonics Spherical harmonics can be separated into two set of functions. One is hemispherical functions (HSH), orthogonal and complete on hemisphere. Another is complementary hemispherical harmonics (CHSH). Generalizations The angle-preserving symmetries of the two-sphere are described by the group of Möbius transformations PSL(2,C). With respect to this group, the sphere is equivalent to the usual Riemann sphere. The group PSL(2,C) is isomorphic to the (proper) Lorentz group, and its action on the two-sphere agrees with the action of the Lorentz group on the celestial sphere in Minkowski space. The analog of the spherical harmonics for the Lorentz group is given by the hypergeometric series; furthermore, the spherical harmonics can be re-expressed in terms of the hypergeometric series, as SO(3) = PSU(2) is a subgroup of PSL(2,C). More generally, hypergeometric series can be generalized to describe the symmetries of any symmetric space; in particular, hypergeometric series can be developed for any Lie group. See also Notes References Cited references Courant, Richard; Hilbert, David (1962), Methods of Mathematical Physics, Volume I, Wiley-Interscience. Edmonds, A.R. (1957), Angular Momentum in Quantum Mechanics, Princeton University Press, Bibcode:1957amqm.book.....E, ISBN 0-691-07912-9 {{citation}}: ISBN / Date incompatibility (help) Eremenko, Alexandre; Jakobson, Dmitry; Nadirashvili, Nikolai (2007), "On nodal sets and nodal domains on S² and R²", Annales de l'Institut Fourier, 57 (7): 2345–2360, doi:10.5802/aif.2335, ISSN 0373-0956, MR 2394544 Hall, Brian C. (2013), Quantum Theory for Mathematicians, Graduate Texts in Mathematics, vol. 267, Springer, Bibcode:2013qtm..book.....H, ISBN 978-1461471158 MacRobert, T.M. (1967), Spherical harmonics: An elementary treatise on harmonic functions, with applications, Pergamon Press. Meijer, Paul Herman Ernst; Bauer, Edmond (2004), Group theory: The application to quantum mechanics, Dover, ISBN 978-0-486-43798-9. Solomentsev, E.D. (2001) [1994], "Spherical harmonics", Encyclopedia of Mathematics, EMS Press. Stein, Elias; Weiss, Guido (1971), Introduction to Fourier Analysis on Euclidean Spaces, Princeton, N.J.: Princeton University Press, ISBN 978-0-691-08078-9. Unsöld, Albrecht (1927), "Beiträge zur Quantenmechanik der Atome", Annalen der Physik, 387 (3): 355–393, Bibcode:1927AnP...387..355U, doi:10.1002/andp.19273870304. Whittaker, E. T.; Watson, G. N. (1927), A Course of Modern Analysis, Cambridge University Press, p. 392. General references E.W. Hobson, The Theory of Spherical and Ellipsoidal Harmonics, (1955) Chelsea Pub. Co., ISBN 978-0-8284-0104-3. C. Müller, Spherical Harmonics, (1966) Springer, Lecture Notes in Mathematics, Vol. 17, ISBN 978-3-540-03600-5. E. U. Condon and G. H. Shortley, The Theory of Atomic Spectra, (1970) Cambridge at the University Press, ISBN 0-521-09209-4, See chapter 3. J.D. Jackson, Classical Electrodynamics, ISBN 0-471-30932-X Albert Messiah, Quantum Mechanics, volume II. (2000) Dover. ISBN 0-486-40924-4. Press, WH; Teukolsky, SA; Vetterling, WT; Flannery, BP (2007), "Section 6.7. Spherical Harmonics", Numerical Recipes: The Art of Scientific Computing (3rd ed.), New York: Cambridge University Press, ISBN 978-0-521-88068-8 D. A. Varshalovich, A. N. Moskalev, V. K. Khersonskii Quantum Theory of Angular Momentum,(1988) World Scientific Publishing Co., Singapore, ISBN 9971-5-0107-4 Weisstein, Eric W. "Spherical harmonics". MathWorld. Maddock, John, Spherical harmonics in Boost.Math External links Spherical Harmonics at MathWorld Spherical Harmonics 3D representation In mathematics, the Yang–Mills–Higgs equations are a set of non-linear partial differential equations for a Yang–Mills field, given by a connection, and a Higgs field, given by a section of a vector bundle (specifically, the adjoint bundle). These equations are D A ∗ F A + ∗ [ Φ , D A Φ ] = 0 , D A ∗ D A Φ = 0 {\displaystyle {\begin{aligned}D_{A}*F_{A}+*[\Phi ,D_{A}\Phi ]&=0,\\D_{A}*D_{A}\Phi &=0\end{aligned}}} with a boundary condition lim | x | → ∞ | Φ | ( x ) = 1 {\displaystyle \lim _{|x|\rightarrow \infty }|\Phi |(x)=1} where A is a connection on a vector bundle, DA is the exterior covariant derivative, FA is the curvature of that connection, Φ is a section of that vector bundle, ∗ is the Hodge star, and [·,·] is the natural, graded bracket. These equations are named after Chen Ning Yang, Robert Mills, and Peter Higgs. They are very closely related to the Ginzburg–Landau equations, when these are expressed in a general geometric setting. M.V. Goganov and L.V. Kapitanskii have shown that the Cauchy problem for hyperbolic Yang–Mills–Higgs equations in Hamiltonian gauge on 4-dimensional Minkowski space have a unique global solution with no restrictions at the spatial infinity. Furthermore, the solution has the finite propagation speed property. Lagrangian The equations arise as the equations of motion of the Lagrangian density where ⟨ ⋅ , ⋅ ⟩ {\displaystyle \langle \cdot ,\cdot \rangle } is an invariant symmetric bilinear form on the adjoint bundle. This is sometimes written as tr {\displaystyle {\text{tr}}} due to the fact that such a form can arise from the trace on g {\displaystyle {\mathfrak {g}}} under some representation; in particular here we are concerned with the adjoint representation, and the trace on this representation is the Killing form. For the particular form of the Yang–Mills–Higgs equations given above, the potential V ( ϕ ) {\displaystyle V(\phi )} is vanishing. Another common choice is V ( ϕ ) = 1 2 m 2 ⟨ ϕ , ϕ ⟩ {\displaystyle V(\phi )={\frac {1}{2}}m^{2}\langle \phi ,\phi \rangle } , corresponding to a massive Higgs field. This theory is a particular case of scalar chromodynamics where the Higgs field ϕ {\displaystyle \phi } is valued in the adjoint representation as opposed to a general representation. See also Yang–Mills equations Stable Yang–Mills–Higgs pair Yang–Mills–Higgs flow Scalar chromodynamics References M.V. Goganov and L.V. Kapitansii, "Global solvability of the initial problem for Yang-Mills-Higgs equations", Zapiski LOMI 147,18–48, (1985); J. Sov. Math, 37, 802–822 (1987). The Schrödinger equation is a partial differential equation that governs the wave function of a non-relativistic quantum-mechanical system.: 1–2 Its discovery was a significant landmark in the development of quantum mechanics. It is named after Erwin Schrödinger, an Austrian physicist, who postulated the equation in 1925 and published it in 1926, forming the basis for the work that resulted in his Nobel Prize in Physics in 1933. Conceptually, the Schrödinger equation is the quantum counterpart of Newton's second law in classical mechanics. Given a set of known initial conditions, Newton's second law makes a mathematical prediction as to what path a given physical system will take over time. The Schrödinger equation gives the evolution over time of the wave function, the quantum-mechanical characterization of an isolated physical system. The equation was postulated by Schrödinger based on a postulate of Louis de Broglie that all matter has an associated matter wave. The equation predicted bound states of the atom in agreement with experimental observations.: II:268 The Schrödinger equation is not the only way to study quantum mechanical systems and make predictions. Other formulations of quantum mechanics include matrix mechanics, introduced by Werner Heisenberg, and the path integral formulation, developed chiefly by Richard Feynman. When these approaches are compared, the use of the Schrödinger equation is sometimes called "wave mechanics". The equation given by Schrödinger is nonrelativistic because it contains a first derivative in time and a second derivative in space, and therefore space and time are not on equal footing. Paul Dirac incorporated special relativity and quantum mechanics into a single formulation that simplifies to the Schrödinger equation in the non-relativistic limit. This is the Dirac equation, which contains a single derivative in both space and time. Another partial differential equation, the Klein–Gordon equation, led to a problem with probability density even though it was a relativistic wave equation. The probability density could be negative, which is physically unviable. This was fixed by Dirac by taking the so-called square root of the Klein–Gordon operator and in turn introducing Dirac matrices. In a modern context, the Klein–Gordon equation describes spin-less particles, while the Dirac equation describes spin-1/2 particles. Definition Preliminaries Introductory courses on physics or chemistry typically introduce the Schrödinger equation in a way that can be appreciated knowing only the concepts and notations of basic calculus, particularly derivatives with respect to space and time. A special case of the Schrödinger equation that admits a statement in those terms is the position-space Schrödinger equation for a single nonrelativistic particle in one dimension: i ℏ ∂ ∂ t Ψ ( x , t ) = [ − ℏ 2 2 m ∂ 2 ∂ x 2 + V ( x , t ) ] Ψ ( x , t ) . {\displaystyle i\hbar {\frac {\partial }{\partial t}}\Psi (x,t)=\left[-{\frac {\hbar ^{2}}{2m}}{\frac {\partial ^{2}}{\partial x^{2}}}+V(x,t)\right]\Psi (x,t).} Here, Ψ ( x , t ) {\displaystyle \Psi (x,t)} is a wave function, a function that assigns a complex number to each point x {\displaystyle x} at each time t {\displaystyle t} . The parameter m {\displaystyle m} is the mass of the particle, and V ( x , t ) {\displaystyle V(x,t)} is the potential energy function that represents the environment in which the particle exists.: 74 The constant i {\displaystyle i} is the imaginary unit, and ℏ {\displaystyle \hbar } is the reduced Planck constant, which has units of action (energy multiplied by time).: 10 Broadening beyond this simple case, the mathematical formulation of quantum mechanics developed by Paul Dirac, David Hilbert, John von Neumann, and Hermann Weyl defines the state of a quantum mechanical system to be a vector | ψ ⟩ {\displaystyle |\psi \rangle } belonging to a separable complex Hilbert space H {\displaystyle {\mathcal {H}}} . This vector is postulated to be normalized under the Hilbert space's inner product, that is, in Dirac notation it obeys ⟨ ψ | ψ ⟩ = 1 {\displaystyle \langle \psi |\psi \rangle =1} . The exact nature of this Hilbert space is dependent on the system – for example, for describing position and momentum the Hilbert space is the space of square-integrable functions L 2 {\displaystyle L^{2}} , while the Hilbert space for the spin of a single proton is the two-dimensional complex vector space C 2 {\displaystyle \mathbb {C} ^{2}} with the usual inner product.: 322 Physical quantities of interest – position, momentum, energy, spin – are represented by observables, which are self-adjoint operators acting on the Hilbert space. A wave function can be an eigenvector of an observable, in which case it is called an eigenstate, and the associated eigenvalue corresponds to the value of the observable in that eigenstate. More generally, a quantum state will be a linear combination of the eigenstates, known as a quantum superposition. When an observable is measured, the result will be one of its eigenvalues with probability given by the Born rule: in the simplest case the eigenvalue λ {\displaystyle \lambda } is non-degenerate and the probability is given by | ⟨ λ | ψ ⟩ | 2 {\displaystyle |\langle \lambda |\psi \rangle |^{2}} , where | λ ⟩ {\displaystyle |\lambda \rangle } is its associated eigenvector. More generally, the eigenvalue is degenerate and the probability is given by ⟨ ψ | P λ | ψ ⟩ {\displaystyle \langle \psi |P_{\lambda }|\psi \rangle } , where P λ {\displaystyle P_{\lambda }} is the projector onto its associated eigenspace. A momentum eigenstate would be a perfectly monochromatic wave of infinite extent, which is not square-integrable. Likewise a position eigenstate would be a Dirac delta distribution, not square-integrable and technically not a function at all. Consequently, neither can belong to the particle's Hilbert space. Physicists sometimes regard these eigenstates, composed of elements outside the Hilbert space, as "generalized eigenvectors". These are used for calculational convenience and do not represent physical states.: 100–105 Thus, a position-space wave function Ψ ( x , t ) {\displaystyle \Psi (x,t)} as used above can be written as the inner product of a time-dependent state vector | Ψ ( t ) ⟩ {\displaystyle |\Psi (t)\rangle } with unphysical but convenient "position eigenstates" | x ⟩ {\displaystyle |x\rangle } : Ψ ( x , t ) = ⟨ x | Ψ ( t ) ⟩ . {\displaystyle \Psi (x,t)=\langle x|\Psi (t)\rangle .} Time-dependent equation The form of the Schrödinger equation depends on the physical situation. The most general form is the time-dependent Schrödinger equation, which gives a description of a system evolving with time:: 143 where t {\displaystyle t} is time, | Ψ ( t ) ⟩ {\displaystyle \vert \Psi (t)\rangle } is the state vector of the quantum system ( Ψ {\displaystyle \Psi } being the Greek letter psi), and H ^ {\displaystyle {\hat {H}}} is an observable, the Hamiltonian operator. The term "Schrödinger equation" can refer to both the general equation, or the specific nonrelativistic version. The general equation is indeed quite general, used throughout quantum mechanics, for everything from the Dirac equation to quantum field theory, by plugging in diverse expressions for the Hamiltonian. The specific nonrelativistic version is an approximation that yields accurate results in many situations, but only to a certain extent (see relativistic quantum mechanics and relativistic quantum field theory). To apply the Schrödinger equation, write down the Hamiltonian for the system, accounting for the kinetic and potential energies of the particles constituting the system, then insert it into the Schrödinger equation. The resulting partial differential equation is solved for the wave function, which contains information about the system. In practice, the square of the absolute value of the wave function at each point is taken to define a probability density function.: 78 For example, given a wave function in position space Ψ ( x , t ) {\displaystyle \Psi (x,t)} as above, we have Pr ( x , t ) = | Ψ ( x , t ) | 2 . {\displaystyle \Pr(x,t)=|\Psi (x,t)|^{2}.} Time-independent equation The time-dependent Schrödinger equation described above predicts that wave functions can form standing waves, called stationary states. These states are particularly important as their individual study later simplifies the task of solving the time-dependent Schrödinger equation for any state. Stationary states can also be described by a simpler form of the Schrödinger equation, the time-independent Schrödinger equation. where E {\displaystyle E} is the energy of the system.: 134 This is only used when the Hamiltonian itself is not dependent on time explicitly. However, even in this case the total wave function is dependent on time as explained in the section on linearity below. In the language of linear algebra, this equation is an eigenvalue equation. Therefore, the wave function is an eigenfunction of the Hamiltonian operator with corresponding eigenvalue(s) E {\displaystyle E} . Properties Linearity The Schrödinger equation is a linear differential equation, meaning that if two state vectors | ψ 1 ⟩ {\displaystyle |\psi _{1}\rangle } and | ψ 2 ⟩ {\displaystyle |\psi _{2}\rangle } are solutions, then so is any linear combination | ψ ⟩ = a | ψ 1 ⟩ + b | ψ 2 ⟩ {\displaystyle |\psi \rangle =a|\psi _{1}\rangle +b|\psi _{2}\rangle } of the two state vectors where a and b are any complex numbers.: 25 Moreover, the sum can be extended for any number of state vectors. This property allows superpositions of quantum states to be solutions of the Schrödinger equation. Even more generally, it holds that a general solution to the Schrödinger equation can be found by taking a weighted sum over a basis of states. A choice often employed is the basis of energy eigenstates, which are solutions of the time-independent Schrödinger equation. In this basis, a time-dependent state vector | Ψ ( t ) ⟩ {\displaystyle |\Psi (t)\rangle } can be written as the linear combination | Ψ ( t ) ⟩ = ∑ n A n e − i E n t / ℏ | ψ E n ⟩ , {\displaystyle |\Psi (t)\rangle =\sum _{n}A_{n}e^{{-iE_{n}t}/\hbar }|\psi _{E_{n}}\rangle ,} where A n {\displaystyle A_{n}} are complex numbers and the vectors | ψ E n ⟩ {\displaystyle |\psi _{E_{n}}\rangle } are solutions of the time-independent equation H ^ | ψ E n ⟩ = E n | ψ E n ⟩ {\displaystyle {\hat {H}}|\psi _{E_{n}}\rangle =E_{n}|\psi _{E_{n}}\rangle } . Unitarity Holding the Hamiltonian H ^ {\displaystyle {\hat {H}}} constant, the Schrödinger equation has the solution | Ψ ( t ) ⟩ = e − i H ^ t / ℏ | Ψ ( 0 ) ⟩ . {\displaystyle |\Psi (t)\rangle =e^{-i{\hat {H}}t/\hbar }|\Psi (0)\rangle .} The operator U ^ ( t ) = e − i H ^ t / ℏ {\displaystyle {\hat {U}}(t)=e^{-i{\hat {H}}t/\hbar }} is known as the time-evolution operator, and it is unitary: it preserves the inner product between vectors in the Hilbert space. Unitarity is a general feature of time evolution under the Schrödinger equation. If the initial state is | Ψ ( 0 ) ⟩ {\displaystyle |\Psi (0)\rangle } , then the state at a later time t {\displaystyle t} will be given by | Ψ ( t ) ⟩ = U ^ ( t ) | Ψ ( 0 ) ⟩ {\displaystyle |\Psi (t)\rangle ={\hat {U}}(t)|\Psi (0)\rangle } for some unitary operator U ^ ( t ) {\displaystyle {\hat {U}}(t)} . Conversely, suppose that U ^ ( t ) {\displaystyle {\hat {U}}(t)} is a continuous family of unitary operators parameterized by t {\displaystyle t} . Without loss of generality, the parameterization can be chosen so that U ^ ( 0 ) {\displaystyle {\hat {U}}(0)} is the identity operator and that U ^ ( t / N ) N = U ^ ( t ) {\displaystyle {\hat {U}}(t/N)^{N}={\hat {U}}(t)} for any N > 0 {\displaystyle N>0} . Then U ^ ( t ) {\displaystyle {\hat {U}}(t)} depends upon the parameter t {\displaystyle t} in such a way that U ^ ( t ) = e − i G ^ t {\displaystyle {\hat {U}}(t)=e^{-i{\hat {G}}t}} for some self-adjoint operator G ^ {\displaystyle {\hat {G}}} , called the generator of the family U ^ ( t ) {\displaystyle {\hat {U}}(t)} . A Hamiltonian is just such a generator (up to the factor of the Planck constant that would be set to 1 in natural units). To see that the generator is Hermitian, note that with U ^ ( δ t ) ≈ U ^ ( 0 ) − i G ^ δ t {\displaystyle {\hat {U}}(\delta t)\approx {\hat {U}}(0)-i{\hat {G}}\delta t} , we have U ^ ( δ t ) † U ^ ( δ t ) ≈ ( U ^ ( 0 ) † + i G ^ † δ t ) ( U ^ ( 0 ) − i G ^ δ t ) = I + i δ t ( G ^ † − G ^ ) + O ( δ t 2 ) , {\displaystyle {\hat {U}}(\delta t)^{\dagger }{\hat {U}}(\delta t)\approx ({\hat {U}}(0)^{\dagger }+i{\hat {G}}^{\dagger }\delta t)({\hat {U}}(0)-i{\hat {G}}\delta t)=I+i\delta t({\hat {G}}^{\dagger }-{\hat {G}})+O(\delta t^{2}),} so U ^ ( t ) {\displaystyle {\hat {U}}(t)} is unitary only if, to first order, its derivative is Hermitian. Changes of basis The Schrödinger equation is often presented using quantities varying as functions of position, but as a vector-operator equation it has a valid representation in any arbitrary complete basis of kets in Hilbert space. As mentioned above, "bases" that lie outside the physical Hilbert space are also employed for calculational purposes. This is illustrated by the position-space and momentum-space Schrödinger equations for a nonrelativistic, spinless particle.: 182 The Hilbert space for such a particle is the space of complex square-integrable functions on three-dimensional Euclidean space, and its Hamiltonian is the sum of a kinetic-energy term that is quadratic in the momentum operator and a potential-energy term: i ℏ d d t | Ψ ( t ) ⟩ = ( 1 2 m p ^ 2 + V ^ ) | Ψ ( t ) ⟩ . {\displaystyle i\hbar {\frac {d}{dt}}|\Psi (t)\rangle =\left({\frac {1}{2m}}{\hat {p}}^{2}+{\hat {V}}\right)|\Psi (t)\rangle .} Writing r {\displaystyle \mathbf {r} } for a three-dimensional position vector and p {\displaystyle \mathbf {p} } for a three-dimensional momentum vector, the position-space Schrödinger equation is i ℏ ∂ ∂ t Ψ ( r , t ) = − ℏ 2 2 m ∇ 2 Ψ ( r , t ) + V ( r ) Ψ ( r , t ) . {\displaystyle i\hbar {\frac {\partial }{\partial t}}\Psi (\mathbf {r} ,t)=-{\frac {\hbar ^{2}}{2m}}\nabla ^{2}\Psi (\mathbf {r} ,t)+V(\mathbf {r} )\Psi (\mathbf {r} ,t).} The momentum-space counterpart involves the Fourier transforms of the wave function and the potential: i ℏ ∂ ∂ t Ψ ~ ( p , t ) = p 2 2 m Ψ ~ ( p , t ) + ( 2 π ℏ ) − 3 / 2 ∫ d 3 p ′ V ~ ( p − p ′ ) Ψ ~ ( p ′ , t ) . {\displaystyle i\hbar {\frac {\partial }{\partial t}}{\tilde {\Psi }}(\mathbf {p} ,t)={\frac {\mathbf {p} ^{2}}{2m}}{\tilde {\Psi }}(\mathbf {p} ,t)+(2\pi \hbar )^{-3/2}\int d^{3}\mathbf {p} '\,{\tilde {V}}(\mathbf {p} -\mathbf {p} '){\tilde {\Psi }}(\mathbf {p} ',t).} The functions Ψ ( r , t ) {\displaystyle \Psi (\mathbf {r} ,t)} and Ψ ~ ( p , t ) {\displaystyle {\tilde {\Psi }}(\mathbf {p} ,t)} are derived from | Ψ ( t ) ⟩ {\displaystyle |\Psi (t)\rangle } by Ψ ( r , t ) = ⟨ r | Ψ ( t ) ⟩ , {\displaystyle \Psi (\mathbf {r} ,t)=\langle \mathbf {r} |\Psi (t)\rangle ,} Ψ ~ ( p , t ) = ⟨ p | Ψ ( t ) ⟩ , {\displaystyle {\tilde {\Psi }}(\mathbf {p} ,t)=\langle \mathbf {p} |\Psi (t)\rangle ,} where | r ⟩ {\displaystyle |\mathbf {r} \rangle } and | p ⟩ {\displaystyle |\mathbf {p} \rangle } do not belong to the Hilbert space itself, but have well-defined inner products with all elements of that space. When restricted from three dimensions to one, the position-space equation is just the first form of the Schrödinger equation given above. The relation between position and momentum in quantum mechanics can be appreciated in a single dimension. In canonical quantization, the classical variables x {\displaystyle x} and p {\displaystyle p} are promoted to self-adjoint operators x ^ {\displaystyle {\hat {x}}} and p ^ {\displaystyle {\hat {p}}} that satisfy the canonical commutation relation [ x ^ , p ^ ] = i ℏ . {\displaystyle [{\hat {x}},{\hat {p}}]=i\hbar .} This implies that: 190 ⟨ x | p ^ | Ψ ⟩ = − i ℏ d d x Ψ ( x ) , {\displaystyle \langle x|{\hat {p}}|\Psi \rangle =-i\hbar {\frac {d}{dx}}\Psi (x),} so the action of the momentum operator p ^ {\displaystyle {\hat {p}}} in the position-space representation is − i ℏ d d x {\textstyle -i\hbar {\frac {d}{dx}}} . Thus, p ^ 2 {\displaystyle {\hat {p}}^{2}} becomes a second derivative, and in three dimensions, the second derivative becomes the Laplacian ∇ 2 {\displaystyle \nabla ^{2}} . The canonical commutation relation also implies that the position and momentum operators are Fourier conjugates of each other. Consequently, functions originally defined in terms of their position dependence can be converted to functions of momentum using the Fourier transform.: 103–104 In solid-state physics, the Schrödinger equation is often written for functions of momentum, as Bloch's theorem ensures the periodic crystal lattice potential couples Ψ ~ ( p ) {\displaystyle {\tilde {\Psi }}(p)} with Ψ ~ ( p + ℏ K ) {\displaystyle {\tilde {\Psi }}(p+\hbar K)} for only discrete reciprocal lattice vectors K {\displaystyle K} . This makes it convenient to solve the momentum-space Schrödinger equation at each point in the Brillouin zone independently of the other points in the Brillouin zone.: 138 Probability current The Schrödinger equation is consistent with local probability conservation.: 238 It also ensures that a normalized wavefunction remains normalized after time evolution. In matrix mechanics, this means that the time evolution operator is a unitary operator. In contrast to, for example, the Klein Gordon equation, although a redefined inner product of a wavefunction can be time independent, the total volume integral of modulus square of the wavefunction need not be time independent. The continuity equation for probability in non relativistic quantum mechanics is stated as: ∂ ∂ t ρ ( r , t ) + ∇ ⋅ j = 0 , {\displaystyle {\frac {\partial }{\partial t}}\rho \left(\mathbf {r} ,t\right)+\nabla \cdot \mathbf {j} =0,} where j = 1 2 m ( Ψ ∗ p ^ Ψ − Ψ p ^ Ψ ∗ ) = − i ℏ 2 m ( ψ ∗ ∇ ψ − ψ ∇ ψ ∗ ) = ℏ m Im ⁡ ( ψ ∗ ∇ ψ ) {\displaystyle \mathbf {j} ={\frac {1}{2m}}\left(\Psi ^{*}{\hat {\mathbf {p} }}\Psi -\Psi {\hat {\mathbf {p} }}\Psi ^{*}\right)=-{\frac {i\hbar }{2m}}(\psi ^{*}\nabla \psi -\psi \nabla \psi ^{*})={\frac {\hbar }{m}}\operatorname {Im} (\psi ^{*}\nabla \psi )} is the probability current or probability flux (flow per unit area). If the wavefunction is represented as ψ ( x , t ) = ρ ( x , t ) exp ⁡ ( i S ( x , t ) ℏ ) , {\textstyle \psi ({\bf {x}},t)={\sqrt {\rho ({\bf {x}},t)}}\exp \left({\frac {iS({\bf {x}},t)}{\hbar }}\right),} where S ( x , t ) {\displaystyle S(\mathbf {x} ,t)} is a real function which represents the complex phase of the wavefunction, then the probability flux is calculated as: j = ρ ∇ S m {\displaystyle \mathbf {j} ={\frac {\rho \nabla S}{m}}} Hence, the spatial variation of the phase of a wavefunction is said to characterize the probability flux of the wavefunction. Although the ∇ S m {\textstyle {\frac {\nabla S}{m}}} term appears to play the role of velocity, it does not represent velocity at a point since simultaneous measurement of position and velocity violates uncertainty principle. Separation of variables If the Hamiltonian is not an explicit function of time, Schrödinger's equation reads: i ℏ ∂ ∂ t Ψ ( r , t ) = [ − ℏ 2 2 m ∇ 2 + V ( r ) ] Ψ ( r , t ) . {\displaystyle i\hbar {\frac {\partial }{\partial t}}\Psi (\mathbf {r} ,t)=\left[-{\frac {\hbar ^{2}}{2m}}\nabla ^{2}+V(\mathbf {r} )\right]\Psi (\mathbf {r} ,t).} The operator on the left side depends only on time; the one on the right side depends only on space. Solving the equation by separation of variables means seeking a solution of the form of a product of spatial and temporal parts Ψ ( r , t ) = ψ ( r ) τ ( t ) , {\displaystyle \Psi (\mathbf {r} ,t)=\psi (\mathbf {r} )\tau (t),} where ψ ( r ) {\displaystyle \psi (\mathbf {r} )} is a function of all the spatial coordinate(s) of the particle(s) constituting the system only, and τ ( t ) {\displaystyle \tau (t)} is a function of time only. Substituting this expression for Ψ {\displaystyle \Psi } into the time dependent left hand side shows that τ ( t ) {\displaystyle \tau (t)} is a phase factor: Ψ ( r , t ) = ψ ( r ) e − i E t / ℏ . {\displaystyle \Psi (\mathbf {r} ,t)=\psi (\mathbf {r} )e^{-i{Et/\hbar }}.} A solution of this type is called stationary, since the only time dependence is a phase factor that cancels when the probability density is calculated via the Born rule.: 143ff The spatial part of the full wave function solves the equation ∇ 2 ψ ( r ) + 2 m ℏ 2 [ E − V ( r ) ] ψ ( r ) = 0 , {\displaystyle \nabla ^{2}\psi (\mathbf {r} )+{\frac {2m}{\hbar ^{2}}}\left[E-V(\mathbf {r} )\right]\psi (\mathbf {r} )=0,} where the energy E {\displaystyle E} appears in the phase factor. This generalizes to any number of particles in any number of dimensions (in a time-independent potential): the standing wave solutions of the time-independent equation are the states with definite energy, instead of a probability distribution of different energies. In physics, these standing waves are called "stationary states" or "energy eigenstates"; in chemistry they are called "atomic orbitals" or "molecular orbitals". Superpositions of energy eigenstates change their properties according to the relative phases between the energy levels. The energy eigenstates form a basis: any wave function may be written as a sum over the discrete energy states or an integral over continuous energy states, or more generally as an integral over a measure. This is an example of the spectral theorem, and in a finite-dimensional state space it is just a statement of the completeness of the eigenvectors of a Hermitian matrix. Separation of variables can also be a useful method for the time-independent Schrödinger equation. For example, depending on the symmetry of the problem, the Cartesian axes might be separated, as in ψ ( r ) = ψ x ( x ) ψ y ( y ) ψ z ( z ) , {\displaystyle \psi (\mathbf {r} )=\psi _{x}(x)\psi _{y}(y)\psi _{z}(z),} or radial and angular coordinates might be separated: ψ ( r ) = ψ r ( r ) ψ θ ( θ ) ψ ϕ ( ϕ ) . {\displaystyle \psi (\mathbf {r} )=\psi _{r}(r)\psi _{\theta }(\theta )\psi _{\phi }(\phi ).} Examples Particle in a box The particle in a one-dimensional potential energy box is the most mathematically simple example where restraints lead to the quantization of energy levels. The box is defined as having zero potential energy inside a certain region and infinite potential energy outside.: 77–78 For the one-dimensional case in the x {\displaystyle x} direction, the time-independent Schrödinger equation may be written − ℏ 2 ψ 2 m x 2 d 2 ψ d x 2 = E ψ . {\displaystyle -{\frac {\hbar ^{2}{\vphantom {\psi }}}{2m{\vphantom {x^{2}}}}}{\frac {d^{2}\psi }{dx^{2}}}=E\psi .} With the differential operator defined by p ^ x = − i ℏ d d x {\displaystyle {\hat {p}}_{x}=-i\hbar {\frac {d}{dx}}} the previous equation is evocative of the classic kinetic energy analogue, 1 2 m p ^ x 2 = E , {\displaystyle {\frac {1}{2m}}{\hat {p}}_{x}^{2}=E,} with state ψ {\displaystyle \psi } in this case having energy E {\displaystyle E} coincident with the kinetic energy of the particle. The general solutions of the Schrödinger equation for the particle in a box are ψ ( x ) = A e i k x + B e − i k x E = ℏ 2 k 2 2 m {\displaystyle \psi (x)=Ae^{ikx}+Be^{-ikx}\qquad \qquad E={\frac {\hbar ^{2}k^{2}}{2m}}} or, from Euler's formula, ψ ( x ) = C sin ⁡ ( k x ) + D cos ⁡ ( k x ) . {\displaystyle \psi (x)=C\sin(kx)+D\cos(kx).} The infinite potential walls of the box determine the values of C , D , {\displaystyle C,D,} and k {\displaystyle k} at x = 0 {\displaystyle x=0} and x = L {\displaystyle x=L} where ψ {\displaystyle \psi } must be zero. Thus, at x = 0 {\displaystyle x=0} , ψ ( 0 ) = 0 = C sin ⁡ ( 0 ) + D cos ⁡ ( 0 ) = D {\displaystyle \psi (0)=0=C\sin(0)+D\cos(0)=D} and D = 0 {\displaystyle D=0} . At x = L {\displaystyle x=L} , ψ ( L ) = 0 = C sin ⁡ ( k L ) , {\displaystyle \psi (L)=0=C\sin(kL),} in which C {\displaystyle C} cannot be zero as this would conflict with the postulate that ψ {\displaystyle \psi } has norm 1. Therefore, since sin ⁡ ( k L ) = 0 {\displaystyle \sin(kL)=0} , k L {\displaystyle kL} must be an integer multiple of π {\displaystyle \pi } , k = n π L n = 1 , 2 , 3 , … . {\displaystyle k={\frac {n\pi }{L}}\qquad \qquad n=1,2,3,\ldots .} This constraint on k {\displaystyle k} implies a constraint on the energy levels, yielding E n = ℏ 2 π 2 n 2 2 m L 2 = n 2 h 2 8 m L 2 . {\displaystyle E_{n}={\frac {\hbar ^{2}\pi ^{2}n^{2}}{2mL^{2}}}={\frac {n^{2}h^{2}}{8mL^{2}}}.} A finite potential well is the generalization of the infinite potential well problem to potential wells having finite depth. The finite potential well problem is mathematically more complicated than the infinite particle-in-a-box problem as the wave function is not pinned to zero at the walls of the well. Instead, the wave function must satisfy more complicated mathematical boundary conditions as it is nonzero in regions outside the well. Another related problem is that of the rectangular potential barrier, which furnishes a model for the quantum tunneling effect that plays an important role in the performance of modern technologies such as flash memory and scanning tunneling microscopy. Harmonic oscillator The Schrödinger equation for this situation is E ψ = − ℏ 2 2 m x 2 d 2 d x 2 ψ + 1 2 x 2 m ω 2 x 2 ψ , {\displaystyle E\psi =-{\frac {\hbar ^{2}}{2m{\vphantom {x^{2}}}}}{\frac {d^{2}}{dx^{2}}}\psi +{\frac {1}{2{\vphantom {x^{2}}}}}m\omega ^{2}x^{2}\psi ,} where x {\displaystyle x} is the displacement and ω {\displaystyle \omega } the angular frequency. Furthermore, it can be used to describe approximately a wide variety of other systems, including vibrating atoms, molecules, and atoms or ions in lattices, and approximating other potentials near equilibrium points. It is also the basis of perturbation methods in quantum mechanics. The solutions in position space are ψ n ( x ) = 1 2 n n ! ( m ω π ℏ ) 1 / 4 e − m ω x 2 2 ℏ H n ( m ω ℏ x ) , {\displaystyle \psi _{n}(x)={\sqrt {\frac {1}{2^{n}\,n!}}}\ \left({\frac {m\omega }{\pi \hbar }}\right)^{1/4}\ e^{-{\frac {m\omega x^{2}}{2\hbar }}}\ {\mathcal {H}}_{n}\left({\sqrt {\frac {m\omega }{\hbar }}}x\right),} where n ∈ { 0 , 1 , 2 , … } {\displaystyle n\in \{0,1,2,\ldots \}} , and the functions H n {\displaystyle {\mathcal {H}}_{n}} are the Hermite polynomials of order n {\displaystyle n} . The solution set may be generated by ψ n ( x ) = 1 n ! ( m ω 2 ℏ ) n ( x − ℏ m ω d d x ) n ( m ω π ℏ ) 1 4 e − m ω x 2 2 ℏ . {\displaystyle \psi _{n}(x)={\frac {1}{\sqrt {n!}}}\left({\sqrt {\frac {m\omega }{2\hbar }}}\right)^{n}\left(x-{\frac {\hbar }{m\omega }}{\frac {d}{dx}}\right)^{n}\left({\frac {m\omega }{\pi \hbar }}\right)^{\frac {1}{4}}e^{\frac {-m\omega x^{2}}{2\hbar }}.} The eigenvalues are E n = ( n + 1 2 ) ℏ ω . {\displaystyle E_{n}=\left(n+{\frac {1}{2}}\right)\hbar \omega .} The case n = 0 {\displaystyle n=0} is called the ground state, its energy is called the zero-point energy, and the wave function is a Gaussian. The harmonic oscillator, like the particle in a box, illustrates the generic feature of the Schrödinger equation that the energies of bound eigenstates are discretized.: 352 Hydrogen atom The Schrödinger equation for the electron in a hydrogen atom (or a hydrogen-like atom) is E ψ = − ℏ 2 2 μ ∇ 2 ψ − q 2 4 π ε 0 r ψ {\displaystyle E\psi =-{\frac {\hbar ^{2}}{2\mu }}\nabla ^{2}\psi -{\frac {q^{2}}{4\pi \varepsilon _{0}r}}\psi } where q {\displaystyle q} is the electron charge, r {\displaystyle \mathbf {r} } is the position of the electron relative to the nucleus, r = | r | {\displaystyle r=|\mathbf {r} |} is the magnitude of the relative position, the potential term is due to the Coulomb interaction, wherein ε 0 {\displaystyle \varepsilon _{0}} is the permittivity of free space and μ = m q m p m q + m p {\displaystyle \mu ={\frac {m_{q}m_{p}}{m_{q}+m_{p}}}} is the 2-body reduced mass of the hydrogen nucleus (just a proton) of mass m p {\displaystyle m_{p}} and the electron of mass m q {\displaystyle m_{q}} . The negative sign arises in the potential term since the proton and electron are oppositely charged. The reduced mass in place of the electron mass is used since the electron and proton together orbit each other about a common center of mass, and constitute a two-body problem to solve. The motion of the electron is of principal interest here, so the equivalent one-body problem is the motion of the electron using the reduced mass. The Schrödinger equation for a hydrogen atom can be solved by separation of variables. In this case, spherical polar coordinates are the most convenient. Thus, ψ ( r , θ , φ ) = R ( r ) Y ℓ m ( θ , φ ) = R ( r ) Θ ( θ ) Φ ( φ ) , {\displaystyle \psi (r,\theta ,\varphi )=R(r)Y_{\ell }^{m}(\theta ,\varphi )=R(r)\Theta (\theta )\Phi (\varphi ),} where R are radial functions and Y l m ( θ , φ ) {\displaystyle Y_{l}^{m}(\theta ,\varphi )} are spherical harmonics of degree ℓ {\displaystyle \ell } and order m {\displaystyle m} . This is the only atom for which the Schrödinger equation has been solved for exactly. Multi-electron atoms require approximate methods. The family of solutions are: ψ n ℓ m ( r , θ , φ ) = ( 2 n a 0 ) 3 ( n − ℓ − 1 ) ! 2 n [ ( n + ℓ ) ! ] e − r / n a 0 ( 2 r n a 0 ) ℓ L n − ℓ − 1 2 ℓ + 1 ( 2 r n a 0 ) ⋅ Y ℓ m ( θ , φ ) {\displaystyle \psi _{n\ell m}(r,\theta ,\varphi )={\sqrt {\left({\frac {2}{na_{0}}}\right)^{3}{\frac {(n-\ell -1)!}{2n[(n+\ell )!]}}}}e^{-r/na_{0}}\left({\frac {2r}{na_{0}}}\right)^{\ell }L_{n-\ell -1}^{2\ell +1}\left({\frac {2r}{na_{0}}}\right)\cdot Y_{\ell }^{m}(\theta ,\varphi )} where a 0 = 4 π ε 0 ℏ 2 m q q 2 {\displaystyle a_{0}={\frac {4\pi \varepsilon _{0}\hbar ^{2}}{m_{q}q^{2}}}} is the Bohr radius, L n − ℓ − 1 2 ℓ + 1 ( ⋯ ) {\displaystyle L_{n-\ell -1}^{2\ell +1}(\cdots )} are the generalized Laguerre polynomials of degree n − ℓ − 1 {\displaystyle n-\ell -1} , n , ℓ , m {\displaystyle n,\ell ,m} are the principal, azimuthal, and magnetic quantum numbers respectively, which take the values n = 1 , 2 , 3 , … , {\displaystyle n=1,2,3,\dots ,} ℓ = 0 , 1 , 2 , … , n − 1 , {\displaystyle \ell =0,1,2,\dots ,n-1,} m = − ℓ , … , ℓ . {\displaystyle m=-\ell ,\dots ,\ell .} Approximate solutions It is typically not possible to solve the Schrödinger equation exactly for situations of physical interest. Accordingly, approximate solutions are obtained using techniques like variational methods and WKB approximation. It is also common to treat a problem of interest as a small modification to a problem that can be solved exactly, a method known as perturbation theory. Semiclassical limit One simple way to compare classical to quantum mechanics is to consider the time-evolution of the expected position and expected momentum, which can then be compared to the time-evolution of the ordinary position and momentum in classical mechanics.: 302 The quantum expectation values satisfy the Ehrenfest theorem. For a one-dimensional quantum particle moving in a potential V {\displaystyle V} , the Ehrenfest theorem says m d d t ⟨ x ⟩ = ⟨ p ⟩ ; d d t ⟨ p ⟩ = − ⟨ V ′ ( X ) ⟩ . {\displaystyle m{\frac {d}{dt}}\langle x\rangle =\langle p\rangle ;\quad {\frac {d}{dt}}\langle p\rangle =-\left\langle V'(X)\right\rangle .} Although the first of these equations is consistent with the classical behavior, the second is not: If the pair ( ⟨ X ⟩ , ⟨ P ⟩ ) {\displaystyle (\langle X\rangle ,\langle P\rangle )} were to satisfy Newton's second law, the right-hand side of the second equation would have to be − V ′ ( ⟨ X ⟩ ) {\displaystyle -V'\left(\left\langle X\right\rangle \right)} which is typically not the same as − ⟨ V ′ ( X ) ⟩ {\displaystyle -\left\langle V'(X)\right\rangle } . For a general V ′ {\displaystyle V'} , therefore, quantum mechanics can lead to predictions where expectation values do not mimic the classical behavior. In the case of the quantum harmonic oscillator, however, V ′ {\displaystyle V'} is linear and this distinction disappears, so that in this very special case, the expected position and expected momentum do exactly follow the classical trajectories. For general systems, the best we can hope for is that the expected position and momentum will approximately follow the classical trajectories. If the wave function is highly concentrated around a point x 0 {\displaystyle x_{0}} , then V ′ ( ⟨ X ⟩ ) {\displaystyle V'\left(\left\langle X\right\rangle \right)} and ⟨ V ′ ( X ) ⟩ {\displaystyle \left\langle V'(X)\right\rangle } will be almost the same, since both will be approximately equal to V ′ ( x 0 ) {\displaystyle V'(x_{0})} . In that case, the expected position and expected momentum will remain very close to the classical trajectories, at least for as long as the wave function remains highly localized in position. The Schrödinger equation in its general form i ℏ ∂ ∂ t Ψ ( r , t ) = H ^ Ψ ( r , t ) {\displaystyle i\hbar {\frac {\partial }{\partial t}}\Psi \left(\mathbf {r} ,t\right)={\hat {H}}\Psi \left(\mathbf {r} ,t\right)} is closely related to the Hamilton–Jacobi equation (HJE) − ∂ ∂ t S ( q i , t ) = H ( q i , ∂ S ∂ q i , t ) {\displaystyle -{\frac {\partial }{\partial t}}S(q_{i},t)=H\left(q_{i},{\frac {\partial S}{\partial q_{i}}},t\right)} where S {\displaystyle S} is the classical action and H {\displaystyle H} is the Hamiltonian function (not operator).: 308 Here the generalized coordinates q i {\displaystyle q_{i}} for i = 1 , 2 , 3 {\displaystyle i=1,2,3} (used in the context of the HJE) can be set to the position in Cartesian coordinates as r = ( q 1 , q 2 , q 3 ) = ( x , y , z ) {\displaystyle \mathbf {r} =(q_{1},q_{2},q_{3})=(x,y,z)} . Substituting Ψ = ρ ( r , t ) e i S ( r , t ) / ℏ {\displaystyle \Psi ={\sqrt {\rho (\mathbf {r} ,t)}}e^{iS(\mathbf {r} ,t)/\hbar }} where ρ {\displaystyle \rho } is the probability density, into the Schrödinger equation and then taking the limit ℏ → 0 {\displaystyle \hbar \to 0} in the resulting equation yield the Hamilton–Jacobi equation. Density matrices Wave functions are not always the most convenient way to describe quantum systems and their behavior. When the preparation of a system is only imperfectly known, or when the system under investigation is a part of a larger whole, density matrices may be used instead.: 74 A density matrix is a positive semi-definite operator whose trace is equal to 1. (The term "density operator" is also used, particularly when the underlying Hilbert space is infinite-dimensional.) The set of all density matrices is convex, and the extreme points are the operators that project onto vectors in the Hilbert space. These are the density-matrix representations of wave functions; in Dirac notation, they are written ρ ^ = | Ψ ⟩ ⟨ Ψ | . {\displaystyle {\hat {\rho }}=|\Psi \rangle \langle \Psi |.} The density-matrix analogue of the Schrödinger equation for wave functions is i ℏ ∂ ρ ^ ∂ t = [ H ^ , ρ ^ ] , {\displaystyle i\hbar {\frac {\partial {\hat {\rho }}}{\partial t}}=[{\hat {H}},{\hat {\rho }}],} where the brackets denote a commutator. This is variously known as the von Neumann equation, the Liouville–von Neumann equation, or just the Schrödinger equation for density matrices.: 312 If the Hamiltonian is time-independent, this equation can be easily solved to yield ρ ^ ( t ) = e − i H ^ t / ℏ ρ ^ ( 0 ) e i H ^ t / ℏ . {\displaystyle {\hat {\rho }}(t)=e^{-i{\hat {H}}t/\hbar }{\hat {\rho }}(0)e^{i{\hat {H}}t/\hbar }.} More generally, if the unitary operator U ^ ( t ) {\displaystyle {\hat {U}}(t)} describes wave function evolution over some time interval, then the time evolution of a density matrix over that same interval is given by ρ ^ ( t ) = U ^ ( t ) ρ ^ ( 0 ) U ^ ( t ) † . {\displaystyle {\hat {\rho }}(t)={\hat {U}}(t){\hat {\rho }}(0){\hat {U}}(t)^{\dagger }.} Unitary evolution of a density matrix conserves its von Neumann entropy.: 267 Relativistic quantum physics and quantum field theory The one-particle Schrödinger equation described above is valid essentially in the nonrelativistic domain. For one reason, it is essentially invariant under Galilean transformations, which form the symmetry group of Newtonian dynamics. Moreover, processes that change particle number are natural in relativity, and so an equation for one particle (or any fixed number thereof) can only be of limited use. A more general form of the Schrödinger equation that also applies in relativistic situations can be formulated within quantum field theory (QFT), a framework that allows the combination of quantum mechanics with special relativity. The region in which both simultaneously apply may be described by relativistic quantum mechanics. Such descriptions may use time evolution generated by a Hamiltonian operator, as in the Schrödinger functional method. Klein–Gordon and Dirac equations Attempts to combine quantum physics with special relativity began with building relativistic wave equations from the relativistic energy–momentum relation E 2 = ( p c ) 2 + ( m 0 c 2 ) 2 , {\displaystyle E^{2}=(pc)^{2}+\left(m_{0}c^{2}\right)^{2},} instead of nonrelativistic energy equations. The Klein–Gordon equation and the Dirac equation are two such equations. The Klein–Gordon equation, − 1 c 2 ∂ 2 ∂ t 2 ψ + ∇ 2 ψ = m 2 c 2 ℏ 2 ψ , {\displaystyle -{\frac {1}{c^{2}}}{\frac {\partial ^{2}}{\partial t^{2}}}\psi +\nabla ^{2}\psi ={\frac {m^{2}c^{2}}{\hbar ^{2}}}\psi ,} was the first such equation to be obtained, even before the nonrelativistic one-particle Schrödinger equation, and applies to massive spinless particles. Historically, Dirac obtained the Dirac equation by seeking a differential equation that would be first-order in both time and space, a desirable property for a relativistic theory. Taking the "square root" of the left-hand side of the Klein–Gordon equation in this way required factorizing it into a product of two operators, which Dirac wrote using 4 × 4 matrices α 1 , α 2 , α 3 , β {\displaystyle \alpha _{1},\alpha _{2},\alpha _{3},\beta } . Consequently, the wave function also became a four-component function, governed by the Dirac equation that, in free space, read ( β m c 2 + c ( ∑ n = ⁡ 1 3 α n p n ) ) ψ = i ℏ ∂ ψ ∂ t . {\displaystyle \left(\beta mc^{2}+c\left(\sum _{n\mathop {=} 1}^{3}\alpha _{n}p_{n}\right)\right)\psi =i\hbar {\frac {\partial \psi }{\partial t}}.} This has again the form of the Schrödinger equation, with the time derivative of the wave function being given by a Hamiltonian operator acting upon the wave function. Including influences upon the particle requires modifying the Hamiltonian operator. For example, the Dirac Hamiltonian for a particle of mass m and electric charge q in an electromagnetic field (described by the electromagnetic potentials φ and A) is: H ^ Dirac = γ 0 [ c γ ⋅ ( p ^ − q A ) + m c 2 + γ 0 q φ ] , {\displaystyle {\hat {H}}_{\text{Dirac}}=\gamma ^{0}\left[c{\boldsymbol {\gamma }}\cdot \left({\hat {\mathbf {p} }}-q\mathbf {A} \right)+mc^{2}+\gamma ^{0}q\varphi \right],} in which the γ = (γ1, γ2, γ3) and γ0 are the Dirac gamma matrices related to the spin of the particle. The Dirac equation is true for all spin-1⁄2 particles, and the solutions to the equation are 4-component spinor fields with two components corresponding to the particle and the other two for the antiparticle. For the Klein–Gordon equation, the general form of the Schrödinger equation is inconvenient to use, and in practice the Hamiltonian is not expressed in an analogous way to the Dirac Hamiltonian. The equations for relativistic quantum fields, of which the Klein–Gordon and Dirac equations are two examples, can be obtained in other ways, such as starting from a Lagrangian density and using the Euler–Lagrange equations for fields, or using the representation theory of the Lorentz group in which certain representations can be used to fix the equation for a free particle of given spin (and mass). In general, the Hamiltonian to be substituted in the general Schrödinger equation is not just a function of the position and momentum operators (and possibly time), but also of spin matrices. Also, the solutions to a relativistic wave equation, for a massive particle of spin s, are complex-valued 2(2s + 1)-component spinor fields. Fock space As originally formulated, the Dirac equation is an equation for a single quantum particle, just like the single-particle Schrödinger equation with wave function Ψ ( x , t ) {\displaystyle \Psi (x,t)} . This is of limited use in relativistic quantum mechanics, where particle number is not fixed. Heuristically, this complication can be motivated by noting that mass–energy equivalence implies material particles can be created from energy. A common way to address this in QFT is to introduce a Hilbert space where the basis states are labeled by particle number, a so-called Fock space. The Schrödinger equation can then be formulated for quantum states on this Hilbert space. However, because the Schrödinger equation picks out a preferred time axis, the Lorentz invariance of the theory is no longer manifest, and accordingly, the theory is often formulated in other ways. History Following Max Planck's quantization of light (see black-body radiation), Albert Einstein interpreted Planck's quanta to be photons, particles of light, and proposed that the energy of a photon is proportional to its frequency, one of the first signs of wave–particle duality. Since energy and momentum are related in the same way as frequency and wave number in special relativity, it followed that the momentum p {\displaystyle p} of a photon is inversely proportional to its wavelength λ {\displaystyle \lambda } , or proportional to its wave number k {\displaystyle k} : p = h λ = ℏ k , {\displaystyle p={\frac {h}{\lambda }}=\hbar k,} where h {\displaystyle h} is the Planck constant and ℏ = h / 2 π {\displaystyle \hbar ={h}/{2\pi }} is the reduced Planck constant. Louis de Broglie hypothesized that this is true for all particles, even particles which have mass such as electrons. He showed that, assuming that the matter waves propagate along with their particle counterparts, electrons form standing waves, meaning that only certain discrete rotational frequencies about the nucleus of an atom are allowed. These quantized orbits correspond to discrete energy levels, and de Broglie reproduced the Bohr model formula for the energy levels. The Bohr model was based on the assumed quantization of angular momentum L {\displaystyle L} according to L = n h 2 π = n ℏ . {\displaystyle L=n{\frac {h}{2\pi }}=n\hbar .} According to de Broglie, the electron is described by a wave, and a whole number of wavelengths must fit along the circumference of the electron's orbit: n λ = 2 π r . {\displaystyle n\lambda =2\pi r.} This approach essentially confined the electron wave in one dimension, along a circular orbit of radius r {\displaystyle r} . In 1921, prior to de Broglie, Arthur C. Lunn at the University of Chicago had used the same argument based on the completion of the relativistic energy–momentum 4-vector to derive what we now call the de Broglie relation. Unlike de Broglie, Lunn went on to formulate the differential equation (presumably by combining the classical Helmholz equation with de Broglie's relations p = ℏ k {\displaystyle p=\hbar k} and E = ℏ ω {\displaystyle E=\hbar \omega } ) now known as the Schrödinger equation and solve for its energy eigenvalues for the hydrogen atom; the paper was rejected by the Physical Review, according to Kamen. Following up on de Broglie's ideas, physicist Peter Debye made an offhand comment that if particles behaved as waves, they should satisfy some sort of wave equation. Inspired by Debye's remark, Schrödinger decided to find a proper 3-dimensional wave equation for the electron. He was guided by William Rowan Hamilton's analogy between mechanics and optics, encoded in the observation that the zero-wavelength limit of optics resembles a mechanical system—the trajectories of light rays become sharp tracks that obey Fermat's principle, an analog of the principle of least action. The equation he found is i ℏ ∂ ∂ t Ψ ( r , t ) = − ℏ 2 2 m ∇ 2 Ψ ( r , t ) + V ( r ) Ψ ( r , t ) . {\displaystyle i\hbar {\frac {\partial }{\partial t}}\Psi (\mathbf {r} ,t)=-{\frac {\hbar ^{2}}{2m}}\nabla ^{2}\Psi (\mathbf {r} ,t)+V(\mathbf {r} )\Psi (\mathbf {r} ,t).} By that time Arnold Sommerfeld had refined the Bohr model with relativistic corrections. Schrödinger used the relativistic energy–momentum relation to find what is now known as the Klein–Gordon equation in a Coulomb potential (in natural units): ( E + e 2 r ) 2 ψ ( x ) = − ∇ 2 ψ ( x ) + m 2 ψ ( x ) . {\displaystyle \left(E+{\frac {e^{2}}{r}}\right)^{2}\psi (x)=-\nabla ^{2}\psi (x)+m^{2}\psi (x).} He found the standing waves of this relativistic equation, but the relativistic corrections disagreed with Sommerfeld's formula. Discouraged, he put away his calculations and secluded himself with a mistress in a mountain cabin in December 1925. While at the cabin, Schrödinger decided that his earlier nonrelativistic calculations were novel enough to publish and decided to leave off the problem of relativistic corrections for the future. Despite the difficulties in solving the differential equation for hydrogen (he had sought help from his friend the mathematician Hermann Weyl: 3 ) Schrödinger showed that his nonrelativistic version of the wave equation produced the correct spectral energies of hydrogen in a paper published in 1926.: 1 Schrödinger computed the hydrogen spectral series by treating a hydrogen atom's electron as a wave Ψ ( x , t ) {\displaystyle \Psi (\mathbf {x} ,t)} , moving in a potential well V {\displaystyle V} , created by the proton. This computation accurately reproduced the energy levels of the Bohr model. The Schrödinger equation details the behavior of Ψ {\displaystyle \Psi } but says nothing of its nature. Schrödinger tried to interpret the real part of Ψ ∂ Ψ ∗ ∂ t {\displaystyle \Psi {\frac {\partial \Psi ^{*}}{\partial t}}} as a charge density, and then revised this proposal, saying in his next paper that the modulus squared of Ψ {\displaystyle \Psi } is a charge density. This approach was, however, unsuccessful. In 1926, just a few days after this paper was published, Max Born successfully interpreted Ψ {\displaystyle \Psi } as the probability amplitude, whose modulus squared is equal to probability density.: 220 Later, Schrödinger himself explained this interpretation as follows: The already ... mentioned psi-function.... is now the means for predicting probability of measurement results. In it is embodied the momentarily attained sum of theoretically based future expectation, somewhat as laid down in a catalog. Interpretation The Schrödinger equation provides a way to calculate the wave function of a system and how it changes dynamically in time. However, the Schrödinger equation does not directly say what, exactly, the wave function is. The meaning of the Schrödinger equation and how the mathematical entities in it relate to physical reality depends upon the interpretation of quantum mechanics that one adopts. In the views often grouped together as the Copenhagen interpretation, a system's wave function is a collection of statistical information about that system. The Schrödinger equation relates information about the system at one time to information about it at another. While the time-evolution process represented by the Schrödinger equation is continuous and deterministic, in that knowing the wave function at one instant is in principle sufficient to calculate it for all future times, wave functions can also change discontinuously and stochastically during a measurement. The wave function changes, according to this school of thought, because new information is available. The post-measurement wave function generally cannot be known prior to the measurement, but the probabilities for the different possibilities can be calculated using the Born rule. Other, more recent interpretations of quantum mechanics, such as relational quantum mechanics and QBism also give the Schrödinger equation a status of this sort. Schrödinger himself suggested in 1952 that the different terms of a superposition evolving under the Schrödinger equation are "not alternatives but all really happen simultaneously". This has been interpreted as an early version of Everett's many-worlds interpretation. This interpretation, formulated independently in 1956, holds that all the possibilities described by quantum theory simultaneously occur in a multiverse composed of mostly independent parallel universes. This interpretation removes the axiom of wave function collapse, leaving only continuous evolution under the Schrödinger equation, and so all possible states of the measured system and the measuring apparatus, together with the observer, are present in a real physical quantum superposition. While the multiverse is deterministic, we perceive non-deterministic behavior governed by probabilities, because we do not observe the multiverse as a whole, but only one parallel universe at a time. Exactly how this is supposed to work has been the subject of much debate. Why should we assign probabilities at all to outcomes that are certain to occur in some worlds, and why should the probabilities be given by the Born rule? Several ways to answer these questions in the many-worlds framework have been proposed, but there is no consensus on whether they are successful. Bohmian mechanics reformulates quantum mechanics to make it deterministic, at the price of adding a force due to a "quantum potential". It attributes to each physical system not only a wave function but in addition a real position that evolves deterministically under a nonlocal guiding equation. The evolution of a physical system is given at all times by the Schrödinger equation together with the guiding equation. See also Notes References External links "Schrödinger equation". Encyclopedia of Mathematics. EMS Press. 2001 [1994]. Quantum Cook Book (PDF) and PHYS 201: Fundamentals of Physics II by Ramamurti Shankar, Yale OpenCourseware The Modern Revolution in Physics – an online textbook. Quantum Physics I at MIT OpenCourseWare The conformal bootstrap is a non-perturbative mathematical method to constrain and solve conformal field theories, i.e. models of particle physics or statistical physics that exhibit similar properties at different levels of resolution. Overview Unlike more traditional techniques of quantum field theory, conformal bootstrap does not use the Lagrangian of the theory. Instead, it operates with the general axiomatic parameters, such as the scaling dimensions of the local operators and their operator product expansion coefficients. A key axiom is that the product of local operators must be expressible as a sum over local operators (thus turning the product into an algebra); the sum must have a non-zero radius of convergence. This leads to decompositions of correlation functions into structure constants and conformal blocks. The main ideas of the conformal bootstrap were formulated in the 1970s by the Soviet physicists Alexander Polyakov and Alexander Migdal and the Italian physicists Sergio Ferrara, Raoul Gatto and Aurelio Grillo. Other early pioneers of this idea were Gerhard Mack and Ivan Todorov. In two dimensions, the conformal bootstrap was demonstrated to work in 1983 by Alexander Belavin, Alexander Polyakov and Alexander Zamolodchikov. Many two-dimensional conformal field theories were solved using this method, notably the minimal models and the Liouville field theory. In higher dimensions, the conformal bootstrap started to develop following the 2008 paper by Riccardo Rattazzi, Slava Rychkov, Erik Tonni and Alessandro Vichi. The method was since used to obtain many general results about conformal and superconformal field theories in three, four, five and six dimensions. Applied to the conformal field theory describing the critical point of the three-dimensional Ising model, it produced the most precise predictions for its critical exponents. Current research The international Simons Collaboration on the Nonperturbative Bootstrap unites researchers devoted to developing and applying the conformal bootstrap and other related techniques in quantum field theory. History of the name The modern usage of the term "conformal bootstrap" was introduced in 1984 by Belavin et al. In the earlier literature, the name was sometimes used to denote a different approach to conformal field theories, nowadays referred to as the skeleton expansion or the "old bootstrap". This older method is perturbative in nature, and is not directly related to the conformal bootstrap in the modern sense of the term. External links Open problems in conformal bootstrap == References == In theoretical physics, Rajesh Gopakumar and Cumrun Vafa introduced in a series of papers numerical invariants of Calabi-Yau threefolds, later referred to as the Gopakumar–Vafa invariants. These physically defined invariants represent the number of BPS states on a Calabi–Yau threefold. In the same papers, the authors also derived the following formula which relates the Gromov–Witten invariants and the Gopakumar-Vafa invariants. ∑ g = 0 ∞ ∑ β ∈ H 2 ( M , Z ) GW ( g , β ) q β λ 2 g − 2 = ∑ g = 0 ∞ ∑ k = 1 ∞ ∑ β ∈ H 2 ( M , Z ) GV ( g , β ) 1 k ( 2 sin ⁡ ( k λ 2 ) ) 2 g − 2 q k β {\displaystyle \sum _{g=0}^{\infty }~\sum _{\beta \in H_{2}(M,\mathbb {Z} )}{\text{GW}}(g,\beta )q^{\beta }\lambda ^{2g-2}=\sum _{g=0}^{\infty }~\sum _{k=1}^{\infty }~\sum _{\beta \in H_{2}(M,\mathbb {Z} )}{\text{GV}}(g,\beta ){\frac {1}{k}}\left(2\sin \left({\frac {k\lambda }{2}}\right)\right)^{2g-2}q^{k\beta }} , where β {\displaystyle \beta } is the class of holomorphic curves with genus g, λ {\displaystyle \lambda } is the topological string coupling, mathematically a formal variable, q β = exp ⁡ ( 2 π i t β ) {\displaystyle q^{\beta }=\exp(2\pi it_{\beta })} with t β {\displaystyle t_{\beta }} the Kähler parameter of the curve class β {\displaystyle \beta } , GW ( g , β ) {\displaystyle {\text{GW}}(g,\beta )} are the Gromov–Witten invariants of curve class β {\displaystyle \beta } at genus g {\displaystyle g} , GV ( g , β ) {\displaystyle {\text{GV}}(g,\beta )} are the Gopakumar–Vafa invariants of curve class β {\displaystyle \beta } at genus g {\displaystyle g} . Notably, Gromov-Witten invariants are generally rational numbers while Gopakumar-Vafa invariants are always integers. As a partition function in topological quantum field theory Gopakumar–Vafa invariants can be viewed as a partition function in topological quantum field theory. They are proposed to be the partition function in Gopakumar–Vafa form: Z t o p = exp ⁡ [ ∑ g = 0 ∞ ∑ k = 1 ∞ ∑ β ∈ H 2 ( M , Z ) GV ( g , β ) 1 k ( 2 sin ⁡ ( k λ 2 ) ) 2 g − 2 q k β ] . {\displaystyle Z_{top}=\exp \left[\sum _{g=0}^{\infty }~\sum _{k=1}^{\infty }~\sum _{\beta \in H_{2}(M,\mathbb {Z} )}{\text{GV}}(g,\beta ){\frac {1}{k}}\left(2\sin \left({\frac {k\lambda }{2}}\right)\right)^{2g-2}q^{k\beta }\right]\ .} Mathematical approaches While Gromov-Witten invariants have rigorous mathematical definitions (both in symplectic and algebraic geometry), there is no mathematically rigorous definition of the Gopakumar-Vafa invariants, except for very special cases. On the other hand, Gopakumar-Vafa's formula implies that Gromov-Witten invariants and Gopakumar-Vafa invariants determine each other. One can solve Gopakumar-Vafa invariants from Gromov-Witten invariants, while the solutions are a priori rational numbers. Ionel-Parker proved that these expressions are indeed integers. See also Gopakumar–Vafa duality Notes References Gopakumar, Rajesh; Vafa, Cumrun (1998a), M-Theory and Topological strings-I, arXiv:hep-th/9809187, Bibcode:1998hep.th....9187G Gopakumar, Rajesh; Vafa, Cumrun (1998b), M-Theory and Topological strings-II, arXiv:hep-th/9812127, Bibcode:1998hep.th...12127G Gopakumar, Rajesh; Vafa, Cumrun (1999), "On the Gauge Theory/Geometry Correspondence", Adv. Theor. Math. Phys., 3 (5): 1415–1443, arXiv:hep-th/9811131, Bibcode:1998hep.th...11131G, doi:10.4310/ATMP.1999.v3.n5.a5, S2CID 13824856 Gopakumar, Rajesh; Vafa, Cumrun (1998d), "Topological Gravity as Large N Topological Gauge Theory", Adv. Theor. Math. Phys., 2 (2): 413–442, arXiv:hep-th/9802016, Bibcode:1998hep.th....2016G, doi:10.4310/ATMP.1998.v2.n2.a8, S2CID 16676561 Ionel, Eleny-Nicoleta; Parker, Thomas H. (2018), "The Gopakumar–Vafa formula for symplectic manifolds", Annals of Mathematics, Second Series, 187 (1): 1–64, arXiv:1306.1516, doi:10.4007/annals.2018.187.1.1, MR 3739228, S2CID 7070264 In optics the noise-equivalent flux density (NEFD) or noise-equivalent irradiance (NEI) of a system is the level of flux density required to be equivalent to the noise present in the system. It is a measure used by astronomers in determining the accuracy of observations. The NEFD can be related to a light detector's noise-equivalent power for a collection area A and a photon bandwidth ν {\displaystyle \nu } by: N E F D = η N E P A ν {\displaystyle \mathrm {NEFD} =\eta {\frac {\mathrm {NEP} }{A\nu }}} , where a factor η {\displaystyle \eta } (often 2, in the case of switching between measuring a source and measuring off-source) accounts for the photon statistics for the mode of operation. See also External quantum efficiency == References == Judd–Ofelt theory is a theory in physical chemistry describing the intensity of electron transitions within the 4f shell of rare-earth ions in solids and solutions. It provides a mathematical framework for predicting and analyzing the spectra of rare-earth ions in solids and solutions, in particular branching ratios, radiative lifetimes, and oscillator strengths. Theory Judd-Ofelt theory may be used to predict and analyze the intensities of intra 4f electronic dipole transitions. Such transitions are disallowed by Electronic Dipole Transition Selection rules in free space, as the initial and final states have the same parity. However, intra 4f transitions have been observed. The transition strengths and the transition changing the orbital angular momentum quantum number are not consistent with them being Magnetic Dipole Transitions. This apparent discrepancy is reconciled by treating the crystal field an ion in a solid experiences as a perturbation to the free space Hamiltonian. This perturbation mixes free space electronic states of opposite parity (namely the rare earth ion's ground 4fn electronic configuration with the opposite parity 4fn-15d). Thus Electronic Dipole transitions between these crystal field perturbed electronic states do not violate this parity change selection rule. The theory quantitatively describes this mixing using three phenomenological parameters particular to the host crystal, denoted as Ω λ {\displaystyle \Omega _{\lambda }} (where λ = 2 , 4 , 6 {\displaystyle \lambda =2,4,6} ). These parameters account for the asymmetric nature of the crystal field and enable the calculation of transition probabilities, oscillator strengths, and radiative lifetimes of excited states, which are crucial for the development of various photonic devices such as lasers and optical amplifiers. Like Russell-Saunders Coupling (LS-Coupling), Judd-Ofelt theory can be simplified to a list of selection rules. The rules for Judd-Ofelt Induced Electric dipole transitions are listed in the following table and compared to LS-coupling Magnetic and Electric dipole transitions. Term symbols in LS-Coupling describe the total orbital angular momentum ( L {\displaystyle L} ), total spin ( S {\displaystyle S} ), and total angular momentum ( J = S + L {\displaystyle J=S+L} ). π {\displaystyle \pi } is the parity of an electronic configuration. Example: Europium (III) For example let us apply these selection rules to triply ionized Europium (Eu3+). Eu3+ has an electronic configuration of [Xe]4f6. The ground term symbol for this configuration (according to Hund's Rules) is 7F0. Applying the above selection rules, transitions from this state to 5D2, 5D4, and 5D6 are allowed, but not to 5D1, 5D0, or 5L7 (violating the restrictions on J {\displaystyle J} ). History The theory was introduced independently in 1962 by Brian R. Judd of the University of California, Berkeley, and PhD candidate George S. Ofelt at Johns Hopkins University. Their work was published in Physical Review and the Journal of Chemical Physics, respectively. Judd and Ofelt did not meet until 2003 at a workshop in Lądek-Zdrój, Poland. Judd-Ofelt Theory has become a standard tool in the field of lanthanide spectroscopy, providing insights into the optical properties of rare earth-doped materials and aiding in the design of materials for color display systems, fluorescent lamps, and lasers. Their work was cited approximately 2000 times between 1962 and 2004. Brian M. Walsh of NASA Langley places Judd and Ofelt's theory at the "forefront" of a 1960s revolution in spectroscopic research on rare-earth ions. Analysis software Judd–Ofelt intensity parameters can be calculated from absorption spectrum of any lanthanide by the RELIC analysis software. The intensity parameters and derived quantities (oscillator strengths, radiative transition probabilities, luminescence branching ratios, excited state radiative lifetimes, and estimates of quantum efficiencies) of Eu3+ doped compounds, can be obtained by the JOES application software from their emission spectrum. Theoretical Judd-Ofelt intensity parameters for Eu3+ can be obtained using the LUMPAC software. Additionally, the JOYSpectra web platform provides these parameters for all Ln3+ ions. See also Parity (physics) Bert Broer Otto Laporte Giulio Racah John Hasbrouck Van Vleck Eugene Wigner Brian Garner Wybourne == References == In quantum physics, light is in a squeezed state if its electric field strength Ԑ for some phases ϑ {\displaystyle \vartheta } has a quantum uncertainty smaller than that of a coherent state. The term squeezing thus refers to a reduced quantum uncertainty. To obey Heisenberg's uncertainty relation, a squeezed state must also have phases at which the electric field uncertainty is anti-squeezed, i.e. larger than that of a coherent state. Since 2019, the gravitational-wave observatories LIGO and Virgo employ squeezed laser light, which has significantly increased the rate of observed gravitational-wave events. Quantum physical background An oscillating physical quantity cannot have precisely defined values at all phases of the oscillation. This is true for the electric and magnetic fields of an electromagnetic wave, as well as for any other wave or oscillation (see figure right). This fact can be observed in experiments and is described by quantum theory. For electromagnetic waves usually just the electric field is considered, because it is the one that mainly interacts with matter. Fig. 1. shows five different quantum states that a monochromatic wave could be in. The difference of the five quantum states is given by different electric field excitations and by different distributions of the quantum uncertainty along the phase ϑ {\displaystyle \vartheta } . For a displaced coherent state, the expectation (mean) value of the electric field shows an oscillation, with an uncertainty independent of the phase (a). Also the phase- (b) and amplitude-squeezed states (c) show an oscillation of the mean electric field, but here the uncertainty depends on phase and is squeezed for some phases. The vacuum state (d) is a special coherent state and is not squeezed. It has zero mean electric field for all phases and a phase-independent uncertainty. It has zero energy on average, i.e. zero photons, and is the ground state of the monochromatic wave we consider. Finally, a squeezed vacuum state has also a zero mean electric field but a phase-dependent uncertainty (e). Generally, quantum uncertainty reveals itself through a large number of identical measurements on identical quantum objects (here: modes of light) that, however, give different results. Let us again consider a continuous-wave monochromatic light wave (as emitted by an ultra-stable laser). A single measurement of Ԑ ( ϑ 1 ) {\displaystyle (\vartheta _{1})} is performed over many periods of the light wave and provides a single number. The next measurements of Ԑ ( ϑ 1 ) {\displaystyle (\vartheta _{1})} will be done consecutively on the same laser beam. Having recorded a large number of such measurements we know the field uncertainty at ϑ 1 {\displaystyle \vartheta _{1}} . In order to get the full picture, and for instance Fig.1(b), we need to record the statistics at many different phases 0 < ϑ i < π {\displaystyle 0<\vartheta _{i}<\pi } . Quantitative description of (squeezed) uncertainty The measured electric field strengths at the wave's phase ϑ {\displaystyle \vartheta } are the eigenvalues of the normalized quadrature operator X ϑ {\displaystyle X_{\vartheta }} , defined as X ^ ϑ = 1 2 [ e − i ϑ a ^ + e i ϑ a ^ † ] = cos ⁡ ( ϑ ) X ^ + sin ⁡ ( ϑ ) Y ^ {\displaystyle {\hat {X}}_{\vartheta }={\frac {1}{2}}\left[e^{-i\vartheta }{\hat {a}}+e^{i\vartheta }{\hat {a}}^{\dagger }\right]=\cos(\vartheta )\,{\hat {X}}+\sin(\vartheta )\,{\hat {Y}}} where a ^ {\displaystyle {\hat {a}}} and a ^ † {\displaystyle {\hat {a}}^{\dagger }} are the annihilation and creation operators, respectively, of the oscillator representing the photon. X ϑ = 0 ∘ ≡ X {\displaystyle X_{\vartheta =0^{\circ }}\equiv X} is the wave's amplitude quadrature, equivalent to the position in optical phase space, and X ϑ = 90 ∘ ≡ Y {\displaystyle X_{\vartheta =90^{\circ }}\equiv Y} is the wave's phase quadrature, equivalent to momentum. X {\displaystyle X} and Y {\displaystyle Y} are non-commuting observables. Although they represent electric fields, they are dimensionless and satisfy the following uncertainty relation: where Δ 2 {\displaystyle \Delta ^{2}} stands for the variance. (The variance is the mean of the squares of the measuring values minus the square of the mean of the measuring values.) If a mode of light is in its ground state | 0 ⟩ {\displaystyle |0\rangle } (having an average photon number of zero), the uncertainty relation above is saturated and the variances of the quadrature are Δ 2 X g = Δ 2 Y g = 1 / 4 {\displaystyle \Delta ^{2}X_{g}=\Delta ^{2}Y_{g}=1/4} . (Other normalizations can also be found in literature. The normalization chosen here has the nice property that the sum of the ground state variances directly provide the zero point excitation of the quantized harmonic oscillator Δ 2 X g + Δ 2 Y g = 1 / 2 {\displaystyle \Delta ^{2}X_{g}+\Delta ^{2}Y_{g}=1/2} ). While coherent states belong to the semi-classical states, since they can be fully described by a semi-classical model, squeezed states of light belong to the so-called nonclassical states, which also include number states (Fock states) and Schrödinger cat states. Squeezed states (of light) were first produced in the mid-1980s. At that time, quantum noise squeezing by up to a factor of about 2 (3 dB) in variance was achieved, i.e. Δ 2 X ϑ ≈ Δ 2 X g / 2 {\displaystyle \Delta ^{2}X_{\vartheta }\approx \Delta ^{2}X_{g}/2} . Today, squeeze factors larger than 10 (10 dB) have been directly observed. A limitation is set by decoherence, mainly in terms of optical loss. The squeeze factor in Decibel (dB) can be computed in the following way: Representation of squeezed states by quasi-probability densities Quantum states such as those in Fig. 1 (a) to (e) are often displayed as Wigner functions, which are quasi-probability density distributions. Two orthogonal quadratures, usually X {\displaystyle X} and Y {\displaystyle Y} , span a phase space diagram, and the third axes provides the quasi probability of yielding a certain combination of [ X ; Y ] {\displaystyle [X;Y]} . Since X {\displaystyle X} and Y {\displaystyle Y} are not precisely defined simultaneously, we cannot talk about a 'probability' as we do in classical physics but call it a 'quasi probability'. A Wigner function is reconstructed from time series of X ( t ) {\displaystyle X(t)} and Y ( t ) {\displaystyle Y(t)} . The reconstruction is also called 'quantum tomographic reconstruction'. For squeezed states, the Wigner function has a Gaussian shape, with an elliptical contour line, see Fig.: 1(f). Physical meaning of measurement quantity and measurement object Quantum uncertainty becomes visible when identical measurements of the same quantity (observable) on identical objects (here: modes of light) give different results (eigen values). In case of a single freely propagating monochromatic laser beam, the individual measurements are performed on consecutive time intervals of identical length. One interval must last much longer than the light's period; otherwise the monochromatic property would be significantly disturbed. Such consecutive measurements correspond to a time series of fluctuating eigen values. Consider an example in which the amplitude quadrature X {\displaystyle X} was repeatedly measured. The time series can be used for a quantum statistical characterization of the modes of light. Obviously, the amplitude of the light wave might be different before and after our measurement, i.e. the time series does not provide any information about very slow changes of the amplitude, which corresponds to very low frequencies. This is a trivial but also fundamental issue, since any data taking lasts for a finite time. Our time series, however, does provide meaningful information about fast changes of the light's amplitude, i.e. changes at frequencies higher than the inverse of the full measuring time. Changes that are faster than the duration of a single measurement, however, are invisible again. A quantum statistical characterization through consecutive measurements on some sort of a carrier is thus always related to a specific frequency interval, for instance described by f ± Δ f / 2 {\displaystyle f\pm \Delta f/2} with f > Δ f / 2 > 0. {\displaystyle f>\Delta f/2>0.} Based on this insight, we can describe the physical meaning of the observable X ϑ {\displaystyle X_{\vartheta }} more clearly: The quantum statistical characterization using identical consecutive modes carried by a laser beam confers to the laser beam's electric field modulation within a frequency interval. The actual observable needs to be labeled accordingly, for instance as X ϑ , f , Δ f {\displaystyle X_{\vartheta ,f,\Delta f}} . X f , Δ f {\displaystyle X_{f,\Delta f}} is the amplitude (or depth) of the amplitude modulation and Y f , Δ f {\displaystyle Y_{f,\Delta f}} the amplitude (or depth) of the phase modulation in the respective frequency interval. This leads to the doggerel expressions 'amplitude quadrature amplitude' and 'phase quadrature amplitude'. Within some limitations, for instance set by the speed of the electronics, f {\displaystyle f} and Δ f {\displaystyle \Delta f} can be freely chosen in course of data acquisition and, in particular, data processing. This choice also defines the measurement object, i.e. the mode that is characterized by the statistics of the eigen values of X f , Δ f {\displaystyle X_{f,\Delta f}} and Y f , Δ f {\displaystyle Y_{f,\Delta f}} . The measurement object thus is a modulation mode that is carried by the light beam. – In many experiments, one is interested in a continuous spectrum of many modulation modes carried by the same light beam. Fig. 2 shows the squeeze factors of many neighboring modulation modes versus f {\displaystyle f} . The upper trace refers to the uncertainties of the same modes being in their vacuum states, which serves as the 0 dB reference. The observables in squeezed light experiments correspond exactly to those being used in optical communication. Amplitude modulation (AM) and frequency modulation (FM) are the classical means to imprint information on a carrier field. (Frequency modulation is mathematically closely related to phase modulation). The observables X f , Δ f {\displaystyle X_{f,\Delta f}} and Y f , Δ f {\displaystyle Y_{f,\Delta f}} also correspond to the measurement quantities in laser interferometers, such as in Sagnac interferometers measuring rotation changes and in Michelson interferometers observing gravitational waves. Squeezed states of light thus have ample applications in optical communication and optical measurements. The most prominent and important application is in gravitational-wave observatories. Arguably, it is the first end-user driven application of quantum correlations. Squeezed light originally was not planned to be implemented in either Advanced LIGO nor in Advanced Virgo, but now it contributes a significant factor towards the observatories design sensitivities and increases the rate of observed gravitational-wave events. Frequency-dependent squeezing Frequency-dependent squeezing is a method being implemented at the LIGO–Virgo–KAGRA collaboration to improve sensitivity using its 300 m long filter cavities to handle light differently according to frequencies which allows to improve accuracy of phases at high frequencies at the cost of more inaccuracy in amplitudes at low frequencies and equivalently better amplitudes at low frequencies but worse phases at high frequencies, manipulating the uncertainty relation by the measurement of interest. Noise at high frequencies is dominated by shot noise while at low frequencies is dominated by radiation pressure noise so when one source is reduced the other increases. Applications Optical high-precision measurements Squeezed light is used to reduce the photon counting noise (shot noise) in optical high-precision measurements, most notably in laser interferometers. There are a large number of proof-of-principle experiments. Laser interferometers split a laser beam in two paths and overlap them again afterwards. If the relative optical path length changes, the interference changes, and the light power in the interferometer's output port as well. This light power is detected with a photo diode providing a continuous voltage signal. If for instance the position of one interferometer mirror vibrates and thereby causes an oscillating path length difference, the output light has an amplitude modulation of the same frequency. Independent of the existence of such a (classical) signal, a beam of light always carries at least the vacuum state uncertainty (see above). The (modulation) signal with respect to this uncertainty can be improved by using a higher light power inside the interferometer arms, since the signal increases with the light power. This is the reason (in fact the only one) why Michelson interferometers for the detection of gravitational waves use very high optical power. High light power, however, produces technical problems. Mirror surfaces absorb parts of the light, become warmer, get thermally deformed and reduce the interferometer's interference contrast. Furthermore, an excessive light power can excite unstable mechanical vibrations of the mirrors. These consequences are mitigated if squeezed states of light are used for improving the signal-to-noise-ratio. Squeezed states of light do not increase the light's power. They also do not increase the signal, but instead reduce the noise. Laser interferometers are usually operated with monochromatic continuous-wave light. The optimal signal-to-noise-ratio is achieved by either operating the differential interferometer arm lengths such that both output ports contain half of the input light power (half fringe) and by recording the difference signal from both ports, or by operating the interferometer close to a dark fringe for one of the output ports where just a single photodiode is placed. The latter operation point is used in gravitational-wave (GW) detectors. For improving an interferometer sensitivity with squeezed states of light, the already existing bright light does not need to be fully replaced. What has to be replaced is just the vacuum uncertainty in the difference of the phase quadrature amplitudes of the light fields in the arms, and only at modulation frequencies at which signals are expected. This is achieved by injecting a (broadband) squeezed vacuum field (Fig. 1e) into the unused interferometer input port (Fig. 3). Ideally, perfect interference with the bright field is achieved. For this the squeezed field has to be in the same mode as the bright light, i.e. has to have the same wavelength, same polarisation, same wavefront curvature, same beam radius, and, of course, the same directions of propagation in the interferometer arms. For the squeezed-light enhancement of a Michelson interferometer operated at dark fringe, a polarising beam splitter in combination with a Faraday rotator is required. This combination constitutes an optical diode. Without any loss, the squeezed field is overlapped with the bright field at the interferometer's central beam splitter, is split and travels along the arms, is retro-reflected, constructively interferes and overlaps with the interferometer signal towards the photo diode. Due to the polarisation rotation of the Faraday rotator, the optical loss on signal and squeezed field is zero (in the ideal case). Generally, the purpose of an interferometer is to transform a differential phase modulation (of two light beams) into an amplitude modulation of the output light . Accordingly, the injected vacuum-squeezed field is injected such that the differential phase quadrature uncertainty in the arms is squeezed. On the output light amplitude quadrature squeezing is observed. Fig. 4 shows the photo voltage of the photo diode in the interferometer output port. Subtracting the constant offset provides the (GW) signal. A source of squeezed states of light were integrated in the gravitational-wave detector GEO600 in 2010, as shown in Fig. 4. The source was built by the research group of R. Schnabel at Leibniz Universität Hannover (Germany). With squeezed light, the sensitivity of GEO600 during observational runs has been increased to values, which for practical reasons were not achievable without squeezed light. In 2018, squeezed light upgrades are also planned for the gravitational wave detectors Advanced LIGO and Advanced Virgo. Going beyond squeezing of photon counting noise, squeezed states of light can also be used to correlate quantum measurement noise (shot noise) and quantum back action noise to achieve sensitivities in the quantum non-demolition (QND) regime. Radiometry and calibration of quantum efficiencies Squeezed light can be used in radiometry to calibrate the quantum efficiency of photo-electric photo detectors without a lamp of calibrated radiance. Here, the term photo detector refers to a device that measures the power of a bright beam, typically in the range from a few microwatts up to about 0.1 W. The typical example is a PIN photo diode. In case of perfect quantum efficiency (100%), such a detector is supposed to convert every photon energy of incident light into exactly one photo electron. Conventional techniques of measuring quantum efficiencies require the knowledge of how many photons hit the surface of the photo detector, i.e. they require a lamp of calibrated radiance. The calibration on the basis of squeezed states of light uses instead the effect, that the uncertainty product Δ 2 X f , Δ f ⋅ Δ 2 Y f , Δ f {\displaystyle \Delta ^{2}X_{f,\Delta f}\cdot \Delta ^{2}Y_{f,\Delta f}} increases the smaller the quantum uncertainty of the detector is. In other words: The squeezed light method uses the fact that squeezed states of light are sensitive against decoherence. Without any decoherence during generation, propagation and detection of squeezed light, the uncertainty product has its minimum value of 1/16 (see above). If optical loss is the dominating decoherence effect, which usually is the case, the independent measurement of all optical losses during generation and propagation together with the value of the uncertainty product directly reveals the quantum uncertainty of the photo detectors used. When a squeezed state with squeezed variance Δ 2 X f , Δ f {\displaystyle \Delta ^{2}X_{f,\Delta f}} is detected with a photo detector of quantum efficiency η {\displaystyle \eta } (with 0 ≤ η ≤ 1 {\displaystyle 0\leq \eta \leq 1} ), the actually observed variance is increased to Δ 2 X f , Δ f o b s = η ⋅ Δ 2 X f , Δ f + ( 1 − η ) / 4 . {\displaystyle \Delta ^{2}X_{f,\Delta f}^{\mathrm {obs} }=\eta \cdot \Delta ^{2}X_{f,\Delta f}+(1-\eta )/4\,.} Optical loss mixes a portion of the vacuum state variance to the squeezed variance, which decreases the squeeze factor. The same equation also describes the influence of a non-perfect quantum efficiency on the anti-squeezed variance. The anti-squeezed variance reduces, however, the uncertainty product increases. Optical loss on a pure squeezed state produces a mixed squeezed state. Entanglement-based quantum key distribution Squeezed states of light can be used to produce Einstein-Podolsky-Rosen-entangled light that is the resource for a high quality level of quantum key distribution (QKD), which is called 'one-sided device independent QKD'. Superimposing on a balanced beam splitter two identical light beams that carry squeezed modulation states and have a propagation length difference of a quarter of their wavelength produces two EPR entangled light beams at the beam splitter output ports. Quadrature amplitude measurements on the individual beams reveal uncertainties that are much larger than those of the ground states, but the data from the two beams show strong correlations: from a measurement value taken at the first beam ( X f , Δ f A {\displaystyle X_{f,\Delta f}^{A}} ), one can infer the corresponding measurement value taken at the second beam ( X f , Δ f B {\displaystyle X_{f,\Delta f}^{B}} ). If the inference shows an uncertainty smaller than that of the vacuum state, EPR correlations exist, see Fig. 5. The aim of quantum key distribution is the distribution of identical, true random numbers to two distant parties A and B in such a way that A and B can quantify the amount of information about the numbers that has been lost to the environment (and thus is potentially in hand of an eavesdropper). To do so, sender (A) sends one of the entangled light beams to receiver (B). A and B measure repeatedly and simultaneously (taking the different propagation times into account) one of two orthogonal quadrature amplitudes. For every single measurement they need to choose whether to measure X {\displaystyle X} or Y {\displaystyle Y} in a truly random way, independently from each other. By chance, they measure the same quadrature in 50% of the single measurements. After having performed a large number of measurements, A and B communicate (publicly) what their choice was for every measurement. The non-matched pairs are discarded. From the remaining data they make public a small but statistically significant amount to test whether B is able to precisely infer the measurement results at A. Knowing the characteristics of the entangled light source and the quality of the measurement at the sender site, the sender gets information about the decoherence that happened during channel transmission and during the measurement at B. The decoherence quantifies the amount of information that was lost to the environment. If the amount of lost information is not too high and the data string not too short, data post processing in terms of error correction and privacy amplification produces a key with an arbitrarily reduced epsilon-level of insecurity. In addition to conventional QKD, the test for EPR correlations not only characterizes the channel over which the light was sent (for instance a glas fibre) but also the measurement at the receiver site. The sender does not need to trust the receivers measurement any more. This higher quality of QKD is called one-sided device independent. This type of QKD works if the natural decoherence is not too high. For this reason, an implementation that uses conventional telecommunication glas fibers would be limited to a distance of a few kilometers. Generation Squeezed light is produced by means of nonlinear optics. The most successful method uses degenerate type I optical-parametric down-conversion (also called optical-parametric amplification) inside an optical resonator. To squeeze modulation states with respect to a carrier field at optical frequency ν {\displaystyle \nu } , a bright pump field at twice the optical frequency is focussed into a nonlinear crystal that is placed between two or more mirrors forming an optical resonator. It is not necessary to inject light at frequency ν {\displaystyle \nu } . (Such light, however, is required for detecting the (squeezed) modulation states). The crystal material needs to have a nonlinear susceptibility and needs to be highly transparent for both optical frequencies used. Typical materials are lithium niobate (LiNbO3) and (periodically poled) potassium titanyl phosphate (KTP). Due to the nonlinear susceptibility of the pumped crystal material, the electric field at frequency ν {\displaystyle \nu } is amplified and deamplified, depending on the relative phase to the pump light. At the pump's electric field maxima, the electric field at frequency ν {\displaystyle \nu } is amplified. At the pump's electric field minima, the electric field at frequency ν {\displaystyle \nu } is squeezed. This way, the vacuum state (Fig. 1e) is transferred to a squeezed vacuum state (Fig. 1d). A displaced coherent state (Fig. 1a) is transferred to a phase squeezed state (Fig. 1b) or to an amplitude squeezed state (Fig. 1c), depending on the relative phase between coherent input field and pump field. A graphical description of these processes can be found in. The existence of a resonator for the field at ν {\displaystyle \nu } is essential. The task of the resonator is shown in Fig. 6. The left resonator mirror has a typical reflectivity of about r 1 2 = 90 % {\displaystyle r_{1}^{2}=90\%} . Correspondingly 0.9 {\displaystyle {\sqrt {0.9}}} of the electric field that (continuously) enters from the left gets reflected. The remaining part is transmitted and resonates between the two mirrors. Due to the resonance, the electric field inside the resonator gets enhanced (even without any medium inside). 10 % {\displaystyle 10\%} of the steady-state light power inside the resonator gets transmitted towards the left and interferes with the beam that was retro-reflected directly. For an empty loss-less resonator, 100% of the light power would eventually propagate towards the left, obeying energy conservation. The principle of the squeezing resonator is the following: The medium parametrically attenuates the electric field inside the resonator to such a value that perfect destructive interference is achieved outside the resonator for the attenuated field quadrature. The optimum field attenuation factor inside the resonator is slightly below 2, depending on the reflectivity of the resonator mirror. This principle also works for electric field uncertainties. Inside the resonator, the squeeze factor is always less than 6 dB, but outside the resonator it can be arbitrarily high. If quadrature X f , Δ f {\displaystyle X_{f,\Delta f}} is squeezed, quadrature Y f , Δ f {\displaystyle Y_{f,\Delta f}} is anti-squeezed – inside as well as outside the resonator. It can be shown that the highest squeeze factor for one quadrature is achieved if the resonator is at its threshold for the orthogonal quadrature. At threshold and above, the pump field is converted into a bright field at optical frequency ν {\displaystyle \nu } . Squeezing resonators are usually operated slightly below threshold, for instance, to avoid damage to the photo diodes due to the bright down-converted field. A squeezing resonator works efficiently at modulation frequencies well inside its linewidth. Only for these frequencies highest squeeze factors can be achieved. At frequencies the optical-parametric gain is strongest, and the time delay between the interfering parts negligible. If decoherence was zero, infinite squeeze factors could be achieved outside the resonator, although the squeeze factor inside the resonator was less than 6 dB. Squeezing resonators have typical linewidths of a few tens of MHz up to GHz. Due to the interest in the interaction between squeezed light and atomic ensemble, narrowband atomic resonance squeezed light have been also generated through crystal and the atomic medium. Detection Squeezed states of light can be fully characterized by a photo-electric detector that is able to (subsequently) measure the electric field strengths at any phase ϑ {\displaystyle \vartheta } . (The restriction to a certain band of modulation frequencies happens after the detection by electronic filtering.) The required detector is a balanced homodyne detector (BHD). It has two input ports for two light beams. One for the (squeezed) signal field, and another for the BHDs local oscillator (LO) having the same wavelength as the signal field. The LO is part of the BHD. Its purpose is to beat with the signal field and to optically amplify it. Further components of the BHD are a balanced beam splitter and two photo diodes (of high quantum efficiency). Signal beam and LO need to be overlapped at the beam splitter. The two interference results in the beam splitter output ports are detected and the difference signal recorded (Fig. 7). The LO needs to be much more intense than the signal field. In this case the differential signal from the photo diodes in the interval f ± Δ f / 2 {\displaystyle f\pm \Delta f/2} is proportional to the quadrature amplitude X ϑ , f , Δ f {\displaystyle X_{\vartheta ,f,\Delta f}} . Changing the differential propagation length before the beam splitter sets the quadrature angle to an arbitrary value. (A change by a quarter of the optical wavelength changes the phase by π / 2 {\displaystyle \pi /2} .) The following should be stated at this point: Any information about the electro-magnetic wave can only be gathered in a quantized way, i.e. by absorbing light quanta (photons). This is also true for the BHD. However, a BHD cannot resolve the discrete energy transfer from the light to the electric current, since in any small time interval a vast number of photons are detected. This is ensured by the intense LO. The observable therefore has a quasi-continuous eigenvalue spectrum, as it is expected for an electric field strength. (In principle, one can also characterize squeezed states, in particular squeezed vacuum states, by counting photons, however, in general the measurement of the photon number statistic is not sufficient for a full characterization of a squeezed state and the full density matrix in the basis of the number states has to be determined.) See also Spin squeezed state == References == In theoretical physics, the Bogoliubov transformation, also known as the Bogoliubov–Valatin transformation, was independently developed in 1958 by Nikolay Bogolyubov and John George Valatin for finding solutions of BCS theory in a homogeneous system. The Bogoliubov transformation is an isomorphism of either the canonical commutation relation algebra or canonical anticommutation relation algebra. This induces an autoequivalence on the respective representations. The Bogoliubov transformation is often used to diagonalize Hamiltonians, which yields the stationary solutions of the corresponding Schrödinger equation. The Bogoliubov transformation is also important for understanding the Unruh effect, Hawking radiation, Davies-Fulling radiation (moving mirror model), pairing effects in nuclear physics, and many other topics. The Bogoliubov transformation is often used to diagonalize Hamiltonians, with a corresponding transformation of the state function. Operator eigenvalues calculated with the diagonalized Hamiltonian on the transformed state function thus are the same as before. Single bosonic mode example Consider the canonical commutation relation for bosonic creation and annihilation operators in the harmonic oscillator basis [ a ^ , a ^ † ] = 1. {\displaystyle \left[{\hat {a}},{\hat {a}}^{\dagger }\right]=1.} Define a new pair of operators b ^ = u a ^ + v a ^ † , {\displaystyle {\hat {b}}=u{\hat {a}}+v{\hat {a}}^{\dagger },} b ^ † = u ∗ a ^ † + v ∗ a ^ , {\displaystyle {\hat {b}}^{\dagger }=u^{*}{\hat {a}}^{\dagger }+v^{*}{\hat {a}},} for complex numbers u and v, where the latter is the Hermitian conjugate of the first. The Bogoliubov transformation is the canonical transformation mapping the operators a ^ {\displaystyle {\hat {a}}} and a ^ † {\displaystyle {\hat {a}}^{\dagger }} to b ^ {\displaystyle {\hat {b}}} and b ^ † {\displaystyle {\hat {b}}^{\dagger }} . To find the conditions on the constants u and v such that the transformation is canonical, the commutator is evaluated, namely, [ b ^ , b ^ † ] = [ u a ^ + v a ^ † , u ∗ a ^ † + v ∗ a ^ ] = ⋯ = ( | u | 2 − | v | 2 ) [ a ^ , a ^ † ] . {\displaystyle \left[{\hat {b}},{\hat {b}}^{\dagger }\right]=\left[u{\hat {a}}+v{\hat {a}}^{\dagger },u^{*}{\hat {a}}^{\dagger }+v^{*}{\hat {a}}\right]=\cdots =\left(|u|^{2}-|v|^{2}\right)\left[{\hat {a}},{\hat {a}}^{\dagger }\right].} It is then evident that | u | 2 − | v | 2 = 1 {\displaystyle |u|^{2}-|v|^{2}=1} is the condition for which the transformation is canonical. Since the form of this condition is suggestive of the hyperbolic identity cosh 2 ⁡ x − sinh 2 ⁡ x = 1 , {\displaystyle \cosh ^{2}x-\sinh ^{2}x=1,} the constants u and v can be readily parametrized as u = e i θ 1 cosh ⁡ r , {\displaystyle u=e^{i\theta _{1}}\cosh r,} v = e i θ 2 sinh ⁡ r . {\displaystyle v=e^{i\theta _{2}}\sinh r.} This is interpreted as a linear symplectic transformation of the phase space. By comparing to the Bloch–Messiah decomposition, the two angles θ 1 {\displaystyle \theta _{1}} and θ 2 {\displaystyle \theta _{2}} correspond to the orthogonal symplectic transformations (i.e., rotations) and the squeezing factor r {\displaystyle r} corresponds to the diagonal transformation. Applications The most prominent application is by Nikolai Bogoliubov himself his theory of a weakly interacting Bose gas. Other applications comprise Hamiltonians and excitations in the theory of antiferromagnetism. When calculating quantum field theory in curved spacetimes the definition of the vacuum changes, and a Bogoliubov transformation between these different vacua is possible. This is used in the derivation of Hawking radiation. Bogoliubov transforms are also used extensively in quantum optics, particularly when working with gaussian unitaries (such as beamsplitters, phase shifters, and squeezing operations). Fermionic mode For the anticommutation relations { a ^ , a ^ } = 0 , { a ^ , a ^ † } = 1 , {\displaystyle \left\{{\hat {a}},{\hat {a}}\right\}=0,\left\{{\hat {a}},{\hat {a}}^{\dagger }\right\}=1,} the Bogoliubov transformation is constrained by u v = 0 , | u | 2 + | v | 2 = 1 {\displaystyle uv=0,|u|^{2}+|v|^{2}=1} . Therefore, the only non-trivial possibility is u = 0 , | v | = 1 , {\displaystyle u=0,|v|=1,} corresponding to particle–antiparticle interchange (or particle–hole interchange in many-body systems) with the possible inclusion of a phase shift. Thus, for a single particle, the transformation can only be implemented (1) for a Dirac fermion, where particle and antiparticle are distinct (as opposed to a Majorana fermion or chiral fermion), or (2) for multi-fermionic systems, in which there is more than one type of fermion. Applications The most prominent application is again by Nikolai Bogoliubov himself, this time for the BCS theory of superconductivity. The point where the necessity to perform a Bogoliubov transform becomes obvious is that in mean-field approximation the Hamiltonian of the system can be written in both cases as a sum of bilinear terms in the original creation and destruction operators, involving finite ⟨ a i + a j + ⟩ {\displaystyle \langle a_{i}^{+}a_{j}^{+}\rangle } terms, i.e. one must go beyond the usual Hartree–Fock method. In particular, in the mean-field Bogoliubov–de Gennes Hamiltonian formalism with a superconducting pairing term such as Δ a i + a j + + h.c. {\displaystyle \Delta a_{i}^{+}a_{j}^{+}+{\text{h.c.}}} , the Bogoliubov transformed operators b , b † {\displaystyle b,b^{\dagger }} annihilate and create quasiparticles (each with well-defined energy, momentum and spin but in a quantum superposition of electron and hole state), and have coefficients u {\displaystyle u} and v {\displaystyle v} given by eigenvectors of the Bogoliubov–de Gennes matrix. Also in nuclear physics, this method is applicable, since it may describe the "pairing energy" of nucleons in a heavy element. Multimode example The Hilbert space under consideration is equipped with these operators, and henceforth describes a higher-dimensional quantum harmonic oscillator (usually an infinite-dimensional one). The ground state of the corresponding Hamiltonian is annihilated by all the annihilation operators: ∀ i a i | 0 ⟩ = 0. {\displaystyle \forall i\qquad a_{i}|0\rangle =0.} All excited states are obtained as linear combinations of the ground state excited by some creation operators: ∏ k = 1 n a i k † | 0 ⟩ . {\displaystyle \prod _{k=1}^{n}a_{i_{k}}^{\dagger }|0\rangle .} One may redefine the creation and the annihilation operators by a linear redefinition: a i ′ = ∑ j ( u i j a j + v i j a j † ) , {\displaystyle a'_{i}=\sum _{j}(u_{ij}a_{j}+v_{ij}a_{j}^{\dagger }),} where the coefficients u i j , v i j {\displaystyle u_{ij},v_{ij}} must satisfy certain rules to guarantee that the annihilation operators and the creation operators a i ′ † {\displaystyle a_{i}^{\prime \dagger }} , defined by the Hermitian conjugate equation, have the same commutators for bosons and anticommutators for fermions. The equation above defines the Bogoliubov transformation of the operators. The ground state annihilated by all a i ′ {\displaystyle a'_{i}} is different from the original ground state | 0 ⟩ {\displaystyle |0\rangle } , and they can be viewed as the Bogoliubov transformations of one another using the operator–state correspondence. They can also be defined as squeezed coherent states. BCS wave function is an example of squeezed coherent state of fermions. Unified matrix description Because Bogoliubov transformations are linear recombination of operators, it is more convenient and insightful to write them in terms of matrix transformations. If a pair of annihilators ( a , b ) {\displaystyle (a,b)} transform as ( α β ) = U ( a b ) {\displaystyle {\begin{pmatrix}\alpha \\\beta \end{pmatrix}}=U{\begin{pmatrix}a\\b\end{pmatrix}}} where U {\displaystyle U} is a 2 × 2 {\displaystyle 2\times 2} matrix. Then naturally ( α † β † ) = U ∗ ( a † b † ) {\displaystyle {\begin{pmatrix}\alpha ^{\dagger }\\\beta ^{\dagger }\end{pmatrix}}=U^{*}{\begin{pmatrix}a^{\dagger }\\b^{\dagger }\end{pmatrix}}} For fermion operators, the requirement of commutation relations reflects in two requirements for the form of matrix U {\displaystyle U} U = ( u v − v ∗ u ∗ ) {\displaystyle U={\begin{pmatrix}u&v\\-v^{*}&u^{*}\end{pmatrix}}} and | u | 2 + | v | 2 = 1 {\displaystyle |u|^{2}+|v|^{2}=1} For boson operators, the commutation relations require U = ( u v v ∗ u ∗ ) {\displaystyle U={\begin{pmatrix}u&v\\v^{*}&u^{*}\end{pmatrix}}} and | u | 2 − | v | 2 = 1 {\displaystyle |u|^{2}-|v|^{2}=1} These conditions can be written uniformly as U Γ ± U † = Γ ± {\displaystyle U\Gamma _{\pm }U^{\dagger }=\Gamma _{\pm }} where Γ ± = ( 1 0 0 ± 1 ) {\displaystyle \Gamma _{\pm }={\begin{pmatrix}1&0\\0&\pm 1\end{pmatrix}}} where Γ ± {\displaystyle \Gamma _{\pm }} applies to fermions and bosons, respectively. Diagonalizing a quadratic Hamiltonian using matrix description Bogoliubov transformation lets us diagonalize a quadratic Hamiltonian H ^ = ( a † b † ) H ( a b ) {\displaystyle {\hat {H}}={\begin{pmatrix}a^{\dagger }&b^{\dagger }\end{pmatrix}}H{\begin{pmatrix}a\\b\end{pmatrix}}} by just diagonalizing the matrix Γ ± H {\displaystyle \Gamma _{\pm }H} . In the notations above, it is important to distinguish the operator H ^ {\displaystyle {\hat {H}}} and the numeric matrix H {\displaystyle H} . This fact can be seen by rewriting H ^ {\displaystyle {\hat {H}}} as H ^ = ( α † β † ) Γ ± U ( Γ ± H ) U − 1 ( α β ) {\displaystyle {\hat {H}}={\begin{pmatrix}\alpha ^{\dagger }&\beta ^{\dagger }\end{pmatrix}}\Gamma _{\pm }U(\Gamma _{\pm }H)U^{-1}{\begin{pmatrix}\alpha \\\beta \end{pmatrix}}} and Γ ± U ( Γ ± H ) U − 1 = D {\displaystyle \Gamma _{\pm }U(\Gamma _{\pm }H)U^{-1}=D} if and only if U {\displaystyle U} diagonalizes Γ ± H {\displaystyle \Gamma _{\pm }H} , i.e. U ( Γ ± H ) U − 1 = Γ ± D {\displaystyle U(\Gamma _{\pm }H)U^{-1}=\Gamma _{\pm }D} . Useful properties of Bogoliubov transformations are listed below. Other applications Fermionic condensates Bogoliubov transformations are a crucial mathematical tool for understanding and describing fermionic condensates. They provide a way to diagonalize the Hamiltonian of an interacting fermion system in the presence of a condensate, allowing us to identify the elementary excitations, or quasiparticles, of the system. In a system where fermions can form pairs, the standard approach of filling single-particle energy levels (the Fermi sea) is insufficient. The presence of a condensate implies a coherent superposition of states with different particle numbers, making the usual creation and annihilation operators inadequate. The Hamiltonian of such a system typically contains terms that create or annihilate pairs of fermions, such as: H ∼ ∑ k ϵ k c k † c k + ∑ k Δ k c k † c − k † + Δ k ∗ c − k c k {\displaystyle H\sim \sum _{k}\epsilon _{k}c_{k}^{\dagger }c_{k}+\sum _{k}\Delta _{k}c_{k}^{\dagger }c_{-k}^{\dagger }+\Delta _{k}^{*}c_{-k}c_{k}} where c k † {\displaystyle c_{k}^{\dagger }} and c k {\displaystyle c_{k}} are the creation and annihilation operators for a fermion with momentum k {\displaystyle k} , ϵ k {\displaystyle \epsilon _{k}} is the single-particle energy, and Δ k {\displaystyle \Delta _{k}} is the pairing amplitude, which characterizes the strength of the condensate. This Hamiltonian is not diagonal in terms of the original fermion operators, making it difficult to directly interpret the physical properties of the system. Bogoliubov transformations provide a solution by introducing a new set of quasiparticle operators, γ k † {\displaystyle \gamma _{k}^{\dagger }} and γ k {\displaystyle \gamma _{k}} , which are linear combinations of the original fermion operators: γ k = u k c k − v k c − k † γ k † = u k ∗ c k † − v k ∗ c − k {\displaystyle {\begin{aligned}\gamma _{k}&=u_{k}c_{k}-v_{k}c_{-k}^{\dagger }\\\gamma _{k}^{\dagger }&=u_{k}^{*}c_{k}^{\dagger }-v_{k}^{*}c_{-k}\end{aligned}}} where u k {\displaystyle u_{k}} and v k {\displaystyle v_{k}} are complex coefficients that satisfy the normalization condition | u k | 2 + | v k | 2 = 1 {\displaystyle |u_{k}|^{2}+|v_{k}|^{2}=1} . This transformation mixes particle and hole creation operators, reflecting the fact that the quasiparticles are a superposition of particles and holes due to the pairing interaction. This transformation was first introduced by N. N. Bogoliubov in his seminal work on superfluidity. The coefficients u k {\displaystyle u_{k}} and v k {\displaystyle v_{k}} are chosen such that the Hamiltonian, when expressed in terms of the quasiparticle operators, becomes diagonal: H = E 0 + ∑ k E k γ k † γ k {\displaystyle H=E_{0}+\sum _{k}E_{k}\gamma _{k}^{\dagger }\gamma _{k}} where E 0 {\displaystyle E_{0}} is the ground state energy and E k {\displaystyle E_{k}} is the energy of the quasiparticle with momentum k {\displaystyle k} . The diagonalization process involves solving the Bogoliubov-de Gennes equations, which are a set of self-consistent equations for the coefficients u k {\displaystyle u_{k}} , v k {\displaystyle v_{k}} , and the pairing amplitude Δ k {\displaystyle \Delta _{k}} . A detailed discussion of the Bogoliubov-de Gennes equations can be found in de Gennes' book on superconductivity. Physical interpretation The Bogoliubov transformation reveals several key features of fermion condensates: Bogoliubov quasiparticles: The elementary excitations of the system are not individual fermions but quasiparticles, which are coherent superpositions of particles and holes. These quasiparticles have a modified energy spectrum E k = ϵ k 2 + | Δ k | 2 {\displaystyle E_{k}={\sqrt {\epsilon _{k}^{2}+|\Delta _{k}|^{2}}}} , which includes a gap of size | Δ k | {\displaystyle |\Delta _{k}|} at zero momentum. This gap represents the energy required to break a Cooper pair and is a hallmark of superconductivity and other fermionic condensate phenomena. Ground state: The ground state of the system is not simply an empty Fermi sea but a state where all quasiparticle levels are unoccupied, i.e., γ k | B C S ⟩ = 0 {\displaystyle \gamma _{k}|\mathrm {BCS} \rangle =0} for all k {\displaystyle k} . This state, often called the BCS state in the context of superconductivity, is a coherent superposition of states with different particle numbers and represents the macroscopic condensate. Broken symmetry: The formation of a fermion condensate is often associated with the spontaneous breaking of a symmetry, such as the U(1) gauge symmetry in superconductors. The Bogoliubov transformation provides a way to describe the system in the broken symmetry phase. The connection between broken symmetry and Bogoliubov transformations is explored in Anderson's work on pseudo-spin and gauge invariance. See also Holstein–Primakoff transformation Jordan–Wigner transformation Jordan–Schwinger transformation Klein transformation References Further reading The whole topic, and a lot of definite applications, are treated in the following textbooks: Blaizot, J.-P.; Ripka, G. (1985). Quantum Theory of Finite Systems. MIT Press. ISBN 0-262-02214-1. Fetter, A.; Walecka, J. (2003). Quantum Theory of Many-Particle Systems. Dover. ISBN 0-486-42827-3. Kittel, Ch. (1987). Quantum theory of solids. Wiley. ISBN 0-471-62412-8. Wagner, M. (1986). Unitary Transformations in Solid State Physics. Elsevier Science. ISBN 0-444-86975-1. In physics, in the area of quantum information theory, a Greenberger–Horne–Zeilinger (GHZ) state is an entangled quantum state that involves at least three subsystems (particle states, qubits, or qudits). Named for the three authors that first described this state, the GHZ state predicts outcomes from experiments that directly contradict predictions by every classical local hidden-variable theory. The state has applications in quantum computing. History The four-particle version was first studied by Daniel Greenberger, Michael Horne and Anton Zeilinger in 1989. The following year Abner Shimony joined in and they published a three-particle version based on suggestions by N. David Mermin. Experimental measurements on such states contradict intuitive notions of locality and causality. GHZ states for large numbers of qubits are theorized to give enhanced performance for metrology compared to other qubit superposition states. Definition The GHZ state is an entangled quantum state for 3 qubits and it can be written | G H Z ⟩ = | 000 ⟩ + | 111 ⟩ 2 . {\displaystyle |\mathrm {GHZ} \rangle ={\frac {|000\rangle +|111\rangle }{\sqrt {2}}}.} where the 0 or 1 values of the qubit correspond to any two physical states. For example the two states may correspond to spin-down and spin up along some physical axis. In physics applications the state may be written | G H Z ⟩ = | 1 , 1 , 1 ⟩ + | − 1 , − 1 , − 1 ⟩ 2 . {\displaystyle |\mathrm {GHZ} \rangle ={\frac {|1,1,1\rangle +|-1,-1,-1\rangle }{\sqrt {2}}}.} where the numbering of the states represents spin eigenvalues. Another example of a GHZ state is three photons in an entangled state, with the photons being in a superposition of being all horizontally polarized (HHH) or all vertically polarized (VVV), with respect to some coordinate system. The GHZ state can be written in bra–ket notation as | G H Z ⟩ = 1 2 ( | H H H ⟩ + | V V V ⟩ ) . {\displaystyle |\mathrm {GHZ} \rangle ={\frac {1}{\sqrt {2}}}(|\mathrm {HHH} \rangle +|\mathrm {VVV} \rangle ).} Prior to any measurements being made, the polarizations of the photons are indeterminate. If a measurement is made on one of the photons using a two-channel polarizer aligned with the axes of the coordinate system, each orientation will be observed, with 50% probability. However the result of all three measurements on the state gives the same result: all three polarizations are observed along the same axis. Generalization The generalized GHZ state is an entangled quantum state of M > 2 subsystems. If each system has dimension d {\displaystyle d} , i.e., the local Hilbert space is isomorphic to C d {\displaystyle \mathbb {C} ^{d}} , then the total Hilbert space of an M {\displaystyle M} -partite system is H t o t = ( C d ) ⊗ M {\displaystyle {\mathcal {H}}_{\rm {tot}}=(\mathbb {C} ^{d})^{\otimes M}} . This GHZ state is also called an M {\displaystyle M} -partite qudit GHZ state. Its formula as a tensor product is | G H Z ⟩ = 1 d ∑ i = 0 d − 1 | i ⟩ ⊗ ⋯ ⊗ | i ⟩ = 1 d ( | 0 ⟩ ⊗ ⋯ ⊗ | 0 ⟩ + ⋯ + | d − 1 ⟩ ⊗ ⋯ ⊗ | d − 1 ⟩ ) {\displaystyle |\mathrm {GHZ} \rangle ={\frac {1}{\sqrt {d}}}\sum _{i=0}^{d-1}|i\rangle \otimes \cdots \otimes |i\rangle ={\frac {1}{\sqrt {d}}}(|0\rangle \otimes \cdots \otimes |0\rangle +\cdots +|d-1\rangle \otimes \cdots \otimes |d-1\rangle )} . In the case of each of the subsystems being two-dimensional, that is for a collection of M qubits, it reads | G H Z ⟩ = | 0 ⟩ ⊗ M + | 1 ⟩ ⊗ M 2 . {\displaystyle |\mathrm {GHZ} \rangle ={\frac {|0\rangle ^{\otimes M}+|1\rangle ^{\otimes M}}{\sqrt {2}}}.} GHZ experiment In the language of quantum computation, the polarization state of each photon is a qubit, the basis of which can be chosen to be | 0 ⟩ ≡ | H ⟩ , | 1 ⟩ ≡ | V ⟩ . {\displaystyle |0\rangle \equiv |\mathrm {H} \rangle ,\qquad |1\rangle \equiv |\mathrm {V} \rangle .} With appropriately chosen phase factors for | H ⟩ {\displaystyle |\mathrm {H} \rangle } and | V ⟩ {\displaystyle |\mathrm {V} \rangle } , both types of measurements used in the experiment becomes Pauli measurements, with the two possible results represented as +1 and −1 respectively: The 45° linear polarizer implements a Pauli X {\displaystyle X} measurement, distinguishing between the eigenstates | + X ⟩ ≡ | + ⟩ = 1 2 ( | H ⟩ + | V ⟩ ) , | − X ⟩ ≡ | − ⟩ = 1 2 ( | H ⟩ − | V ⟩ ) . {\displaystyle |{+X}\rangle \equiv |+\rangle ={\frac {1}{\sqrt {2}}}(|\mathrm {H} \rangle +|\mathrm {V} \rangle ),\qquad |{-X}\rangle \equiv |-\rangle ={\frac {1}{\sqrt {2}}}(|\mathrm {H} \rangle -|\mathrm {V} \rangle ).} The circular polarizer implements a Pauli Y {\displaystyle Y} measurement, distinguishing between the eigenstates | + Y ⟩ ≡ | R ⟩ = 1 2 ( | H ⟩ + i | V ⟩ ) , | − Y ⟩ ≡ | L ⟩ = 1 2 ( | H ⟩ − i | V ⟩ ) . {\displaystyle |{+Y}\rangle \equiv |R\rangle ={\frac {1}{\sqrt {2}}}(|\mathrm {H} \rangle +i|\mathrm {V} \rangle ),\qquad |{-Y}\rangle \equiv |L\rangle ={\frac {1}{\sqrt {2}}}(|\mathrm {H} \rangle -i|\mathrm {V} \rangle ).} A combination of those measurements on each of the three qubits can be regarded as a destructive multi-qubit Pauli measurement, the result of which being the product of each single-qubit Pauli measurement. For example, the combination "circular polarizer on photons 1 and 2, 45° linear polarizer on photon 3" corresponds to a Y 1 Y 2 X 3 {\displaystyle Y_{1}Y_{2}X_{3}} measurement, and the four possible result combinations (RL+, LR+, RR−, LL−) are exactly the ones corresponding to an overall result of −1. The quantum mechanical predictions of the GHZ experiment can then be summarized as ⟨ G H Z | Y 1 Y 2 X 3 | G H Z ⟩ = ⟨ G H Z | Y 1 X 2 Y 3 | G H Z ⟩ = ⟨ G H Z | X 1 Y 2 Y 3 | G H Z ⟩ = − 1 , {\displaystyle \langle \mathrm {GHZ} |Y_{1}Y_{2}X_{3}|\mathrm {GHZ} \rangle =\langle \mathrm {GHZ} |Y_{1}X_{2}Y_{3}|\mathrm {GHZ} \rangle =\langle \mathrm {GHZ} |X_{1}Y_{2}Y_{3}|\mathrm {GHZ} \rangle =-1,} ⟨ G H Z | X 1 X 2 X 3 | G H Z ⟩ = + 1 , {\displaystyle \langle \mathrm {GHZ} |X_{1}X_{2}X_{3}|\mathrm {GHZ} \rangle =+1,} which is consistent in quantum mechanics because all these multi-qubit Paulis commute with each other, and Y 1 Y 2 X 3 ⋅ Y 1 X 2 Y 3 ⋅ X 1 Y 2 Y 3 ⋅ X 1 X 2 X 3 = − 1 {\displaystyle Y_{1}Y_{2}X_{3}\cdot Y_{1}X_{2}Y_{3}\cdot X_{1}Y_{2}Y_{3}\cdot X_{1}X_{2}X_{3}=-1} due to the anticommutativity between X {\displaystyle X} and Y {\displaystyle Y} . These results lead to a contradiction in any local hidden variable theory, where each measurement must have definite (classical) values x i , y i = ± 1 {\displaystyle x_{i},y_{i}=\pm 1} determined by hidden variables, because y 1 y 2 x 3 ⋅ y 1 x 2 y 3 ⋅ x 1 y 2 y 3 ⋅ x 1 x 2 x 3 = x 1 2 x 2 2 x 3 2 y 1 2 y 2 2 y 3 2 {\displaystyle y_{1}y_{2}x_{3}\cdot y_{1}x_{2}y_{3}\cdot x_{1}y_{2}y_{3}\cdot x_{1}x_{2}x_{3}=x_{1}^{2}x_{2}^{2}x_{3}^{2}y_{1}^{2}y_{2}^{2}y_{3}^{2}} must equal +1, not −1. The results of actual experiments agree with the predictions of quantum mechanics, not those of local realism. Properties There is no standard measure of multi-partite entanglement because different, not mutually convertible, types of multi-partite entanglement exist. Nonetheless, many measures define the GHZ state to be a maximally entangled state. Another important property of the GHZ state is that taking the partial trace over one of the three systems yields Tr 3 ⁡ [ ( | 000 ⟩ + | 111 ⟩ 2 ) ( ⟨ 000 | + ⟨ 111 | 2 ) ] = ( | 00 ⟩ ⟨ 00 | + | 11 ⟩ ⟨ 11 | ) 2 , {\displaystyle \operatorname {Tr} _{3}\left[\left({\frac {|000\rangle +|111\rangle }{\sqrt {2}}}\right)\left({\frac {\langle 000|+\langle 111|}{\sqrt {2}}}\right)\right]={\frac {(|00\rangle \langle 00|+|11\rangle \langle 11|)}{2}},} which is an unentangled mixed state. It has certain two-particle (qubit) correlations, but these are of a classical nature. On the other hand, if we were to measure one of the subsystems in such a way that the measurement distinguishes between the states 0 and 1, we will leave behind either | 00 ⟩ {\displaystyle |00\rangle } or | 11 ⟩ {\displaystyle |11\rangle } , which are unentangled pure states. This is unlike the W state, which leaves bipartite entanglements even when we measure one of its subsystems. A pure state | ψ ⟩ {\displaystyle |\psi \rangle } of N {\displaystyle N} parties is called biseparable, if one can find a partition of the parties in two nonempty disjoint subsets A {\displaystyle A} and B {\displaystyle B} with A ∪ B = { 1 , … , N } {\displaystyle A\cup B=\{1,\dots ,N\}} such that | ψ ⟩ = | ϕ ⟩ A ⊗ | γ ⟩ B {\displaystyle |\psi \rangle =|\phi \rangle _{A}\otimes |\gamma \rangle _{B}} , i.e. | ψ ⟩ {\displaystyle |\psi \rangle } is a product state with respect to the partition A | B {\displaystyle A|B} . The GHZ state is non-biseparable and is the representative of one of the two non-biseparable classes of 3-qubit states which cannot be transformed (not even probabilistically) into each other by local quantum operations, the other being the W state, | W ⟩ = ( | 001 ⟩ + | 010 ⟩ + | 100 ⟩ ) / 3 {\displaystyle |\mathrm {W} \rangle =(|001\rangle +|010\rangle +|100\rangle )/{\sqrt {3}}} .: 903 Thus | G H Z ⟩ {\displaystyle |\mathrm {GHZ} \rangle } and | W ⟩ {\displaystyle |\mathrm {W} \rangle } represent two very different kinds of entanglement for three or more particles. The W state is, in a certain sense "less entangled" than the GHZ state; however, that entanglement is, in a sense, more robust against single-particle measurements, in that, for an N-qubit W state, an entangled (N − 1)-qubit state remains after a single-particle measurement. By contrast, certain measurements on the GHZ state collapse it into a mixture or a pure state. Experiments on the GHZ state lead to striking non-classical correlations (1989). Particles prepared in this state lead to a version of Bell's theorem, which shows the internal inconsistency with the notion of "elements of reality" introduced in the famous Einstein–Podolsky–Rosen article. The first laboratory observation of GHZ correlations was by the group of Anton Zeilinger (1998), who was awarded a share of the 2022 Nobel Prize in physics for this work. Many more accurate observations followed. The correlations can be utilized in some quantum information tasks. These include multipartner quantum cryptography (1998) and communication complexity tasks (1997, 2004). Pairwise entanglement Although a measurement of the third particle of the GHZ state that distinguishes the two states results in an unentangled pair, a measurement along an orthogonal direction can leave behind a maximally entangled Bell state. This is illustrated below. The 3-qubit GHZ state can be written as | G H Z ⟩ = 1 2 ( | 000 ⟩ + | 111 ⟩ ) = 1 2 ( | 00 ⟩ + | 11 ⟩ ) ⊗ | + ⟩ + 1 2 ( | 00 ⟩ − | 11 ⟩ ) ⊗ | − ⟩ , {\displaystyle |\mathrm {GHZ} \rangle ={\frac {1}{\sqrt {2}}}\left(|000\rangle +|111\rangle \right)={\frac {1}{2}}\left(|00\rangle +|11\rangle \right)\otimes |+\rangle +{\frac {1}{2}}\left(|00\rangle -|11\rangle \right)\otimes |-\rangle ,} where the third particle is written as a superposition in the X basis (as opposed to the Z basis) as | 0 ⟩ = ( | + ⟩ + | − ⟩ ) / 2 {\displaystyle |0\rangle =(|+\rangle +|-\rangle )/{\sqrt {2}}} and | 1 ⟩ = ( | + ⟩ − | − ⟩ ) / 2 {\displaystyle |1\rangle =(|+\rangle -|-\rangle )/{\sqrt {2}}} . A measurement of the GHZ state along the X basis for the third particle then yields either | Φ + ⟩ = ( | 00 ⟩ + | 11 ⟩ ) / 2 {\displaystyle |\Phi ^{+}\rangle =(|00\rangle +|11\rangle )/{\sqrt {2}}} , if | + ⟩ {\displaystyle |+\rangle } was measured, or | Φ − ⟩ = ( | 00 ⟩ − | 11 ⟩ ) / 2 {\displaystyle |\Phi ^{-}\rangle =(|00\rangle -|11\rangle )/{\sqrt {2}}} , if | − ⟩ {\displaystyle |-\rangle } was measured. In the later case, the phase can be rotated by applying a Z quantum gate to give | Φ + ⟩ {\displaystyle |\Phi ^{+}\rangle } , while in the former case, no additional transformations are applied. In either case, the result of the operations is a maximally entangled Bell state. This example illustrates that, depending on which measurement is made of the GHZ state is more subtle than it first appears: a measurement along an orthogonal direction, followed by a quantum transform that depends on the measurement outcome, can leave behind a maximally entangled state. Applications GHZ states are used in several protocols in quantum communication and cryptography, for example, in secret sharing or in the quantum Byzantine agreement. See also Bell's theorem Local hidden-variable theory NOON state Quantum pseudo-telepathy Dicke state == References == In mathematics and theoretical physics, resummation is a procedure to obtain a finite result from a divergent sum (series) of functions. Resummation involves a definition of another (convergent) function in which the individual terms defining the original function are rescaled, and an integral transformation of this new function to obtain the original function. Borel resummation is probably the most well-known example. The simplest method is an extension of a variational approach to higher order based on a paper by R.P. Feynman and H. Kleinert. Feynman and Kleinert's technique has been extended to arbitrary order in quantum mechanics and quantum field theory. See also Perturbation theory Perturbation theory (quantum mechanics) References Books Hagen Kleinert and V. Schulte-Frohlinde (2001), Critical Properties of φ4-Theories, Singapore: World Scientific, ISBN 981-02-4658-7 (paperback), especially chapters 16-20. In theoretical physics, the Weyl transformation, named after German mathematician Hermann Weyl, is a local rescaling of the metric tensor: g a b → e − 2 ω ( x ) g a b {\displaystyle g_{ab}\rightarrow e^{-2\omega (x)}g_{ab}} which produces another metric in the same conformal class. A theory or an expression invariant under this transformation is called conformally invariant, or is said to possess Weyl invariance or Weyl symmetry. The Weyl symmetry is an important symmetry in conformal field theory. It is, for example, a symmetry of the Polyakov action. When quantum mechanical effects break the conformal invariance of a theory, it is said to exhibit a conformal anomaly or Weyl anomaly. The ordinary Levi-Civita connection and associated spin connections are not invariant under Weyl transformations. Weyl connections are a class of affine connections that is invariant, although no Weyl connection is individual invariant under Weyl transformations. Conformal weight A quantity φ {\displaystyle \varphi } has conformal weight k {\displaystyle k} if, under the Weyl transformation, it transforms via φ → φ e k ω . {\displaystyle \varphi \to \varphi e^{k\omega }.} Thus conformally weighted quantities belong to certain density bundles; see also conformal dimension. Let A μ {\displaystyle A_{\mu }} be the connection one-form associated to the Levi-Civita connection of g {\displaystyle g} . Introduce a connection that depends also on an initial one-form ∂ μ ω {\displaystyle \partial _{\mu }\omega } via B μ = A μ + ∂ μ ω . {\displaystyle B_{\mu }=A_{\mu }+\partial _{\mu }\omega .} Then D μ φ ≡ ∂ μ φ + k B μ φ {\displaystyle D_{\mu }\varphi \equiv \partial _{\mu }\varphi +kB_{\mu }\varphi } is covariant and has conformal weight k − 1 {\displaystyle k-1} . Formulas For the transformation g a b = f ( ϕ ( x ) ) g ¯ a b {\displaystyle g_{ab}=f(\phi (x)){\bar {g}}_{ab}} We can derive the following formulas g a b = 1 f ( ϕ ( x ) ) g ¯ a b − g = − g ¯ f D / 2 Γ a b c = Γ ¯ a b c + f ′ 2 f ( δ b c ∂ a ϕ + δ a c ∂ b ϕ − g ¯ a b ∂ c ϕ ) ≡ Γ ¯ a b c + γ a b c R a b = R ¯ a b + f ″ f − f ′ 2 2 f 2 ( ( 2 − D ) ∂ a ϕ ∂ b ϕ − g ¯ a b ∂ c ϕ ∂ c ϕ ) + f ′ 2 f ( ( 2 − D ) ∇ ¯ a ∂ b ϕ − g ¯ a b ◻ ¯ ϕ ) + 1 4 f ′ 2 f 2 ( D − 2 ) ( ∂ a ϕ ∂ b ϕ − g ¯ a b ∂ c ϕ ∂ c ϕ ) R = 1 f R ¯ + 1 − D f ( f ″ f − f ′ 2 f 2 ∂ c ϕ ∂ c ϕ + f ′ f ◻ ¯ ϕ ) + 1 4 f f ′ 2 f 2 ( D − 2 ) ( 1 − D ) ∂ c ϕ ∂ c ϕ {\displaystyle {\begin{aligned}g^{ab}&={\frac {1}{f(\phi (x))}}{\bar {g}}^{ab}\\{\sqrt {-g}}&={\sqrt {-{\bar {g}}}}f^{D/2}\\\Gamma _{ab}^{c}&={\bar {\Gamma }}_{ab}^{c}+{\frac {f'}{2f}}\left(\delta _{b}^{c}\partial _{a}\phi +\delta _{a}^{c}\partial _{b}\phi -{\bar {g}}_{ab}\partial ^{c}\phi \right)\equiv {\bar {\Gamma }}_{ab}^{c}+\gamma _{ab}^{c}\\R_{ab}&={\bar {R}}_{ab}+{\frac {f''f-f^{\prime 2}}{2f^{2}}}\left((2-D)\partial _{a}\phi \partial _{b}\phi -{\bar {g}}_{ab}\partial ^{c}\phi \partial _{c}\phi \right)+{\frac {f'}{2f}}\left((2-D){\bar {\nabla }}_{a}\partial _{b}\phi -{\bar {g}}_{ab}{\bar {\Box }}\phi \right)+{\frac {1}{4}}{\frac {f^{\prime 2}}{f^{2}}}(D-2)\left(\partial _{a}\phi \partial _{b}\phi -{\bar {g}}_{ab}\partial _{c}\phi \partial ^{c}\phi \right)\\R&={\frac {1}{f}}{\bar {R}}+{\frac {1-D}{f}}\left({\frac {f''f-f^{\prime 2}}{f^{2}}}\partial ^{c}\phi \partial _{c}\phi +{\frac {f'}{f}}{\bar {\Box }}\phi \right)+{\frac {1}{4f}}{\frac {f^{\prime 2}}{f^{2}}}(D-2)(1-D)\partial _{c}\phi \partial ^{c}\phi \end{aligned}}} Note that the Weyl tensor is invariant under a Weyl rescaling. References Weyl, Hermann (1993) [1921]. Raum, Zeit, Materie [Space, Time, Matter]. Lectures on General Relativity (in German). Berlin: Springer. ISBN 3-540-56978-2. Unrestricted Hartree–Fock (UHF) theory is the most common molecular orbital method for open shell molecules where the number of electrons of each spin are not equal. While restricted Hartree–Fock theory uses a single molecular orbital twice, one multiplied by the α spin function and the other multiplied by the β spin function in the Slater determinant, unrestricted Hartree–Fock theory uses different molecular orbitals for the α and β electrons. This has been called a different orbitals for different spins (DODS) method. The result is a pair of coupled Roothaan equations, known as the Pople–Nesbet–Berthier equations. F α C α = S C α ϵ α {\displaystyle \mathbf {F} ^{\alpha }\ \mathbf {C} ^{\alpha }\ =\mathbf {S} \mathbf {C} ^{\alpha }\ \mathbf {\epsilon } ^{\alpha }\ } F β C β = S C β ϵ β {\displaystyle \mathbf {F} ^{\beta }\ \mathbf {C} ^{\beta }\ =\mathbf {S} \mathbf {C} ^{\beta }\ \mathbf {\epsilon } ^{\beta }\ } Where F α {\displaystyle \mathbf {F} ^{\alpha }\ } and F β {\displaystyle \mathbf {F} ^{\beta }\ } are the Fock matrices for the α {\displaystyle \alpha \ } and β {\displaystyle \beta \ } orbitals, C α {\displaystyle \mathbf {C} ^{\alpha }\ } and C β {\displaystyle \mathbf {C} ^{\beta }\ } are the matrices of coefficients for the α {\displaystyle \alpha \ } and β {\displaystyle \beta \ } orbitals, S {\displaystyle \mathbf {S} } is the overlap matrix of the basis functions, and ϵ α {\displaystyle \mathbf {\epsilon } ^{\alpha }\ } and ϵ β {\displaystyle \mathbf {\epsilon } ^{\beta }\ } are the (diagonal, by convention) matrices of orbital energies for the α {\displaystyle \alpha \ } and β {\displaystyle \beta \ } orbitals. The pair of equations are coupled because the Fock matrix elements of one spin contains coefficients of both spin as the orbital has to be optimized in the average field of all other electrons. The final result is a set of molecular orbitals and orbital energies for the α spin electrons and a set of molecular orbitals and orbital energies for the β electrons. This method has one drawback. A single Slater determinant of different orbitals for different spins is not a satisfactory eigenfunction of the total spin operator - S 2 {\displaystyle \mathbf {S} ^{2}} . The ground state is contaminated by excited states. If there is one more electron of α spin than β spin, the ground state is a doublet. The average value of S 2 {\displaystyle \mathbf {S} ^{2}} , written ⟨ S 2 ⟩ {\displaystyle \langle \mathbf {S} ^{2}\rangle } , should be 1 2 ( 1 2 + 1 ) = 0.75 {\displaystyle {\tfrac {1}{2}}({\tfrac {1}{2}}+1)=0.75} but will actually be rather more than this value as the doublet state is contaminated by a quadruplet state. A triplet state with two excess α electrons should have ⟨ S 2 ⟩ {\displaystyle \langle \mathbf {S} ^{2}\rangle } = 1 (1 + 1) = 2, but it will be larger as the triplet is contaminated by a quintuplet state. When carrying out unrestricted Hartree–Fock calculations, it is always necessary to check this contamination. For example, with a doublet state, if ⟨ S 2 ⟩ {\displaystyle \langle \mathbf {S} ^{2}\rangle } = 0.8 or less, it is probably satisfactory. If it is 1.0 or so, it is certainly not satisfactory and the calculation should be rejected and a different approach taken. It requires experience to make this judgment. Even singlet states can suffer from spin-contamination, for example the H2 dissociation curve is discontinuous at the point when spin-contamination states (known as the Coulson–Fischer point). Despite this drawback, the unrestricted Hartree–Fock method is used frequently, and in preference to the restricted open-shell Hartree–Fock (ROHF) method, because UHF is simpler to code, easier to develop post-Hartree–Fock methods with, and returns unique functions unlike ROHF where different Fock operators can give the same final total wave function, but different orbitals. Unrestricted Hartree–Fock theory was discovered by Gaston Berthier and subsequently developed by John Pople; it is found in almost all ab initio programs. == References == In mathematics, the Fredholm determinant is a complex-valued function which generalizes the determinant of a finite dimensional linear operator. It is defined for bounded operators on a Hilbert space which differ from the identity operator by a trace-class operator (i.e. an operator whose singular values sum up to a finite number). The function is named after the mathematician Erik Ivar Fredholm. Fredholm determinants have had many applications in mathematical physics, the most celebrated example being Gábor Szegő's limit formula, proved in response to a question raised by Lars Onsager and C. N. Yang on the spontaneous magnetization of the Ising model. Definition Setup Let H {\displaystyle H} be a Hilbert space and G {\displaystyle G} the set of bounded invertible operators on H {\displaystyle H} of the form I + T {\displaystyle I+T} , where T {\displaystyle T} is a trace-class operator. G {\displaystyle G} is a group because The set of trace-class operators is an ideal in the algebra of bounded linear operators, so ( I + T ) ( I + T ′ ) − I = T + T ′ + T T ′ {\displaystyle (I+T)(I+T')-I=T+T'+TT'} is trace-class. ( I + T ) − 1 − I = − T ( I + T ) − 1 , {\textstyle (I+T)^{-1}-I=-T(I+T)^{-1},} so ( I + T ) − 1 − I {\displaystyle (I+T)^{-1}-I} is trace class if T {\displaystyle T} is. G {\displaystyle G} has a natural metric given by d ( X , Y ) = ‖ X − Y ‖ 1 {\displaystyle d(X,Y)=\|X-Y\|_{1}} , where ‖ X ‖ 1 = ∑ i | λ i ( X ) | {\displaystyle \|X\|_{1}=\sum _{i}|\lambda _{i}(X)|} is the trace-class norm. Definition by exponential trace One definition uses the exponential trace formula. For finite-dimensional matrices, we have det ( I + A ) = e Tr ⁡ ( ln ⁡ ( I + A ) ) {\textstyle \det(I+A)=e^{\operatorname {Tr} (\ln(I+A))}} , which expands in Taylor series to det ⁡ ( I + A ) = exp ⁡ ( ∑ n = 1 ∞ ( − 1 ) n + 1 n Tr ⁡ ( A n ) ) {\displaystyle \operatorname {det} (I+A)=\exp \left(\sum _{n=1}^{\infty }{\frac {(-1)^{n+1}}{n}}\operatorname {Tr} \left(A^{n}\right)\right)} This then generalizes directly to trace-class operators. Definition by exterior powers In the finite-dimensional case, the determinant of an operator can be interpreted as the factor by which it scales the (oriented) volume of a parallelepiped. This can be generalized to infinite dimensions. In finite dimensions, by expanding the definition of determinant as a sum over permutations, det ( I + A ) = ∑ S det ( A S S ) {\displaystyle \det(I+A)=\sum _{S}\det(A_{SS})} where S {\displaystyle S} ranges over all subsets of the index set of A {\displaystyle A} . For example, when the index set is { 1 , 2 } {\displaystyle \{1,2\}} then S = { } , { 1 } , { 2 } , { 1 , 2 } {\displaystyle S=\{\},\{1\},\{2\},\{1,2\}} . If H {\displaystyle H} is an n {\displaystyle n} -dimensional Hilbert space with inner product ( ⋅ , ⋅ ) {\displaystyle (\cdot ,\cdot )} , then the k {\displaystyle k} -th exterior power Λ k H {\displaystyle \Lambda ^{k}H} is also a ( n k ) {\displaystyle {\binom {n}{k}}} -dimensional Hilbert space, with inner product ( v 1 ∧ v 2 ∧ ⋯ ∧ v k , w 1 ∧ w 2 ∧ ⋯ ∧ w k ) = det ( v i , w j ) . {\displaystyle (v_{1}\wedge v_{2}\wedge \cdots \wedge v_{k},w_{1}\wedge w_{2}\wedge \cdots \wedge w_{k})=\det(v_{i},w_{j}).} In particular e i 1 ∧ e i 2 ∧ ⋯ ∧ e i k , ( i 1 < i 2 < ⋯ < i k ) {\displaystyle e_{i_{1}}\wedge e_{i_{2}}\wedge \cdots \wedge e_{i_{k}},\qquad (i_{1}<i_{2}<\cdots <i_{k})} gives an orthonormal basis of Λ k H {\displaystyle \Lambda ^{k}H} if ( e i ) {\displaystyle (e_{i})} is an orthonormal basis of H {\displaystyle H} . If A {\displaystyle A} is an operator on H {\displaystyle H} , then A {\displaystyle A} functorially defines a bounded operator Λ k ( A ) {\displaystyle \Lambda ^{k}(A)} on Λ k H {\displaystyle \Lambda ^{k}H} by Λ k ( A ) v 1 ∧ v 2 ∧ ⋯ ∧ v k = A v 1 ∧ A v 2 ∧ ⋯ ∧ A v k . {\displaystyle \Lambda ^{k}(A)v_{1}\wedge v_{2}\wedge \cdots \wedge v_{k}=Av_{1}\wedge Av_{2}\wedge \cdots \wedge Av_{k}.} By definition of trace, we have Tr ⁡ ( Λ k A ) = ∑ 1 ≤ i 1 < ⋯ < i k ≤ n ( e i 1 ∧ e i 2 ∧ ⋯ ∧ e i k , A e i 1 ∧ A e i 2 ∧ ⋯ ∧ A e i k ) {\displaystyle \operatorname {Tr} \left(\Lambda ^{k}A\right)=\sum _{1\leq i_{1}<\cdots <i_{k}\leq n}(e_{i_{1}}\wedge e_{i_{2}}\wedge \cdots \wedge e_{i_{k}},Ae_{i_{1}}\wedge Ae_{i_{2}}\wedge \cdots \wedge Ae_{i_{k}})} The summand simplifies to det [ ( e i j , A e i j ′ ) ] = det ( A S S ) {\displaystyle \det[(e_{i_{j}},Ae_{i_{j'}})]=\det(A_{SS})} where S = { i 1 , … , i k } {\displaystyle S=\{i_{1},\dots ,i_{k}\}} . Thus Tr ⁡ Λ k ( A ) = ∑ | S | = k det ⁡ ( A S S ) . det ⁡ ( I + A ) = ∑ k = 0 n Tr ⁡ Λ k ( A ) . {\displaystyle {\begin{aligned}&\operatorname {Tr} \Lambda ^{k}(A)=\sum _{|S|=k}\operatorname {det} \left(A_{SS}\right).\\&\operatorname {det} (I+A)=\sum _{k=0}^{n}\operatorname {Tr} \Lambda ^{k}(A).\end{aligned}}} This generalizes to infinite-dimensional Hilbert spaces, and bounded trace-class operators, allowing us to define the Fredholm determinant by det ( I + A ) = ∑ k = 0 ∞ Tr ⁡ Λ k ( A ) {\displaystyle \det(I+A)=\sum _{k=0}^{\infty }\operatorname {Tr} \Lambda ^{k}(A)} To show that the definition makes sense, note that if A {\displaystyle A} is trace-class, then Λ k ( A ) {\displaystyle \Lambda ^{k}(A)} is also trace-class with ‖ Λ k ( A ) ‖ 1 ≤ ‖ A ‖ 1 k / k ! {\textstyle \|\Lambda ^{k}(A)\|_{1}\leq \|A\|_{1}^{k}/k!} , thus ∑ k = 0 ∞ | Tr ⁡ Λ k ( A ) | ≤ e ‖ A ‖ 1 {\displaystyle \sum _{k=0}^{\infty }|\operatorname {Tr} \Lambda ^{k}(A)|\leq e^{\|A\|_{1}}} . Properties By default, all operators are assumed trace-class. det ( I + A ) ⋅ det ( I + B ) = det ( I + A ) ( I + B ) . {\textstyle \det(I+A)\cdot \det(I+B)=\det(I+A)(I+B).} z ↦ det ( I + z A ) = ∑ k = 0 ∞ z k Tr ⁡ Λ k ( A ) {\textstyle z\mapsto \det(I+zA)=\sum _{k=0}^{\infty }z^{k}\operatorname {Tr} \Lambda ^{k}(A)} defines an entire function, with | det ( I + z A ) | ≤ exp ⁡ ( | z | ⋅ ‖ A ‖ 1 ) . {\textstyle \left|\det(I+zA)\right|\leq \exp(|z|\cdot \|A\|_{1}).} The function A ↦ det ( I + A ) {\displaystyle A\mapsto \det(I+A)} is continuous on trace-class operators, with | det ( I + A ) − det ( I + B ) | ≤ ‖ A − B ‖ 1 exp ⁡ ( ‖ A ‖ 1 + ‖ B ‖ 1 + 1 ) . {\displaystyle \left|\det(I+A)-\det(I+B)\right|\leq \|A-B\|_{1}\exp(\|A\|_{1}+\|B\|_{1}+1).} One can improve this inequality slightly to the following, as noted in (Simon 2005, Chapter 5): | det ( I + A ) − det ( I + B ) | ≤ ‖ A − B ‖ 1 exp ⁡ ( max ( ‖ A ‖ 1 , ‖ B ‖ 1 ) + 1 ) . {\displaystyle \left|\det(I+A)-\det(I+B)\right|\leq \|A-B\|_{1}\exp(\max(\|A\|_{1},\|B\|_{1})+1).} The function det {\displaystyle \det } defines a homomorphism of type G → C × {\displaystyle G\to \mathbb {C} ^{\times }} where C × {\displaystyle \mathbb {C} ^{\times }} the multiplicative group of nonzero complex numbers (since elements of G {\displaystyle G} are invertible). If T {\displaystyle T} is in G {\displaystyle G} and X {\displaystyle X} is invertible, det X T X − 1 = det T . {\textstyle \det XTX^{-1}=\det T.} det e A = exp Tr ⁡ ( A ) . {\textstyle \det e^{A}=\exp \,\operatorname {Tr} (A).} log ⁡ det ( I + z A ) = Tr ⁡ ( log ⁡ ( I + z A ) ) = ∑ k = 1 ∞ ( − 1 ) k + 1 Tr ⁡ A k k z k {\textstyle \log \det(I+zA)=\operatorname {Tr} (\log {(I+zA)})=\sum _{k=1}^{\infty }(-1)^{k+1}{\frac {\operatorname {Tr} A^{k}}{k}}z^{k}} Integral operators The Fredholm determinant is often applied to integral operators. Let the trace-class operator T {\displaystyle T} be an integral operator given by a kernel K ( x , y ) {\displaystyle K(x,y)} , then the Fredholm determinant is defined, like before, by det ( I − λ T ) = ∑ n = 0 ∞ ( − λ ) n Tr ⁡ Λ n ( T ) = exp ⁡ ( − ∑ n = 1 ∞ Tr ⁡ ( T n ) n λ n ) {\displaystyle \det(I-\lambda T)=\sum _{n=0}^{\infty }(-\lambda )^{n}\operatorname {Tr} \Lambda ^{n}(T)=\exp {\left(-\sum _{n=1}^{\infty }{\frac {\operatorname {Tr} (T^{n})}{n}}\lambda ^{n}\right)}} where T {\displaystyle T} is an integral operator. The trace of the operator T {\displaystyle T} and its alternating powers is given in terms of the kernel K {\displaystyle K} by Tr ⁡ T = ∫ K ( x , x ) d x {\displaystyle \operatorname {Tr} T=\int K(x,x)\,dx} and Tr ⁡ Λ 2 ( T ) = 1 2 ! ∬ ( K ( x , x ) K ( y , y ) − K ( x , y ) K ( y , x ) ) d x d y {\displaystyle \operatorname {Tr} \Lambda ^{2}(T)={\frac {1}{2!}}\iint \left(K(x,x)K(y,y)-K(x,y)K(y,x)\right)dx\,dy} and in general Tr ⁡ Λ n ( T ) = 1 n ! ∫ ⋯ ∫ det [ K ( x i , x j ) ] i , j ∈ 1 : n d x 1 : n {\displaystyle \operatorname {Tr} \Lambda ^{n}(T)={\frac {1}{n!}}\int \cdots \int \det[K(x_{i},x_{j})]_{i,j\in 1:n}\,dx_{1:n}} The trace is well-defined for these kernels, since these are trace-class or nuclear operators. To see that this is a special case of the previous section's general definition, note that, Tr ⁡ ( Λ k A ) = ∑ 1 ≤ i 1 < ⋯ < i k ≤ n ( e i 1 ∧ e i 2 ∧ ⋯ ∧ e i k , A e i 1 ∧ A e i 2 ∧ ⋯ ∧ A e i k ) {\displaystyle \operatorname {Tr} \left(\Lambda ^{k}A\right)=\sum _{1\leq i_{1}<\cdots <i_{k}\leq n}(e_{i_{1}}\wedge e_{i_{2}}\wedge \cdots \wedge e_{i_{k}},Ae_{i_{1}}\wedge Ae_{i_{2}}\wedge \cdots \wedge Ae_{i_{k}})} is equivalent to 1 k ! ∑ i 1 , ⋯ , i k ∈ 1 : n , all different det ( A S S ) {\displaystyle {\frac {1}{k!}}\sum _{i_{1},\cdots ,i_{k}\in 1:n,{\text{ all different}}}\det(A_{SS})} where S {\displaystyle S} is the ordered sequence i 1 , … , i k {\displaystyle i_{1},\dots ,i_{k}} . Now, to convert this to integral equations, a matrix becomes a kernel, and a summation over indices becomes an integral over coordinates. The above argument is intuitive. A proper definition requires a presentation showing that each of the manipulations are well-defined, convergent, and so on, for the given situation for which the Fredholm determinant is contemplated. Since the kernel K {\displaystyle K} may be defined for a large variety of Hilbert spaces and Banach spaces, this is a non-trivial exercise. Integral equation The original (Fredholm 1903) considered the integral equation u ( x ) + z ∫ a b K ( x , y ) u ( y ) d y = f ( x ) ( x ∈ ( a , b ) ) {\displaystyle u(x)+z\int _{a}^{b}K(x,y)u(y)dy=f(x)\quad (x\in (a,b))} which can be written as ( I + z A ) u = f {\displaystyle (I+zA)u=f} . Fredholm proved that this equation has a unique solution iff det ( I + z A ) ≠ 0 {\displaystyle \det(I+zA)\neq 0} . Commutators A function F ( t ) {\displaystyle F(t)} from ( a , b ) {\displaystyle (a,b)} into G {\displaystyle G} is said to be differentiable if F ( t ) − I {\displaystyle F(t)-I} is differentiable as a map into the trace-class operators, i.e. if the limit F ˙ ( t ) = lim h → 0 F ( t + h ) − F ( t ) h {\displaystyle {\dot {F}}(t)=\lim _{h\to 0}{F(t+h)-F(t) \over h}} exists in trace-class norm. If g ( t ) {\displaystyle g(t)} is a differentiable function with values in trace-class operators, then so too is exp ⁡ g ( t ) {\displaystyle \exp g(t)} and F − 1 F ˙ = id − exp − ad ⁡ g ( t ) ad ⁡ g ( t ) ⋅ g ˙ ( t ) , {\displaystyle F^{-1}{\dot {F}}={\operatorname {id} -\exp -\operatorname {ad} g(t) \over \operatorname {ad} g(t)}\cdot {\dot {g}}(t),} where ad ⁡ ( X ) ⋅ Y = X Y − Y X . {\displaystyle \operatorname {ad} (X)\cdot Y=XY-YX.} Israel Gohberg and Mark Krein proved that if F {\displaystyle F} is a differentiable function into G {\displaystyle G} , then f = det F {\displaystyle f=\det F} is a differentiable map into C ∗ {\displaystyle \mathbb {C} ^{*}} with f − 1 f ˙ = Tr ⁡ F − 1 F ˙ . {\displaystyle f^{-1}{\dot {f}}=\operatorname {Tr} F^{-1}{\dot {F}}.} This result was used by Joel Pincus, William Helton and Roger Howe to prove that if A {\displaystyle A} and B {\displaystyle B} are bounded operators with trace-class commutator A B − B A {\displaystyle AB-BA} , then det e A e B e − A e − B = exp ⁡ Tr ⁡ ( A B − B A ) . {\displaystyle \det e^{A}e^{B}e^{-A}e^{-B}=\exp \operatorname {Tr} (AB-BA).} Szegő limit formula Let H = L 2 ( S 1 ) {\displaystyle H=L^{2}(S^{1})} and let P {\displaystyle P} be the orthogonal projection onto the Hardy space H 2 ( S 1 ) {\displaystyle H^{2}(S^{1})} . If f {\displaystyle f} is a smooth function on the circle, let m ( f ) {\displaystyle m(f)} denote the corresponding multiplication operator on H {\displaystyle H} . The commutator P m ( f ) − m ( f ) P {\displaystyle Pm(f)-m(f)P} is trace-class. Let T ( f ) {\displaystyle T(f)} be the Toeplitz operator on H 2 ( S 1 ) {\displaystyle H^{2}(S^{1})} defined by T ( f ) = P m ( f ) P , {\displaystyle T(f)=Pm(f)P,} then the additive commutator T ( f ) T ( g ) − T ( g ) T ( f ) {\displaystyle T(f)T(g)-T(g)T(f)} is trace-class if f {\displaystyle f} and g {\displaystyle g} are smooth. Berger and Shaw proved that tr ⁡ ( T ( f ) T ( g ) − T ( g ) T ( f ) ) = 1 2 π i ∫ 0 2 π f d g . {\displaystyle \operatorname {tr} (T(f)T(g)-T(g)T(f))={1 \over 2\pi i}\int _{0}^{2\pi }f\,dg.} If f {\displaystyle f} and g {\displaystyle g} are smooth, then T ( e f + g ) T ( e − f ) T ( e − g ) {\displaystyle T(e^{f+g})T(e^{-f})T(e^{-g})} is in G {\displaystyle G} . Harold Widom used the result of Pincus-Helton-Howe to prove that det T ( e f ) T ( e − f ) = exp ⁡ ∑ n > 0 n a n a − n , {\displaystyle \det T(e^{f})T(e^{-f})=\exp \sum _{n>0}na_{n}a_{-n},} where f ( z ) = ∑ a n z n . {\displaystyle f(z)=\sum a_{n}z^{n}.} He used this to give a new proof of Gábor Szegő's celebrated limit formula: lim N → ∞ det P N m ( e f ) P N = exp ⁡ ∑ n > 0 n a n a − n , {\displaystyle \lim _{N\to \infty }\det P_{N}m(e^{f})P_{N}=\exp \sum _{n>0}na_{n}a_{-n},} where P N {\displaystyle P_{N}} is the projection onto the subspace of H {\displaystyle H} spanned by 1 , z , … , z N {\displaystyle 1,z,\ldots ,z^{N}} and a 0 = 0 {\displaystyle a_{0}=0} . Szegő's limit formula was proved in 1951 in response to a question raised by the work Lars Onsager and C. N. Yang on the calculation of the spontaneous magnetization for the Ising model. The formula of Widom, which leads quite quickly to Szegő's limit formula, is also equivalent to the duality between bosons and fermions in conformal field theory. A singular version of Szegő's limit formula for functions supported on an arc of the circle was proved by Widom; it has been applied to establish probabilistic results on the eigenvalue distribution of random unitary matrices. History The Fredholm determinant was first used in (Fredholm 1903) to solve an integral equation. Realizing the potential, Hilbert wrote 6 papers during 1904 to 1910 (collected in (Hilbert 1924)), beginning the theory of compact operators on Hilbert spaces. See (Bornemann 2010) and references therein. The Fredholm determinant was used by physicist John A. Wheeler (1937, Phys. Rev. 52:1107) to help provide mathematical description of the wavefunction for a composite nucleus composed of antisymmetrized combination of partial wavefunctions by the method of Resonating Group Structure. This method corresponds to the various possible ways of distributing the energy of neutrons and protons into fundamental boson and fermion nucleon cluster groups or building blocks such as the alpha-particle, helium-3, deuterium, triton, di-neutron, etc. When applied to the method of Resonating Group Structure for beta and alpha stable isotopes, use of the Fredholm determinant: (1) determines the energy values of the composite system, and (2) determines scattering and disintegration cross sections. The method of Resonating Group Structure of Wheeler provides the theoretical bases for all subsequent Nucleon Cluster Models and associated cluster energy dynamics for all light and heavy mass isotopes (see review of Cluster Models in physics in N.D. Cook, 2006). References Fredholm, Ivar (1903). "Sur une classe d'équations fonctionnelles" (PDF). Acta Mathematica. 27: 365–390. doi:10.1007/BF02421317. ISSN 0001-5962. Retrieved February 7, 2025. Hilbert, D. (1924). Grundzüge einer allgemeinen Theorie der linearen Integralgleichungen. Fortschritte der mathematischen Wissenschaften in Monographien (in German). B. G. Teubner. Gohberg, Israel; Goldberg, Seymour; Krupnik, Nahum (2000). Traces and Determinants of Linear Operators. Basel: Birkhäuser Basel. doi:10.1007/978-3-0348-8401-3. ISBN 978-3-0348-9551-4. Simon, Barry (2005), Trace Ideals and Their Applications, Mathematical Surveys and Monographs, vol. 120, American Mathematical Society, ISBN 0-8218-3581-5 Wheeler, John A. (1937-12-01). "On the Mathematical Description of Light Nuclei by the Method of Resonating Group Structure". Physical Review. 52 (11). American Physical Society (APS): 1107–1122. Bibcode:1937PhRv...52.1107W. doi:10.1103/physrev.52.1107. ISSN 0031-899X. Bornemann, Folkmar (2010), "On the numerical evaluation of Fredholm determinants", Math. Comp., 79 (270), Springer: 871–915, arXiv:0804.2543, doi:10.1090/s0025-5718-09-02280-7 In quantum field theory, an order operator or an order field is a quantum field version of Landau's order parameter whose expectation value characterizes phase transitions. There exists a dual version of it, the disorder operator or disorder field, whose expectation value characterizes a phase transition by indicating the prolific presence of defect or vortex lines in an ordered phase. The disorder operator is an operator that creates a discontinuity of the ordinary order operators or a monodromy for their values. For example, a 't Hooft operator is a disorder operator. So is the Jordan–Wigner transformation. The concept of a disorder observable was first introduced in the context of 2D Ising spin lattices, where a phase transition between spin-aligned (magnetized) and disordered phases happens at some temperature. See also Operator (physics) Books Kleinert, Hagen, Gauge Fields in Condensed Matter, Vol. I, " SUPERFLOW AND VORTEX LINES", pp. 1–742, Vol. II, "STRESSES AND DEFECTS", pp. 743–1456, World Scientific (Singapore, 1989); Paperback ISBN 9971-5-0210-0 (also available online: Vol. I and Vol. II) == References == In quantum field theory, the Casimir effect (or Casimir force) is a physical force acting on the macroscopic boundaries of a confined space which arises from the quantum fluctuations of a field. The term Casimir pressure is sometimes used when it is described in units of force per unit area. It is named after the Dutch physicist Hendrik Casimir, who predicted the effect for electromagnetic systems in 1948. In the same year Casimir, together with Dirk Polder, described a similar effect experienced by a neutral atom in the vicinity of a macroscopic interface which is called the Casimir–Polder force. Their result is a generalization of the London–van der Waals force and includes retardation due to the finite speed of light. The fundamental principles leading to the London–van der Waals force, the Casimir force, and the Casimir–Polder force can be formulated on the same footing. In 1997, a direct experiment by Steven K. Lamoreaux quantitatively measured the Casimir force to be within 5% of the value predicted by the theory. The Casimir effect can be understood by the idea that the presence of macroscopic material interfaces, such as electrical conductors and dielectrics, alters the vacuum expectation value of the energy of the second-quantized electromagnetic field. Since the value of this energy depends on the shapes and positions of the materials, the Casimir effect manifests itself as a force between such objects. Any medium supporting oscillations has an analogue of the Casimir effect. For example, beads on a string as well as plates submerged in turbulent water or gas illustrate the Casimir force. In modern theoretical physics, the Casimir effect plays an important role in the chiral bag model of the nucleon; in applied physics it is significant in some aspects of emerging microtechnologies and nanotechnologies. Physical properties The typical example is of two uncharged conductive plates in a vacuum, placed a few nanometers apart. In a classical description, the lack of an external field means that no field exists between the plates, and no force connects them. When this field is instead studied using the quantum electrodynamic vacuum, it is seen that the plates do affect the virtual photons that constitute the field, and generate a net force – either an attraction or a repulsion depending on the plates' specific arrangement. Although the Casimir effect can be expressed in terms of virtual particles interacting with the objects, it is best described and more easily calculated in terms of the zero-point energy of a quantized field in the intervening space between the objects. This force has been measured and is a striking example of an effect captured formally by second quantization. The treatment of boundary conditions in these calculations is controversial. In fact, "Casimir's original goal was to compute the van der Waals force between polarizable molecules" of the conductive plates. Thus it can be interpreted without any reference to the zero-point energy (vacuum energy) of quantum fields. Because the strength of the force falls off rapidly with distance, it is measurable only when the distance between the objects is small. This force becomes so strong that it becomes the dominant force between uncharged conductors at submicron scales. In fact, at separations of 10 nm – about 100 times the typical size of an atom – the Casimir effect produces the equivalent of about 1 atmosphere of pressure (the precise value depends on surface geometry and other factors). History Dutch physicists Hendrik Casimir and Dirk Polder at Philips Research Labs proposed the existence of a force between two polarizable atoms and between such an atom and a conducting plate in 1947; this special form is called the Casimir–Polder force. After a conversation with Niels Bohr, who suggested it had something to do with zero-point energy, Casimir alone formulated the theory predicting a force between neutral conducting plates in 1948. This latter phenomenon is called the Casimir effect. Predictions of the force were later extended to finite-conductivity metals and dielectrics, while later calculations considered more general geometries. Experiments before 1997 observed the force qualitatively, and indirect validation of the predicted Casimir energy was made by measuring the thickness of liquid helium films. Finally, in 1997 Lamoreaux's direct experiment quantitatively measured the force to within 5% of the value predicted by the theory. Subsequent experiments approached an accuracy of a few percent. Possible causes Vacuum energy The causes of the Casimir effect are described by quantum field theory, which states that all of the various fundamental fields, such as the electromagnetic field, must be quantized at each and every point in space. In a simplified view, a "field" in physics may be envisioned as if space were filled with interconnected vibrating balls and springs, and the strength of the field can be visualized as the displacement of a ball from its rest position. Vibrations in this field propagate and are governed by the appropriate wave equation for the particular field in question. The second quantization of quantum field theory requires that each such ball-spring combination be quantized, that is, that the strength of the field be quantized at each point in space. At the most basic level, the field at each point in space is a simple harmonic oscillator, and its quantization places a quantum harmonic oscillator at each point. Excitations of the field correspond to the elementary particles of particle physics. However, even the vacuum has a vastly complex structure, so all calculations of quantum field theory must be made in relation to this model of the vacuum. The vacuum has, implicitly, all of the properties that a particle may have: spin, polarization in the case of light, energy, and so on. On average, most of these properties cancel out: the vacuum is, after all, "empty" in this sense. One important exception is the vacuum energy or the vacuum expectation value of the energy. The quantization of a simple harmonic oscillator states that the lowest possible energy or zero-point energy that such an oscillator may have is E = 1 2 ℏ ω . {\displaystyle {E}={\tfrac {1}{2}}\hbar \omega \,.} Summing over all possible oscillators at all points in space gives an infinite quantity. Since only differences in energy are physically measurable (with the notable exception of gravitation, which remains beyond the scope of quantum field theory), this infinity may be considered a feature of the mathematics rather than of the physics. This argument is the underpinning of the theory of renormalization. Dealing with infinite quantities in this way was a cause of widespread unease among quantum field theorists before the development in the 1970s of the renormalization group, a mathematical formalism for scale transformations that provides a natural basis for the process. When the scope of the physics is widened to include gravity, the interpretation of this formally infinite quantity remains problematic. There is currently no compelling explanation as to why it should not result in a cosmological constant that is many orders of magnitude larger than observed. However, since we do not yet have any fully coherent quantum theory of gravity, there is likewise no compelling reason as to why it should instead actually result in the value of the cosmological constant that we observe. The Casimir effect for fermions can be understood as the spectral asymmetry of the fermion operator (−1)F, where it is known as the Witten index. Relativistic van der Waals force Alternatively, a 2005 paper by Robert Jaffe of MIT states that "Casimir effects can be formulated and Casimir forces can be computed without reference to zero-point energies. They are relativistic, quantum forces between charges and currents. The Casimir force (per unit area) between parallel plates vanishes as alpha, the fine structure constant, goes to zero, and the standard result, which appears to be independent of alpha, corresponds to the alpha approaching infinity limit", and that "The Casimir force is simply the (relativistic, retarded) van der Waals force between the metal plates." Casimir and Polder's original paper used this method to derive the Casimir–Polder force. In 1978, Schwinger, DeRadd, and Milton published a similar derivation for the Casimir effect between two parallel plates. More recently, Nikolic proved from first principles of quantum electrodynamics that the Casimir force does not originate from the vacuum energy of the electromagnetic field, and explained in simple terms why the fundamental microscopic origin of Casimir force lies in van der Waals forces. Effects Casimir's observation was that the second-quantized quantum electromagnetic field, in the presence of bulk bodies such as metals or dielectrics, must obey the same boundary conditions that the classical electromagnetic field must obey. In particular, this affects the calculation of the vacuum energy in the presence of a conductor or dielectric. Consider, for example, the calculation of the vacuum expectation value of the electromagnetic field inside a metal cavity, such as, for example, a radar cavity or a microwave waveguide. In this case, the correct way to find the zero-point energy of the field is to sum the energies of the standing waves of the cavity. To each and every possible standing wave corresponds an energy; say the energy of the nth standing wave is En. The vacuum expectation value of the energy of the electromagnetic field in the cavity is then ⟨ E ⟩ = 1 2 ∑ n E n {\displaystyle \langle E\rangle ={\tfrac {1}{2}}\sum _{n}E_{n}} with the sum running over all possible values of n enumerating the standing waves. The factor of ⁠1/2⁠ is present because the zero-point energy of the nth mode is ⁠1/2⁠En, where En is the energy increment for the nth mode. (It is the same ⁠1/2⁠ as appears in the equation E = ⁠1/2⁠ħω.) Written in this way, this sum is clearly divergent; however, it can be used to create finite expressions. In particular, one may ask how the zero-point energy depends on the shape s of the cavity. Each energy level En depends on the shape, and so one should write En(s) for the energy level, and ⟨E(s)⟩ for the vacuum expectation value. At this point comes an important observation: The force at point p on the wall of the cavity is equal to the change in the vacuum energy if the shape s of the wall is perturbed a little bit, say by δs, at p. That is, one has F ( p ) = − δ ⟨ E ( s ) ⟩ δ s | p . {\displaystyle F(p)=-\left.{\frac {\delta \langle E(s)\rangle }{\delta s}}\right\vert _{p}\,.} This value is finite in many practical calculations. Attraction between the plates can be easily understood by focusing on the one-dimensional situation. Suppose that a moveable conductive plate is positioned at a short distance a from one of two widely separated plates (distance l apart). With a ≪ l, the states within the slot of width a are highly constrained so that the energy E of any one mode is widely separated from that of the next. This is not the case in the large region l where there is a large number of states (about ⁠l/a⁠) with energy evenly spaced between E and the next mode in the narrow slot, or in other words, all slightly larger than E. Now on shortening a by an amount da (which is negative), the mode in the narrow slot shrinks in wavelength and therefore increases in energy proportional to −⁠da/a⁠, whereas all the ⁠l/a⁠ states that lie in the large region lengthen and correspondingly decrease their energy by an amount proportional to −⁠da/l⁠ (note the different denominator). The two effects nearly cancel, but the net change is slightly negative, because the energy of all the ⁠l/a⁠ modes in the large region are slightly larger than the single mode in the slot. Thus the force is attractive: it tends to make a slightly smaller, the plates drawing each other closer, across the thin slot. Derivation of Casimir effect assuming zeta-regularization In the original calculation done by Casimir, he considered the space between a pair of conducting metal plates at distance a apart. In this case, the standing waves are particularly easy to calculate, because the transverse component of the electric field and the normal component of the magnetic field must vanish on the surface of a conductor. Assuming the plates lie parallel to the xy-plane, the standing waves are ψ n ( x , y , z ; t ) = e − i ω n t e i k x x + i k y y sin ⁡ ( k n z ) , {\displaystyle \psi _{n}(x,y,z;t)=e^{-i\omega _{n}t}e^{ik_{x}x+ik_{y}y}\sin(k_{n}z)\,,} where ψ stands for the electric component of the electromagnetic field, and, for brevity, the polarization and the magnetic components are ignored here. Here, kx and ky are the wavenumbers in directions parallel to the plates, and k n = n π a {\displaystyle k_{n}={\frac {n\pi }{a}}} is the wavenumber perpendicular to the plates. Here, n is an integer, resulting from the requirement that ψ vanish on the metal plates. The frequency of this wave is ω n = c k x 2 + k y 2 + n 2 π 2 a 2 , {\displaystyle \omega _{n}=c{\sqrt {{k_{x}}^{2}+{k_{y}}^{2}+{\frac {n^{2}\pi ^{2}}{a^{2}}}}}\,,} where c is the speed of light. The vacuum energy is then the sum over all possible excitation modes. Since the area of the plates is large, we may sum by integrating over two of the dimensions in k-space. The assumption of periodic boundary conditions yields, ⟨ E ⟩ = ℏ 2 ⋅ 2 ∫ A d k x d k y ( 2 π ) 2 ∑ n = 1 ∞ ω n , {\displaystyle \langle E\rangle ={\frac {\hbar }{2}}\cdot 2\int {\frac {A\,dk_{x}\,dk_{y}}{(2\pi )^{2}}}\sum _{n=1}^{\infty }\omega _{n}\,,} where A is the area of the metal plates, and a factor of 2 is introduced for the two possible polarizations of the wave. This expression is clearly infinite, and to proceed with the calculation, it is convenient to introduce a regulator (discussed in greater detail below). The regulator will serve to make the expression finite, and in the end will be removed. The zeta-regulated version of the energy per unit-area of the plate is ⟨ E ( s ) ⟩ A = ℏ ∫ d k x d k y ( 2 π ) 2 ∑ n = 1 ∞ ω n | ω n | − s . {\displaystyle {\frac {\langle E(s)\rangle }{A}}=\hbar \int {\frac {dk_{x}\,dk_{y}}{(2\pi )^{2}}}\sum _{n=1}^{\infty }\omega _{n}\left|\omega _{n}\right|^{-s}\,.} In the end, the limit s → 0 is to be taken. Here s is just a complex number, not to be confused with the shape discussed previously. This integral sum is finite for s real and larger than 3. The sum has a pole at s = 3, but may be analytically continued to s = 0, where the expression is finite. The above expression simplifies to: ⟨ E ( s ) ⟩ A = ℏ c 1 − s 4 π 2 ∑ n ∫ 0 ∞ 2 π q d q | q 2 + π 2 n 2 a 2 | 1 − s 2 , {\displaystyle {\frac {\langle E(s)\rangle }{A}}={\frac {\hbar c^{1-s}}{4\pi ^{2}}}\sum _{n}\int _{0}^{\infty }2\pi q\,dq\left|q^{2}+{\frac {\pi ^{2}n^{2}}{a^{2}}}\right|^{\frac {1-s}{2}}\,,} where polar coordinates q2 = kx2 + ky2 were introduced to turn the double integral into a single integral. The q in front is the Jacobian, and the 2π comes from the angular integration. The integral converges if Re(s) > 3, resulting in ⟨ E ( s ) ⟩ A = − ℏ c 1 − s π 2 − s 2 a 3 − s 1 3 − s ∑ n | n | 3 − s = − ℏ c 1 − s π 2 − s 2 a 3 − s ( 3 − s ) ∑ n 1 | n | s − 3 . {\displaystyle {\frac {\langle E(s)\rangle }{A}}=-{\frac {\hbar c^{1-s}\pi ^{2-s}}{2a^{3-s}}}{\frac {1}{3-s}}\sum _{n}\left|n\right|^{3-s}=-{\frac {\hbar c^{1-s}\pi ^{2-s}}{2a^{3-s}(3-s)}}\sum _{n}{\frac {1}{\left|n\right|^{s-3}}}\,.} The sum diverges at s in the neighborhood of zero, but if the damping of large-frequency excitations corresponding to analytic continuation of the Riemann zeta function to s = 0 is assumed to make sense physically in some way, then one has ⟨ E ⟩ A = lim s → 0 ⟨ E ( s ) ⟩ A = − ℏ c π 2 6 a 3 ζ ( − 3 ) . {\displaystyle {\frac {\langle E\rangle }{A}}=\lim _{s\to 0}{\frac {\langle E(s)\rangle }{A}}=-{\frac {\hbar c\pi ^{2}}{6a^{3}}}\zeta (-3)\,.} But ζ(−3) = ⁠1/120⁠ and so one obtains ⟨ E ⟩ A = − ℏ c π 2 720 a 3 . {\displaystyle {\frac {\langle E\rangle }{A}}=-{\frac {\hbar c\pi ^{2}}{720a^{3}}}\,.} The analytic continuation has evidently lost an additive positive infinity, somehow exactly accounting for the zero-point energy (not included above) outside the slot between the plates, but which changes upon plate movement within a closed system. The Casimir force per unit area ⁠Fc/A⁠ for idealized, perfectly conducting plates with vacuum between them is F c A = − d d a ⟨ E ⟩ A = − ℏ c π 2 240 a 4 {\displaystyle {\frac {F_{\mathrm {c} }}{A}}=-{\frac {d}{da}}{\frac {\langle E\rangle }{A}}=-{\frac {\hbar c\pi ^{2}}{240a^{4}}}} where ħ is the reduced Planck constant, c is the speed of light, a is the distance between the two plates The force is negative, indicating that the force is attractive: by moving the two plates closer together, the energy is lowered. The presence of ħ shows that the Casimir force per unit area ⁠Fc/A⁠ is very small, and that furthermore, the force is inherently of quantum-mechanical origin. By integrating the equation above it is possible to calculate the energy required to separate to infinity the two plates as: U E ( a ) = ∫ F ( a ) d a = ∫ − ℏ c π 2 A 240 a 4 d a = ℏ c π 2 A 720 a 3 {\displaystyle {\begin{aligned}U_{E}(a)&=\int F(a)\,da=\int -\hbar c\pi ^{2}{\frac {A}{240a^{4}}}\,da\\[4pt]&=\hbar c\pi ^{2}{\frac {A}{720a^{3}}}\end{aligned}}} where ħ is the reduced Planck constant, c is the speed of light, A is the area of one of the plates, a is the distance between the two plates In Casimir's original derivation, a moveable conductive plate is positioned at a short distance a from one of two widely separated plates (distance L apart). The zero-point energy on both sides of the plate is considered. Instead of the above ad hoc analytic continuation assumption, non-convergent sums and integrals are computed using Euler–Maclaurin summation with a regularizing function (e.g., exponential regularization) not so anomalous as |ωn|−s in the above. More recent theory Casimir's analysis of idealized metal plates was generalized to arbitrary dielectric and realistic metal plates by Evgeny Lifshitz and his students. Using this approach, complications of the bounding surfaces, such as the modifications to the Casimir force due to finite conductivity, can be calculated numerically using the tabulated complex dielectric functions of the bounding materials. Lifshitz's theory for two metal plates reduces to Casimir's idealized ⁠1/a4⁠ force law for large separations a much greater than the skin depth of the metal, and conversely reduces to the ⁠1/a3⁠ force law of the London dispersion force (with a coefficient called a Hamaker constant) for small a, with a more complicated dependence on a for intermediate separations determined by the dispersion of the materials. Lifshitz's result was subsequently generalized to arbitrary multilayer planar geometries as well as to anisotropic and magnetic materials, but for several decades the calculation of Casimir forces for non-planar geometries remained limited to a few idealized cases admitting analytical solutions. For example, the force in the experimental sphere–plate geometry was computed with an approximation (due to Derjaguin) that the sphere radius R is much larger than the separation a, in which case the nearby surfaces are nearly parallel and the parallel-plate result can be adapted to obtain an approximate ⁠R/a3⁠ force (neglecting both skin-depth and higher-order curvature effects). However, in the 2010s a number of authors developed and demonstrated a variety of numerical techniques, in many cases adapted from classical computational electromagnetics, that are capable of accurately calculating Casimir forces for arbitrary geometries and materials, from simple finite-size effects of finite plates to more complicated phenomena arising for patterned surfaces or objects of various shapes. Measurement One of the first experimental tests was conducted by Marcus Sparnaay at Philips in Eindhoven (Netherlands), in 1958, in a delicate and difficult experiment with parallel plates, obtaining results not in contradiction with the Casimir theory, but with large experimental errors. The Casimir effect was measured more accurately in 1997 by Steve K. Lamoreaux of Los Alamos National Laboratory, and by Umar Mohideen and Anushree Roy of the University of California, Riverside. In practice, rather than using two parallel plates, which would require phenomenally accurate alignment to ensure they were parallel, the experiments use one plate that is flat and another plate that is a part of a sphere with a very large radius. In 2001, a group (Giacomo Bressi, Gianni Carugno, Roberto Onofrio and Giuseppe Ruoso) at the University of Padua (Italy) finally succeeded in measuring the Casimir force between parallel plates using microresonators. Numerous variations of these experiments are summarized in the 2009 review by Klimchitskaya. In 2013, a conglomerate of scientists from Hong Kong University of Science and Technology, University of Florida, Harvard University, Massachusetts Institute of Technology, and Oak Ridge National Laboratory demonstrated a compact integrated silicon chip that can measure the Casimir force. The integrated chip defined by electron-beam lithography does not need extra alignment, making it an ideal platform for measuring Casimir force between complex geometries. In 2017 and 2021, the same group from Hong Kong University of Science and Technology demonstrated the non-monotonic Casimir force and distance-independent Casimir force, respectively, using this on-chip platform. Regularization In order to be able to perform calculations in the general case, it is convenient to introduce a regulator in the summations. This is an artificial device, used to make the sums finite so that they can be more easily manipulated, followed by the taking of a limit so as to remove the regulator. The heat kernel or exponentially regulated sum is ⟨ E ( t ) ⟩ = 1 2 ∑ n ℏ | ω n | exp ⁡ ( − t | ω n | ) , {\displaystyle \langle E(t)\rangle ={\frac {1}{2}}\sum _{n}\hbar |\omega _{n}|\exp {\bigl (}-t|\omega _{n}|{\bigr )}\,,} where the limit t → 0+ is taken in the end. The divergence of the sum is typically manifested as ⟨ E ( t ) ⟩ = C t 3 + finite {\displaystyle \langle E(t)\rangle ={\frac {C}{t^{3}}}+{\textrm {finite}}\,} for three-dimensional cavities. The infinite part of the sum is associated with the bulk constant C which does not depend on the shape of the cavity. The interesting part of the sum is the finite part, which is shape-dependent. The Gaussian regulator ⟨ E ( t ) ⟩ = 1 2 ∑ n ℏ | ω n | exp ⁡ ( − t 2 | ω n | 2 ) {\displaystyle \langle E(t)\rangle ={\frac {1}{2}}\sum _{n}\hbar |\omega _{n}|\exp \left(-t^{2}|\omega _{n}|^{2}\right)} is better suited to numerical calculations because of its superior convergence properties, but is more difficult to use in theoretical calculations. Other, suitably smooth, regulators may be used as well. The zeta function regulator ⟨ E ( s ) ⟩ = 1 2 ∑ n ℏ | ω n | | ω n | − s {\displaystyle \langle E(s)\rangle ={\frac {1}{2}}\sum _{n}\hbar |\omega _{n}||\omega _{n}|^{-s}} is completely unsuited for numerical calculations, but is quite useful in theoretical calculations. In particular, divergences show up as poles in the complex s plane, with the bulk divergence at s = 4. This sum may be analytically continued past this pole, to obtain a finite part at s = 0. Not every cavity configuration necessarily leads to a finite part (the lack of a pole at s = 0) or shape-independent infinite parts. In this case, it should be understood that additional physics has to be taken into account. In particular, at extremely large frequencies (above the plasma frequency), metals become transparent to photons (such as X-rays), and dielectrics show a frequency-dependent cutoff as well. This frequency dependence acts as a natural regulator. There are a variety of bulk effects in solid state physics, mathematically very similar to the Casimir effect, where the cutoff frequency comes into explicit play to keep expressions finite. (These are discussed in greater detail in Landau and Lifshitz, "Theory of Continuous Media".) Generalities The Casimir effect can also be computed using the mathematical mechanisms of functional integrals of quantum field theory, although such calculations are considerably more abstract, and thus difficult to comprehend. In addition, they can be carried out only for the simplest of geometries. However, the formalism of quantum field theory makes it clear that the vacuum expectation value summations are in a certain sense summations over so-called "virtual particles". More interesting is the understanding that the sums over the energies of standing waves should be formally understood as sums over the eigenvalues of a Hamiltonian. This allows atomic and molecular effects, such as the Van der Waals force, to be understood as a variation on the theme of the Casimir effect. Thus one considers the Hamiltonian of a system as a function of the arrangement of objects, such as atoms, in configuration space. The change in the zero-point energy as a function of changes of the configuration can be understood to result in forces acting between the objects. In the chiral bag model of the nucleon, the Casimir energy plays an important role in showing the mass of the nucleon is independent of the bag radius. In addition, the spectral asymmetry is interpreted as a non-zero vacuum expectation value of the baryon number, cancelling the topological winding number of the pion field surrounding the nucleon. A "pseudo-Casimir" effect can be found in liquid crystal systems, where the boundary conditions imposed through anchoring by rigid walls give rise to a long-range force, analogous to the force that arises between conducting plates. Dynamical Casimir effect The dynamical Casimir effect is the production of particles and energy from an accelerated moving mirror. This reaction was predicted by certain numerical solutions to quantum mechanics equations made in the 1970s. In May 2011 an announcement was made by researchers at the Chalmers University of Technology, in Gothenburg, Sweden, of the detection of the dynamical Casimir effect. In their experiment, microwave photons were generated out of the vacuum in a superconducting microwave resonator. These researchers used a modified SQUID to change the effective length of the resonator in time, mimicking a mirror moving at the required relativistic velocity. If confirmed this would be the first experimental verification of the dynamical Casimir effect. In March 2013 an article appeared on the PNAS scientific journal describing an experiment that demonstrated the dynamical Casimir effect in a Josephson metamaterial. In July 2019 an article was published describing an experiment providing evidence of optical dynamical Casimir effect in a dispersion-oscillating fibre. In 2020, Frank Wilczek et al., proposed a resolution to the information loss paradox associated with the moving mirror model of the dynamical Casimir effect. Constructed within the framework of quantum field theory in curved spacetime, the dynamical Casimir effect (moving mirror) has been used to help understand the Unruh effect. Repulsive forces There are a few instances where the Casimir effect can give rise to repulsive forces between uncharged objects. Evgeny Lifshitz showed (theoretically) that in certain circumstances (most commonly involving liquids), repulsive forces can arise. This has sparked interest in applications of the Casimir effect toward the development of levitating devices. An experimental demonstration of the Casimir-based repulsion predicted by Lifshitz was carried out by Munday et al. who described it as "quantum levitation". Other scientists have also suggested the use of gain media to achieve a similar levitation effect, though this is controversial because these materials seem to violate fundamental causality constraints and the requirement of thermodynamic equilibrium (Kramers–Kronig relations). Casimir and Casimir–Polder repulsion can in fact occur for sufficiently anisotropic electrical bodies; for a review of the issues involved with repulsion see Milton et al. A notable recent development on repulsive Casimir forces relies on using chiral materials. Q.-D. Jiang at Stockholm University and Nobel Laureate Frank Wilczek at MIT show that chiral "lubricant" can generate repulsive, enhanced, and tunable Casimir interactions. Timothy Boyer showed in his work published in 1968 that a conductor with spherical symmetry will also show this repulsive force, and the result is independent of radius. Further work shows that the repulsive force can be generated with materials of carefully chosen dielectrics. Speculative applications It has been suggested that the Casimir forces have application in nanotechnology, in particular silicon integrated circuit technology based micro- and nanoelectromechanical systems, and so-called Casimir oscillators. In 1995 and 1998 Maclay et al. published the first models of a microelectromechanical system (MEMS) with Casimir forces. While not exploiting the Casimir force for useful work, the papers drew attention from the MEMS community due to the revelation that Casimir effect needs to be considered as a vital factor in the future design of MEMS. In particular, Casimir effect might be the critical factor in the stiction failure of MEMS. In 2001, Capasso et al. showed how the force can be used to control the mechanical motion of a MEMS device, The researchers suspended a polysilicon plate from a torsional rod – a twisting horizontal bar just a few microns in diameter. When they brought a metallized sphere close up to the plate, the attractive Casimir force between the two objects made the plate rotate. They also studied the dynamical behaviour of the MEMS device by making the plate oscillate. The Casimir force reduced the rate of oscillation and led to nonlinear phenomena, such as hysteresis and bistability in the frequency response of the oscillator. According to the team, the system's behaviour agreed well with theoretical calculations. The Casimir effect shows that quantum field theory allows the energy density in very small regions of space to be negative relative to the ordinary vacuum energy, and the energy densities cannot be arbitrarily negative as the theory breaks down at atomic distances.: 175 Such prominent physicists such as Stephen Hawking and Kip Thorne, have speculated that such effects might make it possible to stabilize a traversable wormhole. See also Negative energy Scharnhorst effect Van der Waals force Squeezed vacuum References Further reading Introductory readings Casimir effect description from University of California, Riverside's version of the Usenet physics FAQ. A. Lambrecht, The Casimir effect: a force from nothing, Physics World, September 2002. NASA Astronomy Picture of the Day: Casimir effect (17 December 2006) Simpson, W. M. R; Leonhardt, U. (2015). Forces of the Quantum Vacuum: An introduction to Casimir physics. World Scientific. ISBN 978-981-4632-90-4. Papers, books and lectures Casimir, H. B. G.; Polder, D. (1948). "The Influence of Retardation on the London-van der Waals Forces". Physical Review. 73 (4): 360–372. Bibcode:1948PhRv...73..360C. doi:10.1103/PhysRev.73.360. Casimir, H. B. G. (1948). "On the attraction between two perfectly conducting plates" (PDF). Proceedings of the Koninklijke Nederlandse Akademie van Wetenschappen. B51: 793–795. Lamoreaux, S. K. (1997). "Demonstration of the Casimir Force in the 0.6 to 6 μm Range". Physical Review Letters. 78 (1): 5–8. Bibcode:1997PhRvL..78....5L. doi:10.1103/PhysRevLett.78.5. S2CID 25323874. Bordag, M.; Mohideen, U.; Mostepanenko, V. M. (October 2001). "New developments in the Casimir effect". Physics Reports. 353 (1–3): 1–205. arXiv:quant-ph/0106045. Bibcode:2001PhR...353....1B. doi:10.1016/S0370-1573(01)00015-1. S2CID 119352552. Milton, K. A. (2001). The Casimir Effect: Physical Manifestations of Zero-point Energy (Reprint ed.). World Scientific. ISBN 978-981-02-4397-5. Dalvit, Diego; Milonni, Peter; Roberts, David; Da Rosa, Felipe (2011). Dalvit, Diego; Milonni, Peter W.; Roberts, David; da Rosa, Felipe (eds.). Casimir Physics. Lecture Notes in Physics. Vol. 834. arXiv:1007.0966. Bibcode:2011LNP...834.....D. doi:10.1007/978-3-642-20288-9. ISBN 978-3-642-20287-2. ISSN 0075-8450. OCLC 844922239. Bressi, G.; Carugno, G.; Onofrio, R.; Ruoso, G. (2002). "Measurement of the Casimir Force between Parallel Metallic Surfaces". Physical Review Letters. 88 (4): 041804. arXiv:quant-ph/0203002. Bibcode:2002PhRvL..88d1804B. doi:10.1103/PhysRevLett.88.041804. PMID 11801108. S2CID 43354557. Kenneth, O.; Klich, I.; Mann, A.; Revzen, M. (2002). "Repulsive Casimir Forces". Physical Review Letters. 89 (3): 033001. arXiv:quant-ph/0202114. Bibcode:2002PhRvL..89c3001K. doi:10.1103/PhysRevLett.89.033001. PMID 12144387. S2CID 20903628. Barrow, J. D. (2005). "Much Ado About Nothing". Lecture at Gresham College. Archived from the original on 30 September 2007. (Includes discussion of French naval analogy.) Barrow, J. D. (2000). The Book of Nothing: Vacuums, Voids, and the Latest Ideas About the Origins of the Universe. Pantheon Books. ISBN 978-0-09-928845-9. (Also includes discussion of French naval analogy.) Downling, J. P. (1989). "The Mathematics of the Casimir Effect". Mathematics Magazine. 62 (5): 324–331. doi:10.1080/0025570X.1989.11977464. Patent No. PCT/RU2011/000847 Author Urmatskih. Temperature dependence Measurements Recast Usual View of Elusive Force from NIST Nesterenko, V. V.; Lambiase, G.; Scarpetta, G. (2005). "Calculation of the Casimir energy at zero and finite temperature: Some recent results". Rivista del Nuovo Cimento. 27 (6): 1–74. arXiv:hep-th/0503100. Bibcode:2004NCimR..27f...1N. doi:10.1393/ncr/i2005-10002-2. S2CID 14693485. External links Casimir effect article search on arxiv.org G. Lang, The Casimir Force web site, 2002 J. Babb, bibliography on the Casimir Effect web site, 2009 H. Nikolic, The origin of Casimir effect; Vacuum energy or van der Waals force? presentation slides, 2018 Renormalization is a collection of techniques in quantum field theory, statistical field theory, and the theory of self-similar geometric structures, that is used to treat infinities arising in calculated quantities by altering values of these quantities to compensate for effects of their self-interactions. But even if no infinities arose in loop diagrams in quantum field theory, it could be shown that it would be necessary to renormalize the mass and fields appearing in the original Lagrangian. For example, an electron theory may begin by postulating an electron with an initial mass and charge. In quantum field theory a cloud of virtual particles, such as photons, positrons, and others surrounds and interacts with the initial electron. Accounting for the interactions of the surrounding particles (e.g. collisions at different energies) shows that the electron-system behaves as if it had a different mass and charge than initially postulated. Renormalization, in this example, mathematically replaces the initially postulated mass and charge of an electron with the experimentally observed mass and charge. Mathematics and experiments prove that positrons and more massive particles such as protons exhibit precisely the same observed charge as the electron – even in the presence of much stronger interactions and more intense clouds of virtual particles. Renormalization specifies relationships between parameters in the theory when parameters describing large distance scales differ from parameters describing small distance scales. Physically, the pileup of contributions from an infinity of scales involved in a problem may then result in further infinities. When describing spacetime as a continuum, certain statistical and quantum mechanical constructions are not well-defined. To define them, or make them unambiguous, a continuum limit must carefully remove "construction scaffolding" of lattices at various scales. Renormalization procedures are based on the requirement that certain physical quantities (such as the mass and charge of an electron) equal observed (experimental) values. That is, the experimental value of the physical quantity yields practical applications, but due to their empirical nature the observed measurement represents areas of quantum field theory that require deeper derivation from theoretical bases. Renormalization was first developed in quantum electrodynamics (QED) to make sense of infinite integrals in perturbation theory. Initially viewed as a suspect provisional procedure even by some of its originators, renormalization eventually was embraced as an important and self-consistent actual mechanism of scale physics in several fields of physics and mathematics. Despite his later skepticism, it was Paul Dirac who pioneered renormalization. Today, the point of view has shifted: on the basis of the breakthrough renormalization group insights of Nikolay Bogolyubov and Kenneth Wilson, the focus is on variation of physical quantities across contiguous scales, while distant scales are related to each other through "effective" descriptions. All scales are linked in a broadly systematic way, and the actual physics pertinent to each is extracted with the suitable specific computational techniques appropriate for each. Wilson clarified which variables of a system are crucial and which are redundant. Renormalization is distinct from regularization, another technique to control infinities by assuming the existence of new unknown physics at new scales. Self-interactions in classical physics The problem of infinities first arose in the classical electrodynamics of point particles in the 19th and early 20th century. The mass of a charged particle should include the mass–energy in its electrostatic field (electromagnetic mass). Assume that the particle is a charged spherical shell of radius re. The mass–energy in the field is m em = ∫ 1 2 E 2 d V = ∫ r e ∞ 1 2 ( q 4 π r 2 ) 2 4 π r 2 d r = q 2 8 π r e , {\displaystyle m_{\text{em}}=\int {\frac {1}{2}}E^{2}\,dV=\int _{r_{\text{e}}}^{\infty }{\frac {1}{2}}\left({\frac {q}{4\pi r^{2}}}\right)^{2}4\pi r^{2}\,dr={\frac {q^{2}}{8\pi r_{\text{e}}}},} which becomes infinite as re → 0. This implies that the point particle would have infinite inertia and thus cannot be accelerated. Incidentally, the value of re that makes m em {\displaystyle m_{\text{em}}} equal to the electron mass is called the classical electron radius, which (setting q = e {\displaystyle q=e} and restoring factors of c and ε 0 {\displaystyle \varepsilon _{0}} ) turns out to be r e = e 2 4 π ε 0 m e c 2 = α ℏ m e c ≈ 2.8 × 10 − 15 m , {\displaystyle r_{\text{e}}={\frac {e^{2}}{4\pi \varepsilon _{0}m_{\text{e}}c^{2}}}=\alpha {\frac {\hbar }{m_{\text{e}}c}}\approx 2.8\times 10^{-15}~{\text{m}},} where α ≈ 1 / 137 {\displaystyle \alpha \approx 1/137} is the fine-structure constant, and ℏ / ( m e c ) {\displaystyle \hbar /(m_{\text{e}}c)} is the reduced Compton wavelength of the electron. Renormalization: The total effective mass of a spherical charged particle includes the actual bare mass of the spherical shell (in addition to the mass mentioned above associated with its electric field). If the shell's bare mass is allowed to be negative, it might be possible to take a consistent point limit. This was called renormalization, and Lorentz and Abraham attempted to develop a classical theory of the electron this way. This early work was the inspiration for later attempts at regularization and renormalization in quantum field theory. (See also regularization (physics) for an alternative way to remove infinities from this classical problem, assuming new physics exists at small scales.) When calculating the electromagnetic interactions of charged particles, it is tempting to ignore the back-reaction of a particle's own field on itself. (Analogous to the back-EMF of circuit analysis.) But this back-reaction is necessary to explain the friction on charged particles when they emit radiation. If the electron is assumed to be a point, the value of the back-reaction diverges, for the same reason that the mass diverges, because the field is inverse-square. The Abraham–Lorentz theory had a noncausal "pre-acceleration". Sometimes an electron would start moving before the force is applied. This is a sign that the point limit is inconsistent. The trouble was worse in classical field theory than in quantum field theory, because in quantum field theory a charged particle experiences Zitterbewegung due to interference with virtual particle–antiparticle pairs, thus effectively smearing out the charge over a region comparable to the Compton wavelength. In quantum electrodynamics at small coupling, the electromagnetic mass only diverges as the logarithm of the radius of the particle. Divergences in quantum electrodynamics When developing quantum electrodynamics in the 1930s, Max Born, Werner Heisenberg, Pascual Jordan, and Paul Dirac discovered that in perturbative corrections many integrals were divergent (see The problem of infinities). One way of describing the perturbation theory corrections' divergences was discovered in 1947–49 by Hans Kramers, Hans Bethe, Julian Schwinger, Richard Feynman, and Shin'ichiro Tomonaga, and systematized by Freeman Dyson in 1949. The divergences appear in radiative corrections involving Feynman diagrams with closed loops of virtual particles in them. While virtual particles obey conservation of energy and momentum, they can have any energy and momentum, even one that is not allowed by the relativistic energy–momentum relation for the observed mass of that particle (that is, E 2 − p 2 {\displaystyle E^{2}-p^{2}} is not necessarily the squared mass of the particle in that process, e.g. for a photon it could be nonzero). Such a particle is called off-shell. When there is a loop, the momentum of the particles involved in the loop is not uniquely determined by the energies and momenta of incoming and outgoing particles. A variation in the energy of one particle in the loop can be balanced by an equal and opposite change in the energy of another particle in the loop, without affecting the incoming and outgoing particles. Thus many variations are possible. So to find the amplitude for the loop process, one must integrate over all possible combinations of energy and momentum that could travel around the loop. These integrals are often divergent, that is, they give infinite answers. The divergences that are significant are the "ultraviolet" (UV) ones. An ultraviolet divergence can be described as one that comes from the region in the integral where all particles in the loop have large energies and momenta, very short wavelengths and high-frequencies fluctuations of the fields, in the path integral for the field, very short proper-time between particle emission and absorption, if the loop is thought of as a sum over particle paths. So these divergences are short-distance, short-time phenomena. Shown in the pictures at the right margin, there are exactly three one-loop divergent loop diagrams in quantum electrodynamics: The three divergences correspond to the three parameters in the theory under consideration: The field normalization Z. The mass of the electron. The charge of the electron. The second class of divergence called an infrared divergence, is due to massless particles, like the photon. Every process involving charged particles emits infinitely many coherent photons of infinite wavelength, and the amplitude for emitting any finite number of photons is zero. For photons, these divergences are well understood. For example, at the 1-loop order, the vertex function has both ultraviolet and infrared divergences. In contrast to the ultraviolet divergence, the infrared divergence does not require the renormalization of a parameter in the theory involved. The infrared divergence of the vertex diagram is removed by including a diagram similar to the vertex diagram with the following important difference: the photon connecting the two legs of the electron is cut and replaced by two on-shell (i.e. real) photons whose wavelengths tend to infinity; this diagram is equivalent to the bremsstrahlung process. This additional diagram must be included because there is no physical way to distinguish a zero-energy photon flowing through a loop as in the vertex diagram and zero-energy photons emitted through bremsstrahlung. From a mathematical point of view, the IR divergences can be regularized by assuming fractional differentiation w.r.t. a parameter, for example: ( p 2 − a 2 ) 1 2 {\displaystyle \left(p^{2}-a^{2}\right)^{\frac {1}{2}}} is well defined at p = a but is UV divergent; if we take the 3⁄2-th fractional derivative with respect to −a2, we obtain the IR divergence 1 p 2 − a 2 , {\displaystyle {\frac {1}{p^{2}-a^{2}}},} so we can cure IR divergences by turning them into UV divergences. A loop divergence The diagram in Figure 2 shows one of the several one-loop contributions to electron–electron scattering in QED. The electron on the left side of the diagram, represented by the solid line, starts out with 4-momentum pμ and ends up with 4-momentum rμ. It emits a virtual photon carrying rμ − pμ to transfer energy and momentum to the other electron. But in this diagram, before that happens, it emits another virtual photon carrying 4-momentum qμ, and it reabsorbs this one after emitting the other virtual photon. Energy and momentum conservation do not determine the 4-momentum qμ uniquely, so all possibilities contribute equally and we must integrate. This diagram's amplitude ends up with, among other things, a factor from the loop of − i e 3 ∫ d 4 q ( 2 π ) 4 γ μ i ( γ α ( r − q ) α + m ) ( r − q ) 2 − m 2 + i ϵ γ ρ i ( γ β ( p − q ) β + m ) ( p − q ) 2 − m 2 + i ϵ γ ν − i g μ ν q 2 + i ϵ . {\displaystyle -ie^{3}\int {\frac {d^{4}q}{(2\pi )^{4}}}\gamma ^{\mu }{\frac {i(\gamma ^{\alpha }(r-q)_{\alpha }+m)}{(r-q)^{2}-m^{2}+i\epsilon }}\gamma ^{\rho }{\frac {i(\gamma ^{\beta }(p-q)_{\beta }+m)}{(p-q)^{2}-m^{2}+i\epsilon }}\gamma ^{\nu }{\frac {-ig_{\mu \nu }}{q^{2}+i\epsilon }}.} The various γμ factors in this expression are gamma matrices as in the covariant formulation of the Dirac equation; they have to do with the spin of the electron. The factors of e are the electric coupling constant, while the i ϵ {\displaystyle i\epsilon } provide a heuristic definition of the contour of integration around the poles in the space of momenta. The important part for our purposes is the dependency on qμ of the three big factors in the integrand, which are from the propagators of the two electron lines and the photon line in the loop. This has a piece with two powers of qμ on top that dominates at large values of qμ (Pokorski 1987, p. 122): e 3 γ μ γ α γ ρ γ β γ μ ∫ d 4 q ( 2 π ) 4 q α q β ( r − q ) 2 ( p − q ) 2 q 2 . {\displaystyle e^{3}\gamma ^{\mu }\gamma ^{\alpha }\gamma ^{\rho }\gamma ^{\beta }\gamma _{\mu }\int {\frac {d^{4}q}{(2\pi )^{4}}}{\frac {q_{\alpha }q_{\beta }}{(r-q)^{2}(p-q)^{2}q^{2}}}.} This integral is divergent and infinite, unless we cut it off at finite energy and momentum in some way. Similar loop divergences occur in other quantum field theories. Renormalized and bare quantities The solution was to realize that the quantities initially appearing in the theory's formulae (such as the formula for the Lagrangian), representing such things as the electron's electric charge and mass, as well as the normalizations of the quantum fields themselves, did not actually correspond to the physical constants measured in the laboratory. As written, they were bare quantities that did not take into account the contribution of virtual-particle loop effects to the physical constants themselves. Among other things, these effects would include the quantum counterpart of the electromagnetic back-reaction that so vexed classical theorists of electromagnetism. In general, these effects would be just as divergent as the amplitudes under consideration in the first place; so finite measured quantities would, in general, imply divergent bare quantities. To make contact with reality, then, the formulae would have to be rewritten in terms of measurable, renormalized quantities. The charge of the electron, say, would be defined in terms of a quantity measured at a specific kinematic renormalization point or subtraction point (which will generally have a characteristic energy, called the renormalization scale or simply the energy scale). The parts of the Lagrangian left over, involving the remaining portions of the bare quantities, could then be reinterpreted as counterterms, involved in divergent diagrams exactly canceling out the troublesome divergences for other diagrams. Renormalization in QED For example, in the Lagrangian of QED L = ψ ¯ B [ i γ μ ( ∂ μ + i e B A B μ ) − m B ] ψ B − 1 4 F B μ ν F B μ ν {\displaystyle {\mathcal {L}}={\bar {\psi }}_{B}\left[i\gamma _{\mu }\left(\partial ^{\mu }+ie_{B}A_{B}^{\mu }\right)-m_{B}\right]\psi _{B}-{\frac {1}{4}}F_{B\mu \nu }F_{B}^{\mu \nu }} the fields and coupling constant are really bare quantities, hence the subscript B above. Conventionally the bare quantities are written so that the corresponding Lagrangian terms are multiples of the renormalized ones: ( ψ ¯ m ψ ) B = Z 0 ψ ¯ m ψ {\displaystyle \left({\bar {\psi }}m\psi \right)_{B}=Z_{0}{\bar {\psi }}m\psi } ( ψ ¯ ( ∂ μ + i e A μ ) ψ ) B = Z 1 ψ ¯ ( ∂ μ + i e A μ ) ψ {\displaystyle \left({\bar {\psi }}\left(\partial ^{\mu }+ieA^{\mu }\right)\psi \right)_{B}=Z_{1}{\bar {\psi }}\left(\partial ^{\mu }+ieA^{\mu }\right)\psi } ( F μ ν F μ ν ) B = Z 3 F μ ν F μ ν . {\displaystyle \left(F_{\mu \nu }F^{\mu \nu }\right)_{B}=Z_{3}\,F_{\mu \nu }F^{\mu \nu }.} Gauge invariance, via a Ward–Takahashi identity, turns out to imply that we can renormalize the two terms of the covariant derivative piece ψ ¯ ( ∂ + i e A ) ψ {\displaystyle {\bar {\psi }}(\partial +ieA)\psi } together (Pokorski 1987, p. 115), which is what happened to Z2; it is the same as Z1. A term in this Lagrangian, for example, the electron–photon interaction pictured in Figure 1, can then be written L I = − e ψ ¯ γ μ A μ ψ − ( Z 1 − 1 ) e ψ ¯ γ μ A μ ψ {\displaystyle {\mathcal {L}}_{I}=-e{\bar {\psi }}\gamma _{\mu }A^{\mu }\psi -(Z_{1}-1)e{\bar {\psi }}\gamma _{\mu }A^{\mu }\psi } The physical constant e, the electron's charge, can then be defined in terms of some specific experiment: we set the renormalization scale equal to the energy characteristic of this experiment, and the first term gives the interaction we see in the laboratory (up to small, finite corrections from loop diagrams, providing such exotica as the high-order corrections to the magnetic moment). The rest is the counterterm. If the theory is renormalizable (see below for more on this), as it is in QED, the divergent parts of loop diagrams can all be decomposed into pieces with three or fewer legs, with an algebraic form that can be canceled out by the second term (or by the similar counterterms that come from Z0 and Z3). The diagram with the Z1 counterterm's interaction vertex placed as in Figure 3 cancels out the divergence from the loop in Figure 2. Historically, the splitting of the "bare terms" into the original terms and counterterms came before the renormalization group insight due to Kenneth Wilson. According to such renormalization group insights, detailed in the next section, this splitting is unnatural and actually unphysical, as all scales of the problem enter in continuous systematic ways. Running couplings To minimize the contribution of loop diagrams to a given calculation (and therefore make it easier to extract results), one chooses a renormalization point close to the energies and momenta exchanged in the interaction. However, the renormalization point is not itself a physical quantity: the physical predictions of the theory, calculated to all orders, should in principle be independent of the choice of renormalization point, as long as it is within the domain of application of the theory. Changes in renormalization scale will simply affect how much of a result comes from Feynman diagrams without loops, and how much comes from the remaining finite parts of loop diagrams. One can exploit this fact to calculate the effective variation of physical constants with changes in scale. This variation is encoded by beta-functions, and the general theory of this kind of scale-dependence is known as the renormalization group. Colloquially, particle physicists often speak of certain physical "constants" as varying with the energy of interaction, though in fact, it is the renormalization scale that is the independent quantity. This running does, however, provide a convenient means of describing changes in the behavior of a field theory under changes in the energies involved in an interaction. For example, since the coupling in quantum chromodynamics becomes small at large energy scales, the theory behaves more like a free theory as the energy exchanged in an interaction becomes large – a phenomenon known as asymptotic freedom. Choosing an increasing energy scale and using the renormalization group makes this clear from simple Feynman diagrams; were this not done, the prediction would be the same, but would arise from complicated high-order cancellations. For example, I = ∫ 0 a 1 z d z − ∫ 0 b 1 z d z = ln ⁡ a − ln ⁡ b − ln ⁡ 0 + ln ⁡ 0 {\displaystyle I=\int _{0}^{a}{\frac {1}{z}}\,dz-\int _{0}^{b}{\frac {1}{z}}\,dz=\ln a-\ln b-\ln 0+\ln 0} is ill-defined. To eliminate the divergence, simply change lower limit of integral into εa and εb: I = ln ⁡ a − ln ⁡ b − ln ⁡ ε a + ln ⁡ ε b = ln ⁡ a b − ln ⁡ ε a ε b {\displaystyle I=\ln a-\ln b-\ln {\varepsilon _{a}}+\ln {\varepsilon _{b}}=\ln {\tfrac {a}{b}}-\ln {\tfrac {\varepsilon _{a}}{\varepsilon _{b}}}} Making sure ⁠εb/εa⁠ → 1, then I = ln ⁠a/b⁠. Regularization Since the quantity ∞ − ∞ is ill-defined, in order to make this notion of canceling divergences precise, the divergences first have to be tamed mathematically using the theory of limits, in a process known as regularization (Weinberg, 1995). An essentially arbitrary modification to the loop integrands, or regulator, can make them drop off faster at high energies and momenta, in such a manner that the integrals converge. A regulator has a characteristic energy scale known as the cutoff; taking this cutoff to infinity (or, equivalently, the corresponding length/time scale to zero) recovers the original integrals. With the regulator in place, and a finite value for the cutoff, divergent terms in the integrals then turn into finite but cutoff-dependent terms. After canceling out these terms with the contributions from cutoff-dependent counterterms, the cutoff is taken to infinity and finite physical results are recovered. If physics on scales we can measure is independent of what happens at the very shortest distance and time scales, then it should be possible to get cutoff-independent results for calculations. Many different types of regulator are used in quantum field theory calculations, each with its advantages and disadvantages. One of the most popular in modern use is dimensional regularization, invented by Gerardus 't Hooft and Martinus J. G. Veltman, which tames the integrals by carrying them into a space with a fictitious fractional number of dimensions. Another is Pauli–Villars regularization, which adds fictitious particles to the theory with very large masses, such that loop integrands involving the massive particles cancel out the existing loops at large momenta. Yet another regularization scheme is the lattice regularization, introduced by Kenneth Wilson, which pretends that hyper-cubical lattice constructs our spacetime with fixed grid size. This size is a natural cutoff for the maximal momentum that a particle could possess when propagating on the lattice. And after doing a calculation on several lattices with different grid size, the physical result is extrapolated to grid size 0, or our natural universe. This presupposes the existence of a scaling limit. A rigorous mathematical approach to renormalization theory is the so-called causal perturbation theory, where ultraviolet divergences are avoided from the start in calculations by performing well-defined mathematical operations only within the framework of distribution theory. In this approach, divergences are replaced by ambiguity: corresponding to a divergent diagram is a term which now has a finite, but undetermined, coefficient. Other principles, such as gauge symmetry, must then be used to reduce or eliminate the ambiguity. Attitudes and interpretation The early formulators of QED and other quantum field theories were, as a rule, dissatisfied with this state of affairs. It seemed illegitimate to do something tantamount to subtracting infinities from infinities to get finite answers. Dyson argued that these infinities are of a basic nature and cannot be eliminated by any formal mathematical procedures, such as the renormalization method. Dirac's criticism was the most persistent. As late as 1975, he was saying: Most physicists are very satisfied with the situation. They say: 'Quantum electrodynamics is a good theory and we do not have to worry about it any more.' I must say that I am very dissatisfied with the situation because this so-called 'good theory' does involve neglecting infinities which appear in its equations, ignoring them in an arbitrary way. This is just not sensible mathematics. Sensible mathematics involves disregarding a quantity when it is small – not neglecting it just because it is infinitely great and you do not want it! Another important critic was Feynman. Despite his crucial role in the development of quantum electrodynamics, he wrote the following in 1985: The shell game that we play to find n and j is technically called 'renormalization'. But no matter how clever the word, it is still what I would call a dippy process! Having to resort to such hocus-pocus has prevented us from proving that the theory of quantum electrodynamics is mathematically self-consistent. It's surprising that the theory still hasn't been proved self-consistent one way or the other by now; I suspect that renormalization is not mathematically legitimate. Feynman was concerned that all field theories known in the 1960s had the property that the interactions become infinitely strong at short enough distance scales. This property called a Landau pole, made it plausible that quantum field theories were all inconsistent. In 1974, David Gross, Hugh David Politzer and Frank Wilczek showed that another quantum field theory, quantum chromodynamics, does not have a Landau pole. Feynman, along with most others, accepted that quantum chromodynamics was a fully consistent theory. The general unease was almost universal in texts up to the 1970s and 1980s. Beginning in the 1970s, however, inspired by work on the renormalization group and effective field theory, and despite the fact that Dirac and various others—all of whom belonged to the older generation—never withdrew their criticisms, attitudes began to change, especially among younger theorists. Kenneth G. Wilson and others demonstrated that the renormalization group is useful in statistical field theory applied to condensed matter physics, where it provides important insights into the behavior of phase transitions. In condensed matter physics, a physical short-distance regulator exists: matter ceases to be continuous on the scale of atoms. Short-distance divergences in condensed matter physics do not present a philosophical problem since the field theory is only an effective, smoothed-out representation of the behavior of matter anyway; there are no infinities since the cutoff is always finite, and it makes perfect sense that the bare quantities are cutoff-dependent. If quantum field theory holds all the way down past the Planck length (where it might yield to string theory, causal set theory or something different), then there may be no real problem with short-distance divergences in particle physics either; all field theories could simply be effective field theories. In a sense, this approach echoes the older attitude that the divergences in quantum field theory speak of human ignorance about the workings of nature, but also acknowledges that this ignorance can be quantified and that the resulting effective theories remain useful. Be that as it may, Abdus Salam's remark in 1972 seems still relevant Field-theoretic infinities – first encountered in Lorentz's computation of electron self-mass – have persisted in classical electrodynamics for seventy and in quantum electrodynamics for some thirty-five years. These long years of frustration have left in the subject a curious affection for the infinities and a passionate belief that they are an inevitable part of nature; so much so that even the suggestion of a hope that they may, after all, be circumvented — and finite values for the renormalization constants computed – is considered irrational. Compare Russell's postscript to the third volume of his autobiography The Final Years, 1944–1969 (George Allen and Unwin, Ltd., London 1969), p. 221: In the modern world, if communities are unhappy, it is often because they have ignorances, habits, beliefs, and passions, which are dearer to them than happiness or even life. I find many men in our dangerous age who seem to be in love with misery and death, and who grow angry when hopes are suggested to them. They think hope is irrational and that, in sitting down to lazy despair, they are merely facing facts. In quantum field theory, the value of a physical constant, in general, depends on the scale that one chooses as the renormalization point, and it becomes very interesting to examine the renormalization group running of physical constants under changes in the energy scale. The coupling constants in the Standard Model of particle physics vary in different ways with increasing energy scale: the coupling of quantum chromodynamics and the weak isospin coupling of the electroweak force tend to decrease, and the weak hypercharge coupling of the electroweak force tends to increase. At the colossal energy scale of 1015 GeV (far beyond the reach of our current particle accelerators), they all become approximately the same size (Grotz and Klapdor 1990, p. 254), a major motivation for speculations about grand unified theory. Instead of being only a worrisome problem, renormalization has become an important theoretical tool for studying the behavior of field theories in different regimes. If a theory featuring renormalization (e.g. QED) can only be sensibly interpreted as an effective field theory, i.e. as an approximation reflecting human ignorance about the workings of nature, then the problem remains of discovering a more accurate theory that does not have these renormalization problems. As Lewis Ryder has put it, "In the Quantum Theory, these [classical] divergences do not disappear; on the contrary, they appear to get worse. And despite the comparative success of renormalisation theory, the feeling remains that there ought to be a more satisfactory way of doing things." Renormalizability From this philosophical reassessment, a new concept follows naturally: the notion of renormalizability. Not all theories lend themselves to renormalization in the manner described above, with a finite supply of counterterms and all quantities becoming cutoff-independent at the end of the calculation. If the Lagrangian contains combinations of field operators of high enough dimension in energy units, the counterterms required to cancel all divergences proliferate to infinite number, and, at first glance, the theory would seem to gain an infinite number of free parameters and therefore lose all predictive power, becoming scientifically worthless. Such theories are called nonrenormalizable. The Standard Model of particle physics contains only renormalizable operators, but the interactions of general relativity become nonrenormalizable operators if one attempts to construct a field theory of quantum gravity in the most straightforward manner (treating the metric in the Einstein–Hilbert Lagrangian as a perturbation about the Minkowski metric), suggesting that perturbation theory is not satisfactory in application to quantum gravity. However, in an effective field theory, "renormalizability" is, strictly speaking, a misnomer. In nonrenormalizable effective field theory, terms in the Lagrangian do multiply to infinity, but have coefficients suppressed by ever-more-extreme inverse powers of the energy cutoff. If the cutoff is a real, physical quantity—that is, if the theory is only an effective description of physics up to some maximum energy or minimum distance scale—then these additional terms could represent real physical interactions. Assuming that the dimensionless constants in the theory do not get too large, one can group calculations by inverse powers of the cutoff, and extract approximate predictions to finite order in the cutoff that still have a finite number of free parameters. It can even be useful to renormalize these "nonrenormalizable" interactions. Nonrenormalizable interactions in effective field theories rapidly become weaker as the energy scale becomes much smaller than the cutoff. The classic example is the Fermi theory of the weak nuclear force, a nonrenormalizable effective theory whose cutoff is comparable to the mass of the W particle. This fact may also provide a possible explanation for why almost all of the particle interactions we see are describable by renormalizable theories. It may be that any others that may exist at the GUT or Planck scale simply become too weak to detect in the realm we can observe, with one exception: gravity, whose exceedingly weak interaction is magnified by the presence of the enormous masses of stars and planets. Renormalization schemes In actual calculations, the counterterms introduced to cancel the divergences in Feynman diagram calculations beyond tree level must be fixed using a set of renormalisation conditions. The common renormalization schemes in use include: Minimal subtraction (MS) scheme and the related modified minimal subtraction (MS-bar) scheme On-shell scheme Besides, there exists a "natural" definition of the renormalized coupling (combined with the photon propagator) as a propagator of dual free bosons, which does not explicitly require introducing the counterterms. In statistical physics History A deeper understanding of the physical meaning and generalization of the renormalization process, which goes beyond the dilatation group of conventional renormalizable theories, came from condensed matter physics. Leo P. Kadanoff's paper in 1966 proposed the "block-spin" renormalization group. The blocking idea is a way to define the components of the theory at large distances as aggregates of components at shorter distances. This approach covered the conceptual point and was given full computational substance in the extensive important contributions of Kenneth Wilson. The power of Wilson's ideas was demonstrated by a constructive iterative renormalization solution of a long-standing problem, the Kondo problem, in 1974, as well as the preceding seminal developments of his new method in the theory of second-order phase transitions and critical phenomena in 1971. He was awarded the Nobel Prize in Physics for these decisive contributions in 1982. Principles In more technical terms, let us assume that we have a theory described by a certain function Z {\displaystyle Z} of the state variables { s i } {\displaystyle \{s_{i}\}} and a certain set of coupling constants { J k } {\displaystyle \{J_{k}\}} . This function may be a partition function, an action, a Hamiltonian, etc. It must contain the whole description of the physics of the system. Now we consider a certain blocking transformation of the state variables { s i } → { s ~ i } {\displaystyle \{s_{i}\}\to \{{\tilde {s}}_{i}\}} , the number of s ~ i {\displaystyle {\tilde {s}}_{i}} must be lower than the number of s i {\displaystyle s_{i}} . Now let us try to rewrite the Z {\displaystyle Z} function only in terms of the s ~ i {\displaystyle {\tilde {s}}_{i}} . If this is achievable by a certain change in the parameters, { J k } → { J ~ k } {\displaystyle \{J_{k}\}\to \{{\tilde {J}}_{k}\}} , then the theory is said to be renormalizable. The possible macroscopic states of the system, at a large scale, are given by this set of fixed points. Renormalization group fixed points The most important information in the RG flow is its fixed points. A fixed point is defined by the vanishing of the beta function associated to the flow. Then, fixed points of the renormalization group are by definition scale invariant. In many cases of physical interest scale invariance enlarges to conformal invariance. One then has a conformal field theory at the fixed point. The ability of several theories to flow to the same fixed point leads to universality. If these fixed points correspond to free field theory, the theory is said to exhibit quantum triviality. Numerous fixed points appear in the study of lattice Higgs theories, but the nature of the quantum field theories associated with these remains an open question. See also History of quantum field theory Quantum triviality Zeno's paradoxes Nonoblique correction References Further reading General introduction Collins, John (2023). Renormalization: An Introduction to Renormalization, the Renormalization Group and the Operator-Product Expansion. Cambridge University Press. ISBN 9781009401807. DeDeo, Simon; Introduction to Renormalization (2017). Santa Fe Institute Complexity Explorer MOOC. Renormalization from a complex systems point of view, including Markov Chains, Cellular Automata, the real space Ising model, the Krohn-Rhodes Theorem, QED, and rate distortion theory. Delamotte, Bertrand (2004). "A hint of renormalization". American Journal of Physics. 72 (2): 170–184. arXiv:hep-th/0212049. Bibcode:2004AmJPh..72..170D. doi:10.1119/1.1624112. S2CID 2506712. Baez, John; Renormalization Made Easy, (2005). A qualitative introduction to the subject. Blechman, Andrew E.; Renormalization: Our Greatly Misunderstood Friend, (2002). Summary of a lecture; has more information about specific regularization and divergence-subtraction schemes. Cao, Tian Yu; Schweber, Silvan S. (1993). "The conceptual foundations and the philosophical aspects of renormalization theory". Synthese. 97: 33–108. doi:10.1007/BF01255832. S2CID 46968305. Shirkov, Dmitry; Fifty Years of the Renormalization Group, C.E.R.N. Courrier 41(7) (2001). Full text available at : I.O.P Magazines. Mainly: quantum field theory N. N. Bogoliubov, D. V. Shirkov (1959): The Theory of Quantized Fields. New York, Interscience. The first text-book on the renormalization group theory. Ryder, Lewis H.; Quantum Field Theory (Cambridge University Press, 1985), ISBN 0-521-33859-X Highly readable textbook, certainly the best introduction to relativistic Q.F.T. for particle physics. Zee, Anthony; Quantum Field Theory in a Nutshell, Princeton University Press (2003) ISBN 0-691-01019-6. Another excellent textbook on Q.F.T. Weinberg, Steven; The Quantum Theory of Fields (3 volumes) Cambridge University Press (1995). A monumental treatise on Q.F.T. written by a leading expert, Nobel laureate 1979. Pokorski, Stefan; Gauge Field Theories, Cambridge University Press (1987) ISBN 0-521-47816-2. 't Hooft, Gerard; The Glorious Days of Physics – Renormalization of Gauge theories, lecture given at Erice (August/September 1998) by the Nobel laureate 1999 . Full text available at: hep-th/9812203. Rivasseau, Vincent; An introduction to renormalization, Poincaré Seminar (Paris, Oct. 12, 2002), published in : Duplantier, Bertrand; Rivasseau, Vincent (Eds.); Poincaré Seminar 2002, Progress in Mathematical Physics 30, Birkhäuser (2003) ISBN 3-7643-0579-7. Full text available in PostScript. Rivasseau, Vincent; From perturbative to constructive renormalization, Princeton University Press (1991) ISBN 0-691-08530-7. Full text available in PostScript and in PDF (draft version). Iagolnitzer, Daniel & Magnen, J.; Renormalization group analysis, Encyclopaedia of Mathematics, Kluwer Academic Publisher (1996). Full text available in PostScript and pdf here. Scharf, Günter; Finite quantum electrodynamics: The causal approach, Springer Verlag Berlin Heidelberg New York (1995) ISBN 3-540-60142-2. A. S. Švarc (Albert Schwarz), Математические основы квантовой теории поля, (Mathematical aspects of quantum field theory), Atomizdat, Moscow, 1975. 368 pp. Mainly: statistical physics A. N. Vasil'ev; The Field Theoretic Renormalization Group in Critical Behavior Theory and Stochastic Dynamics (Routledge Chapman & Hall 2004); ISBN 978-0-415-31002-4 Nigel Goldenfeld; Lectures on Phase Transitions and the Renormalization Group, Frontiers in Physics 85, Westview Press (June, 1992) ISBN 0-201-55409-7. Covering the elementary aspects of the physics of phases transitions and the renormalization group, this popular book emphasizes understanding and clarity rather than technical manipulations. Zinn-Justin, Jean; Quantum Field Theory and Critical Phenomena, Oxford University Press (4th edition – 2002) ISBN 0-19-850923-5. A masterpiece on applications of renormalization methods to the calculation of critical exponents in statistical mechanics, following Wilson's ideas (Kenneth Wilson was Nobel laureate 1982). Zinn-Justin, Jean; Phase Transitions & Renormalization Group: from Theory to Numbers, Poincaré Seminar (Paris, Oct. 12, 2002), published in : Duplantier, Bertrand; Rivasseau, Vincent (Eds.); Poincaré Seminar 2002, Progress in Mathematical Physics 30, Birkhäuser (2003) ISBN 3-7643-0579-7. Full text available in PostScript Archived October 15, 2005, at the Wayback Machine. Domb, Cyril; The Critical Point: A Historical Introduction to the Modern Theory of Critical Phenomena, CRC Press (March, 1996) ISBN 0-7484-0435-X. Brown, Laurie M. (Ed.); Renormalization: From Lorentz to Landau (and Beyond), Springer-Verlag (New York-1993) ISBN 0-387-97933-6. Cardy, John; Scaling and Renormalization in Statistical Physics, Cambridge University Press (1996) ISBN 0-521-49959-3. Miscellaneous Shirkov, Dmitry; The Bogoliubov Renormalization Group, JINR Communication E2-96-15 (1996). Full text available at: hep-th/9602024 Zinn-Justin, Jean; Renormalization and renormalization group: From the discovery of UV divergences to the concept of effective field theories, in: de Witt-Morette C., Zuber J.-B. (eds), Proceedings of the NATO ASI on Quantum Field Theory: Perspective and Prospective, June 15–26, 1998, Les Houches, France, Kluwer Academic Publishers, NATO ASI Series C 530, 375–388 (1999). Full text available in PostScript. Connes, Alain; Symétries Galoisiennes & Renormalisation, Poincaré Seminar (Paris, Oct. 12, 2002), published in : Duplantier, Bertrand; Rivasseau, Vincent (Eds.); Poincaré Seminar 2002, Progress in Mathematical Physics 30, Birkhäuser (2003) ISBN 3-7643-0579-7. French mathematician Alain Connes (Fields medallist 1982) describe the mathematical underlying structure (the Hopf algebra) of renormalization, and its link to the Riemann-Hilbert problem. Full text (in French) available at arXiv:math/0211199 . External links Quotations related to Renormalization at Wikiquote Wojciech Sylwester Piotr Rubinowicz (February 22, 1889 – October 13, 1974) was a Polish theoretical physicist who made contributions in quantum mechanics, mathematical physics, and the theory of radiation. He is known for the Maggie-Rubinowicz representation of Gustav Kirchhoff’s diffraction formula. Life and career He was born in Sadagóra, Bukovina, in the Austro-Hungarian Empire, the son of Margaret Brodowska and Damian Rubinowicz, Polish patriot and insurgent of the January Uprising of 1863. In 1908, Rubinowicz began his studies at the University of Czernowitz, and he was awarded his doctorate in 1914. In 1916 he began postgraduate studies at the University of Munich under Arnold Sommerfeld and eventually becoming his assistant. In 1918, he became a Privatdozent at the University of Czernowitz. Two years later he took an appointment as professor at the University of Ljubljana, Yugoslavia. He became a professor at the Polytechnic Institute of Lwów, in 1922. During the period from 1937 to 1941, he was a professor at the John Casimir University of Lwów. After World War II, starting in 1946, he was professor of theoretical physics at the University of Warsaw until 1960. He served as president of the Polish Physical Society between 1949–1952 and 1961–1974. While at Munich, Rubinowicz published contributions to radiation theory and to three of Sommerfeld’s major interests, i.e., Sommerfeld’s extension of Bohr’s theory of the atom and both mathematical physics and diffraction theory; he carried these topical interests throughout his career. He eventually published books based on his mathematical physics interests as well as diffraction theory: his work on the polynomial method of solving eigenvalue problems in quantum mechanics was described in "Sommerfeldsche Polynommethode" and his work on diffraction theory, was published in "Die Beugungswelle in der Kirchhoffschen Theorie der Beugung". Also while at Munich, Rubinowicz transformed the Kirchhoff diffraction integral into what has become known as the Rubinowicz representation (also known as the Maggie-Rubinowicz representation) for which scalar and electromagnetic fields are interpreted as a transformation of a surface integral into a line integral – an independent and slightly different derivation from that done in 1888 by G. A. Maggi, one of Gustav Kirchhoff's students in Berlin. Rubinowicz’s form of the representation, as opposed to Maggi’s, is now more commonly accepted. The circle of his disciples and associates included in Lwów Jan Blaton, Wanda Hanusowa, Roman Stanisław Ingarden, Wasyl Milianczuk, Jerzy Rayski, Kazimierz Vetulani, in Warsaw Bohdan Karczewski, Wojciech Królikowski, Adam Kujawski, Jan Petykiewicz, Jerzy Plebański. He died in Warsaw, Poland. Books Adalbert Rubinowicz "Die Beugungswelle in der Kirchhoffschen Theorie der Beugung" (1957, 1966) Adalbert Rubinowicz "Quantum Mechanics" (1968) Adalbert Rubinowicz "Sommerfeldsche Polynommethode: Die Grundlehren der mathematischen Wissenschaften in Einzeldarstellungen mit besonderer Berücksichtigung der Anwendungsgebiete" (Polnischer Verl. d. Wissenschaften, 1972) Adalbert Rubinowicz "Selected Papers" (Polish Scientific Publishers PWN, Warsaw 1975) Notes References Mehra, Jagdish, and Helmut Rechenberg The Historical Development of Quantum Theory. Volume 1 Part 1 The Quantum Theory of Planck, Einstein, Bohr and Sommerfeld 1900 – 1925: Its Foundation and the Rise of Its Difficulties. (Springer, 1982) ISBN 0-387-95174-1 Arnold Sommerfeld, translated from the first German edition by Otto Laporte and Peter A. Moldauer "Optics - Lectures on Theoretical Physics Volume IV" (Academic Press, 1964) External links Media related to Wojciech Rubinowicz at Wikimedia Commons Interview American Institute of Physics 18th May 1963 In the theory of quantum chromodynamics, dual superconductor models attempt to explain confinement of quarks in terms of an electromagnetic dual theory of superconductivity. Overview In an electromagnetic dual theory the roles of electric and magnetic fields are interchanged. The BCS theory of superconductivity explains superconductivity as the result of the condensation of electric charges to Cooper pairs. In a dual superconductor an analogous effect occurs through the condensation of magnetic charges (also called magnetic monopoles). In ordinary electromagnetic theory, no monopoles have been shown to exist. However, in quantum chromodynamics — the theory of color charge which explains the strong interaction between quarks — the color charges can be viewed as (non-abelian) analogues of electric charges and corresponding magnetic monopoles are known to exist. Dual superconductor models posit that condensation of these magnetic monopoles in a superconductive state explains color confinement — the phenomenon that only neutrally colored bound states are observed at low energies. Qualitatively, confinement in dual superconductor models can be understood as a result of the dual to the Meissner effect. The Meissner effect says that a superconducting metal will try to expel magnetic field lines from its interior. If a magnetic field is forced to run through the superconductor, the field lines are compressed in magnetic flux "tubes" known as fluxons. In a dual superconductor the roles of magnetic and electric fields are exchanged and the Meissner effect tries to expel electric field lines. Quarks and antiquarks carry opposite color charges, and for a quark–antiquark pair 'electric' field lines run from the quark to the antiquark. If the quark–antiquark pair are immersed in a dual superconductor, then the electric field lines get compressed to a flux tube. The energy associated to the tube is proportional to its length, and the potential energy of the quark–antiquark is proportional to their separation. The potential energy of colored objects becomes infinite in the limit of large separation, all else being equal, though in reality, when it becomes large enough to form a new quark-anti-quark pair from the vacuum, these split the flux tube and bind to the original anti-quark and quark. A quark–antiquark will therefore always bind regardless of their separation, which explains why no unbound quarks are ever found. Dual superconductors are described by (a dual to) the Landau–Ginzburg model, which is equivalent to the Abelian Higgs model. The MIT bag model boundary conditions for gluon fields are those of the dual color superconductor. The dual superconductor model is motivated by several observations in calculations using lattice gauge theory. The model, however, also has some shortcomings. In particular, although it confines colored quarks, it fails to confine color of some gluons, allowing colored bound states at energies observable in particle colliders. Notes References Ripka, Georges (2004). Dual superconductor models of color confinement. Lecture notes in physics. Vol. 639. Springer. ISBN 978-3-540-20718-4. See also QCD vacuum Maximum Abelian gauge In mathematical physics, the Berezin integral, named after Felix Berezin, (also known as Grassmann integral, after Hermann Grassmann), is a way to define integration for functions of Grassmann variables (elements of the exterior algebra). It is not an integral in the Lebesgue sense; the word "integral" is used because the Berezin integral has properties analogous to the Lebesgue integral and because it extends the path integral in physics, where it is used as a sum over histories for fermions. Definition Let Λ n {\displaystyle \Lambda ^{n}} be the exterior algebra of polynomials in anticommuting elements θ 1 , … , θ n {\displaystyle \theta _{1},\dots ,\theta _{n}} over the field of complex numbers. (The ordering of the generators θ 1 , … , θ n {\displaystyle \theta _{1},\dots ,\theta _{n}} is fixed and defines the orientation of the exterior algebra.) One variable The Berezin integral over the sole Grassmann variable θ = θ 1 {\displaystyle \theta =\theta _{1}} is defined to be a linear functional ∫ [ a f ( θ ) + b g ( θ ) ] d θ = a ∫ f ( θ ) d θ + b ∫ g ( θ ) d θ , a , b ∈ C {\displaystyle \int [af(\theta )+bg(\theta )]\,d\theta =a\int f(\theta )\,d\theta +b\int g(\theta )\,d\theta ,\quad a,b\in \mathbb {C} } where we define ∫ θ d θ = 1 , ∫ d θ = 0 {\displaystyle \int \theta \,d\theta =1,\qquad \int \,d\theta =0} so that : ∫ ∂ ∂ θ f ( θ ) d θ = 0. {\displaystyle \int {\frac {\partial }{\partial \theta }}f(\theta )\,d\theta =0.} These properties define the integral uniquely and imply ∫ ( a θ + b ) d θ = a , a , b ∈ C . {\displaystyle \int (a\theta +b)\,d\theta =a,\quad a,b\in \mathbb {C} .} Take note that f ( θ ) = a θ + b {\displaystyle f(\theta )=a\theta +b} is the most general function of θ {\displaystyle \theta } because Grassmann variables square to zero, so f ( θ ) {\displaystyle f(\theta )} cannot have non-zero terms beyond linear order. Multiple variables The Berezin integral on Λ n {\displaystyle \Lambda ^{n}} is defined to be the unique linear functional ∫ Λ n ⋅ d θ {\displaystyle \int _{\Lambda ^{n}}\cdot {\textrm {d}}\theta } with the following properties: ∫ Λ n θ n ⋯ θ 1 d θ = 1 , {\displaystyle \int _{\Lambda ^{n}}\theta _{n}\cdots \theta _{1}\,\mathrm {d} \theta =1,} ∫ Λ n ∂ f ∂ θ i d θ = 0 , i = 1 , … , n {\displaystyle \int _{\Lambda ^{n}}{\frac {\partial f}{\partial \theta _{i}}}\,\mathrm {d} \theta =0,\ i=1,\dots ,n} for any f ∈ Λ n , {\displaystyle f\in \Lambda ^{n},} where ∂ / ∂ θ i {\displaystyle \partial /\partial \theta _{i}} means the left or the right partial derivative. These properties define the integral uniquely. Notice that different conventions exist in the literature: Some authors define instead ∫ Λ n θ 1 ⋯ θ n d θ := 1. {\displaystyle \int _{\Lambda ^{n}}\theta _{1}\cdots \theta _{n}\,\mathrm {d} \theta :=1.} The formula ∫ Λ n f ( θ ) d θ = ∫ Λ 1 ( ⋯ ∫ Λ 1 ( ∫ Λ 1 f ( θ ) d θ 1 ) d θ 2 ⋯ ) d θ n {\displaystyle \int _{\Lambda ^{n}}f(\theta )\,\mathrm {d} \theta =\int _{\Lambda ^{1}}\left(\cdots \int _{\Lambda ^{1}}\left(\int _{\Lambda ^{1}}f(\theta )\,\mathrm {d} \theta _{1}\right)\,\mathrm {d} \theta _{2}\cdots \right)\mathrm {d} \theta _{n}} expresses the Fubini law. On the right-hand side, the interior integral of a monomial f = g ( θ ′ ) θ 1 {\displaystyle f=g(\theta ')\theta _{1}} is set to be g ( θ ′ ) , {\displaystyle g(\theta '),} where θ ′ = ( θ 2 , … , θ n ) {\displaystyle \theta '=\left(\theta _{2},\ldots ,\theta _{n}\right)} ; the integral of f = g ( θ ′ ) {\displaystyle f=g(\theta ')} vanishes. The integral with respect to θ 2 {\displaystyle \theta _{2}} is calculated in the similar way and so on. Change of Grassmann variables Let θ i = θ i ( ξ 1 , … , ξ n ) , i = 1 , … , n , {\displaystyle \theta _{i}=\theta _{i}\left(\xi _{1},\ldots ,\xi _{n}\right),\ i=1,\ldots ,n,} be odd polynomials in some antisymmetric variables ξ 1 , … , ξ n {\displaystyle \xi _{1},\ldots ,\xi _{n}} . The Jacobian is the matrix D = { ∂ θ i ∂ ξ j , i , j = 1 , … , n } , {\displaystyle D=\left\{{\frac {\partial \theta _{i}}{\partial \xi _{j}}},\ i,j=1,\ldots ,n\right\},} where ∂ / ∂ ξ j {\displaystyle \partial /\partial \xi _{j}} refers to the right derivative ( ∂ ( θ 1 θ 2 ) / ∂ θ 2 = θ 1 , ∂ ( θ 1 θ 2 ) / ∂ θ 1 = − θ 2 {\displaystyle \partial (\theta _{1}\theta _{2})/\partial \theta _{2}=\theta _{1},\;\partial (\theta _{1}\theta _{2})/\partial \theta _{1}=-\theta _{2}} ). The formula for the coordinate change reads ∫ f ( θ ) d θ = ∫ f ( θ ( ξ ) ) ( det D ) − 1 d ξ . {\displaystyle \int f(\theta )\,\mathrm {d} \theta =\int f(\theta (\xi ))(\det D)^{-1}\,\mathrm {d} \xi .} Integrating even and odd variables Definition Consider now the algebra Λ m ∣ n {\displaystyle \Lambda ^{m\mid n}} of functions of real commuting variables x = x 1 , … , x m {\displaystyle x=x_{1},\ldots ,x_{m}} and of anticommuting variables θ 1 , … , θ n {\displaystyle \theta _{1},\ldots ,\theta _{n}} (which is called the free superalgebra of dimension ( m | n ) {\displaystyle (m|n)} ). Intuitively, a function f = f ( x , θ ) ∈ Λ m ∣ n {\displaystyle f=f(x,\theta )\in \Lambda ^{m\mid n}} is a function of m even (bosonic, commuting) variables and of n odd (fermionic, anti-commuting) variables. More formally, an element f = f ( x , θ ) ∈ Λ m ∣ n {\displaystyle f=f(x,\theta )\in \Lambda ^{m\mid n}} is a function of the argument x {\displaystyle x} that varies in an open set X ⊂ R m {\displaystyle X\subset \mathbb {R} ^{m}} with values in the algebra Λ n . {\displaystyle \Lambda ^{n}.} Suppose that this function is continuous and vanishes in the complement of a compact set K ⊂ R m . {\displaystyle K\subset \mathbb {R} ^{m}.} The Berezin integral is the number ∫ Λ m ∣ n f ( x , θ ) d θ d x = ∫ R m d x ∫ Λ n f ( x , θ ) d θ . {\displaystyle \int _{\Lambda ^{m\mid n}}f(x,\theta )\,\mathrm {d} \theta \,\mathrm {d} x=\int _{\mathbb {R} ^{m}}\,\mathrm {d} x\int _{\Lambda ^{n}}f(x,\theta )\,\mathrm {d} \theta .} Change of even and odd variables Let a coordinate transformation be given by x i = x i ( y , ξ ) , i = 1 , … , m ; θ j = θ j ( y , ξ ) , j = 1 , … , n , {\displaystyle x_{i}=x_{i}(y,\xi ),\ i=1,\ldots ,m;\ \theta _{j}=\theta _{j}(y,\xi ),j=1,\ldots ,n,} where x i {\displaystyle x_{i}} are even and θ j {\displaystyle \theta _{j}} are odd polynomials of ξ {\displaystyle \xi } depending on even variables y . {\displaystyle y.} The Jacobian matrix of this transformation has the block form: J = ∂ ( x , θ ) ∂ ( y , ξ ) = ( A B C D ) , {\displaystyle \mathrm {J} ={\frac {\partial (x,\theta )}{\partial (y,\xi )}}={\begin{pmatrix}A&B\\C&D\end{pmatrix}},} where each even derivative ∂ / ∂ y j {\displaystyle \partial /\partial y_{j}} commutes with all elements of the algebra Λ m ∣ n {\displaystyle \Lambda ^{m\mid n}} ; the odd derivatives commute with even elements and anticommute with odd elements. The entries of the diagonal blocks A = ∂ x / ∂ y {\displaystyle A=\partial x/\partial y} and D = ∂ θ / ∂ ξ {\displaystyle D=\partial \theta /\partial \xi } are even and the entries of the off-diagonal blocks B = ∂ x / ∂ ξ , C = ∂ θ / ∂ y {\displaystyle B=\partial x/\partial \xi ,\ C=\partial \theta /\partial y} are odd functions, where ∂ / ∂ ξ j {\displaystyle \partial /\partial \xi _{j}} again mean right derivatives. When the function D {\displaystyle D} is invertible in Λ m ∣ n , {\displaystyle \Lambda ^{m\mid n},} J = ∂ ( x , θ ) ∂ ( y , ξ ) = ( A B C D ) = ( I B 0 D ) ( A − B D − 1 C 0 D − 1 C I ) {\displaystyle \mathrm {J} ={\frac {\partial (x,\theta )}{\partial (y,\xi )}}={\begin{pmatrix}A&B\\C&D\end{pmatrix}}={\begin{pmatrix}I&B\\0&D\end{pmatrix}}{\begin{pmatrix}A-BD^{-1}C&0\\D^{-1}C&I\end{pmatrix}}} So we have the Berezinian (or superdeterminant) of the matrix J {\displaystyle \mathrm {J} } , which is the even function Ber ⁡ J = det ( A − B D − 1 C ) ( det D ) − 1 {\displaystyle \operatorname {Ber} \mathrm {J} =\det \left(A-BD^{-1}C\right)(\det D)^{-1}} Suppose that the real functions x i = x i ( y , 0 ) {\displaystyle x_{i}=x_{i}(y,0)} define a smooth invertible map F : Y → X {\displaystyle F:Y\to X} of open sets X , Y {\displaystyle X,Y} in R m {\displaystyle \mathbb {R} ^{m}} and the linear part of the map ξ ↦ θ = θ ( y , ξ ) {\displaystyle \xi \mapsto \theta =\theta (y,\xi )} is invertible for each y ∈ Y . {\displaystyle y\in Y.} The general transformation law for the Berezin integral reads ∫ Λ m ∣ n f ( x , θ ) d θ d x = ∫ Λ m ∣ n f ( x ( y , ξ ) , θ ( y , ξ ) ) ε Ber ⁡ J d ξ d y = ∫ Λ m ∣ n f ( x ( y , ξ ) , θ ( y , ξ ) ) ε det ( A − B D − 1 C ) det D d ξ d y , {\displaystyle {\begin{aligned}&\int _{\Lambda ^{m\mid n}}f(x,\theta )\,\mathrm {d} \theta \,\mathrm {d} x=\int _{\Lambda ^{m\mid n}}f(x(y,\xi ),\theta (y,\xi ))\varepsilon \operatorname {Ber} \mathrm {J} \,\mathrm {d} \xi \,\mathrm {d} y\\[6pt]={}&\int _{\Lambda ^{m\mid n}}f(x(y,\xi ),\theta (y,\xi ))\varepsilon {\frac {\det \left(A-BD^{-1}C\right)}{\det D}}\,\mathrm {d} \xi \,\mathrm {d} y,\end{aligned}}} where ε = s g n ( det ∂ x ( y , 0 ) / ∂ y {\displaystyle \varepsilon =\mathrm {sgn} (\det \partial x(y,0)/\partial y} ) is the sign of the orientation of the map F . {\displaystyle F.} The superposition f ( x ( y , ξ ) , θ ( y , ξ ) ) {\displaystyle f(x(y,\xi ),\theta (y,\xi ))} is defined in the obvious way, if the functions x i ( y , ξ ) {\displaystyle x_{i}(y,\xi )} do not depend on ξ . {\displaystyle \xi .} In the general case, we write x i ( y , ξ ) = x i ( y , 0 ) + δ i , {\displaystyle x_{i}(y,\xi )=x_{i}(y,0)+\delta _{i},} where δ i , i = 1 , … , m {\displaystyle \delta _{i},i=1,\ldots ,m} are even nilpotent elements of Λ m ∣ n {\displaystyle \Lambda ^{m\mid n}} and set f ( x ( y , ξ ) , θ ) = f ( x ( y , 0 ) , θ ) + ∑ i ∂ f ∂ x i ( x ( y , 0 ) , θ ) δ i + 1 2 ∑ i , j ∂ 2 f ∂ x i ∂ x j ( x ( y , 0 ) , θ ) δ i δ j + ⋯ , {\displaystyle f(x(y,\xi ),\theta )=f(x(y,0),\theta )+\sum _{i}{\frac {\partial f}{\partial x_{i}}}(x(y,0),\theta )\delta _{i}+{\frac {1}{2}}\sum _{i,j}{\frac {\partial ^{2}f}{\partial x_{i}\,\partial x_{j}}}(x(y,0),\theta )\delta _{i}\delta _{j}+\cdots ,} where the Taylor series is finite. Useful formulas The following formulas for Gaussian integrals are used often in the path integral formulation of quantum field theory: ∫ exp ⁡ [ − θ T A η ] d θ d η = det A {\displaystyle \int \exp \left[-\theta ^{T}A\eta \right]\,d\theta \,d\eta =\det A} with A {\displaystyle A} being a complex n × n {\displaystyle n\times n} matrix. ∫ exp ⁡ [ − 1 2 θ T M θ ] d θ = { P f M n even 0 n odd {\displaystyle \int \exp \left[-{\tfrac {1}{2}}\theta ^{T}M\theta \right]\,d\theta ={\begin{cases}\mathrm {Pf} \,M&n{\mbox{ even}}\\0&n{\mbox{ odd}}\end{cases}}} with M {\displaystyle M} being a complex skew-symmetric n × n {\displaystyle n\times n} matrix, and P f M {\displaystyle \mathrm {Pf} \,M} being the Pfaffian of M {\displaystyle M} , which fulfills ( P f M ) 2 = det M {\displaystyle (\mathrm {Pf} \,M)^{2}=\det M} . In the above formulas the notation d θ = d θ 1 ⋯ d θ n {\displaystyle d\theta =d\theta _{1}\cdots \,d\theta _{n}} is used. From these formulas, other useful formulas follow (See Appendix A in) : ∫ exp ⁡ [ θ T A η + θ T J + K T η ] d η 1 d θ 1 … d η n d θ n = det A exp ⁡ [ − K T A − 1 J ] {\displaystyle \int \exp \left[\theta ^{T}A\eta +\theta ^{T}J+K^{T}\eta \right]\,d\eta _{1}\,d\theta _{1}\dots d\eta _{n}d\theta _{n}=\det A\,\,\exp[-K^{T}A^{-1}J]} with A {\displaystyle A} being an invertible n × n {\displaystyle n\times n} matrix. Note that these integrals are all in the form of a partition function. History Berezin integral was probably first presented by David John Candlin in 1956. Later it was independently discovered by Felix Berezin in 1966. Unfortunately Candlin's article failed to attract notice, and has been buried in oblivion. Berezin's work came to be widely known, and has almost been cited universally, becoming an indispensable tool to treat quantum field theory of fermions by functional integral. Other authors contributed to these developments, including the physicists Isaak Khalatnikov (although his paper contains mistakes), Paul Taunton Matthews and Abdus Salam, and J. L. Martin. See also Supermanifold Berezinian Footnote References Further reading Theodore Voronov: Geometric integration theory on Supermanifolds, Harwood Academic Publisher, ISBN 3-7186-5199-8 Berezin, Felix Alexandrovich: Introduction to Superanalysis, Springer Netherlands, ISBN 978-90-277-1668-2 Intrinsic bond orbitals (IBO) are localized molecular orbitals giving exact and non-empirical representations of wave functions. They are obtained by unitary transformation and form an orthogonal set of orbitals localized on a minimal number of atoms. IBOs present an intuitive and unbiased interpretation of chemical bonding with naturally arising Lewis structures. For this reason IBOs have been successfully employed for the elucidation of molecular structures and electron flow along the intrinsic reaction coordinate (IRC). IBOs have also found application as Wannier functions in the study of solids. Theory The IBO method entails molecular wave-functions calculated using self-consistent field (SCF) methods such as Kohn-Sham density functional theory (DFT) which are expressed as linear combinations of localized molecular orbitals. In order to arrive at IBOs, intrinsic atomic orbitals (IAOs) are first calculated as representations of a molecular wave function for which each IAO can be assigned to a specific atom. This allows for a chemically intuitive orbital picture as opposed to the commonly used large and diffuse basis sets for the construction of more complex molecular wavefunctions. IAOs are constructed from tabulated free-atom AOs of standard basis-sets under consideration of the molecular environment. This yields polarized atomic orbitals that resemble the free-atom AOs as much as possible, before orthonormalization of the polarized AOs results in the set of IAOs. IAOs are thus a minimal basis for a given molecule in which atomic contributions can be distinctly assigned. The sum of all IAOs spans exactly over the molecular orbitals which renders them an exact representation of the wavefunction. Since IAOs are associated with a specific atom, they can provide atom specific properties such as the partial charge. Compared to other charges, such as the Mulliken charge, the IAO charges are independent of the employed basis set. IBOs are constructed as a linear combination over IAOs with the condition of minimizing the number of atoms over which the orbital charge is spread. Each IBO can thereby be divided into the contributions of the atoms as the electronic occupation n A ( i ′ ) {\displaystyle n_{A}(i')} of orbital i ′ {\displaystyle i'} on atom A {\displaystyle A} . The localization is performed in the spirit of the Pipek-Mezey localization scheme, maximizing a localization functional L {\displaystyle L} . L = ∑ i o c c ∑ A a t o m [ n A ( i ′ ) ] p {\displaystyle L=\sum _{i}^{occ}\sum _{A}^{atom}[n_{A}(i')]^{p}} with p = 4 {\displaystyle p=4} or 2 {\displaystyle 2} . While the choice of the exponent p {\displaystyle p} does not affect the resulting IBOs in most cases, the choice of p = 4 {\displaystyle p=4} localizes the orbitals in aromatic systems unlike p = 2 {\displaystyle p=2} . The process of IBO construction is performed by unitary tranfomation of canonical MOs, which ensures that the IBOs remain an exact and physically accurate representation of the molecular wavefunction due to the invariance of Slater determinant wavefunctions towards unitary rotations. | i ′ ⟩ = ∑ i o c c | i ⟩ U i i ′ {\displaystyle |i'\rangle =\sum _{i}^{occ}|i\rangle U_{ii'}} The unitary matrix U i i ′ {\displaystyle U_{ii'}} , which produces the localized IBOs upon matrix multiplication with set of occupied MOs | i ⟩ {\displaystyle |i\rangle } , is thereby chosen to effectively minimize spread of IBOs over the atoms of a molecule. The product is a set of localized IBOs, closely resembling the chemically intuitive shapes of molecular orbitals, allowing for distinction of bond types, atomic contributions and polarization. Application in structure and bonding In his original paper introducing IBOs, Knizia showed the versatility of his method for describing not only classical bonding situations, such as the σ and π bond, but also aromatic systems and non-trivial bonds. The differentiation of σ and π bonds in acrylic acid is possible based on IBO geometries, as are the identification of the IBOs corresponding to the oxygen lone pairs. Benzene provided an example of a delocalized aromatic system to test the IBO method. Apart from the C-C and C-H σ-bonds, the six electron π-system is expressed as three delocalized IBOs. Representation of non-Lewis bonding was demonstrated on diborane B2H6, with one IBO stretching over B-H-B, corresponding to the 3-center-2-electron bond. Transition metal compounds IBO analysis was used to explain the stability of electron rich gold-carbene complexes, mimicking reactive intermediates in gold catalysis. While these complexes are sometimes depicted with a Au-C double bond, representing the sigma donation of the carbene and π backbonding of Au, IBO analysis points towards a minimal amount of π-backbonding with the respective orbital mainly localized on Au. The σ-donating carbene orbital is likewise strongly polarized towards C. Stabilization of the compound thus occurs through strong donation of the aromatic carbene substituents into the carbene carbon p-orbitals, outcompeting the Au-π-backdonation. IBO analysis was thus able to negate the double bond character of the gold-carbene complexes and provided deep insight into the electronic structure of Cy3P-Au-C(4-OMe-C6H4)2) (Cy = cyclohexyl). The π-backbonding character was again evaluated for gold-vinylidene complexes, as another common type of gold catalysis intermediates. IBO analysis revealed significantly stronger π-backbonding for the gold-vinylidenes compared to the gold-carbenes. This was attributed to the geometric inability of aromatic vinylidene substituents to compete with Au for π-interactions since the respective orbitals are perpendicular to each other. Knizia and Klein similarly employed IBO for the analysis of [Fe(CO)3NO]–. The even polarization of IBOs between Fe and N points towards a covalently bonded NO ligand. The double bond occurs via two d-p π-interactions and results in a formal Fe0 center. Confirmed by further calculations, IBO proved as a fast and straightforward method to interpret bonding in this case. Making use of the low computational cost, a Cloke-Wilson rearrangement catalyzed by [Fe(CO)3NO]– was investigated by constructing the IBOs for every stationary point along the IRC. It was found that one of the Fe-NO π bonds takes active part in catalysis by electron transfer to and from the substrate, explaining the unique catalytic activity of [Fe(CO)3NO]– compared to the isoelectronic [Fe(CO)4]2–. Apart from the above mentioned compounds, the IBO method has been employed to investigate various other transition metal complexes, such as gold-diarylallenylidenes or diplatinum diboranyl complexes, proving as a valuable tool to gain insight into the extent and nature of bonding. Main group compounds IBO analysis has been employed in main group chemistry to elucidate oftentimes non-trivial electronic structure. The bonding of phosphaaluminirenes was, for example, investigated showing a 3-center-2π-electron bond of the AlCP cycle. Further application was found for confirming the distonic nature of a phosphorus containing radical cation reported by Chen et al. (see figure). While the IAO charge analysis yielded a positive charge on the chelated P, IBOs showed the localization of the unpaired electron on the other P atom, confirming the spatial separation of radical site and charge. Another example is the elucidation of the electronic structure of the hexamethylbenzene dication. Three π bonding IBOs were found between the basal C5Me5 plane and the apical C, reminiscent of Cp* coordination complexes. The three π bonds are thereby polarized towards the apical C, which in turn coordinates to a CH3+ cation with its lone pair. IBO analysis therefore revealed the Lewis-acidic and Lewis-basic character of the apical C. Applications of IBO for cluster compounds have included zirconium doped boron clusters. IBO analysis showed, that the unusual stability of the neutral ZrB12 cluster stems from several multicenter σ bonds. The B-B σ bonding orbitals extend to the central Zr atom, forming the mulicenter bonds. This example displays the method's aptitude to analyze cluster compounds and multicenter bonding. Valence virtual intrinsic bond orbitals Although IBOs typically describe occupied orbitals, the description of unoccupied orbitals can likewise be of value for interpreting chemical interactions. Valence virtual IBOs (vvIBOs) were introduced with the investigation of high valent formal Ni(IV) complexes. The bonding and antibonding manifold of the compound were described using IBOs and vvIBOs respectively. Compared to the widely used HOMO/LUMOs which are often spread over the whole molecule and can be difficult to interpret, vvIBOs allow for more direct interpretation of chemical interactions with unoccupied orbitals. Electron flow along the IRC In 2015, Knizia and Klein introduced the analysis of electron flow in reactions with IBO as a non-empirical and straight-forward method of evaluating curly arrow mechanisms. Since IBOs are exact representations of Kohn-Sham wavefunctions, they can provide physical conformation for curly arrows mechanisms based on first-principles. IBOs usually represent chemical bonds and lone pairs, this method allows for elucidation of bond rearrangements in terms of the elemental steps and their sequence. By calculating the root mean square deviations of the partial charge distributions compared to the initial charge distribution, IBOs taking active part in a reaction can be distinguished from those that remain unchanged along the IRC. Knizia and Klein demonstrate the versatility of this method in their original report, first presenting a simple SN2-type self-exchange reaction of H3CCl and Cl–, followed by the migration of π bonds in a substitution reaction (SN2) and π- to σ-bond transformations in a Claisen rearrangement. Electron flow can be easily followed by observing the migration of an IBO and bond types are easily distinguished based on the geometries of the IBOs. The value of IBO analysis along the IRC especially shows for complex reactions, such as a cyclopropanation reaction with only one transition state and without intermediates, reported by Haven et al. Calculations by Knizia and Klein yielded a precise curly arrow mechanism for this reaction. Closed-shell systems Examples of IBO analysis along the IRC included the investigation of C-H bond activation by gold-vinylidene complexes. Through this method, it is possible to discern between concerted and stepwise reactions. The previously thought single step C-H activation reaction was in this case revealed to consist of three distinct phases: i) hydride transfer, ii) C-C bond formation and iii) sigma to pi rearrangement of the lone pair coordinated to Au. Other reports of IBO analysis along the IRC include the elucidation and confirmation of a previously proposed mechanism for a [3,3]-sigmatropic rearrangement of a Au(I)-vinyl species or the epoxidation of alkene by peracids. For the latter, the textbook four curly arrow mechanism was found to be physically inaccurate. Instead, seven changing IBOs were found, yielding an ideal mechanism featuring seven curly arrows. The combination of IBO analysis with other computational methods, such as natural bond orbital (NBO) analysis for a Ti-catalyzed pyrrole synthesis or natural localized molecular orbital (NLMO) analysis for an intramolecular cycloaddition of a phosphaalkene to an arene has likewise led to insightful results regarding the specifics of the reaction mechanisms. Open-shell systems Klein and Knizia furthermore introduced the first examples of IBOs used for analysis of open-shell systems during proton-coupled electron transfer (PCET) and hydrogen atom transfer (HAT). The differentiation between pCET, a separate but concerted electron and proton transfer, and HAT, the transfer of a hydrogen atom, were shown for two well-studied model systems of enzymatic Fe-oxo active sites. IBOs along the IRC were calculated for the alpha and beta spin manifold respectively. While the IBO of the alpha spin electron travelled together with the proton to take part in the formation of a new H-O bond in case of HAT, the electron was transferred to the Fe-center separated from the transferred proton for PCET. The successful application of the IBO method for these two examples of open-shell systems was suggested to pave the way for broader applications to similar problems. See also Localized molecular orbitals == References == Maria V. Chekhova (born 1963) is a Russian-German physicist known for her research on quantum optics and in particular on the quantum entanglement of pairs of photons. She is a researcher at the Max Planck Institute for the Science of Light in Erlangen, Germany, where she heads an independent research group on quantum radiation, and a professor at the University of Erlangen–Nuremberg, in the chair of experimental physics (optics). Education and career Chekhova was born on 8 June 1963 in Moscow, and educated in physics at Moscow State University, where she earned a master's degree in 1986, completed a Ph.D. in 1989, and earned a habilitation in 2004. She was a full-time researcher at Moscow State University from 1989 to 2010, continuing on a part-time basis until 2020. In the meantime, she took her present position at the Max Planck Institute for the Science of Light in 2010. In 2020, she added a part-time affiliation as professor at the University of Erlangen–Nuremberg. In November 2021 Chekhova was elected for the "Optica Fellow 2022" for "pioneering contributions to the science and applications of photon pairs and twin beams". Book Chekhova is the coauthor, with Peter Banzer, of the textbook Polarization of Light: In Classical, Quantum and Nonlinear Optics (De Gruyter, 2021). References External links Chekhova Research Group Maria V. Chekhova publications indexed by Google Scholar In quantum field theory, the quantum vacuum state (also called the quantum vacuum or vacuum state) is the quantum state with the lowest possible energy. Generally, it contains no physical particles. However, the quantum vacuum is not a simple empty space, but instead contains fleeting electromagnetic waves and particles that pop into and out of the quantum field. The QED vacuum of quantum electrodynamics (or QED) was the first vacuum of quantum field theory to be developed. QED originated in the 1930s, and in the late 1940s and early 1950s, it was reformulated by Feynman, Tomonaga, and Schwinger, who jointly received the Nobel prize for this work in 1965. Today, the electromagnetic interactions and the weak interactions are unified (at very high energies only) in the theory of the electroweak interaction. The Standard Model is a generalization of the QED work to include all the known elementary particles and their interactions (except gravity). Quantum chromodynamics (or QCD) is the portion of the Standard Model that deals with strong interactions, and the QCD vacuum is the vacuum of quantum chromodynamics. It is the object of study in the Large Hadron Collider and the Relativistic Heavy Ion Collider, and is related to the so-called vacuum structure of strong interactions. Non-zero expectation value If the quantum field theory can be accurately described through perturbation theory, then the properties of the vacuum are analogous to the properties of the ground state of a quantum mechanical harmonic oscillator, or more accurately, the ground state of a measurement problem. In this case, the vacuum expectation value of any field operator vanishes. For quantum field theories in which perturbation theory breaks down at low energies (for example, Quantum chromodynamics or the BCS theory of superconductivity), field operators may obtain non-vanishing vacuum expectation values by spontaneous symmetry breaking. In the Standard Model, the Higgs field acquires a non-zero expectation value when the electroweak symmetry is broken, and this explains part of the masses of other particles. Energy The vacuum state is associated with a zero-point energy, and this zero-point energy (equivalent to the lowest possible energy state) has measurable effects. It may be detected as the Casimir effect in the laboratory. In physical cosmology, the energy of the cosmological vacuum appears as the cosmological constant. The energy of a cubic centimeter of empty space has been calculated figuratively to be one trillionth of an erg (or 0.6 eV). An outstanding requirement imposed on a potential Theory of Everything is that the energy of the quantum vacuum state must explain the physically observed cosmological constant. Symmetry For a relativistic field theory, the vacuum is Poincaré invariant, which follows from Wightman axioms but can also be proved directly without these axioms. Poincaré invariance implies that only scalar combinations of field operators have non-vanishing vacuum expectation values. The vacuum may break some of the internal symmetries of the Lagrangian of the field theory. In this case, the vacuum has less symmetry than the theory allows, and one says that spontaneous symmetry breaking has occurred. Non-linear permittivity Quantum corrections to Maxwell's equations are expected to result in a tiny nonlinear electric polarization term in the vacuum, resulting in a field-dependent electrical permittivity ε deviating from the nominal value ε0 of vacuum permittivity. These theoretical developments are described, for example, in Dittrich and Gies. The theory of quantum electrodynamics predicts that the QED vacuum should exhibit a slight nonlinearity so that in the presence of a very strong electric field, the permittivity is increased by a tiny amount with respect to ε0. Subject to ongoing experimental efforts is the possibility that a strong electric field would modify the effective permeability of free space, becoming anisotropic with a value slightly below μ0 in the direction of the electric field and slightly exceeding μ0 in the perpendicular direction. The quantum vacuum exposed to an electric field exhibits birefringence for an electromagnetic wave traveling in a direction other than the electric field. The effect is similar to the Kerr effect but without matter being present. This tiny nonlinearity can be interpreted in terms of virtual pair production A characteristic electric field strength for which the nonlinearities become sizable is predicted to be enormous, about 1.32 × 10 18 {\displaystyle 1.32\times 10^{18}} V/m, known as the Schwinger limit; the equivalent Kerr constant has been estimated, being about 1020 times smaller than the Kerr constant of water. Explanations for dichroism from particle physics, outside quantum electrodynamics, also have been proposed. Experimentally measuring such an effect is challenging, and has not yet been successful. Virtual particles The presence of virtual particles can be rigorously based upon the non-commutation of the quantized electromagnetic fields. Non-commutation means that although the average values of the fields vanish in a quantum vacuum, their variances do not. The term "vacuum fluctuations" refers to the variance of the field strength in the minimal energy state, and is described picturesquely as evidence of "virtual particles". It is sometimes attempted to provide an intuitive picture of virtual particles, or variances, based upon the Heisenberg energy-time uncertainty principle: Δ E Δ t ≥ ℏ 2 , {\displaystyle \Delta E\Delta t\geq {\frac {\hbar }{2}}\,,} (with ΔE and Δt being the energy and time variations respectively; ΔE is the accuracy in the measurement of energy and Δt is the time taken in the measurement, and ħ is the Reduced Planck constant) arguing along the lines that the short lifetime of virtual particles allows the "borrowing" of large energies from the vacuum and thus permits particle generation for short times. Although the phenomenon of virtual particles is accepted, this interpretation of the energy-time uncertainty relation is not universal. One issue is the use of an uncertainty relation limiting measurement accuracy as though a time uncertainty Δt determines a "budget" for borrowing energy ΔE. Another issue is the meaning of "time" in this relation because energy and time (unlike position q and momentum p, for example) do not satisfy a canonical commutation relation (such as [q, p] = i ħ). Various schemes have been advanced to construct an observable that has some kind of time interpretation, and yet does satisfy a canonical commutation relation with energy. Many approaches to the energy-time uncertainty principle are a long and continuing subject. Physical nature of the quantum vacuum According to Astrid Lambrecht (2002): "When one empties out a space of all matter and lowers the temperature to absolute zero, one produces in a Gedankenexperiment [thought experiment] the quantum vacuum state." According to Fowler & Guggenheim (1939/1965), the third law of thermodynamics may be precisely enunciated as follows: It is impossible by any procedure, no matter how idealized, to reduce any assembly to the absolute zero in a finite number of operations. (See also.) Photon-photon interaction can occur only through interaction with the vacuum state of some other field, such as the Dirac electron-positron vacuum field; this is associated with the concept of vacuum polarization. According to Milonni (1994): "... all quantum fields have zero-point energies and vacuum fluctuations." This means that there is a component of the quantum vacuum respectively for each component field (considered in the conceptual absence of the other fields), such as the electromagnetic field, the Dirac electron-positron field, and so on. According to Milonni (1994), some of the effects attributed to the vacuum electromagnetic field can have several physical interpretations, some more conventional than others. The Casimir attraction between uncharged conductive plates is often proposed as an example of an effect of the vacuum electromagnetic field. Schwinger, DeRaad, and Milton (1978) are cited by Milonni (1994) as validly, though unconventionally, explaining the Casimir effect with a model in which "the vacuum is regarded as truly a state with all physical properties equal to zero." In this model, the observed phenomena are explained as the effects of the electron motions on the electromagnetic field, called the source field effect. Milonni writes: The basic idea here will be that the Casimir force may be derived from the source fields alone even in completely conventional QED, ... Milonni provides detailed argument that the measurable physical effects usually attributed to the vacuum electromagnetic field cannot be explained by that field alone, but require in addition a contribution from the self-energy of the electrons, or their radiation reaction. He writes: "The radiation reaction and the vacuum fields are two aspects of the same thing when it comes to physical interpretations of various QED processes including the Lamb shift, van der Waals forces, and Casimir effects." This point of view is also stated by Jaffe (2005): "The Casimir force can be calculated without reference to vacuum fluctuations, and like all other observable effects in QED, it vanishes as the fine structure constant, α, goes to zero." See also References Further reading Free pdf copy of The Structured Vacuum – thinking about nothing by Johann Rafelski and Berndt Muller (1985) ISBN 3-87144-889-3. M. E. Peskin and D. V. Schroeder, An introduction to Quantum Field Theory. H. Genz, Nothingness: The Science of Empty Space. Puthoff, H. E.; Little, S. R.; Ibison, M. (2001). "Engineering the Zero-Point Field and Polarizable Vacuum for Interstellar Flight". arXiv:astro-ph/0107316. E. W. Davis, V. L. Teofilo, B. Haisch, H. E. Puthoff, L. J. Nickisch, A. Rueda and D. C. Cole (2006), "Review of Experimental Concepts for Studying the Quantum Vacuum Field". External links Energy into Matter In the mathematical field of representation theory, a representation of a Lie superalgebra is an action of Lie superalgebra L on a Z2-graded vector space V, such that if A and B are any two pure elements of L and X and Y are any two pure elements of V, then ( c 1 A + c 2 B ) ⋅ X = c 1 A ⋅ X + c 2 B ⋅ X {\displaystyle (c_{1}A+c_{2}B)\cdot X=c_{1}A\cdot X+c_{2}B\cdot X} A ⋅ ( c 1 X + c 2 Y ) = c 1 A ⋅ X + c 2 A ⋅ Y {\displaystyle A\cdot (c_{1}X+c_{2}Y)=c_{1}A\cdot X+c_{2}A\cdot Y} ( − 1 ) A ⋅ X = ( − 1 ) A ( − 1 ) X {\displaystyle (-1)^{A\cdot X}=(-1)^{A}(-1)^{X}} [ A , B ] ⋅ X = A ⋅ ( B ⋅ X ) − ( − 1 ) A B B ⋅ ( A ⋅ X ) . {\displaystyle [A,B]\cdot X=A\cdot (B\cdot X)-(-1)^{AB}B\cdot (A\cdot X).} Equivalently, a representation of L is a Z2-graded representation of the universal enveloping algebra of L which respects the third equation above. Unitary representation of a star Lie superalgebra A * Lie superalgebra is a complex Lie superalgebra equipped with an involutive antilinear map * such that * respects the grading and [a,b]*=[b*,a*]. A unitary representation of such a Lie algebra is a Z2 graded Hilbert space which is a representation of a Lie superalgebra as above together with the requirement that self-adjoint elements of the Lie superalgebra are represented by Hermitian transformations. This is a major concept in the study of supersymmetry together with representation of a Lie superalgebra on an algebra. Say A is an *-algebra representation of the Lie superalgebra (together with the additional requirement that * respects the grading and L[a]*=-(-1)LaL*[a*]) and H is the unitary rep and also, H is a unitary representation of A. These three reps are all compatible if for pure elements a in A, |ψ> in H and L in the Lie superalgebra, L[a|ψ>)]=(L[a])|ψ>+(-1)Laa(L[|ψ>]). Sometimes, the Lie superalgebra is embedded within A in the sense that there is a homomorphism from the universal enveloping algebra of the Lie superalgebra to A. In that case, the equation above reduces to L[a]=La-(-1)LaaL. This approach avoids working directly with a Lie supergroup, and hence avoids the use of auxiliary Grassmann numbers. See also Graded vector space Lie algebra representation Representation theory of Hopf algebras In theoretical physics, the Mandelstam variables are numerical quantities that encode the energy, momentum, and angles of particles in a scattering process in a Lorentz-invariant fashion. They are used for scattering processes of two particles to two particles. The Mandelstam variables were first introduced by physicist Stanley Mandelstam in 1958. If the Minkowski metric is chosen to be d i a g ( 1 , − 1 , − 1 , − 1 ) {\displaystyle \mathrm {diag} (1,-1,-1,-1)} , the Mandelstam variables s , t , u {\displaystyle s,t,u} are then defined by s = ( p 1 + p 2 ) 2 c 2 = ( p 3 + p 4 ) 2 c 2 {\displaystyle s=(p_{1}+p_{2})^{2}c^{2}=(p_{3}+p_{4})^{2}c^{2}} t = ( p 1 − p 3 ) 2 c 2 = ( p 4 − p 2 ) 2 c 2 {\displaystyle t=(p_{1}-p_{3})^{2}c^{2}=(p_{4}-p_{2})^{2}c^{2}} u = ( p 1 − p 4 ) 2 c 2 = ( p 3 − p 2 ) 2 c 2 {\displaystyle u=(p_{1}-p_{4})^{2}c^{2}=(p_{3}-p_{2})^{2}c^{2}} , where p1 and p2 are the four-momenta of the incoming particles and p3 and p4 are the four-momenta of the outgoing particles. s {\displaystyle s} is also known as the square of the center-of-mass energy (invariant mass) and t {\displaystyle t} as the square of the four-momentum transfer. Feynman diagrams The letters s,t,u are also used in the terms s-channel (timelike channel), t-channel, and u-channel (both spacelike channels). These channels represent different Feynman diagrams or different possible scattering events where the interaction involves the exchange of an intermediate particle whose squared four-momentum equals s,t,u, respectively. For example, the s-channel corresponds to the particles 1,2 joining into an intermediate particle that eventually splits into 3,4: the s-channel is the only way that resonances and new unstable particles may be discovered provided their lifetimes are long enough that they are directly detectable. The t-channel represents the process in which the particle 1 emits the intermediate particle and becomes the final particle 3, while the particle 2 absorbs the intermediate particle and becomes 4. The u-channel is the t-channel with the role of the particles 3,4 interchanged. When evaluating a Feynman amplitude one often finds scalar products of the external four momenta. One can use the Mandelstam variables to simplify these: ( p 1 c ) ⋅ ( p 2 c ) = 1 2 ( s − ( m 1 c 2 ) 2 − ( m 2 c 2 ) 2 ) {\displaystyle \left(p_{1}c\right)\cdot \left(p_{2}c\right)={\frac {1}{2}}\left(s-\left(m_{1}c^{2}\right)^{2}-\left(m_{2}c^{2}\right)^{2}\right)} ( p 1 c ) ⋅ ( p 3 c ) = 1 2 ( ( m 1 c 2 ) 2 + ( m 3 c 2 ) 2 − t ) {\displaystyle \left(p_{1}c\right)\cdot \left(p_{3}c\right)={\frac {1}{2}}\left(\left(m_{1}c^{2}\right)^{2}+\left(m_{3}c^{2}\right)^{2}-t\right)} ( p 1 c ) ⋅ ( p 4 c ) = 1 2 ( ( m 1 c 2 ) 2 + ( m 4 c 2 ) 2 − u ) {\displaystyle \left(p_{1}c\right)\cdot \left(p_{4}c\right)={\frac {1}{2}}\left(\left(m_{1}c^{2}\right)^{2}+\left(m_{4}c^{2}\right)^{2}-u\right)} Where m i {\displaystyle m_{i}} is the mass of the particle with corresponding momentum p i {\displaystyle p_{i}} . Sum Note that s + t + u = ( m 1 c 2 ) 2 + ( m 2 c 2 ) 2 + ( m 3 c 2 ) 2 + ( m 4 c 2 ) 2 {\displaystyle s+t+u=\left(m_{1}c^{2}\right)^{2}+\left(m_{2}c^{2}\right)^{2}+\left(m_{3}c^{2}\right)^{2}+\left(m_{4}c^{2}\right)^{2}} where mi is the mass of particle i. Relativistic limit In the relativistic limit, the momentum (speed) is large, so using the relativistic energy-momentum equation, the energy becomes essentially the momentum norm (e.g. E 2 = p ⋅ p + m 0 2 {\displaystyle E^{2}=\mathbf {p} \cdot \mathbf {p} +{m_{0}}^{2}} becomes E 2 ≈ p ⋅ p {\displaystyle E^{2}\approx \mathbf {p} \cdot \mathbf {p} } ). The rest mass can also be neglected. So for example, s / c 2 = ( p 1 + p 2 ) 2 = p 1 2 + p 2 2 + 2 p 1 ⋅ p 2 ≈ 2 p 1 ⋅ p 2 {\displaystyle s/c^{2}=(p_{1}+p_{2})^{2}=p_{1}^{2}+p_{2}^{2}+2p_{1}\cdot p_{2}\approx 2p_{1}\cdot p_{2}} because p 1 2 = ( m 1 c ) 2 {\displaystyle p_{1}^{2}=\left(m_{1}c\right)^{2}} and p 2 2 = ( m 2 c ) 2 {\displaystyle p_{2}^{2}=\left(m_{2}c\right)^{2}} . Thus, See also Feynman diagrams Bhabha scattering Møller scattering Compton scattering References Mandelstam, S. (1958). "Determination of the Pion-Nucleon Scattering Amplitude from Dispersion Relations and Unitarity". Physical Review. 112 (4): 1344. Bibcode:1958PhRv..112.1344M. doi:10.1103/PhysRev.112.1344. Archived from the original on 2000-05-28. Halzen, Francis; Martin, Alan (1984). Quarks & Leptons: An Introductory Course in Modern Particle Physics. John Wiley & Sons. ISBN 0-471-88741-2. Perkins, Donald H. (2000). Introduction to High Energy Physics (4th ed.). Cambridge University Press. ISBN 0-521-62196-8. In physics, the principle of locality states that an object is influenced directly only by its immediate surroundings. A theory that includes the principle of locality is said to be a "local theory". This is an alternative to the concept of instantaneous, or "non-local" action at a distance. Locality evolved out of the field theories of classical physics. The idea is that for a cause at one point to have an effect at another point, something in the space between those points must mediate the action. To exert an influence, something, such as a wave or particle, must travel through the space between the two points, carrying the influence. The special theory of relativity limits the maximum speed at which causal influence can travel to the speed of light, c {\displaystyle c} . Therefore, the principle of locality implies that an event at one point cannot cause a truly simultaneous result at another point. An event at point A {\displaystyle A} cannot cause a result at point B {\displaystyle B} in a time less than T = D / c {\displaystyle T=D/c} , where D {\displaystyle D} is the distance between the points and c {\displaystyle c} is the speed of light in vacuum. The principle of locality plays a critical role in one of the central results of quantum mechanics. In 1935, Albert Einstein, Boris Podolsky, and Nathan Rosen, with their EPR paradox thought experiment, raised the possibility that quantum mechanics might not be a complete theory. They described two systems physically separated after interacting; this pair would be called entangled in modern terminology. They reasoned that without additions, now called hidden variables, quantum mechanics would predict illogical relationships between the physically separated measurements. In 1964, John Stewart Bell formulated Bell's theorem, an inequality which, if violated in actual experiments, implies that quantum mechanics violates local causality (referred to as local realism in later work), a result now considered equivalent to precluding local hidden variables. Progressive variations on those Bell test experiments have since shown that quantum mechanics broadly violates Bell's inequalities. According to some interpretations of quantum mechanics, this result implies that some quantum effects violate the principle of locality. Pre-quantum mechanics During the 17th century, Newton's principle of universal gravitation was formulated in terms of "action at a distance", thereby violating the principle of locality. Newton himself considered this violation to be absurd: It is inconceivable that inanimate Matter should, without the Mediation of something else, which is not material, operate upon, and affect other matter without mutual Contact…That Gravity should be innate, inherent and essential to Matter, so that one body may act upon another at a distance thro' a Vacuum, without the Mediation of any thing else, by and through which their Action and Force may be conveyed from one to another, is to me so great an Absurdity that I believe no