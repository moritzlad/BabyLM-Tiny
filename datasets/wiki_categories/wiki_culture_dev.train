happiness and images of physical beauty—such feelings leave people dissatisfied with their own lives. A joint study by two German universities discovered that one out of three people were more dissatisfied with their lives after visiting Facebook, and another study by Utah Valley University found that college students felt worse about themselves following an increase in time on Facebook. Positive effects include signs of "virtual empathy" with online friends and helping introverted persons learn social skills. A 2020 experimental study in the American Economic Review found that deactivating Facebook led to increased subjective well-being. In a blog post in December 2017, the company highlighted research that has shown "passively consuming" the News Feed, as in reading but not interacting, left users with negative feelings, whereas interacting with messages pointed to improvements in well-being. Politics In February 2008, a Facebook group called "One Million Voices Against FARC" organized an event in which hundreds of thousands of Colombians marched in protest against the Revolutionary Armed Forces of Colombia (FARC). In August 2010, one of North Korea's official government websites and the country's official news agency, Uriminzokkiri, joined Facebook. During the Arab Spring many journalists claimed Facebook played a major role in the 2011 Egyptian revolution. On January 14, the Facebook page of "We are all Khaled Said" was started by Wael Ghoniem to invite the Egyptian people to "peaceful demonstrations" on January 25. In Tunisia and Egypt, Facebook became the primary tool for connecting protesters and led the Egyptian government to ban it, Twitter and other sites. After 18 days, the uprising forced President Hosni Mubarak to resign. In a Bahraini uprising that started on February 14, 2011, Facebook was utilized by the Bahraini regime and regime loyalists to identify, capture and prosecute citizens involved in the protests. A 20-year-old woman named Ayat Al Qurmezi was identified as a protester using Facebook and imprisoned. In 2011, Facebook filed paperwork with the Federal Election Commission to form a political action committee under the name FB PAC. In an email to The Hill, a spokesman for Facebook said "Facebook Political Action Committee will give our employees a way to make their voice heard in the political process by supporting candidates who share our goals of promoting the value of innovation to our economy while giving people the power to share and make the world more open and connected." During the Syrian civil war, the YPG, a libertarian army for Rojava recruited westerners through Facebook in its fight against ISIL. Dozens joined its ranks. The Facebook page's name "The Lions of Rojava" comes from a Kurdish saying which translates as "A lion is a lion, whether it's a female or a male", reflecting the organization's feminist ideology. In recent years, Facebook's News Feed algorithms have been identified as a cause of political polarization, for which it has been criticized. It has likewise been accused of amplifying the reach of 'fake news' and extreme viewpoints, as when it may have enabled conditions which led to the 2015 Rohingya refugee crisis. Facebook first played a role in the American political process in January 2008, shortly before the New Hampshire primary. Facebook teamed up with ABC and Saint Anselm College to allow users to give live feedback about the "back to back" January 5 Republican and Democratic debates. Facebook users took part in debate groups on specific topics, voter registration and message questions. Over a million people installed the Facebook application "US Politics on Facebook" in order to take part which measured responses to specific comments made by the debating candidates. A poll by CBS News, UWIRE and The Chronicle of Higher Education claimed to illustrate how the "Facebook effect" had affected youthful voters, increasing voting rates, support of political candidates, and general involvement. The new social media, such as Facebook and Twitter, connected hundreds of millions of people. By 2008, politicians and interest groups were experimenting with systematic use of social media to spread their message. By the 2016 election, political advertising to specific groups had become normalized. Facebook offered the most sophisticated targeting and analytics platform. ProPublica noted that their system enabled advertisers to direct their pitches to almost 2,300 people who expressed interest in the topics of "Jew hater", "How to burn Jews", or, "History of 'why Jews ruin the world". Facebook has used several initiatives to encourage its users to register to vote and vote. An experiment in 2012 involved showing Facebook users pictures of their friends who reported that they had voted; users who were shown the pictures were about 2% more likely to report that they had also voted compared to the control group, which was not encouraged to vote. In 2020, Facebook announced the goal of helping four million voters register in the US, saying that it had registered 2.5 million by September. The Cambridge Analytica data scandal offered another example of the perceived attempt to influence elections. The Guardian claimed that Facebook knew about the security breach for two years, but did nothing to stop it until it became public. Facebook banned political ads to prevent the manipulation of voters in the US's November's election. Propaganda experts said there are other ways for misinformation to reach voters on social media platforms and blocking political ads will not serve as a proven solution. In March 2024, former US President Donald Trump said that getting rid of TikTok would allow Facebook, which he called the "enemy of the people", to double its business. He spoke after President Biden said he was ready to sign legislation that would require TikTok owner ByteDance to sell the video platform or face a ban in the US. India Ahead of the 2019 general elections in India, Facebook has removed 103 pages, groups and accounts on Facebook and Instagram platforms originating from Pakistan. Facebook said its investigation found a Pakistani military link, along with a mix of real accounts of ISPR employees, and a network of fake accounts created by them that have been operating military fan pages, general interest pages but were posting content about Indian politics while trying to conceal their identity. Owing to the same reasons, Facebook also removed 687 pages and accounts of Congress because of coordinated inauthentic behavior on the platform. Culture Facebook and Zuckerberg have been the subject of music, books, film and television. The 2010 film The Social Network, directed by David Fincher and written by Aaron Sorkin, stars Jesse Eisenberg as Zuckerberg and went on to win three Academy Awards and four Golden Globes. In 2008, Collins English Dictionary declared "Facebook" as its new Word of the Year. In December 2009, the New Oxford American Dictionary declared its word of the year to be the verb "unfriend", defined as "To remove someone as a 'friend' on a social networking site such as Facebook". Internet.org In August 2013, Facebook founded Internet.org in collaboration with six other technology companies to plan and help build affordable Internet access for underdeveloped and developing countries. The service, called Free Basics, includes various low-bandwidth applications such as AccuWeather, BabyCenter, BBC News, ESPN, and Bing. There was severe opposition to Internet.org in India, where the service started in partnership with Reliance Communications in 2015 was banned a year later by the Telecom Regulatory Authority of India (TRAI). In 2018, Zuckerberg claimed that "Internet.org efforts have helped almost 100 million people get access to the internet who may not have had it otherwise." Environment Facebook announced in 2021 that it will make an effort to stop disinformation about climate change. The company will use George Mason University, Yale Program on Climate Change Communication and the University of Cambridge as sources of information. The company will expand its information hub on climate to 16 countries. Users in other countries will be directed to the site of the United Nations Environment Programme for information. See also References Further reading Sue Halpern, "For the Love of Money" (review of Sarah Wynn-Williams, Careless People: A Cautionary Tale of Power, Greed, and Lost Idealism, Flatiron, 382 pp.), The New York Review of Books, vol. LXXII, no. 9 (29 May 2025), pp. 29–30. External links Official website Sharism is a philosophy on sharing content and ideas, developed by Isaac Mao. Inspired by user-generated content, sharism states that the act of sharing something within a community produces a proper value for each of its participants: "the more you share, the more you receive". As knowledge is produced through crowdsourcing, this new kind of shared ownership leads to the production of goods and services where value is distributed through the contributions of everyone involved. History Sharism was coined by Isaac Mao in the essay "Sharism: A Mind Revolution" which was originally published in the book Freesouls. Mao draws a comparison between the open distribution model of online information sharing and the neurological networks of the human brain. Following the analogy of an emerging Social Brain, Mao argues that the process of empowering people through sharing leads to collective ways of rethinking social relationships. Sharism has been particularly focused in China in order to promote the Open Web and combat Internet censorship. Notable proponents of sharism as both a term and practice have included Larry Lessig and Ou Ning. In 2010 during a Creative Commons lecture in Beijing, Lessig mentioned sharism in the context of openness and innovation in creative industries and intellectual property law in China. Also in 2010, Ou in his role as a curator choose sharism as the unifying theme for the Shanghai biennale exhibition "Get It Louder". In an interview about the exhibition, Ou discussed sharism at length and described it as an "Internet concept" that explores the increasingly convoluted relationship between public and private realms." Sharism Lab was created in 2012 with the purpose of providing experimental and theoretical background for a real-world implementation of sharism. Events and by-products Several types of sharism events have been created for people to meet and share things they like or things they make. Sharism Forum was held in October 2010 at the Get It Louder festival in Shanghai, and gathered international speakers, practitioners and activists to discuss the idea of sharism. Another event called "Sharism Presents" offers an informal setup for people to share whatever they want with the attending audience. Since 2010, Sharism Presents have been hosted in many cities throughout the world, included: Amsterdam, Shanghai, Beijing, Madrid, Barcelona, Brussels, Berlin, Montreal, Singapore, Hong Kong, Tokyo and Seoul. Sharism Workshops provide a framework for collective production through the act of sharing. Workshops have been held in Beijing, Doha and Warsaw and have included musicians, digital artists, and designers. In order to offer an easy way to share any kind of work online, the Sharing Agreement has been created in order to work around the increasing complexity of licenses. Criticism Many academics point to the downsides of such uncritical belief in the transformative power of technology. User generated content has been reframed as "Loser Generated Content", as the value of this sharing often ends up with companies, and not in the public domain. Within the art world, it has been suggested that there are "dangers of 'sharism", which "lead people to believe that whatever is contemporary must also be avantgarde." See also Sharing economy Creative Commons Free Software Knowledge sharing Commons-based peer production Copyleft Socialism References External links Video of Isaac Mao on the Concept of Sharism by Thomas Crampton Sharism interview in Digimag (Italian) with Robin Peckham and Isaac Mao A global nomad is a person who is living a mobile and international lifestyle. Global nomads aim to live location-independently, seeking detachment from particular geographical locations and the idea of territorial belonging. Origins and use of the term Nomad originally referred to pastoral nomads who follow their herd according to the seasons. Unlike traditional nomads, global nomads travel alone or in pairs rather than with a family and livestock. They also travel worldwide and via various routes, whereas traditional nomads have a fixed annual or seasonal pattern of movement. Although pastoralists are also professional travelers, they move relatively short distances, mostly walking or riding donkeys, horses, and camels. Air travel and the proliferation of information and communication technologies have afforded more opportunities for modern travelers and also engaged a wider range of people in itinerant lifestyles. In addition to location-independent travelers, the term has also been used for backpackers, lifestyle migrants and third culture kids (highly mobile youth and expatriate children) for highlighting the range and frequency of their travels. Lifestyle The global nomad lifestyle is characterized by high mobility. Global nomads travel from one country to another without a permanent home or job; their ties to their country of origin have also loosened. They might stay in any one location from a few days to several months, but at the end they will always move on. Many of them practice minimalism in order to support their frequent moving. Rather than money and possessions, they focus on experiences, happiness, self discovery and well-being. Many have location-independent vocations in fields such as IT, writing, teaching, and handicraft. See also Backpacking (travel) Davos Man Digital nomad Existential migration Mobilities Nomad Perpetual traveler Third culture kid == References == Aztec influence in Spain can be seen in both the cuisine of Spain and in its architecture. Food Guacamole, an avocado-based dip that was popular in Aztec cuisine as early as the 16th century, was brought back to Spain by the Conquistadors. Its reputation as an aphrodisiac derives from the words that combine to form the word ahuaca-molli ("guacamole" in the Aztec language): molli meant "something mashed or pureed into a sauce" and as well as meaning "avocado" ahuacatl meant "testicle". Architecture The stone work of sun pattern, snakes, panther and birds above the main entrance to the church of Nuestra Señora de Regla, Pájara, Fuerteventura in the Canary Islands, is thought by some specialists to show Aztec influence. See also Columbian exchange Nahuatl–Spanish contact == References == Cultural intelligence or cultural quotient (CQ), refers to an individual's capability to function effectively in culturally diverse settings. The concept was introduced by London Business School professor P. Christopher Earley and Nanyang Business School professor Soon Ang in 2003. While cultural intelligence is comparable to emotional intelligence (EQ), individuals with a high EQ can grasp "what makes us human and, at the same time, what makes each of us different from one another." In contrast, individuals with a high CQ can discern universal, individual, and non-idiosyncratic features within the behavior of a person or group. The authors cited cognitive, behavioral, motivational, and metacognitive (higher-level reflection) aspects of cultural intelligence. Four CQ capabilities The authors described four CQ capabilities: motivation (CQ Drive), cognition (CQ Knowledge), meta-cognition (CQ Strategy), and behavior (CQ Action). CQ Assessments report scores on all four capabilities as well as several sub-dimensions for each capability. Among the four capabilities, motivational CQ, or the interest and enjoyment in cross-cultural interactions, has been identified as a key resource or determinant that enhances personal functioning in cross-cultural environments, leading to improved intercultural adjustment and performance. See also Cosmopolitanism – Idea that all human beings are members of a single community Cultural anthropology – Branch of anthropology focused on the study of cultural variation among humans Intercultural communication – Discipline that studies communication across different cultures and social groups Intercultural competence – Set of behaviours or social skillsPages displaying short descriptions of redirect targets Intelligence cycle – Stages of intelligence information processing Multiculturalism – Existence of multiple cultural traditions within a single country Organizational culture – Customary behaviours in an organization References Further reading Earley, P. Christopher; Ang, S. (2003). Cultural intelligence: individual interactions across cultures. Stanford, Calif: Stanford University Press. ISBN 978-0-8047-4300-6. OCLC 51553576. Bhagat, Rabi S. (2006). "Review of Earley and Ang, Cultural Intelligence, and Hooker, Working Across Cultures". Academy of Management Review. 31 (2): 489–93. doi:10.5465/amr.2006.20208695. JSTOR 20159217. Ang, S. and Van Dyne L (eds). (2008) "The Handbook of Cultural Intelligence." New York: ME Sharpe ISBN 9780765622624 Livermore, David A. (2011). "The Cultural Intelligence Difference." New York: AMACOM ISBN 978-0814417065 Middleton, Julia (2014). "Cultural Intelligence: CQ: The Competitive Edge for Leaders Crossing Borders." London: A&C Black Business Information and Development ISBN 978-1472904812 The Nikkei Asia Prize (Japanese: 日経アジア賞) is an award which recognizes the achievements of people and organizations that have improved the lives of people throughout Asia. The awards were created and presented by Nikkei Inc, one of the largest media corporations in Japan. Launched in 1996, the program honors people in Asia who have made significant contributions in one of the three areas: regional growth; science, technology and innovation; and culture. The category for regional growth (Economic and Business Innovation) is designed to recognize business and economic initiatives that improve living standards and stability in their regions. This could be entrepreneurs who have successfully developed industries and businesses due to innovation. The category for science, technology and environment was established to recognize scientific researches and technological innovations in various fields. The category for Culture is designed to recognize people who have made a difference in their countries and Asia through cultural, artistic or educational activities. Winners Economic and Business Innovation (formerly Regional Growth) 1996: Dr. Widjojo Nitisastro 1997: Dr. Manmohan Singh 1998: Mr. Ni Runfeng 1999: Mr. Shi Wen-long 2000: Dr. Supachai Panitchpakdi 2001: Mr. N. R. Narayana Murthy 2002: Dr. Võ Tòng Xuân 2003: Mr. Lee Hun-jai 2004: Dr. Muhammad Yunus 2005: Dr. Morris Chang 2006: Ms. Olivia Lum 2007: Mr. Mechai Viravaidya 2008: Center of Legal Assistance to Pollution Victims in China at China University of Political Science and Law 2009: Ms. Kiran Mazumdar-Shaw 2010: Mr. Tony Fernandes 2011: Mr. Antonio Meloto 2012: Mr. Yang Yong 2013: Mr. Truong Gia Binh 2014: Dr. Devi Prasad Shetty 2015: Ms. Mai Kieu Lien 2016: Akshaya Patra Foundation 2017: Mr. Nandan Nilekani 2018: Mr. Ma Jun 2019: Mr. Nadiem Makarim Science, Technology and Environment 1996: Prof. Yuan Longping 1997: Dr. Hyung Sup Choi 1998: Rubber Research Institute of Malaysia (RRIM) 1999: Prof. Zhao Qiguo 2000: Institute of Molecular and Cell Biology (Singapore) 2001: Dr. Ho Wang Lee 2002: Department of Medical Microbiology, Faculty of Medicine, University of Malaya 2003: Dr. Yang Huanming 2004: Dr. Yongyuth Yuthavong 2005: Prof. Ko Myoung Sam 2006: Mr. Philip Yeo 2007: Mr. Chang Chun-yen 2008: Dr. C.N.R. Rao 2009: Forest Research Institute Malaysia (FRIM) 2010: Dr. Ding-Shinn Chen 2011: Dr. Maw-Kuen Wu 2012: Dr. Chi-Huey Wong 2013: Prof. Tejraj M. Aminabhavi 2014: Dr. George F. Gao 2015: Dr. Wang Yifang 2016: Dr. Jiang Lei 2017: Dr. Michael M. C. Lai 2018: Prof. Nguyen Thanh Liem 2019: Dr. Liao I-chiu 2020: Prof. Thalappil Pradeep Culture and Community 1996: Dara Kanlaya 1997: José Maceda 1998: Kim Jeong Ok 1999: Dang Nhat Minh 2000: Dr. Pinyo Suwankiri 2001: The Nepal Bhasa Dictionary Committee 2002: Christine Hakim 2003: Urvashi Butalia 2004: Albert Wendt 2005: Guo Dalie 2006: Sophiline Cheam Shapiro 2007: G. Venu 2008: Ahn Sung-ki 2009: Dr. Laretna T. Adishakti 2010: Manteb Soedharsono 2011: Bảo Ninh 2012: Sybil Wettasinghe 2013: Vann Molyvann 2014: Mae Fah Luang Foundation 2015: Asian Youth Orchestra 2016: Dogmid Sosorbaram 2017: Edhi Foundation 2018: Dr. Bindeshwar Pathak 2019: Cinemalaya Foundation Inc. See also List of awards for contributions to culture List of awards for contributions to society References External links and sources Nikkei Asia Prize Winners Garden tourism is a type of niche tourism involving visits to famous gardens and botanical gardens and places which are significant in the history of gardening. Garden tourists often travel individually in countries with which they are familiar but often prefer to join organized garden tours in countries where they might experience difficulties with language, travel or finding accommodation in the vicinity of the garden. In the year 2000 the Alhambra and the Taj Mahal both received over two million visitors. This poses problems for the landscape manager. Probably the oldest traditions of garden tourism are those of China and Japan. In both countries some temples had famous gardens, and in China a private garden could be visited for a small charge by the 11th century. In India, many Mughal gardens around tombs and mosques could be visited, and throughout the Islamic world some gardens were in effect public parks, open to the public, while others remained strictly private. In Early Modern Europe it was generally possible for the public, or at least those respectably dressed, to see large parts of royal palace gardens, at least some of the time, while other areas were a "privy garden" with tightly restricted access. At the same time botanic gardens were being founded, which had being a visitor attraction as an important part of their function. By the 18th century, the English "garden tour" of large country house gardens was well-established, with guide-books and maps of the garden, and special inns. Many tourist visits are to gardens, as part of a broader itinerary or a one-off trip, but the amount of tourism dedicated to seeing a series of gardens is much smaller. Garden tourism of this sort remains a niche commercial enterprise. Throughout the world, there are a limited number of boutique tour operators offering guided tours to the public. History Reliable access is necessary for garden tourism. In China, from the Song dynasty onwards, famous private gardens such as the Classical Gardens of Suzhou seem often to have been opened to the public, either for "festivals and holidays", or at certain seasons, or some combination of the two. Payment was sometimes required, if only to keep numbers manageable. The situation with the many enormous Imperial gardens is less clear, but many visitors managed to enter them and record their impressions. There was probably less access to private Japanese gardens, which tend to run right up to parts of the house, and when first created they seem to have been mostly restricted to visitors from the elite, guests of the owner. But gardens were certainly made to be shown off to visitors; the "stroll garden" with a set walk around it, and recommended stopping points to appreciate the view, is a Japanese feature that later appears in the West. The best Japanese gardens have been continuously and carefully maintained, and probably look more like they did several centuries ago than their equivalents anywhere else in the world. In China large numbers of famous gardens have been restored, probably rather differently from their original appearance. But in India very few older gardens have kept their original appearance. In Europe the gardens of royal palaces seem to have very often had areas accessible to a wide public, if smartly dressed, while other areas were only accessible to courtiers, or just the royal family and those they invited. Access was likely to be easier when the family were not in residence. The grandest private houses, especially in the country, had similar arrangements; in both cases the owners saw their gardens as expressions of their own status and prestige. The owners of great country houses, especially those distant from the capital, often only visited them for brief periods, typically in the summer, when access might be more restricted. Garden tours and literature Michel de Montaigne was one of the earliest garden tourists to record his impressions of gardens (c1580). John Evelyn also recorded his visits to gardens in France and Italy, as did Fynes Moryson. Maggie Campbell-Culver wrote a biography of John Evelyn as she sourced from woods and gardens Evelyn took steps in, and described trees from oak as an Evelyn's symbol to evergreens he favored the most. England and Wales: schemes for charity Initially, the garden tour in England and Wales involves private gardens and gardens that does not accept visitors regularly under the National Gardens Scheme, when "Gardens of England and Wales Open for Charity" (the 'Yellow Book') served as a guide book for those seeking to visit gardens in England and Wales. The first issue of the Yellow Book was published as a supplement to a British magazine "Country Life" in 1931, after Elsie Wagg of an institution serving for district nursing came up with the basic idea of National Gardens Scheme, in which a charity and garden tour was combined when gardening was quite popular in the UK. The movement to open gardens for charity spread to private gardens when it was announced in 1927, and owners of such gardens agreed to collect 1 Shiring fee from each visitors that they donated to the charity. 609 such gardens raised £8,000 and in 1928 the institution renames to The Queen's Institute of District Nursing ("The Queen's Nursing Institute" of later day). With the publication of the first Yellow Book, there were 1,000 gardens to participate in the Scheme, and in 2015 they have donated £4.5 million since 1927. Those owners of private gardens sometimes donated to those charities they choose, amounting to £40,000. As the garden tour expanded since 1948 when the National Gardens Scheme involved the National Trust: while National Trust offered important gardens for garden tours which they have restored and conserved, and number of visitors increased. The Queen's Institute of District Nursing offered them funds which in turn encouraged the Trust to work on additional garden projects. It was in 2013 when the Yellow Page was officially renamed as "Gardens To Visit". At the start of the 21st century, with a history of over 100 years of garden tours, Britain had the largest number of gardens open to the public for tourist visits: in 2013, 3,700 gardens are listed in Gardens of England and Wales Open for Charity, when the Yellow 3,500 gardens are listed in Gardens of England and Wales Open for Charity. Some much-visited gardens This is a very incomplete list of some representative famous gardens which attract garden tourists from afar: Floriade in Australia Sissinghurst Castle Garden and Stourhead in England Versailles, Giverny, Villandry, Rivau in France Keukenhof in the Netherlands Mainau Island in Germany Villa d'Este and Villa Lante in Italy Alhambra in Spain Longwood Gardens and Filoli in the US Taj Mahal in India Ryōan-ji in Japan See also Garden Notes References Sources Batey, Mavis and Lambert, David, The English Garden Tour: A View Into the Past, 1990, John Murray. ISBN 978-0719547751 "Oxford": The Oxford Companion to Gardens, eds. Geoffrey Jellicoe, Susan Jellicoe, Patrick Goode and Michael Lancaster, 1986, OUP, ISBN 0192861387 Identified patient (IP) is a clinical term often used in family therapy discussion. It describes one family member in a dysfunctional family who is used as an expression of the family's authentic inner conflicts. As a family system is dynamic, the overt symptoms of an identified patient draw attention away from the "elephants in the living room" no one can talk about which need to be discussed. If covert abuse occurs between family members, the overt symptoms can draw attention away from the perpetrators. The identified patient is a kind of diversion and a kind of scapegoat. Often a child, this is "the split-off false carrier of a breakdown in the entire family system," which may be a transgenerational disturbance or trauma. While the concept has evolved beyond Jung's original interpretation, some modern authors still use the term to describe a family member who is blamed for the family's problems, even though all members experience mental illness, rather than the one officially labeled as mentally ill – positing that the IP may actually be the least troubled member of a dysfunctional family nexus. Origins and characteristics The term emerged from the work of the Bateson Project on family homeostasis, as a way of identifying a largely unconscious pattern of behavior whereby an excess of painful feelings in a family lead to one member being identified as the cause of all the difficulties – a scapegoating of the IP. The identified patient – also called the "symptom-bearer" or "presenting problem" – may display unexplainable emotional or physical symptoms, and is often the first person to seek help, perhaps at the request of the family. However, while family members will typically express concern over the IP's problems, they may instinctively react to any improvement on the identified patient's part by attempting to reinstate the status quo. Virginia Satir, the wellspring of family systems theory, who knew Bateson, viewed the identified patient as a way of both concealing and revealing a family's secret agendas. Conjoint family therapy stressed accordingly the importance in group therapy of bringing not only the identified patient but the extended family in which their problems arose into the therapy – with the ultimate goal of relieving the IP of the broader family feelings they have been carrying. In such circumstances, not only the IP but their siblings as well may end up feeling the benefits. R. D. Laing saw the IP as a function of the family nexus: "the person who gets diagnosed is part of a wider network of extremely disturbed and disturbing patterns of communication." Later formulations suggest that the patient may be an "emissary" of sorts from the family to the wider world, in an implicit familial call for help, as with the reading of juvenile delinquency as a coded cry for help by a child on their parents' behalf. There may then be an element of altruism in the IP's behavior – 'playing' sick to prevent worse things happening in the family, such as a total family breakdown. Examples In a family where the parents need to assert themselves as powerful figures and caretakers, often due to their own insecurities, they may designate one or more of their children as being inadequate, unconsciously assigning to the child the role of someone who cannot cope by themselves. For example, the child may exhibit some irrational problem requiring the constant care and attention of the parents. In Dibs in Search of Self, an account of a child therapy, Virginia Axline considered that perhaps the parents, "quite unconsciously...chose to see Dibs as a mental defective rather than as an intensified personification of their own emotional and social inadequacy". Gregory Bateson considered sometimes "the identified patient sacrifices himself to maintain the sacred illusion that what the parent says makes sense", and that "the identified patient exhibits behavior which is almost a caricature of that loss of identity which is characteristic of all the family members". Literary and biographical In the play The Family Reunion, T. S. Eliot writes of the protagonist: "It is possible You are the consciousness of your unhappy family, Its bird sent flying through the purgatorial flame". Carl Jung, who viewed individual neurosis as often deriving from whole family or social groups, considered himself a case in point: "I feel very strongly I am under the influence of things or questions left incomplete and unanswered by my parents and grandparents and more distant ancestors...an impersonal karma within a family, which is passed on from parents to children". The term is also used in analyzing dysfunction in businesses where an individual becomes the carrier of a group problem. See also References Further reading Patterson, JoEllen (1998). Essential skills in family therapy: from the first interview to termination. The Guilford Press. ISBN 1-57230-307-7 In lawn care, thatch is a layer of organic matter that accumulates on a lawn around the base of the grass plants. Thatch is a combination of living and dead plant matter including crowns, stolons, rhizomes, and roots. Grass clippings do not generally contribute to thatch buildup as they can be easily broken down by soil microorganisms. Thatch is composed of about 25% lignin, a complex organic polymer that is highly resistant to decomposition. Thatch buildup can be caused by several factors: Certain grass species are especially prone to thatch production Acidic soils may not be able to support sufficient populations of decomposing microorganisms Certain fungicides can stimulate excessive root and rhizome growth Application of insecticides may reduce earthworm activity, leading to decreased bioturbation Over-application of nitrogen fertilizers can stimulate excess growth as well as contribute to soil acidity A small amount of thatch may provide a beneficial insulating effect against fluctuations in temperature and moisture. However, excessive thatch can cause root problems and lawn mower difficulties. A dethatcher may be used to remove thatch from a lawn. See also Plant litter – Dead plant material that has fallen to the ground References Landschoot, Peter (January 9, 2023). Managing Thatch in Lawns (Report). Penn State College of Agricultural Sciences. p. 1. Retrieved January 30, 2024. External links NDIS Lawn Mowing Professional Lawn Care Ark's Landscaping Obsessive–compulsive disorder (OCD) is a mental disorder in which an individual has intrusive thoughts (an obsession) and feels the need to perform certain routines (compulsions) repeatedly to relieve the distress caused by the obsession, to the extent where it impairs general function. Obsessions are persistent unwanted thoughts, mental images, or urges that generate feelings of anxiety, disgust, or discomfort. Some common obsessions include fear of contamination, obsession with symmetry, the fear of acting blasphemously, sexual obsessions, and the fear of possibly harming others or themselves. Compulsions are repeated actions or routines that occur in response to obsessions to achieve a relief from anxiety. Common compulsions include excessive hand washing, cleaning, counting, ordering, repeating, avoiding triggers, hoarding, neutralizing, seeking assurance, praying, and checking things. OCD can also manifest exclusively through mental compulsions, such as mental avoidance and excessive rumination. This manifestation is sometimes referred to as primarily obsessional obsessive–compulsive disorder. Compulsions occur often and typically take up at least one hour per day, impairing one's quality of life. Compulsions cause relief in the moment, but cause obsessions to grow over time due to the repeated reward-seeking behavior of completing the ritual for relief. Many adults with OCD are aware that their compulsions do not make sense, but they still perform them to relieve the distress caused by obsessions. For this reason, thoughts and behaviors in OCD are usually considered egodystonic (inconsistent with one's ideal self-image). In contrast, thoughts and behaviors in obsessive–compulsive personality disorder (OCPD) are usually considered egosyntonic (consistent with one's ideal self-image), helping differentiate between OCPD and OCD. Although the exact cause of OCD is unknown, several regions of the brain have been implicated in its neuroanatomical model including the anterior cingulate cortex, orbitofrontal cortex, amygdala, and BNST. The presence of a genetic component is evidenced by the increased likelihood for both identical twins to be affected than both fraternal twins. Risk factors include a history of child abuse or other stress-inducing events such as during the postpartum period or after streptococcal infections. Diagnosis is based on clinical presentation and requires ruling out other drug-related or medical causes; rating scales such as the Yale–Brown Obsessive–Compulsive Scale (Y-BOCS) assess severity. Other disorders with similar symptoms include generalized anxiety disorder, major depressive disorder, eating disorders, tic disorders, body-focused repetitive behavior, and obsessive–compulsive personality disorder. Personality disorders are a common comorbidity, with schizotypal and OCPD having poor treatment response. The condition is also associated with a general increase in suicidality. The phrase obsessive–compulsive is sometimes used in an informal manner unrelated to OCD to describe someone as excessively meticulous, perfectionistic, absorbed, or otherwise fixated. However, the actual disorder can vary in presentation and individuals with OCD may not be concerned with cleanliness or symmetry. OCD is chronic and long-lasting with periods of severe symptoms followed by periods of improvement. Treatment can improve ability to function and quality of life, and is usually reflected by improved Y-BOCS scores. Treatment for OCD may involve psychotherapy, pharmacotherapy such as antidepressants or surgical procedures such as deep brain stimulation or, in extreme cases, psychosurgery. Psychotherapies derived from cognitive behavioral therapy (CBT) models, such as exposure and response prevention, acceptance and commitment therapy, and inference based-therapy, are more effective than non-CBT interventions. Selective serotonin reuptake inhibitors (SSRIs) are more effective when used in excess of the recommended depression dosage; however, higher doses can increase side effect intensity. Commonly used SSRIs include sertraline, fluoxetine, fluvoxamine, paroxetine, citalopram, and escitalopram. Some patients fail to improve after taking the maximum tolerated dose of multiple SSRIs for at least two months; these cases qualify as treatment-resistant and can require second-line treatment such as clomipramine or atypical antipsychotic augmentation. While SSRIs continue to be first-line, recent data for treatment-resistant OCD supports adjunctive use of neuroleptic medications, deep brain stimulation and neurosurgical ablation. There is growing evidence to support the use of deep brain stimulation and repetitive transcranial magnetic stimulation for treatment-resistant OCD. Signs and symptoms OCD can present with a wide variety of symptoms. Certain groups of symptoms usually occur together as dimensions or clusters, which may reflect an underlying process. The standard assessment tool for OCD, the Yale–Brown Obsessive–Compulsive Scale (Y-BOCS), has 13 predefined categories of symptoms. These symptoms fit into three to five groupings. A meta-analytic review of symptom structures found a four-factor grouping structure to be most reliable: symmetry factor, forbidden thoughts factor, cleaning factor and hoarding factor. The symmetry factor correlates highly with obsessions related to ordering, counting and symmetry, as well as repeating compulsions. The forbidden thoughts factor correlates highly with intrusive thoughts of a violent, religious or sexual nature. The cleaning factor correlates highly with obsessions about contamination and compulsions related to cleaning. The hoarding factor only involves hoarding-related obsessions and compulsions, and was identified as being distinct from other symptom groupings. When looking into the onset of OCD, one study suggests that there are differences in the age of onset between males and females, with the average age of onset of OCD being 9.6 for male children and 11.0 for female children. Children with OCD often have other mental disorders, such as ADHD, depression, anxiety and disruptive behavior disorder. Continually, children are more likely to struggle in school and experience difficulties in social situations (Lack 2012). When looking at both adults and children a study found the average ages of onset to be 21 and 24 for males and females respectively. While some studies have shown that OCD with earlier onset is associated with greater severity, other studies have not been able to validate this finding. Looking at women specifically, a different study suggested that 62% of participants found that their symptoms worsened at a premenstrual age. Across the board, all demographics and studies showed a mean age of onset of less than 25. Some OCD subtypes have been associated with improvement in performance on certain tasks, such as pattern recognition (washing subtype) and spatial working memory (obsessive thought subtype). Subgroups have also been distinguished by neuroimaging findings and treatment response, though neuroimaging studies have not been comprehensive enough to draw conclusions. Subtype-dependent treatment response has been studied and the hoarding subtype has consistently been least responsive to treatment. While OCD is considered a homogeneous disorder from a neuropsychological perspective, many of the symptoms may be the result of comorbid disorders. For example, adults with OCD have exhibited more symptoms of attention deficit hyperactivity disorder (ADHD) and autism spectrum disorder (ASD) than adults without OCD. In regards to the cause of onset, researchers asked participants in one study what they felt was responsible for triggering the initial onset of their illness. 29% of patients answered that there was an environmental factor in their life that did so. Specifically, the majority of participants who answered with that noted their environmental factor to be related to an increased responsibility. Obsessions Obsessions are stress-inducing thoughts that recur and persist, despite efforts to ignore or confront them. People with OCD frequently perform tasks, or compulsions, to seek relief from obsession-related anxiety. Within and among individuals, initial obsessions vary in clarity and vividness. A relatively vague obsession could involve a general sense of disarray or tension, accompanied by a belief that life cannot proceed as normal while the imbalance remains. A more intense obsession could be a preoccupation with the thought or image of a close family member or friend dying, or intrusive thoughts related to relationship rightness. Other obsessions concern the possibility that someone or something other than oneself—such as God, the devil or disease—will harm either the patient or the people or things the patient cares about. Others with OCD may experience the sensation of invisible protrusions emanating from their bodies or feel that inanimate objects are ensouled. Another common obsession is scrupulosity, the pathological guilt/anxiety about moral or religious issues. In scrupulosity, a person's obsessions focus on moral or religious fears, such as the fear of being an evil person or the fear of divine retribution for sin. Mysophobia, a pathological fear of contamination and germs, is another common obsession theme. Some people with OCD experience sexual obsessions that may involve intrusive thoughts or images of "kissing, touching, fondling, oral sex, anal sex, intercourse, incest and rape" with "strangers, acquaintances, parents, children, family members, friends, coworkers, animals and religious figures" and can include heterosexual or homosexual contact with people of any age. Similar to other intrusive thoughts or images, some disquieting sexual thoughts are normal at times, but people with OCD may attach extraordinary significance to such thoughts. For example, obsessive fears about sexual orientation can appear to the affected individual, and even to those around them, as a crisis of sexual identity. Furthermore, the doubt that accompanies OCD leads to uncertainty regarding whether one might act on the troubling thoughts, resulting in self-criticism or self-loathing. Most people with OCD understand that their thoughts do not correspond with reality; however, they feel that they must act as though these ideas are correct or realistic. For example, someone who engages in compulsive hoarding might be inclined to treat inorganic matter as if it had the sentience or rights of living organisms, despite accepting that such behavior is irrational on an intellectual level. There is debate as to whether hoarding should be considered an independent syndrome from OCD. Compulsions Some people with OCD perform compulsive rituals because they inexplicably feel that they must do so, while others act compulsively to mitigate the anxiety that stems from obsessive thoughts. The affected individual might feel that these actions will either prevent a dreaded event from occurring or push the event from their thoughts. In any case, their reasoning is so idiosyncratic or distorted that it results in significant distress, either personally or for those around the affected individual. Excessive skin picking, hair pulling, nail biting and other body-focused repetitive behavior disorders are all on the obsessive–compulsive spectrum. Some individuals with OCD are aware that their behaviors are not rational, but they feel compelled to follow through with them to fend off feelings of panic or dread. Furthermore, compulsions often stem from memory distrust, a symptom of OCD characterized by insecurity in one's skills in perception, attention and memory, even in cases where there is no clear evidence of a deficit. Common compulsions may include hand washing, cleaning, checking things (such as locks on doors), repeating actions (such as repeatedly turning on and off switches), ordering items in a certain way and requesting reassurance. Although some individuals perform actions repeatedly, they do not necessarily perform these actions compulsively; for example, morning or nighttime routines and religious practices are not usually compulsions. Whether behaviors qualify as compulsions or mere habit depends on the context in which they are performed. For instance, arranging and ordering books for eight hours a day would be expected of someone who works in a library, but this routine would seem abnormal in other situations. In other words, habits tend to bring efficiency to one's life, while compulsions tend to disrupt it. Furthermore, compulsions are different from tics (such as touching, tapping, rubbing or blinking) and stereotyped movements (such as head banging, body rocking or self-biting), which are usually not as complex and not precipitated by obsessions. It can sometimes be difficult to tell the difference between compulsions and complex tics, and about 10–40% of people with OCD also have a lifetime tic disorder. People with OCD rely on compulsions as an escape from their obsessive thoughts; however, they are aware that relief is only temporary and that intrusive thoughts will return. Some affected individuals use compulsions to avoid situations that may trigger obsessions. Compulsions may be actions directly related to the obsession, such as someone obsessed with contamination compulsively washing their hands, but they can be unrelated as well. In addition to experiencing the anxiety and fear that typically accompanies OCD, affected individuals may spend hours performing compulsions every day. In such situations, it can become difficult for the person to fulfill their work, familial or social roles. These behaviors can also cause adverse physical symptoms; for example, people who obsessively wash their hands with antibacterial soap and hot water can make their skin red and raw with dermatitis. Individuals with OCD often use rationalizations to explain their behavior; however, these rationalizations do not apply to the behavioral pattern, but to each individual occurrence. For example, someone compulsively checking the front door may argue that the time and stress associated with one check is less than the time and stress associated with being robbed, and checking is consequently the better option. This reasoning often occurs in a cyclical manner and can continue for as long as the affected person needs it to in order to feel safe. OCD sometimes manifests in mental instead of overt compulsions. This manifestation may be termed "primarily obsessional OCD" and typically involves mental compulsions, such as mental avoidance or excessive rumination. OCD without overt compulsions could, by one estimate, characterize as many as 50–60% of OCD cases. Insight and overvalued ideation The Diagnostic and Statistical Manual of Mental Disorders (DSM-5), identifies a continuum for the level of insight in OCD, ranging from good insight (the least severe) to no insight (the most severe). Good or fair insight is characterized by the acknowledgment that obsessive–compulsive beliefs are not or may not be true, while poor insight is characterized by the belief that obsessive–compulsive beliefs are probably true. The absence of insight altogether, in which the individual is completely convinced that their beliefs are true, is also identified as a delusional thought pattern and occurs in about 4% of people with OCD. When cases of OCD with no insight become severe, affected individuals have an unshakable belief in the reality of their delusions, which can make their cases difficult to differentiate from psychotic disorders. Some people with OCD exhibit what is known as overvalued ideas, ideas that are abnormal compared to affected individuals' respective cultures, and more treatment-resistant than most negative thoughts and obsessions. After some discussion, it is possible to convince the individual that their fears are unfounded. It may be more difficult to practice exposure and response prevention therapy (ERP) on such people, as they may be unwilling to cooperate, at least initially. Similar to how insight is identified on a continuum, obsessive-compulsive beliefs are characterized on a spectrum, ranging from obsessive doubt to delusional conviction. In the United States, overvalued ideation (OVI) is considered most akin to poor insight—especially when considering belief strength as one of an idea's key identifiers. Furthermore, severe and frequent overvalued ideas are considered similar to idealized values, which are so rigidly held by, and so important to affected individuals, that they end up becoming a defining identity. In adolescent OCD patients, OVI is considered a severe symptom. Historically, OVI has been thought to be linked to poorer treatment outcome in patients with OCD, but it is currently considered a poor indicator of prognosis. The Overvalued Ideas Scale (OVIS) has been developed as a reliable quantitative method of measuring levels of OVI in patients with OCD. Research has suggested that overvalued ideas are more stable for those with more extreme OVIS scores. Cognitive performance Though OCD was once believed to be associated with above-average intelligence, this does not appear to necessarily be the case. A 2013 review reported that people with OCD may sometimes have mild but wide-ranging cognitive deficits, most significantly those affecting spatial memory and to a lesser extent with verbal memory, fluency, executive function and processing speed, while auditory attention was not significantly affected. People with OCD show impairment in formulating an organizational strategy for coding information, set-shifting, and motor and cognitive inhibition. Specific subtypes of symptom dimensions in OCD have been associated with specific cognitive deficits. For example, the results of one meta-analysis comparing washing and checking symptoms reported that washers outperformed checkers on eight out of ten cognitive tests. The symptom dimension of contamination and cleaning may be associated with higher scores on tests of inhibition and verbal memory. Video game addiction Pediatric OCD Approximately 1–2% of children are affected by OCD. The clinical presentation of OCD in children shares many similarities with that observed in adults. OCD is considered a highly familial disorder, with a phenotypic heritability of around 50%. Symptoms tend to develop more frequently in children 10–14 years of age, with males displaying symptoms at an earlier age, and at a more severe level than females. In children, symptoms can be grouped into at least four types, including sporadic and tic-related OCD. The Children's Yale–Brown Obsessive–Compulsive Scale (CY-BOCS) is the gold standard measure for assessment of pediatric OCD. It follows the Y-BOCS format, but with a Symptom Checklist that is adapted for developmental appropriateness. Insight, avoidance, indecisiveness, responsibility, pervasive slowness and doubting are not included in a rating of overall severity. The CY-BOCS has demonstrated good convergent validity with clinician-rated OCD severity and good to fair discriminant validity from measures of closely related anxiety, depression and tic severity. The CY-BOCS Total Severity score is an important monitoring tool as it is responsive to pharmacotherapy and psychotherapy. Positive treatment response is characterized by 25% reduction in CY-BOCS total score and diagnostic remission is associated with a 45%-50% reduction in Total Severity score (or a score <15). CBT is the first line treatment for mild to moderate cases of OCD in children, while medication plus CBT is recommended for moderate to severe cases. Selective serotonin reuptake inhibitors (SSRIs) are first-line medications for OCD in children with established AACAP guidelines for dosing. Medication in addition to a CBT intervention like exposure and response prevention (ERP) is more beneficial than only using medication in the treatment of OCD in children. Associated conditions People with OCD may be diagnosed with other conditions as well, such as obsessive–compulsive personality disorder, major depressive disorder, bipolar disorder, generalized anxiety disorder, anorexia nervosa, social anxiety disorder, bulimia nervosa, Tourette syndrome, transformation obsession, ASD, ADHD, dermatillomania, body dysmorphic disorder and trichotillomania. More than 50% of people with OCD experience suicidal tendencies and 15% have attempted suicide. Depression, anxiety and prior suicide attempts increase the risk of future suicide attempts. It has been found that between 18 and 34% of females currently experiencing OCD scored positively on an inventory measuring disordered eating. Another study found that 7% are likely to have an eating disorder, while another found that fewer than 5% of males have OCD and an eating disorder. Individuals with OCD have also been found to be affected by delayed sleep phase disorder at a substantially higher rate than the general public. Moreover, severe OCD symptoms are consistently associated with greater sleep disturbance. Reduced total sleep time and sleep efficiency have been observed in people with OCD, with delayed sleep onset and offset. Some research has demonstrated a link between drug addiction and OCD. For example, there is a higher risk of drug addiction among those with any anxiety disorder, likely as a way of coping with the heightened levels of anxiety. However, drug addiction among people with OCD may be a compulsive behavior. Depression is also extremely prevalent among people with OCD. One explanation for the high depression rate among OCD populations was posited by Mineka, Watson and Clark (1998), who explained that people with OCD, or any other anxiety disorder, may feel "out of control". Someone exhibiting OCD signs does not necessarily have OCD. Behaviors that present as obsessive–compulsive can also be found in a number of other conditions, including obsessive–compulsive personality disorder (OCPD), autism spectrum disorder (ASD) or disorders in which perseveration is a possible feature (ADHD, PTSD, bodily disorders or stereotyped behaviors). Some cases of OCD present symptoms typically associated with Tourette syndrome, such as compulsions that may appear to resemble motor tics; this has been termed tic-related OCD or Tourettic OCD. OCD frequently occurs comorbidly with both bipolar disorder and major depressive disorder. Between 60 and 80% of those with OCD experience a major depressive episode in their lifetime. Comorbidity rates have been reported at between 19 and 90%, as a result of methodological differences. Between 9–35% of those with bipolar disorder also have OCD, compared to 1–2% in the general population. About 50% of those with OCD experience cyclothymic traits or hypomanic episodes. OCD is also associated with anxiety disorders. Lifetime comorbidity for OCD has been reported at 22% for specific phobia, 18% for social anxiety disorder, 12% for panic disorder and 30% for generalized anxiety disorder. The comorbidity rate for OCD and ADHD has been reported to be as high as 51%. Pedophilia-themed OCD Pedophilia-themed obsessive–compulsive disorder (also known as pedophile OCD or P-OCD) is an OCD subtype regarding reocurring compulsions and obsessions over one being a pedophile. Causes The cause of OCD is unknown. Both environmental and genetic factors are believed to play a role. Risk factors include a history of adverse childhood experiences or other stress-inducing events. Drug-induced OCD Some medications, toxin exposures and drugs, such as methamphetamine or cocaine, can induce obsessive–compulsive symptoms in people without a history of OCD. Atypical antipsychotics such as olanzapine and clozapine can induce OCD in some people, particularly individuals with schizophrenia. The diagnostic criteria include: General OCD symptoms (obsessions, compulsions, skin picking, hair pulling, etc.) that developed soon after exposure to the substance or medication which can produce such symptoms. The onset of symptoms cannot be explained by an obsessive–compulsive and related disorder that is not substance/medication-induced and should last for a substantial period of time (about 1 month) This disturbance does not only occur during delirium. Clinically induces distress or impairment in social, occupational or other important areas of functioning. Genetics There appear to be some genetic components of OCD causation, with identical twins more often affected than fraternal twins. Furthermore, individuals with OCD are more likely to have first-degree family members exhibiting the same disorders than matched controls. In cases in which OCD develops during childhood, there is a much stronger familial link in the disorder than with cases in which OCD develops later in adulthood. In general, genetic factors account for 45–65% of the variability in OCD symptoms in children diagnosed with the disorder. A 2007 study found evidence supporting the possibility of a heritable risk for OCD. OCD is believed to be a heterogeneous disorder. Research has found there to be a genetic correlation between anorexia nervosa and OCD, suggesting a strong etiology. First and second hand relatives of probands with OCD have a greater risk of developing anorexia nervosa as genetic relatedness increases. A mutation has been found in the human serotonin transporter gene hSERT in unrelated families with OCD. A systematic review found that while neither allele was associated with OCD overall, in Caucasians, the L allele was associated with OCD. Another meta-analysis observed an increased risk in those with the homozygous S allele, but found the LS genotype to be inversely associated with OCD. A genome-wide association study found OCD to be linked with single-nucleotide polymorphisms (SNPs) near BTBD3 and two SNPs in DLGAP1 in a trio-based analysis, but no SNP reached significance when analyzed with case-control data. One meta-analysis found a small but significant association between a polymorphism in SLC1A1 and OCD. The relationship between OCD and Catechol-O-methyltransferase (COMT) has been inconsistent, with one meta-analysis reporting a significant association, albeit only in men, and another meta analysis reporting no association. It has been postulated by evolutionary psychologists that moderate versions of compulsive behavior may have had evolutionary advantages. Examples would be moderate constant checking of hygiene, the hearth or the environment for enemies. Similarly, hoarding may have had evolutionary advantages. In this view, OCD may be the extreme statistical tail of such behaviors, possibly the result of a high number of predisposing genes. Brain structure and functioning Imaging studies have shown differences in the frontal cortex and subcortical structures of the brain in patients with OCD. There appears to be a connection between the OCD symptoms and abnormalities in certain areas of the brain, but such a connection is not clear. Some people with OCD have areas of unusually high activity in their brain or low levels of the chemical serotonin, which is a neurotransmitter that some nerve cells use to communicate with each other, and is thought to be involved in regulating many functions, influencing emotions, mood, memory and sleep. Autoimmune A controversial hypothesis is that some cases of rapid onset of OCD in children and adolescents may be caused by a syndrome connected to Group A streptococcal infections (GABHS), known as pediatric autoimmune neuropsychiatric disorders associated with streptococcal infections (PANDAS). OCD and tic disorders are hypothesized to arise in a subset of children as a result of a post-streptococcal autoimmune process. The PANDAS hypothesis is unconfirmed and unsupported by data and two new categories have been proposed: PANS (pediatric acute-onset neuropsychiatric syndrome) and CANS (childhood acute neuropsychiatric syndrome). The CANS and PANS hypotheses include different possible mechanisms underlying acute-onset neuropsychiatric conditions, but do not exclude GABHS infections as a cause in a subset of individuals. PANDAS, PANS and CANS are the focus of clinical and laboratory research, but remain unproven. Whether PANDAS is a distinct entity differing from other cases of tic disorders or OCD is debated. A review of studies examining anti-basal ganglia antibodies in OCD found an increased risk of having anti-basal ganglia antibodies in those with OCD versus the general population. Environment OCD may be more common in people who have been bullied, abused or neglected, and it sometimes starts after a significant life event, such as childbirth or bereavement. It has been reported in some studies that there is a connection between childhood trauma and obsessive-compulsive symptoms. More research is needed to understand this relationship better. Mechanisms Neuroimaging Functional neuroimaging during symptom provocation has observed abnormal activity in the orbitofrontal cortex (OFC), left dorsolateral prefrontal cortex (dlPFC), right premotor cortex, left superior temporal gyrus, globus pallidus externus, hippocampus and right uncus. Weaker foci of abnormal activity were found in the left caudate, posterior cingulate cortex and superior parietal lobule. However, an older meta-analysis of functional neuroimaging in OCD reported that the only consistent functional neuroimaging finding was increased activity in the orbital gyrus and head of the caudate nucleus, while anterior cingulate cortex (ACC) activation abnormalities were too inconsistent. A meta-analysis comparing affective and nonaffective tasks observed differences with controls in regions implicated in salience, habit, goal-directed behavior, self-referential thinking and cognitive control. For nonaffective tasks, hyperactivity was observed in the insula, ACC and head of the caudate/putamen, while hypoactivity was observed in the medial prefrontal cortex (mPFC) and posterior caudate. Affective tasks were observed to relate to increased activation in the precuneus and posterior cingulate cortex, while decreased activation was found in the pallidum, ventral anterior thalamus and posterior caudate. The involvement of the cortico-striato-thalamo-cortical loop in OCD, as well as the high rates of comorbidity between OCD and ADHD, have led some to draw a link in their mechanism. Observed similarities include dysfunction of the anterior cingulate cortex and prefrontal cortex, as well as shared deficits in executive functions. The involvement of the orbitofrontal cortex and dorsolateral prefrontal cortex in OCD is shared with bipolar disorder and may explain the high degree of comorbidity. Decreased volumes of the dorsolateral prefrontal cortex related to executive function has also been observed in OCD. People with OCD evince increased grey matter volumes in bilateral lenticular nuclei, extending to the caudate nuclei, with decreased grey matter volumes in bilateral dorsal medial frontal/anterior cingulate gyri. These findings contrast with those in people with other anxiety disorders, who evince decreased (rather than increased) grey matter volumes in bilateral lenticular/caudate nuclei, as well as decreased grey matter volumes in bilateral dorsal medial frontal/anterior cingulate gyri. Increased white matter volume and decreased fractional anisotropy in anterior midline tracts has been observed in OCD, possibly indicating increased fiber crossings. Cognitive models Generally, two categories of models for OCD have been postulated. The first category involves deficits in executive dysfunction and is based on the observed structural and functional abnormalities in the dlPFC, striatum and thalamus. The second category involves dysfunctional modulatory control and primarily relies on observed functional and structural differences in the ACC, mPFC and OFC. One proposed model suggests that dysfunction in the orbitalfrontal cortex (OFC) leads to improper valuation of behaviors and decreased behavioral control, while the observed alterations in amygdala activations leads to exaggerated fears and representations of negative stimuli. Due to the heterogeneity of OCD symptoms, studies differentiating various symptoms have been performed. Symptom-specific neuroimaging abnormalities include the hyperactivity of caudate and ACC in checking rituals, while finding increased activity of cortical and cerebellar regions in contamination-related symptoms. Neuroimaging differentiating content of intrusive thoughts has found differences between aggressive as opposed to taboo thoughts, finding increased connectivity of the amygdala, ventral striatum and ventromedial prefrontal cortex in aggressive symptoms, while observing increased connectivity between the ventral striatum and insula in sexual or religious intrusive thoughts. Another model proposes that affective dysregulation links excessive reliance on habit-based action selection with compulsions. This is supported by the observation that those with OCD demonstrate decreased activation of the ventral striatum when anticipating monetary reward, as well as increased functional connectivity between the VS and the OFC. Furthermore, those with OCD demonstrate reduced performance in Pavlovian fear-extinction tasks, hyperresponsiveness in the amygdala to fearful stimuli and hyporesponsiveness in the amygdala when exposed to positively valanced stimuli. Stimulation of the nucleus accumbens has also been observed to effectively alleviate both obsessions and compulsions, supporting the role of affective dysregulation in generating both. Neurobiological From the observation of the efficacy of antidepressants in OCD, a serotonin hypothesis of OCD has been formulated. Studies of peripheral markers of serotonin, as well as challenges with proserotonergic compounds have yielded inconsistent results, including evidence pointing towards basal hyperactivity of serotonergic systems. Serotonin receptor and transporter binding studies have yielded conflicting results, including higher and lower serotonin receptor 5-HT2A and serotonin transporter binding potentials that were normalized by treatment with SSRIs. Despite inconsistencies in the types of abnormalities found, evidence points towards dysfunction of serotonergic systems in OCD. Orbitofrontal cortex overactivity is attenuated in people who have successfully responded to SSRI medication, a result believed to be caused by increased stimulation of serotonin receptors 5-HT2A and 5-HT2C. A complex relationship between dopamine and OCD has been observed. Although antipsychotics, which act by antagonizing dopamine receptors, may improve some cases of OCD, they frequently exacerbate others. Antipsychotics, in the low doses used to treat OCD, may actually increase the release of dopamine in the prefrontal cortex, through inhibiting autoreceptors. Further complicating things is the efficacy of amphetamines, decreased dopamine transporter activity observed in OCD, and low levels of D2 binding in the striatum. Furthermore, increased dopamine release in the nucleus accumbens after deep brain stimulation correlates with improvement in symptoms, pointing to reduced dopamine release in the striatum playing a role in generating symptoms. Abnormalities in glutamatergic neurotransmission have been implicated in OCD. Findings such as increased cerebrospinal glutamate, less consistent abnormalities observed in neuroimaging studies, and the efficacy of some glutamatergic drugs (such as the glutamate-inhibiting riluzole) have implicated glutamate in OCD. OCD has been associated with reduced N-Acetylaspartic acid in the mPFC, which is thought to reflect neuron density or functionality, although the exact interpretation has not been established. Diagnosis Formal diagnosis may be performed by a psychologist, psychiatrist, clinical social worker or other licensed mental health professional. OCD, like other mental and behavioral health disorders, cannot be diagnosed by a medical exam, nor are there any medical exams that can predict if one will fall victim to such illnesses. To be diagnosed with OCD, a person must have obsessions, compulsions or both, according to the Diagnostic and Statistical Manual of Mental Disorders (DSM). The DSM notes that there are multiple characteristics that can turn obsessions and compulsions from normalized behavior to "clinically significant". There has to be recurring and strong thoughts or impulsive that intrude on the day-to-day lives of the patients and cause noticeable levels of anxiousness. These thoughts, impulses or images are of a degree or type that lies outside the normal range of worries about conventional problems. A person may attempt to ignore or suppress such obsessions, neutralize them with another thought or action, or try to rationalize their anxiety away. People with OCD tend to recognize their obsessions as irrational. Compulsions become clinically significant when a person feels driven to perform them in response to an obsession or according to rules that must be applied rigidly and when the person consequently feels or causes significant distress. Therefore, while many people who do not have OCD may perform actions often associated with OCD (such as ordering items in a pantry by height), the distinction with clinically significant OCD lies in the fact that the person with OCD must perform these actions to avoid significant psychological distress. These behaviors or mental acts are aimed at preventing or reducing distress or preventing some dreaded event or situation; however, these activities are not logically or practically connected to the issue, or, they are excessive. Moreover, the obsessions or compulsions must be time-consuming, often taking up more than one hour per day or cause impairment in social, occupational or scholastic functioning. It is helpful to quantify the severity of symptoms and impairment before and during treatment for OCD. In addition to the person's estimate of the time spent each day harboring obsessive-compulsive thoughts or behaviors, concrete tools can be used to gauge the person's condition. This may be done with rating scales, such as the Yale–Brown Obsessive–Compulsive Scale (Y-BOCS; expert rating) or the obsessive–compulsive inventory (OCI-R; self-rating). With measurements such as these, psychiatric consultation can be more appropriately determined, as it has been standardized. In regards to diagnosing, the health professional also looks to make sure that the signs of obsessions and compulsions are not the results of any drugs, prescription or recreational, that the patient may be taking. There are several types of obsessive thoughts that are found commonly in those with OCD. Some of these include fear of germs, hurting loved ones, embarrassment, neatness, societally unacceptable sexual thoughts etc. Within OCD, these specific categories are often diagnosed into their own type of OCD. OCD is sometimes placed in a group of disorders called the obsessive–compulsive spectrum. Another criterion in the DSM is that a person's mental illness does not fit one of the other categories of a mental disorder better. That is to say, if the obsessions and compulsions of a patient could be better described by trichotillomania, it would not be diagnosed as OCD. That being said, OCD does often go hand in hand with other mental disorders. For this reason, one may be diagnosed with multiple mental disorders at once. A different aspect of the diagnoses is the degree of insight had by the individual in regards to the truth of the obsessions. There are three levels, good/fair, poor and absent/delusional. Good/fair indicated that the patient is aware that the obsessions they have are not true or probably not true. Poor indicates that the patient believes their obsessional beliefs are probably true. Absent/delusional indicates that they fully believe their obsessional thoughts to be true. Approximately 4% or fewer individuals with OCD will be diagnosed as absent/delusional. Additionally, as many as 30% of those with OCD also have a lifetime tic disorder, meaning they have been diagnosed with a tic disorder at some point in their life. There are several different types of tics that have been observed in individuals with OCD. These include but are not limited to, "grunting", "jerking" or "shrugging" body parts, sniffling and excessive blinking. There has been a significant amount of progress over the last few decades and as of 2022 there is statically significant improvement in the diagnostic process for individuals with OCD. One study found that of two groups of individuals, one with participants under the age of 27.25 and one with participants over that age, those in the younger group experienced a significantly faster time between the onset of OCD tendencies and their formal diagnoses. Differential diagnosis OCD is often confused with the separate condition obsessive–compulsive personality disorder (OCPD). OCD is egodystonic, meaning that the disorder is incompatible with the individual's self-concept. As egodystonic disorders go against a person's self-concept, they tend to cause much distress. OCPD, on the other hand, is egosyntonic, marked by the person's acceptance that the characteristics and behaviors displayed as a result are compatible with their self-image, or are otherwise appropriate, correct or reasonable. As a result, people with OCD are often aware that their behavior is not rational and are unhappy about their obsessions, but nevertheless feel compelled by them. By contrast, people with OCPD are not aware of anything abnormal; they will readily explain why their actions are rational. It is usually impossible to convince them otherwise and they tend to derive pleasure from their obsessions or compulsions. Management Cognitive behavioral therapy (CBT) and psychotropic medications are the first-line treatments for OCD. Therapy In cognitive behavioral therapy (CBT), OCD patients are asked to overcome intrusive thoughts by not indulging in any compulsions. They are taught that rituals keep OCD strong, while not performing them causes OCD to become weaker. This position is supported by the pattern of memory distrust; the more often compulsions are repeated, the more weakened memory trust becomes and this cycle continues as memory distrust increases compulsion frequency. One specific CBT technique used is called exposure and response prevention (ERP), which involves teaching the person to deliberately come into contact with situations that trigger obsessive thoughts and fears (exposure), without carrying out the usual compulsive acts associated with the obsession (response prevention). This technique causes patients to gradually learn to tolerate the discomfort and anxiety associated with not performing their compulsions. For many patients, ERP is the add-on treatment of choice when selective serotonin reuptake inhibitors (SSRIs) or serotonin–norepinephrine reuptake inhibitors (SNRIs) medication does not effectively treat OCD symptoms, or vice versa, for individuals who begin treatment with psychotherapy. This technique is considered superior to others due to the lack of medication used. However, up to 25% of patients will discontinue treatment due to the severity of their tics. CBT normally lasts anywhere from 12-16 sessions, with homework assigned to the patient in between meetings with a therapist. (Lack 2012). Modalities differ in ERP treatment but both virtual reality based as well as unguided computer assisted treatment programs have shown effective results in treatment programs. For example, a patient might be asked to touch something very mildly contaminated (exposure) and wash their hands only once afterward (response prevention). Another example might entail asking the patient to leave the house and check the lock only once (exposure), without going back to check again (response prevention). After succeeding at one stage of treatment, the patient's level of discomfort in the exposure phase can be increased. When this therapy is successful, the patient will quickly habituate to an anxiety-producing situation, discovering a considerable drop in anxiety level. ERP has a strong evidence base and is considered the most effective treatment for OCD. However, this claim was doubted by some researchers in 2000, who criticized the quality of many studies. While ERP can lead a majority of clients to improvements, many do not reach remission or become asymptomatic; some therapists are also hesitant to use this approach. The recent development of remotely technology-delivered CBT is increasing access to therapy options for those living with OCD and remote versions appear to equally as effective as in-person therapy options. The development of smartphone interventions for OCD that utilize CBT techniques are another alternative that is expanding access to therapy while allowing therapies to be personalized for each patient. Acceptance and commitment therapy (ACT), a newer therapy also used to treat anxiety and depression, has also been found to be effective in treatment of OCD. ACT uses acceptance and mindfulness strategies to teach patients not to overreact to or avoid unpleasant thoughts and feelings but rather "move toward valued behavior". Inference-based therapy (IBT) is a form of cognitive therapy specifically developed for treating OCD. The therapy posits that individuals with OCD put a greater emphasis on an imagined possibility than on what can be perceived with the senses, and confuse the imagined possibility with reality, in a process called inferential confusion. According to inference-based therapy, obsessional thinking occurs when the person replaces reality and real probabilities with imagined possibilities. The goal of inference-based therapy is to reorient clients towards trusting the senses and relating to reality in a normal, non-effortful way. Differences between normal and obsessional doubts are presented and clients are encouraged to use their senses and reasoning as they do in non-obsessive–compulsive disorder situations. Research on Inference-Based Cognitive-Behavior Therapy (I-CBT) suggests it can lead to improvements for those with OCD. For body-focused repetitive behaviors such as trichotillomania (hair pulling), skin picking and onychophagia (nail biting), behavioral interventions such as habit reversal training and decoupling are recommended for the treatment of compulsive behaviors. A 2007 Cochrane review found that psychological interventions derived from CBT models, such as ERP, ACT and IBT, were more effective than non-CBT interventions. Other forms of psychotherapy, such as psychodynamics and psychoanalysis, may help in managing some aspects of the disorder. However, in 2007, the American Psychiatric Association (APA) noted a lack of controlled studies showing their efficacy, "in dealing with the core symptoms of OCD". For body-focused repetitive behaviors, behavioral interventions such as habit-reversal training and decoupling are recommended. Psychotherapy in combination with psychiatric medication may be more effective than either option alone for individuals with severe OCD. ERP coupled with weight restoration and serotonin reuptake inhibitors has proven the most effective when treating OCD and an eating disorder simultaneously. Medication The medications most frequently used to treat OCD are antidepressants, including selective serotonin reuptake inhibitors (SSRIs) and serotonin–norepinephrine reuptake inhibitors (SNRIs). SSRIs such as sertraline and fluoxetine are effective in treating OCD for children and adolescents. However, ERP alone may be as effective as ERP combined with an SSRI for OCD symptoms. SSRIs are a second-line treatment of adult OCD with mild functional impairment and as first-line treatment for those with moderate or severe impairment. In children, SSRIs can be considered as a second-line therapy in those with moderate to severe impairment, with close monitoring for psychiatric adverse effects. Patients treated with SSRIs are about twice as likely to respond to treatment as are those treated with placebo, so this treatment is qualified as efficacious. Efficacy has been demonstrated both in short-term (6–24 weeks) treatment trials and in discontinuation trials with durations of 28–52 weeks. Clomipramine, a medication belonging to the class of tricyclic antidepressants, appears to work as well as SSRIs, but has a higher rate of side effects. Clomipramine has been shown to be possibly more effective than a placebo. In 2006, the National Institute for Health and Care Excellence (NICE) guidelines recommended augmentative second-generation (atypical) antipsychotics for treatment-resistant OCD. Atypical antipsychotics are not useful when used alone and no evidence supports the use of first-generation antipsychotics. For OCD treatment specifically, there is tentative evidence for risperidone and insufficient evidence for olanzapine. Quetiapine is no better than placebo with regard to primary outcomes, but small effects were found in terms of Y-BOCS score. The efficacy of quetiapine and olanzapine are limited by an insufficient number of studies. A 2014 review article found two studies that indicated that aripiprazole was "effective in the short-term" and found that "[t]here was a small effect-size for risperidone or antipsychotics in general in the short-term"; however, the study authors found "no evidence for the effectiveness of quetiapine or olanzapine in comparison to placebo." While quetiapine may be useful when used in addition to an SSRI/SNRI in treatment-resistant OCD, these drugs are often poorly tolerated and have metabolic side effects that limit their use. A guideline by the American Psychological Association suggested that dextroamphetamine may be considered by itself after more well-supported treatments have been attempted. Procedures Electroconvulsive therapy has been found to have effectiveness in some severe and refractory cases. Transcranial magnetic stimulation has shown to provide therapeutic benefits in alleviating symptoms. Surgery may be used as a last resort in people who do not improve with other treatments. In this procedure, a surgical lesion is made in an area of the brain (the cingulate cortex). In one study, 30% of participants benefitted significantly from this procedure. Deep brain stimulation and vagus nerve stimulation are possible surgical options that do not require destruction of brain tissue. However, because deep brain stimulation results in such an instant and intense change, individuals may experience identity challenges afterward. In the United States, the Food and Drug Administration approved deep brain stimulation for the treatment of OCD under a humanitarian device exemption, requiring that the procedure be performed only in a hospital with special qualifications to do so. In the United States, psychosurgery for OCD is a treatment of last resort and will not be performed until the person has failed several attempts at medication (at the full dosage) with augmentation, and many months of intensive cognitive behavioral therapy with exposure and ritual/response prevention. Likewise, in the United Kingdom, psychosurgery cannot be performed unless a course of treatment from a suitably qualified cognitive–behavioral therapist has been carried out. Children Therapeutic treatment may be effective in reducing ritual behaviors of OCD for children and adolescents. Similar to the treatment of adults with OCD, cognitive behavioral therapy, along with exposure and response prevention (ERP) therapy, stands as an effective and validated first line of treatment of OCD in children. Family involvement, in the form of behavioral observations and reports, is a key component to the success of such treatments. Parental interventions also provide positive reinforcement for a child who exhibits appropriate behaviors as alternatives to compulsive responses. In a recent meta-analysis of evidenced-based treatment of OCD in children, family-focused individual CBT was labeled as "probably efficacious", establishing it as one of the leading psychosocial treatments for youth with OCD. After one or two years of therapy, in which a child learns the nature of their obsession and acquires strategies for coping, they may acquire a larger circle of friends, exhibit less shyness and become less self-critical. Trials have shown that children and adolescents with OCD should begin treatment with the combination of CBT with a selective serotonin reuptake inhibitor or CBT alone, rather than only an SSRI. A 2024 systematic review of the literature found that combining ERP therapy with selective serotonin reuptake inhibitors can enhance treatment outcomes compared to using SSRIs alone. ERP therapy can be done in-office or via telehealth since there was no statistically significant difference in effectiveness as shown in the AHRQ study. Although the known causes of OCD in younger age groups range from brain abnormalities to psychological preoccupations, life stress such as bullying and traumatic familial deaths may also contribute to childhood cases of OCD, and acknowledging these stressors can play a role in treating the disorder. Prognosis Quality of life is reduced across all domains in OCD. While psychological or pharmacological treatment can lead to a reduction of OCD symptoms and an increase in reported quality of life, symptoms may persist at moderate levels even following adequate treatment courses, and completely symptom-free periods are uncommon. In pediatric OCD, around 40% still have the disorder in adulthood and around 40% qualify for remission. The risk of having at least one comorbid personality disorder in OCD is 52%, which is the highest among anxiety disorders and greatly impacts its management and prognosis. Epidemiology Obsessive–compulsive disorder affects about 2.3% of people at some point in their life, with the yearly rate about 1.2%. OCD occurs worldwide. It is unusual for symptoms to begin after the age of 35 and half of people develop problems before 20. Males and females are affected about equally. However, there is an earlier age for onset for males than females. History Plutarch, an ancient Greek philosopher and historian, describes an ancient Roman man who possibly had scrupulosity, which could be a symptom of OCD or OCPD. This man is described as "turning pale under his crown of flowers", praying with a "faltering voice" and scattering "incense with trembling hands". In the 7th century AD, John Climacus records an instance of a young monk plagued by constant and overwhelming "temptations to blasphemy" consulting an older monk, who told him: "My son, I take upon myself all the sins which these temptations have led you, or may lead you, to commit. All I require of you is that for the future you pay no attention to them whatsoever.": 212 The Cloud of Unknowing, a Christian mystical text from the late 14th century, recommends dealing with recurring obsessions by attempting to ignore them, and, if that fails, to "cower under them like a poor wretch and a coward overcome in battle, and reckon it to be a waste of your time for you to strive any longer against them", a technique now known as emotional flooding.: 213 Abu Zayd Al-Balkhi, the 9th century Islamic polymath, was likely the first to classify OCD into different types and pioneer cognitive behavioral therapy, in a fashion unique to his era and which was not popular in Greek medicine. In his medical treatise entitled Sustenance of the Body and Soul, Al-Balkhi describes obsessions particular to the disorder as "Annoying thoughts that are not real. These intrusive thoughts prevent enjoying life, and performing daily activities. They affect concentration and interfere with ability to carry out different tasks." As treatment, Al-Balkhi suggests treating obsessive thoughts with positive thoughts and mind-based therapy. From the 14th to the 16th century in Europe, it was believed that people who experienced blasphemous, sexual or other obsessive thoughts were possessed by the devil.: 213 Based on this reasoning, treatment involved banishing the "evil" from the "possessed" person through exorcism. The vast majority of people who thought that they were possessed by the devil did not have hallucinations or other "spectacular symptoms" but "complained of anxiety, religious fears, and evil thoughts.": 213 In 1584, a woman from Kent, England, named Mrs. Davie, described by a justice of the peace as "a good wife", was nearly burned at the stake after she confessed that she experienced constant, unwanted urges to murder her family.: 213 The English term obsessive–compulsive arose as a translation of German Zwangsvorstellung (obsession) used in the first conceptions of OCD by Karl Westphal. Westphal's description went on to influence Pierre Janet, who further documented features of OCD. In the early 1910s, Sigmund Freud attributed obsessive–compulsive behavior to unconscious conflicts that manifest as symptoms. Freud describes the clinical history of a typical case of "touching phobia" as starting in early childhood, when the person has a strong desire to touch an item. In response, the person develops an "external prohibition" against this type of touching. However, this "prohibition does not succeed in abolishing" the desire to touch; all it can do is repress the desire and "force it into the unconscious." Freudian psychoanalysis remained the dominant treatment for OCD until the mid-1980s, even though medicinal and therapeutic treatments were known and available, because it was widely thought that these treatments would be detrimental to the effectiveness of the psychotherapy.: 210–211 In the mid-1980s, this approach changed and practitioners began treating OCD primarily with medicine and practical therapy rather than through psychoanalysis.: 210 One of the first successful treatments of OCD, exposure and response prevention, emerged during the 1960s, when psychologist Vic Meyer exposed two hospitalized patients to anxiety-inducing situations while preventing them from performing any compulsions. Eventually, both patients' anxiety level dropped to manageable levels. Meyer devised this procedure from his analysis of fear extinguishment in animals via flooding. The success of ERP clinically and scientifically has been summarized as "spectacular" by prominent OCD researcher Stanley Rachman decades following Meyer's creation of the method. In 1967, psychiatrist Juan José López-Ibor reported that the drug clomipramine was effective in treating OCD. Many reports of its success in treatment followed and several studies had confirmed its effectiveness by the 1980s. However, clomipramine was subsequently displaced by new SSRIs developed in the 1970s, such as fluoxetine and sertraline, which were shown to have fewer side effects. Obsessive–compulsive symptoms worsened during the early stages of the COVID-19 pandemic, particularly for individuals with contamination-related OCD. Notable cases John Bunyan (1628–1688), the author of The Pilgrim's Progress, displayed symptoms of OCD (which had not yet been named). During the most severe period of his condition, he would mutter the same phrase over and over again to himself while rocking back and forth.: 53–54 He later described his obsessions in his autobiography Grace Abounding to the Chief of Sinners, stating, "These things may seem ridiculous to others, even as ridiculous as they were in themselves, but to me they were the most tormenting cogitations.": 53–54 He wrote two pamphlets advising those with similar anxieties.: 217–218 In one of them, he warns against indulging in compulsions: "Have care of putting off your trouble of spirit in the wrong way: by promising to reform yourself and lead a new life, by your performances or duties.": 217–218 British poet, essayist and lexicographer Samuel Johnson (1709–1784) had OCD. He had elaborate rituals for crossing the thresholds of doorways and repeatedly walked up and down staircases counting the steps.: 54–55 He would touch every post on the street as he walked past, only step in the middle of paving stones and repeatedly perform tasks as though they had not been done properly the first time.: 55 The "Rat Man", real name Ernst Lanzer, a patient of Sigmund Freud, suffered from what was then called "obsessional neurosis". Lanzer's illness was characterised most famously by a pattern of distressing intrusive thoughts in which he feared that his father or a female friend would be subjected to a purported Chinese method of torture in which rats would be encouraged to gnaw their way out of a victim's body by a hot poker. American aviator and filmmaker Howard Hughes is known to have had OCD, primarily an obsessive fear of germs and contamination. Friends of Hughes have also mentioned his obsession with minor flaws in clothing. This was conveyed in The Aviator (2004), a film biography of Hughes. English singer-songwriter George Ezra has spoken about his life-long struggle with OCD, particularly primarily obsessional obsessive–compulsive disorder. Swedish climate activist Greta Thunberg is also known to have OCD, among other mental health conditions. Passing for Normal: A Memoir of Compulsion (2000) is a memoir by Amy Wilensky about her experience of OCD and Tourette syndrome. American actor James Spader has spoken about his OCD. In 2014, when interviewed for Rolling Stone he said: "I'm obsessive-compulsive. I have very, very strong obsessive-compulsive issues. I'm very particular. ... It's very hard for me, you know? It makes you very addictive in behavior, because routine and ritual become entrenched. But in work, it manifests itself in obsessive attention to detail and fixation. It serves my work very well: Things don't slip by. But I'm not very easygoing. In 2022 the president of Chile Gabriel Boric stated that he had OCD, saying: "I have an obsessive–compulsive disorder that's completely under control. Thank God I've been able to undergo treatment and it doesn't make me unable to carry out my responsibilities as the President of the Republic." In a documentary released in 2023, the footballer David Beckham discussed his compelling cleaning rituals, need for symmetry in the fridge and the impact of OCD on his life. Society and culture Art, entertainment and media Movies and television shows may portray idealized or incomplete representations of disorders such as OCD. Compassionate and accurate literary and on-screen depictions may help counteract the potential stigma associated with an OCD diagnosis and lead to increased public awareness, understanding and sympathy for such disorders. The play and film adaptations of The Odd Couple based around the character of Felix, who shows some of the common symptoms of OCD. In the film As Good as It Gets (1997), actor Jack Nicholson portrays a man with OCD who performs ritualistic behaviors that disrupt his life. The film Matchstick Men (2003) portrays a con man named Roy (Nicolas Cage) with OCD who opens and closes doors three times while counting aloud before he can walk through them. In the television series Monk (2002–2009), the titular character Adrian Monk fears both human contact and dirt. The one-man show The Life and Slimes of Marc Summers (2016), a stage adaptation of Marc Summers' 1999 memoir which recounts how OCD affected his entertainment career. In the novel Turtles All the Way Down (2017) by John Green, teenage main character Aza Holmes struggles with OCD that manifests as a fear of the human microbiome. Throughout the story, Aza repeatedly opens an unhealed callus on her finger to drain out what she believes are pathogens. The novel is based on Green's own experiences with OCD. He explained that Turtles All the Way Down is intended to show how "most people with chronic mental illnesses also live long, fulfilling lives." The British TV series Pure (2019) stars Charly Clive as a 24-year-old Marnie who is plagued by disturbing sexual thoughts, as a kind of primarily obsessional obsessive compulsive disorder. In the film The House That Jack Built, the titular character compulsively cleans a crime scene after obsessing over leaving evidence. Research Cortical thickness differences in adults with OCD are linked to a network of brain-specific, developmentally expressed genes. OCD patients have thicker precentral and paracentral brain regions compared to controls and those with thinner precentral areas showed greater improvement with transcranial direct-current stimulation treatment, suggesting these structural differences may serve as neural biomarkers for predicting treatment response. Other animals Advocacy Many organizations and charities around the world advocate for the wellbeing of people with OCD, stigma reduction, research and awareness. The International OCD Foundation (IOCDF) is the largest 501(c)3 nonprofit organization dedicated to serving a broad community of individuals with OCD and related disorders, their family members and loved ones, and mental health professionals and researchers around the world. Since 1986, the IOCDF provides up-to-date education and resources, strengthens community engagement worldwide, delivers quality professional training to clinicians and funds groundbreaking research. See also Delusional disorder Hypochondriasis References External links National Institute Of Mental Health American Psychiatric Association APA Division 12 treatment page for obsessive-compulsive disorder Davis LJ (2008). Obsession: A History. University of Chicago Press. ISBN 978-0-226-13782-7. A familect or marriage language is a set of invented words or phrases with meanings understood within members of a family or other small intimate group. Among the pioneers of research on familects is Cynthia Gordon, professor of linguistics at Georgetown University, who discussed the concept in her 2009 book Making Meanings, Creating Family. Familects fall within the intimate register of communication. Familects often gain vocabulary through the words young children create as they learn to talk, when these words are adopted by the family. Familects also gain vocabulary through slips of the tongue and word invention. See also Idioglossia References Further reading Cynthia Gordon (2009). Making Meanings, Creating Family. Oxford University Press. ISBN 978-0195373837. The English Project (2008). Kitchen Table Lingo. Virgin Books. ISBN 978-0753518199. External links Georgetown University Linguistics faculty page, with info on Cynthia Gordon Kitchen Table Lingo, The English Project Mignon Fogarty, What's Your Family Slang, Quick and Dirty Tips Human uses of living things, including animals, plants, fungi, and microbes, take many forms, both practical, such as the production of food and clothing, and symbolic, as in art, mythology, and religion. Social sciences including archaeology, anthropology and ethnography are starting to take a multispecies view of human interactions with nature, in which living things are not just resources to be exploited, practically or symbolically, but are involved as participants. Plants provide the greater part of the food for people and their domestic animals: much of civilisation came into being through agriculture. While many plants have been used for food, a small number of staple crops including wheat, rice, and maize provide most of the food in the world today. In turn, animals provide much of the meat eaten by the human population, whether farmed or hunted, and until the arrival of mechanised transport, terrestrial mammals provided a large part of the power used for work and transport. A variety of living things serve as models in biological research, such as in genetics, and in drug testing. Until the 19th century, plants yielded most of the medicinal drugs in common use, as described in the 1st century by Dioscorides. Plants are the source of many psychoactive drugs, some such as coca known to have been used for thousands of years. Yeast, a fungus, has been used to ferment cereals such as wheat and barley to make bread and beer; other fungi such as Psilocybe and fly agaric mushrooms have been gathered as psychoactive drugs. Many species of animal are kept as pets, the most popular being mammals, especially dogs and cats. Plants are grown for pleasure in gardens and greenhouses, yielding flowers, shade, and decorative foliage; some, such as cactuses, able to tolerate dry conditions, are grown as houseplants. Animals such as horses and deer are among the earliest subjects of art, being found in the Upper Paleolithic cave paintings such as at Lascaux. Living things play a wide variety of symbolic roles in literature, film, mythology, and religion. Sometimes a major disease like tuberculosis is depicted in art and literature, in its case being associated for some reason with artistic creativity. Scope Scholars have traditionally divided uses of animals, plants, and other living things into two categories: practical use for food and other resources; and symbolic use such as in art and religion. More recently, scholars have added a third type of interaction, where living things, whether animals, plants, fungi or microbes function as participants. This makes the relationships bidirectional, explicitly implying various forms of symbiosis in a complex ecology. These three types are described in turn. Practical uses For food and materials The human population exploits and depends on many animal and plant species for food, mainly through agriculture, but also by exploiting wild populations, notably of marine fish. Livestock animals are raised for meat across the world; they include (2011) around 1.4 billion cattle, 1.2 billion sheep and 1 billion domestic pigs. Plants provide the greater part of food for humans, and for their domestic animals. They have played a key role in the history of world civilizations. Agriculture includes agronomy for arable crops, horticulture for vegetables and fruit, and forestry for timber. About 7,000 species of plant have been used for food, though most of today's food is derived from only 30 species. The major staples include cereals such as rice and wheat, starchy roots and tubers such as cassava and potato, and legumes such as peas and beans. Vegetable oils such as olive oil provide lipids, while fruit and vegetables contribute vitamins and minerals to the diet. Plants grown as industrial crops are the source of a wide range of products used in manufacturing, sometimes so intensively as to risk harm to the environment. Nonfood products include essential oils, natural dyes, pigments, waxes, resins, tannins, alkaloids, amber and cork. Products derived from plants include soaps, shampoos, perfumes, cosmetics, paint, varnish, turpentine, rubber, latex, lubricants, linoleum, plastics, inks, and gums. Renewable fuels from plants include firewood, peat and other biofuels. The fossil fuels coal, petroleum and natural gas are derived from the remains of aquatic organisms including phytoplankton in geological time. Structural resources and fibres from plants are used to construct dwellings and to manufacture clothing. Wood is used not only for buildings, boats, and furniture, but also for smaller items such as musical instruments and sports equipment. Wood is pulped to make paper and cardboard. Cloth is often made from cotton, flax, ramie or synthetic fibres such as rayon and acetate derived from plant cellulose. Thread used to sew cloth likewise comes in large part from cotton. Plants are a primary source of basic chemicals, both for their medicinal and physiological effects, and for the industrial synthesis of a vast array of organic chemicals. Textiles are made from both animal fibres, including wool and silk, and plant fibres, including cotton and flax. Dyestuffs too are made both from animals, including carmine from the bodies of insects, from plants including indigo and madder, and from lichens. For work and transport Working domestic animals including cattle, horses, yaks, camels, and elephants have been used for work and transport from the origins of agriculture, their numbers declining with the arrival of mechanised transport and agricultural machinery. In 2004 they still provided some 80% of the power for the mainly small farms in the third world, and some 20% of the world's transport, again mainly in rural areas. In mountainous regions unsuitable for wheeled vehicles, pack animals continue to transport goods. In science Biology studies the whole range of living things. Animals such as the fruit fly Drosophila melanogaster, the zebrafish, the chicken and the house mouse, serve a major role in science as experimental models, both in fundamental biological research, such as in genetics, and in the development of new medicines, which must be tested exhaustively to demonstrate their safety. Millions of mammals, especially mice and rats, are used in experiments each year. Knockout mice are used to help discover the functions of genes. Basic biological research has been done with plants. In genetics, the breeding of pea plants allowed Gregor Mendel to derive the basic laws governing inheritance, and examination of chromosomes in maize allowed Barbara McClintock to demonstrate their connection to inherited traits. The plant Arabidopsis thaliana is used in laboratories as a model organism to understand how genes control the growth and development of plant structures. Space stations or space colonies may one day rely on plants for life support. For medicines and drugs Vaccines have been made using animals and microbes since their discovery by Edward Jenner in the 18th century. He noted that inoculation with live cowpox afforded protection against the more dangerous smallpox. In the 19th century, Louis Pasteur developed an attenuated (weakened) vaccine for rabies. In the 20th century, vaccines for the viral diseases mumps and polio were developed using animal cells grown in vitro. An increasing variety of medicinal drugs are based on toxins and other molecules of animal origin. The cancer drug Yondelis was isolated from the tunicate Ecteinascidia turbinata. One of dozens of toxins made by the deadly cone snail Conus geographus is used as Prialt in pain relief. Since classical times and possibly much earlier, hundreds of species of plants have provided drugs to treat a wide range of conditions. Dioscorides's De Materia Medica, written by 70 AD, listed some 600 medicinal plants and around 1000 drugs made from them, including substances known to be effective such as aconite, aloes, colocynth, colchicum, henbane, opium and squill. The book remained a standard reference for nearly two thousand years, and was the basis of the European pharmacopoeia until the end of the 19th century. Also since the earliest times, people have exploited some of the many psychoactive substances manufactured by plants in religious rituals and for pleasure. Among the most widely used throughout history are alcohol, produced by fermenting cereals with yeast (a fungus), tobacco, coffee, tea, chocolate, cannabis, coca (used as leaf for some 8,000 years in Peru, and in recent times also purified to cocaine), mescaline (from a cactus) and psilocybin (from a fungus). For pleasure Both animals and plants are used to provide pleasure, through a range of activities including keeping pets, hunting, fishing, and gardening. A wide variety of animals are kept as pets, from invertebrates such as tarantulas and octopuses, insects including praying mantises, reptiles such as snakes and chameleons, and birds including canaries, parakeets and parrots all finding a place. Mammals are the most popular pets in the Western world, with the most kept species being dogs, cats, and rabbits. For example, in America in 2012 there were some 78 million dogs, 86 million cats, and 3.5 million rabbits. There is a tension between the role of animals as companions to humans, and their existence as individuals with rights of their own. Many animals are hunted for sport. The aquatic animals most often hunted for sport are fish, including many species from large marine predators such as sharks and tuna, to freshwater fish such as trout and carp. Birds such as partridges, pheasants and ducks, and mammals such as deer and wild boar, are among the terrestrial game animals most often hunted. Thousands of plant species are cultivated for aesthetic purposes as well as to provide shade, modify temperatures, reduce wind, abate noise, provide privacy, and prevent soil erosion. Plants are the basis of a multibillion-dollar per year tourism industry, which includes travel to historic gardens, national parks, rainforests, forests with colorful autumn leaves, and festivals such as Japan's and America's cherry blossom festivals. While some gardens are planted with food crops, many are planted for aesthetic, ornamental, or conservation purposes. Arboretums and botanical gardens are public collections of living plants. In private outdoor gardens, lawn grasses, shade trees, ornamental trees, shrubs, vines, herbaceous perennials and bedding plants are used. Gardens may cultivate the plants in a naturalistic state, or may sculpture their growth, as with topiary or espalier. Gardening is the most popular leisure activity in the U.S., and working with plants or horticulture therapy is beneficial for rehabilitating people with disabilities. Plants may also be grown or kept indoors as houseplants, or in specialized buildings such as greenhouses that are designed for the care and cultivation of living plants. Venus Flytrap, sensitive plant and resurrection plant are examples of plants sold as novelties. There are also art forms specializing in the arrangement of cut or living plant, such as bonsai, ikebana, and the arrangement of cut or dried flowers. Ornamental plants have sometimes changed the course of history, as in tulipomania. Symbolic uses In art Both animals and plants are significant in art, whether as background or as main subjects. Animals, often mammals but including fish and insects among other groups, have been the subjects of art from the earliest times, in both early history as in Ancient Egypt, and prehistory, as in the cave paintings at Lascaux and other sites in the Dordogne, France and elsewhere. Major artistic depictions of animals include Albrecht Dürer's 1515 The Rhinoceros, and George Stubbs's c. 1762 horse portrait Whistlejacket. Plants appear in art, either to illustrate their botanical appearance, or for the purposes of the artist, which may include decoration or religious symbolism. For example, the Virgin Mary was compared by the Venerable Bede to a lily, the white petals denoting purity of body, while the yellow anthers signified the radiant light of the soul; accordingly, European portraits of the Virgin's Annunciation may depict a vase of white lilies in her room to indicate her attributes. Plants are also often used as backgrounds or features in portraits, and as main subjects in still lifes. In literature and film Animals, plants, and microbes feature in literature and film. Animals as varied as bees, beetles, mice, foxes, crocodiles and elephants play a wide variety of roles in literature and film. A genre of films has been based on oversized insects, including the pioneering 1954 Them!, featuring giant ants mutated by radiation, and the 1957 The Deadly Mantis. Birds have occasionally featured in film, as in Alfred Hitchcock's 1963 The Birds, loosely based on Daphne du Maurier's story of the same name, which tells the tale of sudden attacks on people by violent flocks of birds. Ken Loach's admired 1969 Kes, based on Barry Hines's 1968 novel A Kestrel for a Knave, tells a story of a boy coming of age by training a kestrel. Parasitoids have inspired science fiction authors and screenwriters to create disgusting and terrifying parasitic alien species that kill their human hosts, such as in Ridley Scott's 1979 film Alien. Plants too, both real and invented, play many roles in literature and film. Plants' roles may be evil, as with the triffids, carnivorous plants with a whip-like poisonous sting as well as mobility provided by three foot-like appendages, from John Wyndham's 1951 science fiction novel The Day of the Triffids, and subsequent adaptations for film and radio. J. R. R. Tolkien's fictional world Middle-earth features many named kinds of plant, including the healing herb athelas the yellow star-flower elanor which grows in special places such as Cerin Amroth in Lothlórien, and the tall mallorn tree of the elves. Tolkien names several individual trees of significance in the narrative, including the Party Tree in the Shire with its happy associations, and the malevolent Old Man Willow in the Old Forest. Trees feature in many of Ursula K. Le Guin's books, including the forest world of Athshe and the Immanent Grove on Roke in the Earthsea series, to such an extent that in her introduction to her collection The Wind's Twelve Quarters, she admits to "a certain obsession with trees" and describes herself as "the most arboreal science fiction writer". James Cameron's 2009 film Avatar features a giant tree named Hometree, the sacred gathering place of the humanoid Na'vi tribe; the interconnected tree, tribe and planet are threatened by mining: the tribe and the film's hero fight to save them. Trees are common subjects in poetry, including Joyce Kilmer's 1913 lyric poem named "Trees". Flowers, similarly, are the subjects of many poems by poets such as William Blake, Robert Frost, and Rabindranath Tagore. Tuberculosis played a role in art and literature, associated for some reason with artistic creativity, becoming known as "the romantic disease". Many artistic figures including the poet John Keats, the composer Frederic Chopin and the artist Edvard Munch either had the disease or were close to others who did. Tuberculosis played prominent and recurring roles in diverse fields. These included literature, as in Thomas Mann's The Magic Mountain, set in a sanatorium; in music, as in Van Morrison's song "T.B. Sheets"; in opera, as in Puccini's La bohème and Verdi's La Traviata; in art, as in Monet's painting of his first wife Camille on her deathbed; and in film, such as the 1945 The Bells of St. Mary's starring Ingrid Bergman as a nun with tuberculosis. In mythology and religion Animals including many insects and mammals feature in mythology and religion; indeed, animals and plants appear in what has been suggested to be the world's first religion in the Paleolithic era. Among the insects, in both Japan and Europe, as far back as ancient Greece and Rome, a butterfly was seen as the personification of a person's soul, both while they were alive and after their death. The scarab beetle was sacred in ancient Egypt, while the praying mantis was considered a god in southern African Khoi and San tradition for its praying posture. Among the mammals, cattle, deer, horses, lions and wolves (and werewolves), are the subjects of myths and worship. Of the twelve signs of the Western zodiac, six—Aries (ram), Taurus (bull), Cancer (crab), Leo (lion), Scorpio (scorpion), and Pisces (fish)—are animals, while two others, Sagittarius (horse/man) and Capricorn (fish/goat) are hybrid animals; the name zodiac indeed means a circle of animals. All twelve signs of the Chinese zodiac are animals. Plants including trees are important in mythology and religion, where they symbolise themes such as fertility, growth, immortality and rebirth, and may be more or less magical. Thus in Latvian mythology, Austras koks is a tree which grows from the start of the Sun's daily journey across the sky. A different cosmic tree is Yggdrasil, the World tree of Norse mythology, on which Odin hung. Different again is the barnacle tree, believed in the Middle Ages to have barnacles that opened to reveal geese, a story which may perhaps have started from an observation of goose barnacles growing on driftwood. Greek mythology mentions many plants and flowers, where for example the lotus tree bears a fruit that causes a pleasant drowsiness, while moly is a magic herb mentioned by Homer in the Odyssey with a black root and white blossoms. Magic plants are found, too, in Serbian mythology, where the raskovnik is supposed to be able to open any lock. In Buddhist symbolism, both the lotus and the Bodhi Tree are significant. The lotus is one of the Ashtamangala (eight auspicious signs) shared between Buddhism, Jainism and Hinduism, representing the primordial purity of body, speech, and mind, floating above the muddy waters of attachment and desire. The Bodhi Tree is the sacred fig tree under which the Buddha is said to have attained enlightenment; the name is also given to other Bodhi trees thought to have been propagated from the original tree. As participants Social sciences including anthropology, ethnography, and archaeology have long investigated human interactions with living things. Anthropology and ethnography have traditionally studied these interactions in two opposed ways: as physical resources that humans used; and as symbols or concepts through totemism and animism. More recently, these scientists have also seen living things as participants in human social interactions, in what has been called "multispecies ethnography". The anthropologists S. Eben Kirksey and Stefan Helmreich wrote: Creatures previously appearing on the margins of anthropology—as part of the landscape, as food for humans, as symbols—have been pressed into the foreground in recent ethnographies. Animals, plants, fungi, and microbes once confined in anthropological accounts to the realm of zoe or 'bare life'—that which is killable—have started to appear alongside humans in the realm of bios, with legibly biographical and political lives. Archaeology, too, has traditionally centred ecological interactions on the human side, rather than, in Suzanne E. Pilaar Birch's words, emphasizing the uniqueness of our species rather than viewing [ecological] novelty as a collective shift shared amongst multiple species and their habitats. Birch includes animals, plants, fungi, and microbes among critical interactions with humans: plants too are incredibly important determinants: for mobile hunter-gatherers, they might dictate a seasonal move; for sedentary agriculturalists, the reliability of your crop yields means the difference between survival and extinction. Fungi and microbes may also be given short shrift in archaeology because they are more difficult to study ... perhaps only ... their physical traces ... Yet they are huge determining factors that cannot be overlooked. So too we might include proteins and DNA in our summary of what might be defined as multispecies archaeology. Whereas historically, Birch states, humans saw themselves as exceptional, such as in the medieval great chain of being, an integrated multispecies approach would assemble expertise "in diverse areas, including archaeology, human-animal studies, biology, ecology, evolutionary theory, and philosophy". == References == Diffusion of innovations is a theory that seeks to explain how, why, and at what rate new ideas and technology spread. The theory was popularized by Everett Rogers in his book Diffusion of Innovations, first published in 1962. Rogers argues that diffusion is the process by which an innovation is communicated through certain channels over time among the participants in a social system. The origins of the diffusion of innovations theory are varied and span multiple disciplines. Rogers proposes that five main elements influence the spread of a new idea: the innovation itself, adopters, communication channels, time, and a social system. This process relies heavily on social capital. The innovation must be widely adopted in order to self-sustain. Within the rate of adoption, there is a point at which an innovation reaches critical mass. In 1989, management consultants working at the consulting firm Regis McKenna, Inc. theorized that this point lies at the boundary between the early adopters and the early majority. This gap between niche appeal and mass (self-sustained) adoption was originally labeled "the marketing chasm". The categories of adopters are innovators, early adopters, early majority, late majority, and laggards. Diffusion manifests itself in different ways and is highly subject to the type of adopters and innovation-decision process. The criterion for the adopter categorization is innovativeness, defined as the degree to which an individual adopts a new idea. History The concept of diffusion was first studied by the French sociologist Gabriel Tarde in late 19th century and by German and Austrian anthropologists and geographers such as Friedrich Ratzel and Leo Frobenius. The study of diffusion of innovations took off in the subfield of rural sociology in the midwestern United States in the 1920s and 1930s. Agriculture technology was advancing rapidly, and researchers started to examine how independent farmers were adopting hybrid seeds, equipment, and techniques. A study of the adoption of hybrid corn seed in Iowa by Ryan and Gross (1943) solidified the prior work on diffusion into a distinct paradigm that would be cited consistently in the future. Since its start in rural sociology, Diffusion of Innovations has been applied to numerous contexts, including medical sociology, communications, marketing, development studies, health promotion, organizational studies, knowledge management, conservation biology and complexity studies, with a particularly large impact on the use of medicines, medical techniques, and health communications. In organizational studies, its basic epidemiological or internal-influence form was formulated by H. Earl Pemberton, such as postage stamps and standardized school ethics codes. In 1962, Everett Rogers, a professor of rural sociology at Ohio State University, published his seminal work: Diffusion of Innovations. Rogers synthesized research from over 508 diffusion studies across the fields that initially influenced the theory: anthropology, early sociology, rural sociology, education, industrial sociology and medical sociology. Rogers applied it to the healthcare setting to address issues with hygiene, cancer prevention, family planning, and drunk driving. Using his synthesis, Rogers produced a theory of the adoption of innovations among individuals and organizations. Diffusion of Innovations and Rogers' later books are among the most often cited in diffusion research. His methodologies are closely followed in recent diffusion research, even as the field has expanded into, and been influenced by, other methodological disciplines such as social network analysis and communication. Elements The key elements in diffusion research are: Characteristics of innovations Studies have explored many characteristics of innovations. Meta-reviews have identified several characteristics that are common among most studies. These are in line with the characteristics that Rogers initially cited in his reviews. Rogers describes five characteristics that potential adopters evaluate when deciding whether to adopt an innovation: Compatibility: How well does this innovation fit with existing values, patterns of behavior, or tools? Trialability: Can you try it before you buy it? Relative advantage: In what way is this innovation better than the alternatives? Observability: Are its benefits noticeable? If someone else is using the innovation, can I see it being used? Simplicity / Complexity: The easier it is to learn or grasp, the faster it diffuses. These qualities interact and are judged as a whole. For example, an innovation might be extremely complex, reducing its likelihood to be adopted and diffused, but it might be very compatible with a large advantage relative to current tools. Even with this high learning curve, potential adopters might adopt the innovation anyway. Studies also identify other characteristics of innovations, but these are not as common as the ones that Rogers lists above. The fuzziness of the boundaries of the innovation can impact its adoption. Specifically, innovations with a small core and large periphery are easier to adopt. Innovations that are less risky are easier to adopt as the potential loss from failed integration is lower. Innovations that are disruptive to routine tasks, even when they bring a large relative advantage, might not be adopted because of added instability. Likewise, innovations that make tasks easier are likely to be adopted. Closely related to relative complexity, knowledge requirements are the ability barrier to use presented by the difficulty to use the innovation. Even when there are high knowledge requirements, support from prior adopters or other sources can increase the chances for adoption. Characteristics of individual adopters Like innovations, adopters have been determined to have traits that affect their likelihood to adopt an innovation. A bevy of individual personality traits have been explored for their impacts on adoption, but with little agreement. Ability and motivation, which vary on situation unlike personality traits, have a large impact on a potential adopter's likelihood to adopt an innovation. Unsurprisingly, potential adopters who are motivated to adopt an innovation are likely to make the adjustments needed to adopt it. Motivation can be impacted by the meaning that an innovation holds; innovations can have symbolic value that encourage (or discourage) adoption. First proposed by Ryan and Gross (1943), the overall connectedness of a potential adopter to the broad community represented by a city. Potential adopters who frequent metropolitan areas are more likely to adopt an innovation. Finally, potential adopters who have the power or agency to create change, particularly in organizations, are more likely to adopt an innovation than someone with less power over his choices. Complementary to the diffusion framework, behavioral models such as Technology acceptance model (TAM) and Unified theory of acceptance and use of technology (UTAUT) are frequently used to understand individual technology adoption decisions in greater details. Characteristics of organizations Organizations face more complex adoption possibilities because organizations are both the aggregate of its individuals and its own system with a set of procedures and norms. Three organizational characteristics match well with the individual characteristics above: tension for change (motivation and ability), innovation-system fit (compatibility), and assessment of implications (observability). Organizations can feel pressured by a tension for change. If the organization's situation is untenable, it will be motivated to adopt an innovation to change its fortunes. This tension often plays out among its individual members. Innovations that match the organization's pre-existing system require fewer coincidental changes and are easy to assess and more likely to be adopted. The wider environment of the organization, often an industry, community, or economy, exerts pressures on the organization, too. Where an innovation is diffusing through the organization's environment for any reason, the organization is more likely to adopt it. Innovations that are intentionally spread, including by political mandate or directive, are also likely to diffuse quickly. h individual decisions where behavioral models (e.g. TAM and UTAUT) can be used to complement the diffusion framework and reveal further details, these models are not directly applicable to organizational decisions. However, research suggested that simple behavioral models can still be used as a good predictor of organizational technology adoption when proper initial screening procedures are introduced. Process Diffusion occurs through a five–step decision-making process. It occurs through a series of communication channels over a period of time among the members of a similar social system. Ryan and Gross first identified adoption as a process in 1943. Rogers' five stages (steps): awareness, interest, evaluation, trial, and adoption are integral to this theory. An individual might reject an innovation at any time during or after the adoption process. Abrahamson examined this process critically by posing questions such as: How do technically inefficient innovations diffuse and what impedes technically efficient innovations from catching on? Abrahamson makes suggestions for how organizational scientists can more comprehensively evaluate the spread of innovations. In later editions of Diffusion of Innovation, Rogers changes his terminology of the five stages to: knowledge, persuasion, decision, implementation, and confirmation. However, the descriptions of the categories have remained similar throughout the editions. Decisions Two factors determine what type a particular decision is: Whether the decision is made freely and implemented voluntarily Who makes the decision Based on these considerations, three types of innovation-decisions have been identified. Rate of adoption The rate of adoption is defined as the relative speed at which participants adopt an innovation. Rate is usually measured by the length of time required for a certain percentage of the members of a social system to adopt an innovation. The rates of adoption for innovations are determined by an individual's adopter category. In general, individuals who first adopt an innovation require a shorter adoption period (adoption process) when compared to late adopters. Within the adoption curve at some point the innovation reaches critical mass. This is when the number of individual adopters ensures that the innovation is self-sustaining. Adoption strategies Rogers outlines several strategies in order to help an innovation reach this stage, including when an innovation adopted by a highly respected individual within a social network and creating an instinctive desire for a specific innovation. Another strategy includes injecting an innovation into a group of individuals who would readily use said technology, as well as providing positive reactions and benefits for early adopters. Diffusion vis-à-vis adoption Adoption is an individual process detailing the series of stages one undergoes from first hearing about a product to finally adopting it. Diffusion signifies a group phenomenon, which suggests how an innovation spreads. Adopter categories Rogers defines an adopter category as a classification of individuals within a social system on the basis of innovativeness. In the book Diffusion of Innovations, Rogers suggests a total of five categories of adopters in order to standardize the usage of adopter categories in diffusion research. The adoption of an innovation follows an S curve when plotted over a length of time. The categories of adopters are innovators, early adopters, early majority, late majority and laggards. In addition to the gatekeepers and opinion leaders who exist within a given community, change agents may come from outside the community. Change agents bring innovations to new communities – first through the gatekeepers, then through the opinion leaders, and so on through the community. Failed diffusion Failed diffusion does not mean that the technology was adopted by no one. Rather, failed diffusion often refers to diffusion that does not reach or approach 100% adoption due to its own weaknesses, competition from other innovations, or simply a lack of awareness. From a social networks perspective, a failed diffusion might be widely adopted within certain clusters but fail to make an impact on more distantly related people. Networks that are over-connected might suffer from a rigidity that prevents the changes an innovation might bring, as well. Sometimes, some innovations also fail as a result of lack of local involvement and community participation. For example, Rogers discussed a situation in Peru involving the implementation of boiling drinking water to improve health and wellness levels in the village of Los Molinos. The residents had no knowledge of the link between sanitation and illness. The campaign worked with the villagers to try to teach them to boil water, burn their garbage, install latrines and report cases of illness to local health agencies. In Los Molinos, a stigma was linked to boiled water as something that only the "unwell" consumed, and thus, the idea of healthy residents boiling water prior to consumption was frowned upon. The two-year educational campaign was considered to be largely unsuccessful. This failure exemplified the importance of the roles of the communication channels that are involved in such a campaign for social change. An examination of diffusion in El Salvador determined that there can be more than one social network at play as innovations are communicated. One network carries information and the other carries influence. While people might hear of an innovation's uses, in Rogers' Los Molinos sanitation case, a network of influence and status prevented adoption. Heterophily and communication channels Lazarsfeld and Merton first called attention to the principles of homophily and its opposite, heterophily. Using their definition, Rogers defines homophily as "the degree to which pairs of individuals who interact are similar in certain attributes, such as beliefs, education, social status, and the like". When given the choice, individuals usually choose to interact with someone similar to themselves. Homophilous individuals engage in more effective communication because their similarities lead to greater knowledge gain as well as attitude or behavior change. As a result, homophilous people tend to promote diffusion among each other. However, diffusion requires a certain degree of heterophily to introduce new ideas into a relationship; if two individuals are identical, no diffusion occurs because there is no new information to exchange. Therefore, an ideal situation would involve potential adopters who are homophilous in every way, except in knowledge of the innovation. Promotion of healthy behavior provides an example of the balance required of homophily and heterophily. People tend to be close to others of similar health status. As a result, people with unhealthy behaviors like smoking and obesity are less likely to encounter information and behaviors that encourage good health. This presents a critical challenge for health communications, as ties between heterophilous people are relatively weaker, harder to create, and harder to maintain. Developing heterophilous ties to unhealthy communities can increase the effectiveness of the diffusion of good health behaviors. Once one previously homophilous tie adopts the behavior or innovation, the other members of that group are more likely to adopt it, too. Role of social systems Opinion leaders Not all individuals exert an equal amount of influence over others. In this sense opinion leaders are influential in spreading either positive or negative information about an innovation. Rogers relies on the ideas of Katz & Lazarsfeld and the two-step flow theory in developing his ideas on the influence of opinion leaders. Opinion leaders have the most influence during the evaluation stage of the innovation-decision process and on late adopters. In addition opinion leaders typically have greater exposure to the mass media, more cosmopolitan, greater contact with change agents, more social experience and exposure, higher socioeconomic status, and are more innovative than others. Research was done in the early 1950s at the University of Chicago attempting to assess the cost-effectiveness of broadcast advertising on the diffusion of new products and services. The findings were that opinion leadership tended to be organized into a hierarchy within a society, with each level in the hierarchy having most influence over other members in the same level, and on those in the next level below it. The lowest levels were generally larger in numbers and tended to coincide with various demographic attributes that might be targeted by mass advertising. However, it found that direct word of mouth and example were far more influential than broadcast messages, which were only effective if they reinforced the direct influences. This led to the conclusion that advertising was best targeted, if possible, on those next in line to adopt, and not on those not yet reached by the chain of influence. Research on actor-network theory (ANT) also identifies a significant overlap between the ANT concepts and the diffusion of innovation which examine the characteristics of innovation and its context among various interested parties within a social system to assemble a network or system which implements innovation. Other research relating the concept to public choice theory finds that the hierarchy of influence for innovations need not, and likely does not, coincide with hierarchies of official, political, or economic status. Elites are often not innovators, and innovations may have to be introduced by outsiders and propagated up a hierarchy to the top decision makers. Electronic communication social networks Prior to the introduction of the Internet, it was argued that social networks had a crucial role in the diffusion of innovation particularly tacit knowledge in the book The IRG Solution – hierarchical incompetence and how to overcome it. The book argued that the widespread adoption of computer networks of individuals would lead to much better diffusion of innovations, with greater understanding of their possible shortcomings and the identification of needed innovations that would not have otherwise occurred. The social model proposed by Ryan and Gross is expanded by Valente who uses social networks as a basis for adopter categorization instead of solely relying on the system-level analysis used by Ryan and Gross. Valente also looks at an individual's personal network, which is a different application than the organizational perspective espoused by many other scholars. Recent research by Wear shows, that particularly in regional and rural areas, significantly more innovation takes place in communities which have stronger inter-personal networks. Organizations Innovations are often adopted by organizations through two types of innovation-decisions: collective innovation decisions and authority innovation decisions. The collective decision occurs when adoption is by consensus. The authority decision occurs by adoption among very few individuals with high positions of power within an organization. Unlike the optional innovation decision process, these decision processes only occur within an organization or hierarchical group. Research indicated that, with proper initial screening procedures, even simple behavioral model can serve as a good predictor for technology adoption in many commercial organizations. Within an organization certain individuals are termed "champions" who stand behind an innovation and break through opposition. The champion plays a very similar role as the champion used within the efficiency business model Six Sigma. The process contains five stages that are slightly similar to the innovation-decision process that individuals undertake. These stages are: agenda-setting, matching, redefining/restructuring, clarifying and routinizing. Extensions of the theory Policy Diffusion of Innovations has been applied beyond its original domains. In the case of political science and administration, policy diffusion focuses on how institutional innovations are adopted by other institutions, at the local, state, or country level. An alternative term is 'policy transfer' where the focus is more on the agents of diffusion and the diffusion of policy knowledge, such as in the work of Diane Stone. Specifically, policy transfer can be defined as "knowledge about how policies administrative arrangements, institutions, and ideas in one political setting (past or present) is used in the development of policies, administrative arrangements, institutions, and ideas in another political setting". The first interests with regards to policy diffusion were focused in time variation or state lottery adoption, but more recently interest has shifted towards mechanisms (emulation, learning and coercion) or in channels of diffusion where researchers find that regulatory agency creation is transmitted by country and sector channels. At the local level, examining popular city-level policies make it easy to find patterns in diffusion through measuring public awareness. At the international level, economic policies have been thought to transfer among countries according to local politicians' learning of successes and failures elsewhere and outside mandates made by global financial organizations. As a group of countries succeed with a set of policies, others follow, as exemplified by the deregulation and liberalization across the developing world after the successes of the Asian Tigers. The reintroduction of regulations in the early 2000s also shows this learning process, which would fit under the stages of knowledge and decision, can be seen as lessons learned by following China's successful growth. Technology Peres, Muller and Mahajan suggested that diffusion is "the process of the market penetration of new products and services that is driven by social influences, which include all interdependencies among consumers that affect various market players with or without their explicit knowledge". Eveland evaluated diffusion from a phenomenological view, stating, "Technology is information, and exists only to the degree that people can put it into practice and use it to achieve values". Diffusion of existing technologies has been measured using "S curves". These technologies include radio, television, VCR, cable, flush toilet, clothes washer, refrigerator, home ownership, air conditioning, dishwasher, electrified households, telephone, cordless phone, cellular phone, per capita airline miles, personal computer and the Internet. These data can act as a predictor for future innovations. Diffusion curves for infrastructure reveal contrasts in the diffusion process of personal technologies versus infrastructure. Consequences of adoption Both positive and negative outcomes are possible when an individual or organization chooses to adopt a particular innovation. Rogers states that this area needs further research because of the biased positive attitude that is associated with innovation. Rogers lists three categories for consequences: desirable vs. undesirable, direct vs. indirect, and anticipated vs. unanticipated. In contrast Wejnert details two categories: public vs. private and benefits vs. costs. Public vis-à-vis private Public consequences comprise the impact of an innovation on those other than the actor, while private consequences refer to the impact on the actor. Public consequences usually involve collective actors, such as countries, states, organizations or social movements. The results are usually concerned with issues of societal well-being. Private consequences usually involve individuals or small collective entities, such as a community. The innovations are usually concerned with the improvement of quality of life or the reform of organizational or social structures. Benefits vis-à-vis costs Benefits of an innovation obviously are the positive consequences, while the costs are the negative. Costs may be monetary or nonmonetary, direct or indirect. Direct costs are usually related to financial uncertainty and the economic state of the actor. Indirect costs are more difficult to identify. An example would be the need to buy a new kind of pesticide to use innovative seeds. Indirect costs may also be social, such as social conflict caused by innovation. Marketers are particularly interested in the diffusion process as it determines the success or failure of a new product. It is quite important for a marketer to understand the diffusion process so as to ensure proper management of the spread of a new product or service. Intended vis-à-vis Unintended The diffusion of innovations theory has been used to conduct research on the unintended consequences of new interventions in public health. In the book multiple examples of the unintended negative consequences of technological diffusion are given. The adoption of automatic tomato pickers developed by Midwest agricultural colleges led to the adoption of harder tomatoes (disliked by consumers) and the loss of thousands of jobs leading to the collapse of thousands of small farmers. In another example, the adoption of snowmobiles in Saami reindeer herding culture is found to lead to the collapse of their society with widespread alcoholism and unemployment for the herders, ill-health for the reindeer (such as stress ulcers, miscarriages) and a huge increase in inequality. Mathematical treatment The diffusion of an innovation typically follows an S-shaped curve which often resembles a logistic function. Roger's diffusion model concludes that the popularity of a new product will grow with time to a saturation level and then decline, but it cannot predict how much time it will take and what the saturation level will be. Bass (1969) and many other researchers proposed modeling the diffusion based on parametric formulas to fill this gap and to provide a means for a quantitative forecast of adoption timing and levels. The Bass model focuses on the first two (Introduction and Growth). Some of the Bass-Model extensions present mathematical models for the last two (Maturity and Decline). MS-Excel or other tools can be used to solve the Bass model equations, and other diffusion models equations, numerically. Mathematical programming models such as the S-D model apply the diffusion of innovations theory to real data problems. In addition to that, agent-based models follow a more intuitive process by designing individual-level rules to model the diffusion of ideas and innovations. Complex systems models Complex network models can also be used to investigate the spread of innovations among individuals connected to each other by a network of peer-to-peer influences, such as in a physical community or neighborhood. Such models represent a system of individuals as nodes in a network (or graph). The interactions that link these individuals are represented by the edges of the network and can be based on the probability or strength of social connections. In the dynamics of such models, each node is assigned a current state, indicating whether or not the individual has adopted the innovation, and model equations describe the evolution of these states over time. In threshold models, the uptake of technologies is determined by the balance of two factors: the (perceived) usefulness (sometimes called utility) of the innovation to the individual as well as barriers to adoption, such as cost. The multiple parameters that influence decisions to adopt, both individual and socially motivated, can be represented by such models as a series of nodes and connections that represent real relationships. Borrowing from social network analysis, each node is an innovator, an adopter, or a potential adopter. Potential adopters have a threshold, which is a fraction of his neighbors who adopt the innovation that must be reached before he will adopt. Over time, each potential adopter views his neighbors and decides whether he should adopt based on the technologies they are using. When the effect of each individual node is analyzed along with its influence over the entire network, the expected level of adoption was seen to depend on the number of initial adopters and the network's structure and properties. Two factors emerge as important to successful spread of the innovation: the number of connections of nodes with their neighbors and the presence of a high degree of common connections in the network (quantified by the clustering coefficient). These models are particularly good at showing the impact of opinion leaders relative to others. Computer models are often used to investigate this balance between the social aspects of diffusion and perceived intrinsic benefit to the individuals. Criticism Even though there have been more than four thousand articles across many disciplines published on Diffusion of Innovations, with a vast majority written after Rogers created a systematic theory, there have been few widely adopted changes to the theory. Although each study applies the theory in slightly different ways, critics say this lack of cohesion has left the theory stagnant and difficult to apply with consistency to new problems. Diffusion is difficult to quantify because humans and human networks are complex. It is extremely difficult, if not impossible, to measure what exactly causes adoption of an innovation. This variety of variables has also led to inconsistent results in research, reducing heuristic value. Compared to other modes of diffusion in natural sciences, diffusion models of innovation also lack a clear understanding of the spatial structure on which innovation is propagated. Product management can shape the topology of the diffusion space in numerous different ways by the means of segmentation, product portfolios, and lifecycle management. Rogers placed the contributions and criticisms of diffusion research into four categories: pro-innovation bias, individual-blame bias, recall problem, and issues of equality. The pro-innovation bias, in particular, implies that all innovation is positive and that all innovations should be adopted. Cultural traditions and beliefs can be consumed by another culture's through diffusion, which can impose significant costs on a group of people. The one-way information flow, from sender to receiver, is another weakness of this theory. The message sender has a goal to persuade the receiver, and there is little to no reverse flow. The person implementing the change controls the direction and outcome of the campaign. In some cases, this is the best approach, but other cases require a more participatory approach. In complex environments where the adopter is receiving information from many sources and is returning feedback to the sender, a one-way model is insufficient and multiple communication flows need to be examined. See also References Stone, Diane (January 2004). "Transfer agents and global networks in the 'transnationalization' of policy" (PDF). Journal of European Public Policy (Submitted manuscript). 11 (3): 545–566. doi:10.1080/13501760410001694291. S2CID 153837868. Stone, Diane (January 2000). "Non-governmental policy transfer: the strategies of independent policy institutes". Governance. 13 (1): 45–70. doi:10.1111/0952-1895.00123. Stone, Diane (February 1999). "Learning lessons and transferring policy across time, space and disciplines". Politics. 19 (1): 51–59. doi:10.1111/1467-9256.00086. S2CID 143819195. Noel, Hayden (2009). Consumer behaviour. Lausanne, Switzerland La Vergne, TN: AVA Academia Distributed in the USA by Ingram Publisher Services. ISBN 9782940439249. Loudon, David L.; Bitta, Albert J. Della (1993). Consumer behavior: concepts and applications. McGraw-Hill Series in Marketing (4th ed.). New York: McGraw-Hill. ISBN 9780070387584. Rogers, Everett M. (1962). Diffusion of innovations (1st ed.). New York: Free Press of Glencoe. OCLC 254636. Rogers, Everett M. (1983). Diffusion of innovations (3rd ed.). New York: Free Press of Glencoe. ISBN 9780029266502. Wejnert, Barbara (August 2002). "Integrating models of diffusion of innovations: a conceptual framework". Annual Review of Sociology. 28: 297–326. doi:10.1146/annurev.soc.28.110601.141051. JSTOR 3069244. S2CID 14699184. Notes External links The Diffusion Simulation Game, about adopting an innovation in education. The Pencil Metaphor on diffusion of innovation. Diffusion of Innovations, Strategy and Innovations. The D.S.I Framework by Francisco Rodrigues Gomes, Academia.edu share research. Water marble nails are a finger nail art technique involving dropping nail lacquers into clear water and creating a pattern on the water surface; the pattern is then transferred to the nails. History The water marble nail technique was originally developed by professional nail technicians in Japanese nail salons. In the 1990s, it was popularized by commercial publications released by shopping centers in Chiba, Japan. In 2010, water marble nail art was adapted to use acrylic artificial nails and gels. The water marble nail technique has gained popularity across the globe through features in magazines, websites, polish makers and videos. Styles There are two main types of water marble nail art methods: free-dragging and free-dropping. Free-dragging is more common. Free-dragging Dragged patterns range from simple circular shapes to complicated drawings. Patterns such as marble, hearts/peacocks, animals, flowers, leaves, parallel lines, psychedelics, spiderwebs, and random patterns in the style of designer Emilio Pucci are generally performed with nail lacquers and some kind of tool. Free-dropping Dropped patterns range from colored shapes to complicated drawings such as spirals and geometric designs. The free patterns are created by colored drops of nail lacquer. For free-dropping, lacquer colors are dropped straight or diagonally onto the water. The pattern floats on the surface of the water. Technique Water marble nail art requires clean water, nail lacquers for free-dropping, and a stick for drawing patterns. Before patterns are created, the nails are painted with a light-colored nail polish that establishes a good contrast with the colors chosen to create the water marble. Some techniques use a matte coat to provide contrast and maintain an even look. After the base coat has dried, tape is put around the fingers to gather the excess nail polish after the marbling. One or two drops of colored lacquer are chosen for the design and are added to a cup of clean water. The drops will create a circle on the water surface. The next color is added on top of the circle created by the previous drop. The resulting pattern is ready for application to the nail, but it can still be modified with a stick or toothpick to create different shapes. The nail is dipped into the pattern on the water and kept under water as a Q-tip is used to "grab" the remaining polish. Examples Notes == References == Pop art is an art movement that emerged in the United Kingdom and the United States during the mid- to late 1950s. The movement presented a challenge to traditions of fine art by including imagery from popular and mass culture, such as advertising, comic books and mundane mass-produced objects. One of its aims is to use images of popular culture in art, emphasizing the banal or kitschy elements of any culture, most often through the use of irony. It is also associated with the artists' use of mechanical means of reproduction or rendering techniques. In pop art, material is sometimes visually removed from its known context, isolated, or combined with unrelated material. Amongst the first artists that shaped the pop art movement were Eduardo Paolozzi and Richard Hamilton in Britain, and Larry Rivers, Ray Johnson, Robert Rauschenberg and Jasper Johns among others in the United States. Pop art is widely interpreted as a reaction to the then-dominant ideas of abstract expressionism, as well as an expansion of those ideas. Due to its utilization of found objects and images, it is similar to Dada. Pop art and minimalism are considered to be art movements that precede postmodern art, or are some of the earliest examples of postmodern art themselves. Pop art often takes imagery that is currently in use in advertising. Product labeling and logos figure prominently in the imagery chosen by pop artists, seen in the labels of Campbell's Soup Cans, by Andy Warhol. Even the labeling on the outside of a shipping box containing food items for retail has been used as subject matter in pop art, as demonstrated by Warhol's Campbell's Tomato Juice Box, 1964 (pictured). Origins The origins of pop art in North America developed differently from those in Great Britain. In the United States, pop art emerged as a reaction by artists; it marked a return to hard-edged composition and representational art. The artists employed impersonal, mundane reality, irony, and parody to "defuse" the personal symbolism and "painterly looseness" of abstract expressionism. In the U.S., some works by Larry Rivers, Alex Katz and Man Ray anticipated pop art. By contrast, the origins of pop art in post-war Britain, while employing irony and parody, were more academic. British artists focused on the dynamic and paradoxical imagery of American pop culture as powerful, manipulative symbolic devices that were affecting whole patterns of life, while simultaneously improving the prosperity of a society. Early pop art in Britain was a matter of ideas fueled by American popular culture when viewed from afar. Similarly, pop art was both an extension and a repudiation of Dadaism. While pop art and Dadaism explored some of the same subjects, pop art replaced the destructive, satirical, and anarchic impulses of the Dada movement with a detached affirmation of the artifacts of mass culture. Among those artists in Europe seen as producing work leading up to pop art are: Pablo Picasso, Marcel Duchamp, and Kurt Schwitters. Proto-pop Although both British and American pop art began during the 1950s, Marcel Duchamp and others in Europe like Francis Picabia and Man Ray predate the movement; in addition there were some earlier American proto-pop origins which utilized "as found" cultural objects. During the 1920s, American artists Patrick Henry Bruce, Gerald Murphy, Charles Demuth and Stuart Davis created paintings that contained pop culture imagery (mundane objects culled from American commercial products and advertising design), almost "prefiguring" the pop art movement. United Kingdom: the Independent Group The Independent Group (IG), founded in London in 1952, is regarded as the precursor to the pop art movement. They were a gathering of young painters, sculptors, architects, writers and critics who were challenging prevailing modernist approaches to culture as well as traditional views of fine art. Their group discussions centered on pop culture implications from elements such as mass advertising, movies, product design, comic strips, science fiction and technology. At the first Independent Group meeting in 1952, co-founding member, artist and sculptor Eduardo Paolozzi presented a lecture using a series of collages titled Bunk! that he had assembled during his time in Paris between 1947 and 1949. This material of "found objects" such as advertising, comic book characters, magazine covers and various mass-produced graphics mostly represented American popular culture. One of the collages in that presentation was Paolozzi's I was a Rich Man's Plaything (1947), which includes the first use of the word "pop", appearing in a cloud of smoke emerging from a revolver. Following Paolozzi's seminal presentation in 1952, the IG focused primarily on the imagery of American popular culture, particularly mass advertising. According to the son of John McHale, the term "pop art" was first coined by his father in 1954 in conversation with Frank Cordell, although other sources credit its origin to British critic Lawrence Alloway. (Both versions agree that the term was used in Independent Group discussions by mid-1955.) "Pop art" as a moniker was then used in discussions by IG members in the Second Session of the IG in 1955, and the specific term "pop art" first appeared in published print in the article "But Today We Collect Ads" by IG members Alison and Peter Smithson in Ark magazine in 1956. However, the term is often credited to British art critic/curator Lawrence Alloway for his 1958 essay titled The Arts and the Mass Media, even though the precise language he uses is "popular mass culture". "Furthermore, what I meant by it then is not what it means now. I used the term, and also 'Pop Culture' to refer to the products of the mass media, not to works of art that draw upon popular culture. In any case, sometime between the winter of 1954–55 and 1957 the phrase acquired currency in conversation..." Nevertheless, Alloway was one of the leading critics to defend the inclusion of the imagery of mass culture in the fine arts. Alloway clarified these terms in 1966, at which time pop art had already transited from art schools and small galleries to a major force in the artworld. But its success had not been in England. Practically simultaneously, and independently, New York City had become the hotbed for pop art. In London, the annual Royal Society of British Artists (RBA) exhibition of young talent in 1960 first showed American pop influences. In January 1961, the most famous RBA-Young Contemporaries of all put David Hockney, the American R B Kitaj, New Zealander Billy Apple, Allen Jones, Derek Boshier, Joe Tilson, Patrick Caulfield, Peter Phillips, Pauline Boty and Peter Blake on the map; Apple designed the posters and invitations for both the 1961 and 1962 Young Contemporaries exhibitions. Hockney, Kitaj and Blake went on to win prizes at the John-Moores-Exhibition in Liverpool in the same year. Apple and Hockney traveled together to New York during the Royal College's 1961 summer break, which is when Apple first made contact with Andy Warhol – both later moved to the United States and Apple became involved with the New York pop art scene. United States Although pop art began in the early 1950s, in America it was given its greatest impetus during the 1960s. The term "pop art" was officially introduced in December 1962; the occasion was a "Symposium on Pop Art" organized by the Museum of Modern Art. By this time, American advertising had adopted many elements of modern art and functioned at a very sophisticated level. Consequently, American artists had to search deeper for dramatic styles that would distance art from the well-designed and clever commercial materials. As the British viewed American popular culture imagery from a somewhat removed perspective, their views were often instilled with romantic, sentimental and humorous overtones. By contrast, American artists, bombarded every day with the diversity of mass-produced imagery, produced work that was generally more bold and aggressive. According to historian, curator and critic Henry Geldzahler, "Ray Johnson's collages Elvis Presley No. 1 and James Dean stand as the Plymouth Rock of the Pop movement." Author Lucy Lippard wrote that "The Elvis ... and Marilyn Monroe [collages] ... heralded Warholian Pop." Johnson worked as a graphic designer, met Andy Warhol by 1956 and both designed several book covers for New Directions and other publishers. Johnson began mailing out whimsical flyers advertising his design services printed via offset lithography. He later became known as the father of mail art as the founder of his "New York Correspondence School," working small by stuffing clippings and drawings into envelopes rather than working larger like his contemporaries. A note about the cover image in January 1958's Art News pointed out that "[Jasper] Johns' first one-man show ... places him with such better-known colleagues as Rauschenberg, Twombly, Kaprow and Ray Johnson". Indeed, two other important artists in the establishment of America's pop art vocabulary were the painters Jasper Johns and Robert Rauschenberg. Rauschenberg, who like Ray Johnson attended Black Mountain College in North Carolina after World War II, was influenced by the earlier work of Kurt Schwitters and other Dada artists, and his belief that "painting relates to both art and life" challenged the dominant modernist perspective of his time. His use of discarded readymade objects (in his Combines) and pop culture imagery (in his silkscreen paintings) connected his works to topical events in everyday America. The silkscreen paintings of 1962–64 combined expressive brushwork with silkscreened magazine clippings from Life, Newsweek, and National Geographic. Johns' paintings of flags, targets, numbers, and maps of the U.S. as well three-dimensional depictions of ale cans drew attention to questions of representation in art. Johns' and Rauschenberg's work of the 1950s is frequently referred to as Neo-Dada, and is visually distinct from the prototypical American pop art which exploded in the early 1960s. Roy Lichtenstein is of equal importance to American pop art. His work, and its use of parody, probably defines the basic premise of pop art better than any other. Selecting the old-fashioned comic strip as subject matter, Lichtenstein produces a hard-edged, precise composition that documents while also parodying in a soft manner. Lichtenstein used oil and Magna paint in his best known works, such as Drowning Girl (1963), which was appropriated from the lead story in DC Comics' Secret Hearts #83. (Drowning Girl is part of the collection of the Museum of Modern Art.) His work features thick outlines, bold colors and Ben-Day dots to represent certain colors, as if created by photographic reproduction. Lichtenstein said, "[abstract expressionists] put things down on the canvas and responded to what they had done, to the color positions and sizes. My style looks completely different, but the nature of putting down lines pretty much is the same; mine just don't come out looking calligraphic, like Pollock's or Kline's." Pop art merges popular and mass culture with fine art while injecting humor, irony, and recognizable imagery/content into the mix. The paintings of Lichtenstein, like those of Andy Warhol, Tom Wesselmann and others, share a direct attachment to the commonplace image of American popular culture, but also treat the subject in an impersonal manner clearly illustrating the idealization of mass production. Andy Warhol is probably the most famous figure in pop art. In fact, art critic Arthur Danto once called Warhol "the nearest thing to a philosophical genius the history of art has produced". Warhol attempted to take pop beyond an artistic style to a life style, and his work often displays a lack of human affectation that dispenses with the irony and parody of many of his peers. Early U.S. exhibitions Claes Oldenburg, Jim Dine and Tom Wesselmann had their first shows in the Judson Gallery in 1959 and 1960 and later in 1960 through 1964 along with James Rosenquist, George Segal and others at the Green Gallery on 57th Street in Manhattan. In 1960, Martha Jackson showed installations and assemblages, New Media – New Forms featured Hans Arp, Kurt Schwitters, Jasper Johns, Claes Oldenburg, Robert Rauschenberg, Jim Dine and May Wilson. 1961 was the year of Martha Jackson's spring show, Environments, Situations, Spaces. Andy Warhol held his first solo exhibition in Los Angeles in July 1962 at Irving Blum's Ferus Gallery, where he showed 32 paintings of Campbell's soup cans, one for every flavor. Warhol sold the set of paintings to Blum for $1,000; in 1996, when the Museum of Modern Art acquired it, the set was valued at $15 million. Donald Factor, the son of Max Factor Jr., and an art collector and co-editor of avant-garde literary magazine Nomad, wrote an essay in the magazine's last issue, Nomad/New York. The essay was one of the first on what would become known as pop art, though Factor did not use the term. The essay, "Four Artists", focused on Roy Lichtenstein, James Rosenquist, Jim Dine, and Claes Oldenburg. In the 1960s, Oldenburg, who became associated with the pop art movement, created many happenings, which were performance art-related productions of that time. The name he gave to his own productions was "Ray Gun Theater". The cast of colleagues in his performances included: artists Lucas Samaras, Tom Wesselmann, Carolee Schneemann, Öyvind Fahlström and Richard Artschwager; dealer Annina Nosei; art critic Barbara Rose; and screenwriter Rudy Wurlitzer. His first wife, Patty Mucha, who sewed many of his early soft sculptures, was a constant performer in his happenings. This brash, often humorous, approach to art was at great odds with the prevailing sensibility that, by its nature, art dealt with "profound" expressions or ideas. In December 1961, he rented a store on Manhattan's Lower East Side to house The Store, a month-long installation he had first presented at the Martha Jackson Gallery in New York, stocked with sculptures roughly in the form of consumer goods. Opening in 1962, Willem de Kooning's New York art dealer, the Sidney Janis Gallery, organized the groundbreaking International Exhibition of the New Realists, a survey of new-to-the-scene American, French, Swiss, Italian New Realism, and British pop art. The fifty-four artists shown included Richard Lindner, Wayne Thiebaud, Roy Lichtenstein (and his painting Blam), Andy Warhol, Claes Oldenburg, James Rosenquist, Jim Dine, Robert Indiana, Tom Wesselmann, George Segal, Peter Phillips, Peter Blake (The Love Wall from 1961), Öyvind Fahlström, Yves Klein, Arman, Daniel Spoerri, Christo and Mimmo Rotella. The show was seen by Europeans Martial Raysse, Niki de Saint Phalle and Jean Tinguely in New York, who were stunned by the size and look of the American artwork. Also shown were Marisol, Mario Schifano, Enrico Baj and Öyvind Fahlström. Janis lost some of his abstract expressionist artists when Mark Rothko, Robert Motherwell, Adolph Gottlieb and Philip Guston quit the gallery, but gained Dine, Oldenburg, Segal and Wesselmann. At an opening-night soiree thrown by collector Burton Tremaine, Willem de Kooning appeared and was turned away by Tremaine, who ironically owned a number of de Kooning's works. Rosenquist recalled: "at that moment I thought, something in the art world has definitely changed". Turning away a respected abstract artist proved that, as early as 1962, the pop art movement had begun to dominate art culture in New York. A bit earlier, on the West Coast, Roy Lichtenstein, Jim Dine and Andy Warhol from New York City; Phillip Hefferton and Robert Dowd from Detroit; Edward Ruscha and Joe Goode from Oklahoma City; and Wayne Thiebaud from California were included in the New Painting of Common Objects show. This first pop art museum exhibition in America was curated by Walter Hopps at the Pasadena Art Museum. Pop art was ready to change the art world. New York followed Pasadena in 1963, when the Guggenheim Museum exhibited Six Painters and the Object, curated by Lawrence Alloway. The artists were Jim Dine, Jasper Johns, Roy Lichtenstein, Robert Rauschenberg, James Rosenquist, and Andy Warhol. Another pivotal early exhibition was The American Supermarket organised by the Bianchini Gallery in 1964. The show was presented as a typical small supermarket environment, except that everything in it—the produce, canned goods, meat, posters on the wall, etc.—was created by prominent pop artists of the time, including Apple, Warhol, Lichtenstein, Wesselmann, Oldenburg, and Johns. This project was recreated in 2002 as part of the Tate Gallery's Shopping: A Century of Art and Consumer Culture. By 1962, pop artists started exhibiting in commercial galleries in New York and Los Angeles; for some, it was their first commercial one-man show. The Ferus Gallery presented Andy Warhol in Los Angeles (and Ed Ruscha in 1963). In New York, the Green Gallery showed Rosenquist, Segal, Oldenburg, and Wesselmann. The Stable Gallery showed R. Indiana and Warhol (in his first New York show). The Leo Castelli Gallery presented Rauschenberg, Johns, and Lichtenstein. Martha Jackson showed Jim Dine and Allen Stone showed Wayne Thiebaud. By 1966, after the Green Gallery and the Ferus Gallery closed, the Leo Castelli Gallery represented Rosenquist, Warhol, Rauschenberg, Johns, Lichtenstein and Ruscha. The Sidney Janis Gallery represented Oldenburg, Segal, Dine, Wesselmann and Marisol, while Allen Stone continued to represent Thiebaud, and Martha Jackson continued representing Robert Indiana. In 1968, the São Paulo 9 Exhibition – Environment U.S.A.: 1957–1967 featured the "Who's Who" of pop art. Considered as a summation of the classical phase of the American pop art period, the exhibit was curated by William Seitz. The artists were Edward Hopper, James Gill, Robert Indiana, Jasper Johns, Roy Lichtenstein, Claes Oldenburg, Robert Rauschenberg, Andy Warhol and Tom Wesselmann. France Nouveau réalisme refers to an artistic movement founded in 1960 by the art critic Pierre Restany and the artist Yves Klein during the first collective exposition in the Apollinaire gallery in Milan. Pierre Restany wrote the original manifesto for the group, titled the "Constitutive Declaration of New Realism," in April 1960, proclaiming, "Nouveau Réalisme—new ways of perceiving the real." This joint declaration was signed on 27 October 1960, in Yves Klein's workshop, by nine people: Yves Klein, Arman, Martial Raysse, Pierre Restany, Daniel Spoerri, Jean Tinguely and the Ultra-Lettrists, Francois Dufrêne, Raymond Hains, Jacques de la Villeglé; in 1961 these were joined by César, Mimmo Rotella, then Niki de Saint Phalle and Gérard Deschamps. The artist Christo showed with the group. It was dissolved in 1970. Contemporary of American pop art—often conceived as its transposition in France—new realism was along with Fluxus and other groups one of the numerous tendencies of the avant-garde in the 1960s. The group initially chose Nice, on the French Riviera, as its home base since Klein and Arman both originated there; new realism is thus often retrospectively considered by historians to be an early representative of the École de Nice movement. In spite of the diversity of their plastic language, they perceived a common basis for their work; this being a method of direct appropriation of reality, equivalent, in the terms used by Restany; to a "poetic recycling of urban, industrial and advertising reality". Spain In Spain, the study of pop art is associated with the "new figurative", which arose from the roots of the crisis of informalism. Eduardo Arroyo could be said to fit within the pop art trend, on account of his interest in the environment, his critique of our media culture which incorporates icons of both mass media communication and the history of painting, and his scorn for nearly all established artistic styles. However, the Spanish artist who could be considered most authentically part of "pop" art is Alfredo Alcaín, because of the use he makes of popular images and empty spaces in his compositions. Also in the category of Spanish pop art is the "Chronicle Team" (El Equipo Crónica), which existed in Valencia between 1964 and 1981, formed by the artists Manolo Valdés and Rafael Solbes. Their movement can be characterized as "pop" because of its use of comics and publicity images and its simplification of images and photographic compositions. Filmmaker Pedro Almodóvar emerged from Madrid's "La Movida" subculture of the 1970s making low budget super 8 pop art movies, and he was subsequently called the Andy Warhol of Spain by the media at the time. In the book Almodovar on Almodovar, he is quoted as saying that the 1950s film "Funny Face" was a central inspiration for his work. One pop trademark in Almodovar's films is that he always produces a fake commercial to be inserted into a scene. New Zealand In New Zealand, pop art has predominately flourished since the 1990s, and is often connected to Kiwiana. Kiwiana is a pop-centered, idealised representation of classically Kiwi icons, such as meat pies, kiwifruit, tractors, jandals, Four Square supermarkets; the inherent campness of this is often subverted to signify cultural messages. Dick Frizzell is a famous New Zealand pop artist, known for using older Kiwiana symbols in ways that parody modern culture. For example, Frizzell enjoys imitating the work of foreign artists, giving their works a unique New Zealand view or influence. This is done to show New Zealand's historically subdued impact on the world; naive art is connected to Aotearoan pop art this way. This can be also done in an abrasive and deadpan way, as with Michel Tuffery's famous work Pisupo Lua Afe (Corned Beef 2000). Of Samoan ancestry, Tuffery constructed the work, which represents a bull, out of processed food cans known as pisupo. It is an unusual work of western pop art because Tuffery includes themes of neocolonialism and racism against non-western cultures (signified by the food cans the work is made of, which represent economic dependence brought on Samoans by the west). The undeniable indigenous viewpoint makes it stand out against more common non-indigenous works of pop art. Other New Zealand pop artists to deal with similar subject matter are Māori artists Michael Parekōwhai and Reuben Paterson. One of New Zealand's earliest and famous pop artists is Billy Apple, one of the few non-British members of the Royal Society of British Artists. Featured among the likes of David Hockney, American R.B. Kitaj and Peter Blake in the January 1961 RBA exhibition Young Contemporaries, Apple quickly became an iconic international artist of the 1960s. This was before he conceived his moniker of "Billy Apple", and his work was displayed under his birth name of Barrie Bates. He sought to distinguish himself by appearance as well as name, so bleached his hair and eyebrows with Lady Clairol Instant Creme Whip. Later, Apple was associated with the 1970s Conceptual Art movement. Japan In Japan, pop art evolved from the nation's prominent avant-garde scene. The use of images of the modern world, copied from magazines in the photomontage-style paintings produced by Harue Koga in the late 1920s and early 1930s, foreshadowed elements of pop art. The Japanese Gutai movement led to a 1958 Gutai exhibition at Martha Jackson's New York gallery that preceded by two years her famous New Forms New Media show that put pop art on the map. The work of Yayoi Kusama contributed to the development of pop art and influenced many other artists, including Andy Warhol. In the mid-1960s, graphic designer Tadanori Yokoo became one of the most successful pop artists and an international symbol for Japanese pop art. He is well known for his advertisements and creating artwork for pop culture icons such as commissions from The Beatles, Marilyn Monroe, and Elizabeth Taylor, among others. Another leading pop artist at that time was Keiichi Tanaami. Iconic characters from Japanese manga and anime have also become symbols for pop art, such as Speed Racer and Astro Boy. Japanese manga and anime also influenced later pop artists such as Takashi Murakami and his superflat movement. Italy In Italy, by 1964 pop art was known and took different forms, such as the "Scuola di Piazza del Popolo" in Rome, with pop artists such as Mario Schifano, Franco Angeli, Giosetta Fioroni, Tano Festa, Claudio Cintoli, and some artworks by Piero Manzoni, Lucio Del Pezzo, Mimmo Rotella and Valerio Adami. Italian pop art originated in 1950s culture – the works of the artists Enrico Baj and Mimmo Rotella to be precise, rightly considered the forerunners of this scene. In fact, it was around 1958–1959 that Baj and Rotella abandoned their previous careers (which might be generically defined as belonging to a non-representational genre, despite being thoroughly post-Dadaist), to catapult themselves into a new world of images, and the reflections on them, which was springing up all around them. Rotella's torn posters showed an ever more figurative taste, often explicitly and deliberately referring to the great icons of the times. Baj's compositions were steeped in contemporary kitsch, which turned out to be a "gold mine" of images and the stimulus for an entire generation of artists. The novelty came from the new visual panorama, both inside "domestic walls" and out-of-doors. Cars, road signs, television, all the "new world", everything can belong to the world of art, which itself is new. In this respect, Italian pop art takes the same ideological path as that of the international scene. The only thing that changes is the iconography and, in some cases, the presence of a more critical attitude toward it. Even in this case, the prototypes can be traced back to the works of Rotella and Baj, both far from neutral in their relationship with society. Yet this is not an exclusive element; there is a long line of artists, including Gianni Ruffi, Roberto Barni, Silvio Pasotti, Umberto Bignardi, and Claudio Cintoli, who take on reality as a toy, as a great pool of imagery from which to draw material with disenchantment and frivolity, questioning the traditional linguistic role models with a renewed spirit of "let me have fun" à la Aldo Palazzeschi. Belgium In Belgium, pop art was represented to some extent by Paul Van Hoeydonck, whose sculpture Fallen Astronaut was left on the Moon during one of the Apollo missions, as well as by other notable pop artists. Internationally recognized artists such as Marcel Broodthaers ( 'vous êtes doll? "), Evelyne Axell and Panamarenko are indebted to the pop art movement; Broodthaers's great influence was George Segal. Another well-known artist, Roger Raveel, mounted a birdcage with a real live pigeon in one of his paintings. By the end of the 1960s and early 1970s, pop art references disappeared from the work of some of these artists when they started to adopt a more critical attitude towards America because of the Vietnam War's increasingly gruesome character. Panamarenko, however, has retained the irony inherent in the pop art movement up to the present day. Evelyne Axell from Namur was a prolific pop-artist in the 1964–1972 period. Axell was one of the first female pop artists, had been mentored by Magritte and her best-known painting is Ice Cream. Netherlands While there was no formal pop art movement in the Netherlands, there were a group of artists that spent time in New York during the early years of pop art, and drew inspiration from the international pop art movement. Representatives of Dutch pop art include Daan van Golden, Gustave Asselbergs, Jacques Frenken, Jan Cremer, Wim T. Schippers, and Woody van Amen. They opposed the Dutch petit bourgeois mentality by creating humorous works with a serious undertone. Examples of this nature include Sex O'Clock, by Woody van Amen, and Crucifix / Target, by Jacques Frenken. Russia Russia arrived later to the movement, with pop-esque pieces emerging in the 1970s. This was likely a result of Russia's postwar political climate, which closely supervised artistic expression. Russia's version of pop art was Soviet-themed and was referred to as Sots Art. Compared to western pop art, it functioned as a counter-culture reaction against the state's approved art-movements. Afer the fall of the Berlin Wall, Russian pop art took on another form, epitomised by Dmitri Vrubel and his painting My God, Help Me to Survive This Deadly Love. Notable artists See also References Further reading Bloch, Mark. The Brooklyn Rail. "Gutai: 1953 –1959", June 2018. Diggory, Terence (2013) Encyclopedia of the New York School Poets (Facts on File Library of American Literature). ISBN 978-1-4381-4066-7 Francis, Mark and Foster, Hal (2010) Pop. London and New York: Phaidon. Haskell, Barbara (1984) BLAM! The Explosion of Pop, Minimalism and Performance 1958–1964. New York: W.W. Norton & Company, Inc. in association with the Whitney Museum of American Art. Lifshitz, Mikhail, The Crisis of Ugliness: From Cubism to Pop-Art. Translated and with an Introduction by David Riff. Leiden: BRILL, 2018 (originally published in Russian by Iskusstvo, 1968). Lippard, Lucy R. (1966) Pop Art, with contributions by Lawrence Alloway, Nancy Marmer, Nicolas Calas, Frederick A. Praeger, New York. Selz, Peter (moderator); Ashton, Dore; Geldzahler, Henry; Kramer, Hilton; Kunitz, Stanley and Steinberg, Leo (April 1963) "A symposium on Pop Art" Arts Magazine, pp. 36–45. Transcript of symposium held at the Museum of Modern Art on 13 December 1962. External links Pop Art: A Brief History, MoMA Learning Pop Art in Modern and Contemporary Art, The Met Brooklyn Museum Exhibitions: Seductive Subversion: Women Pop Artists, 1958–1968, Oct. 2010-Jan. 2011 Brooklyn Museum, Wiki/Pop (Women Pop Artists) Tate Glossary term for Pop art This is a list of cultural heritage sites that have been damaged or destroyed accidentally, deliberately, or by a natural disaster. The list is sorted by continent, then by country. Cultural heritage can be subdivided into two main types: tangible and intangible. Tangible heritage includes built heritage (such as religious buildings, museums, monuments, and archaeological sites) and movable heritage (such as works of art and manuscripts). Intangible cultural heritage includes customs, music, fashion, and other traditions. This article mainly deals with the destruction of built heritage; the destruction of movable collectible heritage is dealt with in art destruction, whilst the destruction of movable industrial heritage remains almost totally ignored. The deliberate and systematic destruction of cultural heritage, such as that carried out by ISIL and other terrorist organizations, is regarded as a form of cultural genocide. Africa Egypt The Library of Alexandria was destroyed during the Palmyrene invasion of Egypt and the following Roman counterattack during the 3rd century AD. The Lighthouse of Alexandria, one of the Seven Wonders of the Ancient World, was damaged then destroyed by earthquakes over the 8th to 14th centuries, collapsing around 1341 CE. Villa Aghion, a modernist villa built by Auguste Perret in Alexandria was demolished for redevelopment in 2014. A rock-cut portion of the Temple of Gerf Hussein, flooded by Lake Nasser. The building the Institut d'Égypte occupied was burned down and many documents with it on 17 December 2011 during anti-government demonstrations of the Arab Spring. It reopened in December 2012. Libya During the 2011 Libyan Civil War, various sites were vandalized, looted, or destroyed. The Islamic State destroyed the tomb of Zuhayr ibn Qays, located at the As-Sahabah Mosque in Derna, in 2012 and May 2014. In March 2015, during the second civil war, the Islamic State destroyed Sufi shrines near Tripoli. Madagascar In November 1995, a fire broke out in the Rova of Antananarivo, a royal palace of the Merina Kingdom since the 17th century. The fire destroyed or severely damaged all of its buildings. The last two reconstruction phases started in 2010, and by July 2020 the entire structure had been refurbished. Mali Parts of the World Heritage Site of Timbuktu were intentionally destroyed in the aftermath of the 2012 Fall of Timbuktu. Nigeria During the Benin Expedition of 1897, the British Empire launched a military campaign against Benin City, the capital of the Kingdom of Benin, during which much of the city was burned and numerous artefacts were looted. South Africa The 2021 Table Mountain fire partially or completely gutted several significant buildings and collections in the University of Cape Town. This included: Mostert's Mill, which had been built in 1796 The university's Special Collections Library, which held over 1,300 collections and over 85,000 books and other items, including, A historically significant Bible, An original illustration of The Jungle Book, drawings, Maps and transcripts of stories from the indigenous peoples of the Cape, A major dictionary of the Xhosa language; copies of historic Xhosa language newspapers, Papers by Ray Alexander Simons, and archives of papers relating to many anti-apartheid movements. The fire completely gutted the Library's Reading Room. The vast majority of the African Studies Published Print Collection (about 70,000 items) and the entirety of the African Studies Film Collection DVDs (about 3,500 items) was destroyed, along with documents relating to the university itself. Sudan Faras Cathedral was flooded by Lake Nasser. Some paintings were salvaged and are now in the Faras Gallery in Warsaw in Poland, and others were transferred to the National Museum of Sudan in Khartoum, which may now also be lost or destroyed as a result of the Sudanese civil war. Most of the artifacts from the National Museum of Sudan were stolen or destroyed by the Rapid Support Forces between 2023 and 2025 during the Sudanese civil war. Zimbabwe The medieval city of Great Zimbabwe has faced the removal of gold and artifacts in amateur digging by early colonial antiquarians. Notably, digs by Richard Nicklin Hall, who was determined to find evidence that the monument was not built by indigenous Africans until he eventually relinquished this belief. More extensive damage was caused by the mining of ruins for gold. Reconstruction attempts since 1980 caused further damage, alienating local communities. Visitors have caused further damage, with many cases of people climbing the walls, walking over archaeological deposits, and the over-use of certain paths. This is in conjunction with natural damage from vegetation growth, foundations settling, and weathering. Asia Abkhazia A fire in 2024 destroyed the National Art Gallery in Sukhumi and all but 150 of its collection of 4,000 paintings. Afghanistan A pair of 6th-century monumental statues known as the Buddhas of Bamiyan were dynamited by the Taliban in March 2001, who had declared them heretical idols. The world's oldest oil paintings were discovered in Bamiyan, though some were damaged by knives and attempts to destroy them. These paintings, which were identified during UNESCO research in 2008, suffered significant harm from vandalism. Armenia In 1870, a report by the Viceroyalty of the Caucasus recorded 269 Shia mosques in the region. After 1917, many of the city's religious buildings were demolished in accordance with the Soviet government's modernization and anti-religious policies. A mosque in Yerevan was pulled down with a bulldozer at the beginning of the year 1990. Today only one mosque remains in the city. Azerbaijan Azerbaijani authorities destroyed the Armenian cemetery in Julfa in December 2005, in the region of Nakhchivan. The Azerbaijani representative of Nakhchivan denied that there was an Armenian cemetery there in the first place. Azerbaijani authorities demolished the Church of the Holy Virgin in Baku in 1992, as part of their "de-Armenisation" campaign, which also took place in the Armenian cemetery in Julfa. The rest was turned into a restaurant. Azerbaijani authorities demolished the Kanach Zham of Shusha, in the disputed region of Nagorno-Karabakh, between December 2023 and April 2024. Bahrain At least 43 Shia mosques, including the ornate 400-year-old Amir Mohammed Braighi mosque, and many other religious structures were destroyed by the Bahraini government during the Bahraini uprising of 2011. Bangladesh Several landmarks associated with the founding leader of Bangladesh, Sheikh Mujibur Rahman, were destroyed or damaged in arson attacks and looting that followed the Non-cooperation movement (2024), including his former residence in Dhaka, which had been converted into the Bangabandhu Memorial Museum, as well as the Mujibnagar Memorial Complex. Other cultural institutions were also destroyed in the violence, including the 19th-century Bir Chandra Public Library in Comilla. Cambodia The Roman Catholic Cathedral of Phnom Penh was the first building to be destroyed by the Khmer Rouge after the establishment of their regime. The Cathedral of Battambang was also destroyed in 1975. China The Famen Temple went through several periods of destruction. First erected during the Eastern Han dynasty (AD 25–220), it was destroyed during the years of the Northern Zhou dynasty (557–581). After being rebuilt, it was destroyed again by an earthquake during the Longqing's years (1567–1572) of the Ming dynasty. After another reconstruction, it was destroyed again during the Cultural Revolution of 1966–1976. The present structure was completed in 1987 before being opened to the public as a museum in 1988. The Huang Chao rebellion (874–884) devastated the city of Chang'an, a historical capital of several ancient Chinese empires. The city was sacked and occupied by the rebels who looted and demolished the buildings, whose materials were then reused to build the subsequent capital city of Luoyang. Chang'an never recovered after this obliteration, and it was followed by the decline of the Tang dynasty. Huang Chao's former lieutenant Zhu Wen completed the destruction by dismantling Chang'an and transporting the materials east to Luoyang. During the systematic persecution of Buddhists in AD 845 by the Taoist Emperor Wuzong of Tang, more than 4,600 Buddhist temples were destroyed across the empire. In 955, Emperor Shizong of the Later Zhou ordered the systematic destruction of Buddha statues for copper to mint coins. The ordinance led to the destruction of 3,336 of China's 6,030 Buddhist temples. In 1739, the Pagoda of Chengtian Temple was destroyed after a large earthquake struck the city of Yinchuan. The pagoda was restored in 1820. The Porcelain Tower of Nanjing, which dates back to the 15th century, was destroyed during the course of the Taiping Rebellion (1850–1864). A modern life-size replica was built in 2015. In 1860, much of the Old Summer Palace, a Qing-era imperial palace, was set on fire and sacked during the Second Opium War. The palace was later sacked again and destroyed by the Eight-Nation Alliance when they invaded Beijing. Beijing city fortifications which date back to the 15th–16th century were destroyed through the course of the decline of the Qing dynasty in the late 19th to early 20th century. They were severely damaged during the Boxer Rebellion (1898–1901), with the gate towers and watchtowers destroyed and troops of the Eight-Nation Alliance tearing down much of the Outer city walls. After the collapse of the Qing in 1912, and end of the Republic of China in 1949, the fortifications were dismantled to build modern ring roads around Beijing. Today, nothing of the Outer city remains intact. In 1921, Buddhist murals at the Mogao Caves were damaged and vandalized by White Russian soldiers fleeing the Russian Civil War. Buddhist murals at the Bezeklik Thousand Buddha Caves were damaged by the local Muslim population. The eyes and mouths in particular were often gouged out. Pieces of murals were also broken off for use as fertilizer by the locals. During the Kumul Rebellion in Xinjiang in the 1930s, Buddhist murals were vandalized by Muslims. Yongdingmen, the former front gate of the outer city wall of the Beijing city fortifications, which dates back to 1553, was demolished in the 1950s to make way for the new road system. It was rebuilt in 2005. The Gate of China in Beijing was demolished by the Chinese government in 1954 to make way for the expansion of Tiananmen Square. The Chairman Mao Memorial Hall occupies the former site of the site of the gate. A shrine dedicated to Wei Yan was destroyed by the Chinese government in 1968. A stone tablet which contained the record of his presence was lost after the demolition. The shrine was rebuilt in 1995. During the Cultural Revolution of the 1960s and 1970s, many artifacts, monuments, and buildings belonging to the Four Olds were attacked and destroyed, including: White Horse Temple in Luoyang, the oldest Buddhist temple in China. Some historical artifacts are still missing. Famen Buddhist Temple, Shaanxi. Tomb contents and human remains from the Ding Mausoleum, the tomb of Ming Emperor Wanli and his empresses. China's development has resulted in the destruction of more than 30,000 items listed by the China's National Cultural Heritage Administration, compiled from various archaeological and historic sites. One conservation campaigner reported that the rate of destruction is worse than during the Cultural Revolution. Destroyed heritage sites include the old town in Dinghai, the old town of Laoximen in Shanghai, a centuries-old market street in Qianmen, and a section of the Great Wall of China. Historical neighborhoods of Beijing and Nanjing were also razed. The construction of the Three Gorges Dam on the Yangtze River caused water levels to rise, destroying entire cities and historical locations along the river. In 2016, the Chinese government ordered the demolition of historical houses in the Larung Gar Tibetan Buddhist institution. By 2017, the old town of Kashgar had been destroyed by the Chinese government, and replaced by a significantly smaller and lower-quality "theme park" version of the site. During the 2020 China floods, multiple historic bridges were destroyed, including the Lecheng Bridge and the Zhenhai Bridge. India In 1024, during the reign of Bhima I, the prominent Afghan ruler Mahmud of Ghazni raided Gujarat, plundered and destroyed the Somnath temple and broke its jyotirlinga. In 1299, Alauddin Khalji's army under the leadership of Ulugh Khan defeated Karandev II of the Vaghela dynasty, and sacked the (rebuilt) Somnath temple. By 1665, the rebuilt temple was once again ordered destroyed by Mughal emperor Aurangzeb. In 1702, he ordered that if Hindus had revived worship there, it should be demolished completely. Around 1200 CE, the most prominent seats of learning in Ancient India, Nalanda University, Buddhist Monasteries and the educational centers of Vikramasila and Odantapuri were sacked and destroyed by Bengal ruler Muhammad bin Bakhtiyar Khalji. The famous Martand Sun Temple, located in Jammu and Kashmir, was destroyed by Sultan Sikandar Butshikan in the early 15th century, with demolition lasting a year. In 14th century Hampi, the second biggest city in the world at the time, was destroyed by the armies of the Delhi Sultanate, those of Alauddin Khalji and Muhammad bin Tughlaq. In 1565 CE, after the Battle of Talikota, the capital city of Vijayanagara, was sacked and destroyed by an invading army raised by the five Bahamani Sultanates. What remains now are the ruins of Hampi. The Shiva temples of Puneshwar and Narayaneshwar in the city of Pune were destroyed by the invading army of Alauddin Khalji. Later a tomb of a Muslim preacher was erected at the sites. In 1664, Aurangzeb destroyed the Kashi Vishwanath Mandir and built the Gyanvapi Mosque over its walls. The remnants of the temple wall can still be seen today, as depicted in the 19th century sketch by James Prinsep. In 1696, the Madrasa Mahmud Gawan of Bidar was struck by lightning and a part of it was destroyed. On 6 December 1992, the Babri Masjid was destroyed by Hindu nationalists. On 26 April 2016, the National Museum of Natural History, New Delhi, and its valuable collection of animal fossils and taxidermy animals was destroyed by fire. The Nalanda University, one of the oldest residential universities in the world, was destroyed in 1193 AD by Bakhtiyar Khilji. The library, known for its vast collection of manuscripts, was set on fire, reportedly burning for several months. The Martand Sun Temple in Jammu and Kashmir, an important Hindu temple built in the 8th century, was destroyed in the 15th century during the reign of Sultan Sikandar. Indonesia Kraton Majapahit, the royal palace of Majapahit emperors, was destroyed in a rebellion. What remained of the palace and fortifications around it was further looted by treasure hunters during the Dutch colonial era. Iran In 330 BC, Alexander the Great sent the main force of his army to Persepolis by the Royal Road and destroyed it. During the reign of Naser al-Din Shah Qajar, Nahavand Castle was ruined hoping to find treasures beneath. In 861 AD, a sacred tree in Zoroastrianism called the Cypress of Kashmar was felled on the order of Abbasid caliph Al-Mutawakkil. Iraq The Hanging Gardens of Babylon, one of the Seven Wonders of the Ancient World, are believed to have been destroyed sometime after the 1st century AD. Their existence is not confirmed by archaeology, and there have been suggestions that the gardens were purely mythical. The Round City of Baghdad, the seat of the Abbasid caliph, was sacked by the Mongols led by Hulegu in 1258. Large sections of the city as well as irrigation system and the House of Wisdom, a library and intellectual center, were destroyed. The city was attacked again by Tamerlane in 1401, leading to the almost destruction. Several historical gates of Baghdad dating back to the 12th century were demolished by the occupying Allied and Ottoman forces during the First World War. Since the U.S.-led invasion of Iraq in 2003, various archaeological sites and museums have been looted, including the ancient cities of Adab, Hatra and Isin where U.S. military protection was absent. The most prominent among them being the Iraq Museum where as much as 170,000 items were looted, including the 5,000 year old statues. In addition, several sites such as Babylon saw the destruction of its archaeology-rich subsoil as a result of military planning. During the Iraqi Civil War (2006–2008), several historical sites were destroyed by various groups. In 2006 and 2007, Al-Askari Mosque was bombed by Sunni militants twice in the course of two years. In 2006, the Minaret of Anah and the statue of Al-Mansur were bombed by Shia militants and destroyed. The buildings were later reconstructed. The Islamic State of Iraq and Syria (ISIS) destroyed much of the cultural heritage in the areas it controlled in Iraq. At least 28 religious buildings were looted and destroyed, including Shiite mosques, tombs, shrines and churches. Numerous ancient and medieval sites and artifacts, including the ancient cities of Nimrud and Hatra, parts of the wall of Nineveh, the ruins of Bash Tapia Castle and Dair Mar Elia, and artifacts from the Mosul Museum were also destroyed. However, from 2018, a year after IS was defeated and driven out of Mosul, many churches, mosques and historic buildings were rebuilt. Israel and Palestine The First Temple in Jerusalem was destroyed by the Neo-Babylonian Empire in 587/586 BCE, and the Second Temple by the Roman Empire in 70 CE. Following the conquest of the Old City of Jerusalem by the Arab Legion in 1948, under the Jordanian annexation, Jewish sites were systematically damaged and destroyed. In particular, all but one of the thirty-five synagogues of the Jewish Quarter were destroyed. The Shrine of Husayn's Head was built by the Fatimids on a hilltop adjacent to Ascalon that was reputed to have held the head of Husayn ibn Ali between c. 906 CE and 1153 CE. It was described as the most magnificent building in the ancient city, and developed into the most important and holiest Shi'a site in Palestine. The shrine was destroyed in 1950 by the Israeli army, more than a year after hostilities ended, on the orders of Moshe Dayan. This was in accordance with a 1950s Israeli policy of erasing Muslim historical sites within Israel, and in line with efforts to expel the remaining Palestinian Arabs from the region. Following Israel's victory during the 1967 Six-Day War the Israeli military destroyed a large part of the Moroccan Quarter in Jerusalem's Old Town to make room for a plaza in front of the Western Wall. In the 1970s a Late Neolithic archaeological site was bulldozed for the construction of Shiqma Reservoir near Zikim, a kibbutz in Israel. The Baptist Church of Jerusalem in Narkis Street was burned down in 1982 in a suspected arson, and subjected to an arson attack in 2007. In 1996, the Jerusalem Islamic Waqf began unauthorized construction on the Temple Mount, damaging ancient structures and weakening the stability of the Southern Wall. 300 truckloads of topsoil were excavated and dumped in the Kidron Valley without permitting proper archaeological care. (See Temple Mount Sifting Project). In 2015, in one of a series of attacks on churches in Israel and the West Bank by Jewish extremist groups, a former settler on the West Bank torched and set fire to the Church of the Multiplication in Tabgha on the shore of the Sea of Galilee in northern Israel. This was the church where some Christians believe Jesus to have carried out his miracle of the feeding of the 5000. Joseph's Tomb in the city of Nablus has been repeatedly vandalized, with Palestinian mobs burning and pillaging it immediately after the withdrawal of Israeli forces in 2000, in 2003, and in 2009, when the tomb was vandalized with graffiti including swastikas. The tomb was vandalized again by Palestinian rioters in 2015 and 2022. During the Gaza war, Israel damaged or destroyed more than 100 heritage sites in Gaza, including the Church of Saint Porphyrius, the Great Mosque of Gaza, and the Rafah Museum. This figure was reached by mid-November 2023. Japan The majority of Japanese castles were smashed and destroyed in the late 19th century during the Meiji restoration. This was done by the Japanese people and government in order to modernize and westernize Japan and break from their past feudal era of the Daimyo and Shoguns. Concrete replicas of the castles were built for tourists prior to the1964 Summer Olympics in Japan. The vast majority of castles in Japan today are new replicas made out of concrete. In 1959 a concrete keep was built for Nagoya castle. An earth wall with uneven stones made up the original base of Komine Castle before it collapsed in the 1970s due to rain. The Japanese local government repaired it with concrete and the entire section of the repaired wall was destroyed by the earthquake in 2011 due to using concrete. The Japanese government then sought photographs of the original wall from local citizens as they had no references to repair it to its original state. The destroyed Kumamoto Castle, Fushimi Castle, Hiroshima Castle were rebuilt with concrete after World War II and Tokyo Imperial Palace was rebuilt after World War II. Kinkaku-ji was rebuilt after a monk burned it down. Kyoto Imperial Palace was rebuilt in 1855. Ryōunkaku, Japan's first skyscraper, was severely damaged during the 1923 Great Kantō earthquake. 12 people inside the tower were killed and another injured. It was demolished less than a month later on 23 September 1923. The Japanese used mostly concrete in 1934 to rebuild the Togetsukyo Bridge, unlike the original destroyed wooden version of the bridge from 836. Horyuji temple was rebuilt referencing historic paintings. During the Meiji restoration's Shinbutsu bunri, tens of thousands of Buddhist religious statues and temples were destroyed. The government then closed tens of thousands of traditional Shinto shrines under the Shrine Consolidation Policy. The Meiji government built the new modern 15 shrines of the Kenmu restoration as a political move to link the Meiji restoration to the Kenmu restoration for their new State Shinto cult. The Bodhisattva Avalokiteśvara (Kannon) statue at a temple Ryozen Kannon in Kyoto was rebuilt with concrete after World War II. In 1958 the Kannon-do temple at the Senso-ji Temple in Tokyo was rebuilt with concrete after it was destroyed in 1945 during World War II. Kumamoto Castle, Kumamoto: Seriously damaged in 1877 during the Siege of Kumamoto Castle, part of the larger Satsuma Rebellion; subsequently rebuilt in the 1960s, with further historical restoration work completed from 1998 to 2008. The castle was again seriously damaged during 2016 Kumamoto earthquakes, with the required rebuilding effort estimated to take several decades. Shuri Castle, a palace of the Ryukyu Kingdom first built in the 14th century, was destroyed during the Battle of Okinawa in World War II. Japanese forces had set up a defense perimeter which went through the underground of the castle. The battleship USS Mississippi shelled the location for three days in May 1945. The castle burned down after. It was later reconstructed in the 1990s. On the morning of 31 October 2019, the main courtyard structures of the castle were again destroyed in a fire. The Kinkaku-ji (Golden Pavilion) of Kyoto was burnt down by an arsonist in 1950, and was restored in 1955. A large number of Important Cultural Property, libraries, museums, and other archives were damaged or destroyed by the 2011 Tōhoku earthquake and tsunami. Lebanon On 9 October 2024, during the Israeli invasion of Lebanon, a 19th-century church in Derdghaya was destroyed by an IDF airstrike. Tibnin Castle was damaged during the Israeli invasion of Lebanon in 2024 and one of its walls collapsed. Malaysia Candi Number 11, also known as Candi Sungai Batu Estate, a 1,200 year old ruin of a tomb-temple located in the Bujang Valley historical complex in Kedah, was demolished in 2013 by housing developers who claimed not to have known the historical significance of the stone edifice. Maldives On 7 February 2012, in the aftermath of the coup in which President Mohamed Nasheed was overthrown, the National Museum was stormed by Islamists who destroyed Buddhist artifacts. Most of the Buddhist physical history of the Maldives was obliterated. Hindu artifacts were also targeted for obliteration and the actions have been compared to the attacks on the Buddhas by the Taliban. Myanmar Mandalay Palace, a former 19th century royal palace of the Burmese kings, was burned down to the ground in 1945 during the Battle of Mandalay between Allied and Japanese forces. Shwedagon Paya temple complex in Yangon, built between 6th and 10th centuries AD, was severely damaged when Cyclone Nargis struck the region in 2008, which caused the worst natural disaster in the recorded history of Myanmar. Nepal The 7.8 Mw Nepal earthquake in 2015 demolished heritage sites in Kathmandu valley. It destroyed centuries old medieval temples and palaces in the Kathmandu, Bhaktapur and Patan Durbar Squares along with the tower of Dharahara, the temple of Changunarayan, some temples of the Pahupatinath complex, the main stupa of Boudhanath and the temples of Swayambhunath Stupa. Oman The Arabian Oryx Sanctuary became the first of now three Former UNESCO World Heritage Sites in 2007. It was delisted by UNESCO after having been reduced to a tenth of its size to allow for oil-drilling Pakistan The archaeological site of Harappa which dated back to 2600 BCE was heavily damaged during the Indian Rebellion of 1857. Bricks from the ruins were brought out and used as track ballast during the construction of Lahore–Multan railway line. Since the discovery, the site was damaged by the local farmers in the process of turning it into an agriculture land. The Multan Sun Temple, a grand Hindu temple dedicated to the Sun deity built in 614 CE or earlier, was destroyed in the late 10th century by Ismaili rulers. A mosque was built atop it, which was also destroyed in the 11th century by Mahmud of Ghazni. The ruins of the temple exist in modern-day Multan, Pakistan. The Prahladpuri Temple, Multan, was destroyed by Muslims in 1992 in the aftermath of Babri mosque destruction in India. The Shaheed Ganj Mosque in Lahore was demolished by the Sikhs in 1935. Sikhs had been occupying the public square near the mosque since the capture of Lahore by Bhangi Misl in the 18th century. The conflict concerning the mosque had heightened during the colonial era, as Muslims were forbidden to pray there by the mosque administration. The demolishing of the mosque had led to the Muslims protesters holding marches toward the mosque, which was dispersed by the police opening fire on them. Looters and the Taliban destroyed much of Pakistan's Buddhist artifacts left over from the Buddhist Gandhara civilization especially in Swat Valley. Gandhara Buddhist relics were deliberately targeted by the Taliban for destruction, and illegally looted by smugglers. Kushan era Buddhist stupas and statues in Swat valley, including the Jehanabad Buddha's face, were demolished by the Taliban.The government was criticized for doing nothing to safeguard the statue after the initial attempt at destroying the Buddha, which did not cause permanent harm, and when the second attack took place on the statue the feet, shoulders, and face were demolished. A rehabilitation attempt on the Buddha was made by Luca Olivieri and a group from Italy. Philippines During the Spanish Colonization of the Philippine islands, the Spanish observed native structures called Kota or citadels made of large wooden houses or lime stones which made up the ancient cosmopolitan city-states of Luzon, Visayas and even in Mindanao. The City of Cainta was a fortified city. According to the descriptions by early Spanish chroniclers, it was surrounded by bamboo thickets, defended by a log wall, stone bulwarks and several lantakas, and an arm of the Pasig River flowed through the middle of the city, dividing it into two settlements.: 145 It had a population with about a thousand inhabitants. As described in the anonymous 1572 account documented in Volume 3 of Blair and Robertson's compiled translations: In August 1571, Miguel Lopez de Legazpi assigned his nephew, Juan de Salcedo, to "pacify" Cainta. After travelling several days upriver, Salcedo lay siege to the city, and eventually found a weak spot on the wall. The final Spanish attack over 400 residents of Cainta killed including their leader Gat Maitan. Kota Selurong was the walled city of Manila along the south bank of the Pasig River. Kota Seludong, the seat of the power of the Kingdom of Maynila that was protected by a rammed earth fortress equipped with stockades, battlements and cannons. the Kota were destroyed in 1570 siege, after the Spanish forces invaded the city and Martin de Goiti ordered his men to set the city in fire. During the Battle of Manila in 1945, most of the city's unique architecture was destroyed. After the battle, in the business district, only two buildings dating to before the war remained intact, and these buildings' plumbing had been looted. After the war ended, much of Manila was rebuilt in a modernist style, and thus the original architectural heritage of the city is largely lost. Manila Jai Alai Building, a historic jai alai venue demolished in 2000 which was opposed by heritage conservationists. The demolition led to the passage of the National Cultural Heritage Act of 2009. Several historic buildings were damaged or destroyed during the 2013 Bohol earthquake, including the Loboc Church, the Loon Church, the Maribojoc Church and the Baclayon Church. The Philippine Su Kuang Institute building was demolished in 2017 after the owners sold the building to a private developer within the same year. The 1940s era building was the last Art Deco wooden school structure in Binondo, Manila. In 2023, the Manila Post Palace burned down, destroying their valuable stamp collection. Saudi Arabia Various mosques and other historic sites such as the Ajyad Fortress, especially those relating to early Islam, have been destroyed in Saudi Arabia. This is done both for economic reasons, to create room to accommodate hajj pilgrims (including luxury facilities for wealthy guests) and for ideological reasons related to the iconoclastic religious doctrine of the state Wahhabi sect. The Ajyad Fortress of the Ottomans was demolished for commercial development of the Mecca Royal Hotel Clock Tower. Singapore The Singapore Stone was blown up in 1843 to make way for Fort Fullerton. One fragment survives and is currently displayed at the National Museum of Singapore. It has been designated as a national treasure of Singapore. South Korea Hwangnyongsa, a Buddhist temple in Gyeongju which dates back to the 7th century, was burned down by the Mongolians during their invasion in 1238. Hundreds of Buddhist monasteries were shut down or destroyed during the Joseon period as a part of anti-Buddhism policy. In 1407, during the reign of Taejong, the regulations were imposed on the number of Buddhist temples which limited to 88. Sejong the Great further reduced the number to 36. Many Buddhist statues were also destroyed during the reign of Jungjong (1506–1544). The 1954 Busan Yongdusan fires destroyed many important cultural and historical artifacts in storage, most prominently 30 of the 48 remaining Portraits of Joseon Dynasty Royals, with most of the remaining portraits heavily damaged. Namdaemun was damaged by arson in 2008. It reopened in 2013. In March 2021, a main hall of the historic Naejangsan temple in Jeongeup, was burned into ashes by a 53-year-old monk arsonist. Sri Lanka The Palace of King Parakramabahu I of Polonnaruwa was set on fire by the Kalinga Magha-led Indian invaders in the 11th century. The ruins and the effect of the fire is still visible. The Library of Jaffna, which had over 97,000 manuscripts, was burned in 1981, as a precursor to the Sri Lankan civil war. Syria The Aleppo Codex, the authoritative Hebrew Bible text, was partially destroyed during anti-Jewish riots in Syria in 1947. Much of Syria's cultural heritage was damaged, destroyed, or looted during the Syrian Civil War. Destroyed buildings include the minaret of the Great Mosque of Aleppo and the Al-Madina Souq, while others such as Krak des Chevaliers were damaged. Khusruwiyah Mosque (Husrev Mosque). The Islamic State of Iraq and the Levant (ISIL) destroyed the Lion of Al-lāt, the temples of Bel and Baalshamin, the Arch of Triumph, and other sites in Palmyra. The group also destroyed the Monastery of St. Elian, the Armenian Genocide Memorial Church, and several ancient sculptures in the city of Raqqa. During the Turkish military operation in Afrin in 2018, Turkish shelling seriously damaged the ancient temple of Ain Dara in Afrin. Thailand The historical Bangkok tramway system, opened in 1888, ended its operations in 1968. The original Dusit Zoo, in Bangkok, Thailand, built in 1938, was demolished in 2018. The Parliament House of Thailand, which housed the Legislative Branch of the Government of Thailand from 1974 to 2018, was demolished in 2019. Turkey The Library of Antioch was ordered destroyed by the Roman Emperor Jovian in 363 AD. The Temple of Artemis, one of the Seven Wonders of the Ancient World, was destroyed by arson in 356 BC. It was later rebuilt, but was damaged in a raid by Goths in 268 AD. Its stones were subsequently used in other buildings. A few fragments of the structure still survive in situ. The Mausoleum at Halicarnassus, another Wonder of the Ancient World, was destroyed by a series of earthquakes between the 12th and 15th centuries. Most of the remaining marble blocks were burnt into lime, but some were used in the construction of Bodrum Castle by the Knights Hospitaller, where they can still be seen today. The only other surviving remains of the mausoleum are some foundations in situ, a few sculptures in the British Museum, and some marble blocks which were used to build a dockyard in Malta's Grand Harbour Port city of İzmir (Smyrna) during the Great fire of Smyrna in the aftermath of the Greco-Turkish war. The abandonment and confiscation of Armenian monasteries and cultural heritage in places such as Ani contributed to their eventual destruction. In 1974, UNESCO stated that after 1923, out of 913 Armenian historical monuments left in Eastern Turkey, 464 had vanished completely, 252 were in ruins, and 197 needed repair. In 2011, there were 34 Armenian churches functioning in Turkey, primarily in Istanbul. Turkmenistan The Church of the Transfiguration in Ashgabat was destroyed in 1932 by the Soviet Government. The country's only Bahá'í Temple, in Ashgabat (called Ishqabad by its followers) which was completed in 1908 was later destroyed in 1962 after being damaged in the 1948 Earthquake. Europe Albania Sulejman Pasha Mosque, the dome and mosque were destroyed during World War II, while its minaret remained until 1967, when the communist regime of Enver Hoxha built a war memorial in its place. Austria Vienna's Cathedral of St. Stephen was severely damaged by fire in 1945, towards the end of the Second World War. Incendiary bombs and shelling had set the roof on fire, and the cathedral's original larch girders, said to be made from an entire forest of larches, were destroyed, as were the Rollinger choir stalls, carved in 1487. The building was rebuilt soon after the war. Belgium The Palace of Coudenberg in Brussels burned down in 1731 and its ruins were demolished half a century later. Many churches and abbeys were demolished during the French occupation in the late 18th century, amongst them the St. Lambert's Cathedral in Liège, the St. Donatian's Cathedral and Eekhout Abbey in Bruges, Florennes Abbey in Florennes, and St. Michael's Abbey in Antwerp. The Herkenrode Abbey in Hasselt survived the French Revolution, but subsequently fell into disrepair. In 1826 a fire destroyed much of the church, and the remaining ruins were demolished in 1844. On 25 August 1914, during World War I, the university library of Leuven was destroyed by the Germans against the backdrop of other war crimes. 230,000 volumes were lost, including medieval and Renaissance manuscripts and more than 1,000 incunabula. After the war, a new library was built. During World War II, the new building was again set on fire and nearly a million books were lost. During World War I, the city of Ypres was destroyed, including its Town Hall and Cloth Hall. These monuments were later rebuilt. The Maison du Peuple in Brussels, one of the largest works of architect Victor Horta, was demolished in 1965 to make way for an office building. The surviving townhouses in Brussels designed by Horta were declared UNESCO World Heritage in 2000. Château Miranda, a 19th-century neo-Gothic castle in Celles was demolished in 2016–17. The Valemprez farm, a 13th-century farmhouse rebuilt in the 18th century in Dottignies, was demolished in 2008 Bosnia and Herzegovina Through the course of the Bosnian War, numerous sites of cultural and religious heritage were destroyed: During the Siege of Sarajevo, culturicide was committed by the Army of Republika Srpska. The National and University Library of Bosnia and Herzegovina was specifically targeted and besieged by cannons positioned all around the city and it was destroyed in the fire, along with 80 percent of its contents. Some 3 million books destroyed, along with hundreds of original documents from the Ottoman Empire and the Austro-Hungarian monarchy. Muslim heritage sites suffered the most, with 614 mosques and several other religious facilities, schools, and institutions destroyed by the authorities of the Republic of Srpska as a part of the ethnic cleansing campaign against the local Muslim populations. The most well known among them include Mehmed Pasha Kukavica Mosque, Arnaudija Mosque, and Ferhat Pasha Mosque. A substantial proportion of these mosques dated back to the Ottoman and Austro-Hungarian era. Many of them, such as the Ferhadija and Arnaudija mosques, have since been rebuilt with financial and professional assistance from Turkey. The Ottoman clock tower of Banja Luka Banjalučka sahat-kula was also destroyed in line with efforts to destroy all Ottoman heritage sites in the region. Roman Catholic sites also suffered with more than 269 churches being destroyed, which was associated with the killings of Bosnian Croats, mostly by Bosnian Serbs. As many as 125 Serbian Orthodox religious buildings were destroyed in the war, such as the 13th-century Sase Monastery and Vozuća Monastery. Parts of the old city of Mostar, including the Stari Most, were destroyed by the Croatian Defence Council. The Stari Most has been rebuilt. Another symbol of the city, the monumental Serbian Orthodox Cathedral of the Holy Trinity was shelled, set afire, and finally blown up by the local Croat forces. The reconstruction of the church is ongoing as of 2020. Croatia In the Independent State of Croatia 450 Serb Orthodox churches and monasteries were destroyed along with monumental iconostasis, thousands of icons and number of manuscripts and books which included archival books about births, weddings and deaths. The destroyed ritual items were of great cultural and historical importance and beauty. War damage of the Croatian War of Independence (1991–1995) has been assessed on 2,271 protected cultural monuments, with the damage cost being estimated at 407 million DM. The largest numbers – 683 damaged cultural monuments – are located in the area of Dubrovnik and Neretva County. Most are situated in Dubrovnik itself. The entire buildings and possessions of 481 Roman Catholic churches, several synagogues, and several Serbian Orthodox churches were badly damaged or destroyed. Valuable inventories were looted from over 100 churches. The most drastic example of destruction of cultural monuments, art objects, and artifacts took place in Vukovar. After the occupation of the devastated city by the Yugoslav Army and Serbian paramilitary forces, portable cultural property was removed from shelters and museums in Vukovar to museums and archives in Serbia. Church of St. Nicholas, Karlovac, destroyed between 1991 and 1993. Renovated in 2007. Medieval Dragović monastery, Vrlika, destroyed in 1995. Reestablished in 2004. After Croatia gained independence, about 3,000 memorials dedicated to the anti-fascist resistance and the victims of fascism were destroyed. In September 1991, Croatian forces entered the memorial site of the Jasenovac Concentration Camp and vandalized the museum building, while exhibitions and documentation were destroyed, damaged and looted. Cyprus Following Cypriot intercommunal violence (which ultimately led to the 1974 Cypriot coup d'état and the 1974 Turkish invasion of Cyprus), many Ottoman-era mosques were destroyed after the Turkish Cypriots left. Czech Republic The Old Town Hall in Prague was severely damaged by fire during the Prague uprising of 1945. The chamber where George of Poděbrady was elected King of Bohemia was devastated; the town hall's bell, the oldest in Bohemia, dating from 1313, was melted; and the city archives, comprising 70,000 volumes (most of which were transported to the outskirts of Prague due to the fear of the bombardment), as well as historically priceless manuscripts, were destroyed. The Vinohrady Synagogue (one of Europe's largest Synagogues) was destroyed during the Bombing of Prague. Denmark In the Copenhagen Fire of 1728, a great part of medieval Copenhagen was destroyed. Christiansborg Palace, the main residence of the Danish Kings, was destroyed by fire in 1794. In the Copenhagen Fire of 1795, a great part of medieval and renaissance Copenhagen vanished. Hirschholm Palace, the summer residence of the Danish Kings, was demolished in 1809–1813 after its role in the affair between Johann Friedrich Struensee and Queen Caroline Matilda of Great Britain in the 1770s. Frederiksborg Castle, residence and coronation castle of the Danish king, severely damaged by fire in 1859. The Børsen, a 17th-century stock exchange building in Copenhagen, was partially destroyed, including its iconic spire, in a fire on 16 April 2024. Estonia During World War II, 98% of the town of Narva was destroyed due to Soviet bombing raids. Only 3 pre-war buildings, including the town hall, are still remaining. France In the aftermath of the French Revolution, many historic structures, such as castles and monasteries, were destroyed by the revolutionaries as tangible symbols of the ancien régime. Examples include the Bastille (rapidly demolished in 1789), Cluny Abbey (the largest church in Christendom when built, demolished between 1800 and 1810, and quarried for stone thereafter) and Jumièges Abbey. Most of the French Crown Jewels, including the famous Crown of Charlemagne, were melted down or destroyed during the French Revolution. During the Siege of Strasbourg that took place at the height of the Franco-Prussian War in 1870, the total destruction by shelling and fire of the municipal library and the municipal art and archaeology collections resulted in the loss of 400,000 books, 3,446 Medieval manuscripts, and thousands of incunables, as well as of hundreds of paintings, stained glass windows, and archaeological artefacts. The most famous lost object was the original manuscript of the Hortus deliciarum. On 23 May 1871, the Tuileries Palace, which had been the usual Parisian residence of French monarchs, was almost entirely gutted in a fire set by members of the Paris Commune, leaving only the stone shell. It was subsequently demolished in 1883. The Château de Saint-Cloud was destroyed by Prussians during the Franco-Prussian war. Other destroyed palaces in France include the Château de Meudon and the Château de Bellevue. In 1914, Reims Cathedral was burned as a result of shelling during the initial phase of the First World War. The cathedral was rebuilt after the war. In 1917, the Château de Coucy was blown up by the retreating Germans, destroying its important 13th century donjon. The 1978 Palace of Versailles bombing severely damaged parts of the Palace of Versailles, including several priceless pieces of art. The palace was rebuilt and reopened to the public within four years. On 15 April 2019, the roof of the Notre-Dame de Paris caught fire, severely damaging the bell towers and resulting in the total collapse of the central spire and roof. The fire is believed to have been caused by the ongoing restoration, though an investigation is ongoing. Repairs to the building were completed in 2024, and the cathedral reopened in December 2024. Germany Many historically and architecturally significant buildings were destroyed or severely damaged during World War II and the post-war period as a result of the Allied policy of area bombing of cities aimed at destroying or weakening infrastructure and war-related industry in the German Reich, as well as demoralizing the population by destroying urban cores and residential neighborhoods. Several hundred cities were destroyed, many of them by more than 80 percent. Examples are palaces like Berlin Palace, Monbijou Palace, and City Palace, Potsdam, as well as churches like Dresden Frauenkirche, Berlin Cathedral, and Kaiser Wilhelm Memorial Church. Several have been rebuilt since 1990 (including all those mentioned except Monbijou Palace and Kaiser Wilhelm Memorial Church). The Paulinerkirche was a medieval church from 1231 in Leipzig. The church survived the war practically unscathed but was dynamited in 1968 during the communist regime of East Germany. After the reunification of Germany, a new building in a contemporary style, the Paulinum, was built on the site. East German authorities destroyed several other religious and aristocracy-related buildings as not in line with their vision of a "Worker and Peasant State" – some of those buildings had already been damaged by the war. The building housing the Historical Archive of the City of Cologne collapsed on 3 March 2009, during construction for an extension of the Cologne Stadtbahn. The Church of St. Lambertus in Immerath was demolished on 9 January 2018 as part of the demolition of the entire village to make way for an expansion of the Garzweiler surface mine. The church had been added to the list of heritage monuments in Erkelenz on 14 May 1985. In October 2020, artworks displayed at museums at Museumsinsel in Berlin were vandalized with a liquid that left stains on the artifacts. Numerous synagogues throughout Germany were destroyed during the Nazi era (1933–1945), particularly during or slightly after the November Pogroms of 1938. Their post-war reconstruction was hampered by the Jewish community having fled or been murdered and in some cases Old Nazis remaining in local administrative positions preventing rebuilding. After reunification some important representative buildings of East Germany were demolished, most notably Palast der Republik where asbestos contamination was cited as a reason for demolition In the course of post-war reconstruction in line with the vision of an automotive city several old towns were destroyed or significantly impacted to make way for highways, parking lots and other car-related infrastructure The Duchess Anna Amalia Library in Weimar containing numerous irreplaceable rare books burned down in 2004 Much of Germany's industrial heritage, including railways, historic factories and canals has been destroyed. Very little of the Bavarian Ludwig Railway (Germany's first passenger steam railway) remains, the Ludwig-Donau-Main-Kanal was abandoned and much of it subsequently filled in to build German federal highway A73 and the historic Lehrter Bahnhof in Berlin was torn down to make way for the current Berlin Hauptbahnhof The construction of Waldschlösschen Bridge led UNESCO to withdraw its designation of the Dresden Elbe Valley as a UNESCO World Heritage Site. It's one of only three Former UNESCO World Heritage Sites in the world. During and after the Protestant Reformation religiously motivated iconoclasts destroyed religious art and architecture. This process lasted until the end of the Thirty Years War Besides deliberate acts of destruction like the Sack of Magdeburg which all but depopulated a city of some 25,000 inhabitants, the Thirty Years War also destroyed most church records and many buildings due to ransacking, plunder and arson. As most genealogical records were kept in churches at the time, genealogical research meets a bottleneck during that historical era. Greece The Colossus of Rhodes, one of the Seven Wonders of the Ancient World, was destroyed in the 226 BC Rhodes earthquake, and its remains were destroyed in the 7th century AD while Rhodes was under Arab rule. In December 2015, a group of European architects announced plans to build a modern Colossus where the original once stood. The Statue of Zeus at Olympia, also a Wonder of the Ancient World, was destroyed around the 5th century CE, although it is not known exactly when or how. The Parthenon was extensively damaged in 1687 in the Morean theatre of the Great Turkish War (1683–1699). The Ottoman army fortified the Acropolis of Athens and used the Parthenon as a gunpowder magazine and a shelter for members of the local Turkish community. On 26 September, a Venetian mortar round blew up the magazine, and the explosion blew out the building's central portion. About three hundred people were killed in the explosion, which caused fires that burned until the following day and consumed many homes. The Parthenon was extensively and permanently damaged when Thomas Bruce, the 7th Earl of Elgin and ambassador to the Ottoman Empire (occupiers of Greece in the early 19th century), who admired the Parthenon's extensive collection of ancient marble sculptures, began extracting and expatriating them to Britain in 1801. More damage to the site's heritage came after independence, when all Medieval and Ottoman features of the Acropolis (most notably the Frankish Tower) were destroyed by Heinrich Schliemann in a project to rid the site of all post-Classical influence. Hungary Numerous historical buildings in Budapest were damaged or destroyed during World War II, including the Hungarian Parliament Building, the Chain Bridge, and the Sándor Palace. Ireland During the Battle of Dublin at the beginning of the Irish Civil War in 1922, munitions were stored at the Four Courts building, which housed 1,000 years of Irish records in the Public Record Office. Under circumstances that are disputed, the munitions exploded, destroying much of Ireland's historical record. The Irish Republican Army followed a policy of deliberate destruction of Irish country houses (1919–1923). Italy Many ancient Roman temples, statues, scrolls, buildings and entire cities. In around 1505, Pope Pius II had the historic ancient St. Peter's Basilica demolished to make way for a replacement. The architect, Donato Bramante, also carelessly destroyed many of the tombs in the Basilica. Many historic gardens and villas were destroyed in Rome in the 19th century, including Villa Ludovisi, Villa Negroni and Villa Astalli; The Tower of Paul III and Convent of Aracoeli were demolished to make room for the Victor Emmanuel Monument. Various historic buildings were demolished in the 19th and 20th centuries to make way for railways, industrial areas, or other modern buildings. Examples include the Castello di Villagonia and the Real Cittadella in Sicily. Many historic buildings in Italy were destroyed or damaged during World War II. These include the monastery of Monte Cassino, which was destroyed during the Battle of Monte Cassino. Several historic buildings, books, paintings, and sculptures were destroyed during the Florence Flood of 1966. Several churches and other heritage sites were damaged or destroyed during earthquakes such as the 1997 Umbria and Marche earthquake, the 2009 L'Aquila earthquake, and the August 2016 Central Italy earthquake. Kosovo During the Yugoslavia period there was destruction of Albanian heritage endorsed by the state. A number of Albanian cultural sites in Kosovo were destroyed during the Kosovo conflict (1998–1999) which constituted a war crime violating the Hague and Geneva Conventions. 225 out of 600 mosques in Kosovo were damaged, vandalised, or destroyed alongside other Islamic architecture and Islamic libraries and archives with records spanning 500 years. Additionally 500 Albanian owned kulla dwellings (traditional stone tower houses) and three out of four well preserved Ottoman period urban centres located in Kosovo cities were badly damaged resulting in great loss of traditional architecture. Kosovo's public libraries, in particular 65 out of 183 were completely destroyed with a loss of 900,588 volumes. During the war, Islamic architectural heritage posed for Yugoslav Serb paramilitary and military forces as Albanian patrimony with destruction of non-Serbian architectural heritage being a methodical and planned component of ethnic cleansing in Kosovo. During World War II, a number of Serbian Orthodox religious sites were damaged or destroyed. During the 1968 and 1981 protests, Serbian Orthodox religious sites were the target of vandalism. This continued during the 1980s. NATO bombing in March–June 1999 resulted in some accidental damage to churches and a mosque. Revenge attacks against Serbian religious sites commenced following the conflict and the return of hundreds of thousands of Kosovo Albanian refugees to their homes. Serbian cultural sites in Kosovo were systematically destroyed in the aftermath of the Kosovo War and 2004 ethnic violence. According to the International Center for Transitional Justice this includes 155 destroyed Serbian Orthodox churches and monasteries as well as Medieval Monuments in Kosovo, which were inscribed on the List of World Heritage in Danger. Malta Parts of the megalithic Xagħra Stone Circle in Gozo were deliberately destroyed in around 1834–1835 and its megaliths were broken down to form masonry which was used in the construction of a nearby farmhouse. The site was forgotten for over a century before being rediscovered in the late 20th century. A number of buildings of historical or architectural importance which had been included on the Antiquities List were destroyed by aerial bombardment during World War II, including Auberge d'Auvergne, Auberge de France and the Slaves' Prison in Valletta, the Clock Tower, Auberge d'Allemagne and Auberge d'Italie in Birgu, and two out of three megalithic temples at Kordin. Others such as Fort Manoel also suffered severe damage, but were rebuilt after the war. Other buildings which were not included on the Antiquities List but which had significant cultural importance were also destroyed during the war. The most notable of these was the Royal Opera House in Valletta, which is considered as "one of the major architectural and cultural projects undertaken by the British" by the Superintendence of Cultural Heritage. The Gourgion Tower in Xewkija, which was included on the Antiquities List, was demolished by American forces in 1943 to make way for an airfield. Many of its inscriptions and decorated stones were retrieved and they are in storage at Heritage Malta. Palazzo Fremaux, a building included on the Antiquities List and which was scheduled as a Grade 2 property, was gradually demolished between 1990 and 2003. The demolition was condemned by local residents, the local government and non-governmental organizations. The Azure Window, a 28-metre-tall (92 ft) limestone natural arch on the island of Gozo in Malta was one of Malta's major tourist attractions and was featured in several films. It was located in Dwejra Bay in the limits of San Lawrenz, close to the Inland Sea and the Fungus Rock. The formation was anchored on the east end by the seaside cliff, arching over open water, to be anchored to a free standing pillar in the sea to the west of the cliff. It was created when two limestone sea caves collapsed. Following years of natural erosion causing parts of the arch to fall into the sea, the arch and free standing pillar collapsed completely during a storm in March 2017. Villa St Ignatius, a 19th-century villa with historical and architectural significance, was partially demolished in late 2017. This was condemned by numerous non-governmental organizations and other entities. Netherlands The German bombing of Rotterdam that took place on 14 May 1940, also known as the Rotterdam Blitz, decimated most of the historical city center of the Dutch city. During the bombing, hundreds of years worth of architecture and artwork were destroyed within hours. De Noord, a tower mill which had survived the Rotterdam Blitz, suffered a fire in July 1954 and was demolished soon after. Norway From 1992 to 1995, members of the Norwegian black metal scene began a wave of arson attacks on medieval Christian churches. By 1996, there had been at least 50 attacks. Poland Warsaw Old Town, including the Royal Castle, Warsaw, Warsaw New Town, Łazienki Park including the Łazienki Palace, and Ujazdowski Castle, were destroyed by Nazi Germany in 1944, and later rebuilt from the 1950s to 1980s. Portugal Lisbon was almost destroyed during the 1755 Lisbon earthquake and subsequent fire and tsunami. A small section of the 19th-century quarter Chiado was destroyed by fire on 25 August 1988. The eighteen damaged buildings were rebuilt in the following 20 years. Romania The 60-meter-high tower of Rotbav fortified church, dating back to the 13th century, collapsed on 20 February 2016. Many historical buildings were demolished to construct the Centrul Civic in Bucharest. Many old towns of different cities were destroyed partially or completely because of communist urban planning. Old towns of cities like Bacău, Bârlad, Câmpina, Galați, Orșova, Pitești, Ploiești, Râmnicu Vâlcea, and Suceava were completely demolished. The 1989 fire of the Central University Library in Bucharest destroyed over 500,000 books and 3,700 manuscripts, including manuscripts of famous Romanian writings, such as Mircea Eliade's novel manuscripts. Russia In Moscow alone losses of 1917–2006 are estimated at over 640 notable buildings (including 150 to 200 listed buildings, out of a total inventory of 3,500) – some disappeared completely, others were replaced with concrete replicas. President Boris Yeltsin ordered the shelling of the White House, seat of the Russian government, during his 1993 consolidation of power, causing a large fire and considerable damage to the top floors. 'Mephistopheles', figure on a St Petersburg building on Lakhtinksaya Street known as the House with Mephistopheles, smashed by a fundamentalist Orthodox group in 2015. The original buildings of Metrowagonmash plant, founded by Savva Mamontov in 1897 and built in Russian Gothic style, were demolished between 2016 and 2019 to make way for block houses. Serbia A number of culturally and historically important buildings were destroyed in Belgrade during Operation Retribution, the Allied bombing of Yugoslavia in World War II, and other battles. Destroyed buildings include the National Library of Serbia and the library's collection of 500,000 volumes and invaluable collection of medieval Cyrillic manuscripts and charters, King Alexander Bridge, Old Post Office, El Kal Synagogue, Beth Israel synagogue and others. Socialist Federal Republic of Yugoslavia demolished dozens of synagogues in Vojvodina after World War II as preserving religious buildings and relics wasn't considered to be important. The Yugoslav Ministry of Defence building, a cultural monument, was partially destroyed during the NATO bombing of Yugoslavia in 1999. NATO bombing also resulted in the damaging of medieval monuments, such as Gračanica Monastery, the Patriarchate of Peć and the Visoki Dečani, which are on the UNESCO's World Heritage list today. The University Hospital Center Dr Dragiša Mišović, founded in 1922, was destroyed. The Avala Tower, one of the most iconic symbols of the Serbian capital, was destroyed during the bombing. Slovenia Partisan forces or their successors destroyed approximately 100 castles and manors during and after the Second World War. Examples include Ajman Manor, Belnek Castle, Boštanj Castle, Brdo Castle, Čušperk Castle, Dol Mansion, Dolena Castle, Gracar Castle, Haasberg Castle, Klevevž Castle, Kolovec Castle, Križ Castle, Krupa Castle, Mokronog Castle, Pogonik Castle, Radelstein Castle, Soteska Castle, Špitalič Manor, Turn Castle, and Volčji Potok Manor. An Allied raid heavily damaged Žužemberk Castle during the Second World War. Many churches were destroyed during and after World War II. Examples include the churches in Ajbelj, Dragatuš, Dvor, Gabrje, Hinje, Koče, Kočevska Reka, Morava, Plešivica, Ptuj, Srobotnik pri Velikih Laščah, Stari Log, Trava, Velika Račna, Zafara, and Žužemberk. Soviet Union During February–March 1944, the Soviet conducted the expulsion of the Chechens and Ingush from the North Caucasus as a part of the Soviet forced settlement program of the non-Russian ethnic minorities. The operation resulted in the deportation of 496,000 Chechens and Ingush populations, and the death of around a quarter of them. It was also accompanied by the destruction of local cultural and societal heritages; names of these nations were erased from the books and records; placenames were replaced with Russian ones; mosques were demolished; villages were razed; and the historical Nakh language manuscripts were almost destroyed. The native Crimean Tatars were deported by the Soviets from the peninsula in May 1944. Afterward, the government engaged in a full-scale detatarization campaign to continue the ethnic cleansing campaign, all the Tatar placenames being replaced with Russian ones, and the Muslim graveyards and religious objects were destroyed or converted into secular places. A new anti-religious campaign was launched in 1929 and the destruction of churches in the cities peaked around 1932. Several churches were demolished, including the Cathedral of Christ the Saviour in Moscow and St. Michael's Cathedral in Izhevsk. Both of these were rebuilt in the 1990s and 2000s. In 1959 Nikita Khrushchev launched his anti-religious campaign. By 1964 over 10 thousand churches out of 20 thousand were shut down (mostly in rural areas) and many were demolished. Of 58 monasteries and convents operating in 1959, only sixteen remained by 1964; of Moscow's fifty churches operating in 1959, thirty were closed and six demolished. Spain Because of the Ecclesiastical confiscations of Mendizábal, secularization of church properties in 1835–1836, several hundreds of church buildings, monasteries, or civil buildings owned by the Church were partly or completely demolished. Many of the art works, libraries and archives contained were lost or pillaged in the time the buildings were abandoned and without owners. Among them were important buildings as Santa Caterina convent (the first gothic building in Iberian Peninsula) and Sant Francesc convent (gothic too, one of the richest in the country), both in Barcelona, or San Pedro de Arlanza Roman monastery, near Burgos, now ruined. Several monuments demolished in Calatayud: the church of Convent of Dominicos of San Pedro Mártir (1856), Convent of Trinidad (1856), Church of Santiago (1863), Church of San Torcuato and Santa Lucía (1869) and Church of San Miguel (1871). The leaning Torre Nueva in Zaragoza was demolished in 1892 amidst fears that it would topple. Palacio de los Lasso de Castilla was a 15th-century palace in Madrid which became the palace or residence of the Catholic Monarchs. It was demolished during the mid 19th century. Churches, monasteries, convents and libraries were destroyed during the Spanish Civil War. A Virxe da Barca sanctuary, located in Muxia, was destroyed by a fire started by lightning. Iglesia de San Pío X, a church located in Todoque, Canary Islands, was destroyed by the 2021 Cumbre Vieja volcanic eruption on 26 September 2021. Sweden Tre Kronor, main residence of the Swedish Kings, destroyed by fire in 1697. Several important documents of the history of Sweden were lost in the fire. Klarakvarteren, a part of Stockholm from the 17th century. It was demolished in the 1960–70. The city of Norrköping was razed in 1719 by Russians. It was reconstructed with grid pattern streets and using the surviving Johannesborg fort as a quarry. Switzerland The city of Basel was devastated by the 1356 Basel earthquake. Pfäfers Abbey was destroyed in 1665 by fire. The city of Sion with Majoria and Tourbillon castles were destroyed by fire in 1788. Disentis Abbey was destroyed by fire in 1799 with its library and archives. The Kapellbrücke (Chapel Bridge) in Luzern (Lucerne) was substantially destroyed in 1993 by fire. Ukraine Antonov An-225 Mriya, Kyiv: Severely damaged in 2022 during the Battle of Antonov Airport, part of the 2022 Russian invasion of Ukraine. The aircraft's owner, defense contractor Ukroboronprom, has announced that following the invasion, they will attempt to rebuild the aircraft. Babi Yar Holocaust Memorial Center, Kyiv: Damaged on 1 March 2022 during the Battle of Kyiv, part of the 2022 Russian invasion of Ukraine. The memorial complex, which was under construction at the time, suffered structural damage to a museum building, as well as damage to the adjacent cemetery; other intrinsic elements of the site, including the memorial's synagogue and menorah sculpture, were not damaged. Brotherhood Monastery, Kyiv: Demolished by Soviet authorities in 1935. Building of the Chernihiv Regional Youth Center, Chernihiv: Destroyed by a Russian air strike on 27 February 2022. Church of the Tithes, Kyiv: The original 10th century church was destroyed by Mongol forces in the Siege of Kiev (1240). A new church was built on the site in the 19th century, but it was destroyed by Soviet authorities in 1935. Dormition Cathedral, Kharkiv: Damaged in 2022 during the Battle of Kharkiv, part of the 2022 Russian invasion of Ukraine. Artwork and stained glass in the cathedral were damaged. Great Suburb Synagogue, Lviv: Demolished by invading Nazi forces in 1941. Golden Rose Synagogue, Lviv: Oldest synagogue in Ukraine, sacked in 1941 and demolished in 1942 by the Nazi occupation forces. Ivankiv Historical and Local History Museum, Ivankiv: Destroyed on 27 February 2022 during the Battle of Ivankiv, part of the 2022 Russian invasion of Ukraine. The museum contained folk artwork, including paintings by Maria Prymachenko and textile works of Hanna Veres. The number of artworks by Prymachenko, Veres, and other artists which were destroyed or damaged is currently unknown. Khreshchatyk, Kyiv: The main street of Kyiv, containing many historic buildings. It was heavily mined by retreating Soviet forces in 1941, and as a result most buildings were destroyed. Some buildings were restored after the war, but most were replaced with new structures in the style of Stalinist architecture. Kuindzhi Art Museum, Mariupol: Destroyed on 21 March 2022 during the Siege of Mariupol, part of the 2022 Russian invasion of Ukraine. The museum was dedicated to the life and work of Ukrainian-born artist Arkhip Kuindzhi. Although the works by Kuindzhi held by the museum were reportedly removed from the building prior to its destruction, the whereabouts of the artwork are unknown. Additionally, the status of the remainder of the museum's collection, which included around 2,000 works by fellow Ukrainian artists Ivan Aivazovsky, Mykola Hlushchenko, Tetyana Yablonska, Mykhailo Derehus, and others, remains unknown. St. Michael's Golden-Domed Monastery, Kyiv: Demolished by Soviet authorities from 1934 to 1936. Some frescoes and mosaics were removed and taken to museums in the Russian FSR before demolition, only a portion of which were returned when the cathedral was rebuilt in the 1990s. St. Nicholas Military Cathedral, Kyiv: Demolished by Soviet authorities in 1934. Slovo Building, Kharkiv: Damaged in 2022 during the Battle of Kharkiv, part of the 2022 Russian invasion of Ukraine. Transfiguration Cathedral, Odesa: Severely damaged by a Russian air strike on 23 July 2023. United Kingdom 13th–17th centuries Dunwich, the historic capital of East Anglia, and a major port city of medieval England, has largely fallen into the sea due to gradual coastal erosion following two great storms in 1287. Eight churches present in the 13th century and 400 houses have been swept into the sea. A popular local legend says that at certain tides church bells can still be heard from beneath the waves. Hastings Castle was originally built as a wooden motte and bailey castle in 1066, after William the Conqueror first landed in England, and was rebuilt as a stone fortified castle in 1070. It was dismantled on the orders of King John, who feared it being taken by French Dauphin Louis. It was then rebuilt and refortified by King Henry II around 1220 to 1225. In the South England flood of February 1287, the cliff supporting the castle's south wall collapsed due to a violent storm, causing a large portion of the wall to fall into the North Sea, then in 1337 and 1339 it was attacked by French troops. The destroyed remains of the castle were excavated in the 1820s when the sandstone cliff was cut back to make room for the construction of the neo-classica Pelham Arcade. It last suffered damage as a target for bombs during World War II. The Dissolution of the Monasteries in the 1530s led to many monasteries, relics, and books being destroyed, such as Glastonbury Abbey, Fountains Abbey, Walsingham Priory, Waltham Abbey, Rievaulx Abbey and Furness Abbey. Some monastic churches survived in use as parish churches or cathedrals, as for example at Bath, Romsey and Gloucester, some monasteries were converted to houses, like Coombe Abbey and Lacock Abbey, some fell into ruin and some disappeared completely. In total around 900 monasteries were closed. In addition, the abolition of chantries in 1547 and the conversion to Protestantism led to iconoclastic destruction of artwork in many churches. For a complete list of dissolved monasteries, see List of monastic houses in England and List of monastic houses in Wales. In Scotland, the Reformation happened later, in 1560, and the monks were generally not evicted, but merely left in their monasteries to die out. By the 1590s most monks had died, and in the early 17th century King James VI reconstructed the monastic estates as temporal lordships. The new owners then either destroyed the derelict monasteries or converted them to residential use. In the English Civil War, many castles and stately homes were destroyed in sieges (an example being Old Wardour Castle) or slighted or demolished by the victorious Parliamentarians. This was done both to render them militarily untenable and as a symbolic destruction of the old order. Parliament could not afford to garrison al the many castles in England against Royalist insurgents, and an ungarrisoned castle could easily be used as a base by supporters of King Charles. This happened at Pontefract Castle, where the castle was left standing after the first civil war, was retaken by a party of Royalists, and had to be taken again in a lengthy siege. To prevent this recurring, the castle was thoroughly demolished. Raglan Castle was an example of a punitive demolition – the Marquis of Worcester had held out long after every other castle except Pendennis. As punishment his castle was ransacked and, in contrast to the preservation of that at Oxford, his library was deliberately burnt. Examples of destroyed or damaged castles include Corfe, Winchester, Pembroke, Aberystwyth, Helmsley, Bolton and Basing. The walls of the city of Coventry were also destroyed. The Great Fire of London in 1666 destroyed much of the old city, including Old St Paul's Cathedral, 87 parish churches, 44 Company Halls, the Royal Exchange, the Custom House, and the Bridewell Palace. The Palace of Whitehall, the main residence of the English and later British monarchs, was destroyed by fire in 1698. 18th–20th centuries The Cotton library owned by Sir Robert Bruce Cotton was partly destroyed in a house fire in 1731, resulting in the loss of a number of important Late Antique and medieval manuscripts and serious damage to a number of others, including a copy of Magna Carta. The surviving works are now held by the British Library. Arthur's O'on, a Roman temple or triumphal monument located near the Antonine Wall in Scotland, was demolished by a local landowner in 1743. St Mary's Church in Reculver, an exemplar of Anglo-Saxon architecture and sculpture, was partially demolished in 1809. The Palace of Westminster was almost destroyed by fire on 16 October 1834, and many documents about Britain's political history were lost. Only Westminster Hall, the crypt of St Stephen's Chapel and the Jewel Tower survived. The Temple of the Sun, a Gothic folly in Kew Gardens designed by William Chambers in 1761, was destroyed when a nearby cedar tree fell on it in a storm in 1916. Strangely, Chambers had also planted the cedar earlier, in 1725. The Crystal Palace in London was destroyed by fire on 30 November 1936. St Michael's Church in Coventry was a 14th-century cathedral that was nearly destroyed by the German Luftwaffe during the Coventry Blitz of 14 November 1940. Only the tower, the spire, the outer wall, and the bronze effigy and tomb of its first bishop, Huyshe Yeatman-Biggs, survived. The ruins of this cathedral remain hallowed ground and are listed at Grade I. Charles Church in Plymouth was entirely burned out by incendiary bombs dropped by the Luftwaffe on the nights of 21 and 22 March 1941. It has since been encircled by a roundabout and turned into "a memorial to those citizens of Plymouth who were killed in air-raids on the city in the 1939–45 war." Coleshill House, a historic mansion in Oxfordshire (historically Berkshire) was destroyed in a fire in 1952, and many historic items within were lost. The ruins were demolished in 1958. This was part of a wave of country house demolitions in the aftermath of the Second World War and the decline of the aristocracy, which continued into the 1970s. Several historic structures, such as the Euston Arch in London and the Royal Arch in Dundee, were demolished in the 1960s to make way for redeveloped infrastructure. Urban renewal in many historic cities, like Exeter, Coventry and York, in the 1960s and 70s compounded the damage of the Blitz by destroying many historic buildings to create roads that were believed to be more suitable for traffic. The Imperial Hotel, London, designed by Charles Fitzroy Doll and built from 1905 to 1911, was demolished in 1966–67. York Minster was severely damaged by fire in 1984, believed to have been caused by a lightning strike on the south transept. The Baltic Exchange at 24-28 St Mary Axe in the City of London, was destroyed by a bomb placed there by the Provisional IRA in 1992. The site is now occupied by The Gherkin and the Baltic Exchange Memorial Glass can be seen in the National Maritime Museum. A major fire in 1992 caused extensive damage to Windsor Castle, the largest inhabited castle in the world. 21st century The original Wembley Stadium was closed in October 2000 for redevelopment, and demolition commenced in December 2002, completing in 2003. The top of one of the twin towers was erected as a memorial in the park on the north side of Overton Close in the Saint Raphael's Estate. The Carlton Tavern, an historic pub in Kilburn, London and the only building on its street to survive the Blitz during World War II, was demolished by its owner without prior permission in April 2015. The pub was subsequently rebuilt and re-opened following a community campaign and planning appeals. Clandon Park House, a historic mansion in Surrey, was severely damaged by fire on 29 April 2015, leaving the house "essentially a shell" and destroying thousands of historic items, including one of the footballs kicked across no-man's land on the first day of the Battle of the Somme in 1916. The Royal Clarence Hotel in Exeter, considered England's oldest hotel, was almost destroyed by fire on 28 October 2016. The Mackintosh Building of the Glasgow School of Art was extensively damaged by fire in May 2014, including the destruction of the artistically significant Mackintosh library; but as restoration was completed and nearing reopening a far more devastating fire broke out on the night of 15 June 2018, destroying the building's interior. Alan Dunlop, the school's professor of architecture, said: "I can't see any restoration possible for the building itself. It looks destroyed." The Beehive Mills, in Bolton, Lancashire, a Grade II Bolton listed building built in 1895, was demolished by agreement of the local authority in 2019 for the building of 121 houses. The Crooked House, a historic 18th-century pub and former farmhouse in Staffordshire, was destroyed by fire in August 2023, and the ruins demolished. Liverpool Maritime Mercantile City lost its designation as a UNESCO World Heritage Site in 2021 following the decision to build the new Everton Stadium. It is one of only three former World Heritage Sites. North America Belize Several Maya sites such as San Estevan and Nohmul have been partly demolished. This has been done by contractors to illegally extract gravel for roadworks. Canada also see: Heritage conservation in Canada; "Worst Losses Archive" by National Trust for Canada The 1620 Colony of Avalon was destroyed in 1696 in the Siege of Ferryland during King William's War. In 1696, the Cathedral of St. John the Baptist in St. John's, Newfoundland, was destroyed by the French under the command of Pierre Le Moyne d'Iberville. The present cathedral was extensively damaged in The Great Fire of 1892. The 1754 St. Matthew's United Church (Halifax) was destroyed by fire in 1857 and rebuilt. In January 1839, St. James Anglican Church in Toronto was destroyed by a fire and rebuilt as St. James Cathedral by December 1839. This building was destroyed by another fire in 1849 and replaced by the current structure, the Cathedral Church of St. James, in 1853. Brock's Monument was heavily damaged after a bombing on 17 April 1840 and subsequently demolished; the monument was rebuilt in 1859. On the night of 25 April 1849, the Canadian Parliament buildings in Montreal were set ablaze by Loyalist rioters. The resulting fire consumed the Parliament's two libraries, parts of the archives of Upper Canada and Lower Canada, as well as more recent public documents. Over 23,000 volumes, forming the collections of the two parliamentary libraries, were lost. Christ Church Cathedral (Montreal) was rebuilt in 1859, replacing the previous structure destroyed in a fire in 1856. The 1665 Fort William in St. John's, Newfoundland, was demolished in 1881 to make room for the Newfoundland Railway. The 1881 St. James Anglican Church (Vancouver) was destroyed by a fire in 1886 and only rebuilt after 1935 (completed in 1937). Knox Presbyterian Church (Toronto) was severely damaged by fire in 1895 and eventually relocated in 1909. Crystal Palace (Montreal) was destroyed by fire in 1896. Centre Block of Parliament Hill in Ottawa was destroyed by fire on 3 February 1916, and immediately rebuilt. The Church of Nativité-de-la-Sainte-Vierge-d'Hochelaga was rebuilt in 1921 after fire destroyed the original 1877 church. Montreal City Hall was gutted by fire in 1922 and rebuilt by 1932. Church of the Ascension (Windsor, Ontario) was destroyed by fire in 1926 (rebuilt in 1927) and again in 1990 (repaired the same year). Metropolitan United Church was destroyed by fire in 1928 and rebuilt in 1929 to match the original 1874 building. Saint Boniface Cathedral in Winnipeg, built in 1830, was destroyed by fire in 1860, rebuilt in 1862, relocated in 1906, and destroyed by fire again in 1968. The current cathedral was rebuilt in 1972. Saint-Jacques Cathedral (Montreal) was destroyed by three fires in 1852, 1858, and 1933. The last rebuilt church was mostly demolished after 1973 with only the entrance preserved as Pavillon Judith-Jasmin for the Université du Québec à Montréal. 21st Century The Old Government House, Built in 1876. From 1876 to 1883 Battleford was the seat of government and known as the Territorial Capital of the Northwest Territories. Battleford and the N.W.M.P. played a significant role during the 1885 North West Rebellion. Burned down by arson in 2003 St. Jude's Cathedral (Iqaluit) was destroyed by arson in 2005, along with Inuit art and artifacts. The Quebec City Armoury, built 1885–1888, was mostly destroyed by a fire in 2008 and rebuilt by 2016. St. Anne's Anglican Church (Toronto) was destroyed by fire in June 2024. St. Mary & St. George Anglican Church (Jasper, Alberta) was destroyed by the Jasper wildfire on July 24, 2024. Following unsupported reports of mass graves of Indigenous children across the country, arson attacks in Canada between May 2021 and December 2023 burned down at least 24 churches. Built in 1921, the Notre-Dame-des-Sept-Allégresses church in Québec burned down in October 2024. Guatemala The Maya codices were destroyed by Spanish priest Diego de Landa. Iglesia del Carmen, a colonial church in Antigua, Guatemala, was damaged by several earthquakes. The convent of Santa Clara in Antigua Guatemala was severely damaged during the earthquakes, today only its ruins survive. Tikal Temple 33 was destroyed in the 1960s by archaeologists to uncover earlier phases of construction of the pyramid. Haiti Much of Haiti's heritage was damaged or destroyed in the devastating earthquake in 2010, including the National Palace and the Port-au-Prince Cathedral. Honduras The Catholic church of La Iglesia de Nuestro Señor de los Reyes in the city of Comayagua was built in 1555. It was damaged by an earthquake in 1808 and the mayor's office ordered it demolished in 1829. The church of Santa Lucia de Jeto in Comayagua was built in 1558 and collapsed in 1808 after an earthquake. The Catholic church of the La limpia de la Inmaculada Concepcion in Tegucigalpa was built in 1621. It suffered a fire in 1746, and stopped being used frequently. It was finally demolished in 1858 due to its poor condition. The colonial era building Caxa Real of Comayagua was heavily damaged due to earthquakes; it remained in ruins until it was rebuilt and re opened in 2013. Tenampúa, a ceremonial center of the Lenca culture from the classic Mesoamerican period, was heavily damaged during the Second Honduran Civil War in 1924. The Choir of the Immaculate Conception cathedral was a unique architectonic element in Honduras from the early 18th Century that implemented Baroque and Neoclassical decoration along with Golden pieces inside it. The structure was demolished in 1930 due to the amplification of the cathedral and the possible poor preservation conditions on the structure. The original National bank of Honduras was a renaissance style building located in the central park of Tegucigalpa, built during the late 19th Century. It was demolished during the 1970s to be replaced by a new building that holds government office to this days. Castillo Bogran is an abandoned 19th-century historical building in Santa Barbara that belonged to President Marco Bogran. The building has deteriorated extensively due to heavy rains, hurricanes, and wind. Only 30% of the structure survives. Salitrón Viejo, archaeological site of the Lenca culture that ended up covered under water after the construction of the El Cajon dam, possessing various structures. In April 2009 a fire at the museum of the Saint Agustin College of Comayagua destroyed several pieces of art dating from the Spanish colonial era, including paintings made in Spain and relics that had belonged to national heroes. On 30 November 2017 a fire damaged the Museum del hombre In Tegucigalpa, damaging the structure of the building. Several pieces were saved but suffered extensive damage. On 12 March 2019 there was a fire in the Museum of the Palace of Telecommunications in Tegucigalpa. 30% of the collection was destroyed and another part damaged. Mexico The Chapel of the Christ, San Pablo del Monte, an 18th-century chapel in Tlaxcala, Mexico, was burnt down on July 25, 2015, in an act of arson. Nicaragua Much of the historic downtown of Managua was destroyed by two earthquakes in the 20th century – one in 1931 and a second more devastating one in 1972. Reconstruction efforts after the 1972 earthquake were marred by corruption of the Anastasio Somoza Debayle-regime and much of what could have been saved was lost to graft, incompetence and an ideology of "redesigning" the capital according to then prevalent ideas of city planning The Nicaraguan Revolution and subsequent Contra War led to the destruction of cultural heritage – for example the colonial era fortress of San Carlos, Rio San Juan was destroyed during an FSLN-led commando raid on the Somocista prison housed in the building United States Pennsylvania Station was a Beaux-Arts style "architectural jewel" of New York City. Controversially, the above-ground portions of the station were demolished in 1963, making way for constructing the Madison Square Garden arena that opened in 1968. The architecturally significant beaux-arts Singer Building in New York City, briefly the tallest building in the world from 1906 to 1908, was demolished between 1967 and 1969. Since the start of the 1960s, the National Historic Landmark (NHL) program and the 1966 start of the National Register of Historic Places (NRHP), numerous landmarks designated in those programs have been destroyed. In some cases, the destruction was mitigated by documentation of the artifact or reproduction. Losses by flood and wind damage include: The Old Blenheim Bridge, built in 1855 and the longest-surviving covered bridge in the United States, was destroyed by Hurricane Irene-related flooding in 2011. Numerous NRHP-listed coastal properties in Mississippi and Louisiana were destroyed or significantly damaged by Hurricane Katrina in 2005. Losses by fire, arson, or otherwise include: Leesylvania (plantation), built c. 1750, was destroyed by fire in 1790. The Russian-built Fort Ross Chapel, pre-1841, was destroyed in 1970 and subsequently reproduced. The National Personnel Records Center fire of 1973 destroyed about 80% of the military personnel records held at the National Personnel Records Center in St. Louis, MO. The Provo Tabernacle (NRHP) was destroyed in a fire on 17 December 2010. It was subsequently rebuilt as the Provo City Center Temple, dedicated in 2016. On 30 May 2020, multiple historical documents and artifacts were either damaged or destroyed when the Memorial to the Women of the Confederacy was attacked by rioters in Richmond, Virginia. On 15 May 2025, Nottoway Plantation, the largest antebellum mansion in the Southern United States by square footage, was destroyed in a fire. Losses by permitted processes include: The Edwin H. Armstrong House in Yonkers, New York, was demolished in 1983. Soldier Field stadium in Chicago, Illinois, built in 1924, was altered during a 2002 renovation. The Army Medical Museum and Library in Washington, D.C., built in 1887, was demolished in 1969. The ruins of Leesylvania were demolished in 1960 to make way for a road. NASA wind tunnels including the Eight-Foot High Speed Tunnel (1936–2011) and Full Scale 30-by 60-Foot Tunnel (1936–2010). Ships broken up include: Wapama (steam schooner) (1915–2013), scrapped, though documented by Historic American Engineering Record (HAER) throughout its dismantling. President (steamboat) (1924–2009), disassembled. Other losses of covered bridges, landmarked or not, include: Dooley Station Covered Bridge (1917–1960), arson; replaced by move of 1856-built Portland Mills Covered Bridge. Bridgeton Covered Bridge (1868–2005), arson, replaced by a replica. Jeffries Ford Covered Bridge (1915–2002), arson. Welle Hess Covered Bridge No. S1 (1871–1981), collapsed, partially reproduced off-site. Whites Bridge (1869–2013), arson. Babb's Bridge (1840/43/64–1973), arson, replaced by a replica. Honey Run Covered Bridge (1886–2018), destroyed during the 2018 Camp Fire. In 2014, a 4,500-year-old Coast Miwok Indian burial ground and village was found near Larkspur, California, and destroyed to make way for a multimillion-dollar housing development. Grand Coulee Dam, constructed between 1933 and 1942 on the Columbia River, disturbed burial grounds and destroyed ancient villages on 18,000 acres (7,300 ha) of the Colville Indian Reservation, home to a dozen tribes at the time. NRHD Jobbers Canyon Historic District, in 1989, all 24 buildings were demolished for development, representing the largest National Register historic district loss to date. Rich Bar in Plumas County, California, was a ghost town dating back to California Gold Rush, whose history was documented by Louise Clappe in her famed "Shirley Letters". One notable building, the Kellogg House, still contained original furnishings from the 1800s, and was continuously inhabited by Eva Eyraud, the famed "Woman on Indian Hill", from 1888 up until 1977. Plans to refurbish the house were thwarted when it was destroyed in the Dixie Fire on 23 or 24 July 2021. The Georgia Guidestones were heavily damaged in a bombing on 6 July 2022 and demolished completely later that same day. In the course of the construction of the Interstate Highway System through central areas of many cities and towns, much of their historic architecture was destroyed to accommodate the new roads. Only with the freeway revolts of the 1960s and 1970s did the process slow down. Other buildings were destroyed to make room for surface parking lots demanded by mandatory parking minimums or the desires of business owners. Big box stores also led to a withering of smaller-scale retail and in many cases the abandonment and subsequent demolition of the buildings that formerly hosted such businesses. Much of the tangible and intangible cultural heritage of Native Americans was deliberately or accidentally destroyed in the course of the Native American genocide. The system of residential schools in many cases broke oral tradition and led to language death or severe language decline. The latter was openly admitted policy for decades under the slogan "Kill the Indian, Save the Man". Oceania Australia The Garden Palace in Sydney was destroyed by fire on 22 September 1882. The original Her Majesty's Theatre in Sydney was demolished in June 1933 to make way for a Woolworths retail store. The Jubilee Exhibition Building in Adelaide was demolished in 1962. It hosted the Adelaide Jubilee International Exhibition of 1887. The Federal Coffee Palace in Melbourne was demolished in 1971. The APA Building, also in Melbourne, was demolished in 1980. The Bellevue Hotel in Brisbane was demolished on 20 April 1979 by the Bjelke-Petersen Queensland State government amid mass protest. The Cloudland Dance Hall in Brisbane was demolished in 1982 to make way for an apartment complex. The Regent Theatre in Sydney was demolished in 1988. Tasmanian Aboriginal cave paintings, believed to be 800–8,000 years old, were vandalised in 2016. Juukan Gorge cave, a site of Aboriginal cultural significance in the Pilbara region of Western Australia, where the estimated age of human artefacts was 46,000 years, was destroyed by mining company Rio Tinto. Murujuga (Burrup), a site of Aboriginal cultural significance in the Burrup Peninsula, Western Australia. The establishment of the Karratha Gas Plant, operated by Woodside Energy on behalf of the North West Shelf Joint Venturers, saw the destruction of approximately 5,000 rock art sites and motifs (petroglyphs). New Zealand The Exchange Building in Princes Street, Dunedin was demolished in 1967 to make way for new office buildings. The Cathedral of the Blessed Sacrament in Christchurch was demolished in 2021 by order of Bishop Paul Martin following damage in the 2010 and 2011 Canterbury earthquakes. The cathedral was listed as a category 1 heritage building. Previous Bishop Barry Jones had approved a plan to restore the building) but these plans were thrown out following his death in 2016. After extensive destruction of significant heritage buildings in the quakes and the loss of many community hubs within Christchurch, the decision to demolish not only the cathedral but also many other Catholic churches (damaged and undamaged) was regarded by many in the city as an act of cultural vandalism. The Anglican ChristChurch Cathedral was severely damaged in the 2010 and 2011 Christchurch earthquakes. Demolition was planned and partially done before being stopped entirely in 2012 after government concerns. In 2017 it was announced that the church would be reinstated. Cramner Court, Christchurch, was demolished in 2012, after suffering from damage owing to the 2011 Canterbury earthquake. Like many other heritage buildings in Christchurch, its demolition was seen as controversial. South America Argentina Several grand buildings in Buenos Aires were demolished over years, including Pabellón Argentino, Grand Hotel, Ortiz Basualdo Palace, Unzué Palace, Odeón Theater, Coliseo Theater and various palaces in Avenida Alvear. In 1935 the old church of San Nicholás of Bari was demolished to make way for 9 de Julio Avenue, whose construction demanded the razing of multiple city blocks, including several mansions along for example Cerrito St. The church was later rebuilt on another location. In response to the Bombing of Plaza de Mayo in 1955 several churches in Buenos Aires were burned and looted, including Santo Domingo convent, St. Ignatius Church, Basilica of San Francesco, St. Michael's Church and Basilica of Saint Nicholas of Bari. The ornate building of the Jockey Club on Florida Avenue was also destroyed. Historic buildings in the city of San Juan were destroyed by an earthquake in 1944, including the Cathedral and the Government House. The 1773 Marquez Bridge over the Reconquista River was renewed in 1964 and declared a National Historic Monument of Argentina. In 1997, the Company Autopistas del Oeste demolished it. On 18 October 1977, a fire burned the Teatro Argentino to the ground in La Plata. The building was later rebuilt, but in a different style. On 3 April 1979 a fire destroyed about half of the Hotel Castilla along Avenida de Mayo. The ornate Avenida Theatre housed inside survived intact, but the destroyed portions of the building have not been rebuilt. Brazil On 8 July 1978, the Museum of Modern Art of Rio de Janeiro was destroyed by fire. On 17 May 2010, the natural history collection of the Instituto Butantan was destroyed by fire. On 2 September 2018, the National Museum of Brazil was destroyed by fire. Peru Many of the quipu, an Andea system of encoding information in ropes via color and knots used by the Inca and other civilizations have been lost to decay of organic material and deliberate destruction. The knowledge of reading quipu was still present well into the colonial era but has since been lost. Uruguay On 16 July 1969, an original Flag of the Treinta y Tres from the Cisplatine War was stolen from the history museum by a revolutionary group called OPR-33. The historical flag was last seen in 1975 in Buenos Aires but has been considered missing since the day of its theft. This is still a matter of political debate. Venezuela On 17 October 2004, a fire in the Parque Central Complex of Caracas, Venezuela, destroyed the tower's planoteca, an archive containing the entire history of the country's public building plans spanning two centuries, including aqueduct and sewer systems. See also Art destruction Book burning and list of book-burning incidents List of destroyed libraries List of landmarks destroyed or damaged by climate change List of missing treasures List of World Heritage in Danger Lost work, lost artworks and list of lost films Slighting Virtual heritage World Monuments Fund References Sources Gaya Nuño, Juan Antonio. La arquitectura española en sus monumentos desaparecidos. Madrid, Espasa-Calpe, 1961. Gernet, Jacques (1996), A History of Chinese Civilization (2nd ed.), New York: Cambridge University Press, ISBN 978-0-521-49781-7 Mileusnić, Slobodan (1997). Spiritual Genocide: A survey of destroyed, damaged and desecrated churches, monasteries and other church buildings during the war 1991–1995 (1997). Belgrade: Museum of the Serbian Orthodox Church. Williams, Thomas J. T. (2012). "A Blaze in the Northern Sky: Black Metal and Crimes Against Culture". Public Archaeology. 11 (2): 59–72. doi:10.1179/1465518712Z.0000000006. S2CID 145058950. Давидов, Динко (2015). Independent State of Croatia: Total Genocide, 1941-1945. Svet Knjige. ISBN 978-86-7396-536-9. Perica, Vjekoslav (2002). Balkan Idols: Religion and Nationalism in Yugoslav States. Oxford University Press. ISBN 9780195174298. Petersen, A. (2017). "Shrine of Husayn's Head". Bones of Contention: Muslim Shrines in Palestine. Heritage Studies in the Muslim World. Springer Singapore. ISBN 978-981-10-6965-9. Retrieved 6 January 2023. Talmon-Heller, Daniella; Kedar, B. Z.; Reiter, Y. (January 2016). "Vicissitudes of a Holy Place: Construction, Destruction and Commemoration of Mashhad Ḥusayn in Ascalon". Der Islam. 93 (1). Walter de Gruyter. doi:10.1515/islam-2016-0008. ISSN 1613-0928. Thapar, Romila (2004). Somanatha: The Many Voices of a History. Penguin Books India. ISBN 1-84467-020-1. Yagnik, Achyut; Sheth, Suchitra (2005), The Shaping of Modern Gujarat: Plurality, Hindutva, and Beyond, Penguin Books India, p. 39, ISBN 978-0-14-400038-8 Further reading Askew, Rachel (2016). "Political iconoclasm: the destruction of Eccleshall Castle during the English Civil Wars". Post-Medieval Archaeology. 50 (2): 279–288. doi:10.1080/00794236.2016.1203547. S2CID 157307448. Bajgora, Sabri (2014). Destruction of Islamic Heritage in the Kosovo War 1998–1999. Pristina: Interfaith Kosovo, Ministry of Foreign Affairs of the Republic of Kosovo. ISBN 978-9951-595-02-5. Narain, Harsh (1993). The Ayodhya Temple Mosque Dispute: Focus on Muslim Sources. Delhi: Penman Publishers. ISBN 9788185504162 Rakoczy, Lila (2007). Archaeology of Destruction: A Reinterpretation of Castle Slightings in the English Civil War (PhD). University of York. OCLC 931130655. Shourie, Arun, S.R. Goel, Harsh Narain, J. Dubashi and Ram Swarup. Hindu Temples - What Happened to Them Vol. I, (A Preliminary Survey) (1990) ISBN 81-85990-49-2 External links Targeting History and Memory, SENSE - Transitional Justice Center (dedicated to the study, research, and documentation of the destruction and damage of historic heritage during the Balkan Wars of the 1990s. The website contains judicial documents from the International Criminal Tribunal for the former Yugoslavia). http://www.cracked.com/article_20149_6-mind-blowing-archeological-discoveries-destroyed-by-idiocy_p2.html The term digital native describes a person who has grown up in the information age. The term "digital native" was coined by Marc Prensky, an American writer, speaker and technologist who wrote several articles referencing this subject. This term specifically applied to the generation that grew up in the "digital age," predominantly regarding individuals born from 1980 onwards, namely Millennials, Generation Z, and Generation Alpha. Individuals from these demographic cohorts can quickly and comfortably locate, consume and send digital information through electronic devices and platforms such as computers, mobile phones, and social media. Digital natives are distinguished from digital immigrants, people who grew up in a world dominated by print and television, because they were born before the advent of the Internet. The digital generation grew up with increased confidence in the technology that they were encircled and engulfed in. This was thanks in part to their predecessors growing interest into a subject that was previously an unknown. Due to their upbringing, this digital generation of youth became fixated on their technologies as it became an ingrained, integral and essential way of life. Prensky concluded that due to the volume of daily interactions with technology, the digital native generation had developed a completely different way of thinking. Though the brains may not have changed physically, pathways and thinking patterns had evolved, and brains had changed to be physiologically different than those of the bygone era. Repeated exposure had helped grow and stimulate certain regions of the brain, while other unused parts of the brain were reduced in size. The terms digital native and digital immigrant are often used to describe the digital generation gap in terms of the ability of technological use among people born after 1980 and those born before. The term digital native is a highly contested concept, being considered by many education researchers as a persistent myth not founded on empirical evidence, and many argue for a more nuanced approach in understanding the relationship between digital media, learning and youth. Origin Native–immigrant analogy terms, referring to age groups' relationships with and understanding of the Internet, were used as early as 1995 by John Perry Barlow in an interview, and used again in 1996 as part of the Declaration of the Independence of Cyberspace. The specific terms digital native and digital immigrant were popularized by education consultant Marc Prensky in his 2001 article entitled Digital Natives, Digital Immigrants, in which he relates the contemporary decline in American education to educators' failure to understand the needs of modern students. His article posited that "the arrival and rapid dissemination of digital technology in the last decade of the 20th century" had changed the way students think and process information, making it difficult for them to excel academically using the outdated teaching methods of the day. In other words, children raised in a digital, media-saturated world, require a media-rich learning environment to hold their attention, and Prensky dubbed these children "digital natives". He also goes on to say that Digital Natives have "spent their entire lives surrounded by and using computers and videogames, digital music players, videocams, cell phones and all other toys and tools of the digital age". Globally, 30 percent of the population born between 1988 and 1998 had used the Internet for over five years as of 2013. Conceptualization and development Digital Natives, Digital Immigrants Marc Prensky defines the term "digital native" and applies it to a new group of students enrolling in educational establishments referring to the young generation as "native speakers" of the digital language of computers, videos, video games, social media and other sites on the internet. Contextually, his ideas were introduced after a decade of worry over increased diagnosis of children with ADD and ADHD, which itself turned out to be largely overblown. Prensky did not strictly define the digital native in his 2001 article, but it was later, arbitrarily, applied to children born after 1980, because computer bulletin board systems and Usenet were already in use at the time. The idea became popular among educators and parents whose children fell within Prensky's definition of a digital native, and has since been embraced as an effective marketing tool. It is important to note that Prensky's original paper was not a scientific one, and that no empirical data exists to support his claims. However, the concept has been widely addressed in the academic literature since, mainly in education research, but also in health research. A review published in 2024 found nearly 1,900 academic articles to have used the term digital native since the mid-2000s, noting how the meaning of the term had evolved to cover everything from businesses and start-ups to new generations of "AI natives". Prensky has since abandoned his digital native metaphor in favor of "digital wisdom". The Digital Visitor and Resident idea has been proposed as one alternative to understanding the various ways individuals engage with digital technology. It is also argued that digital native and digital immigrant are labels that oversimplify the classification scheme and that there are categories that can be considered "undetermined" based from the framework of the previous assignations. The critique of Prensky's conceptualization has resulted in further refinement of the terms. For instance, digital natives have been further classified into three types: the avoiders, minimalists, and enthusiastic participants. The avoiders are those who do not depend on technological devices and use technology minimally while the minimalists make use of the trends, although not as often as the enthusiastic participants. People who were "born digital" first appeared in a series of presentations by Josh Spear beginning in May 2007. A Digital Native research project is being run jointly by the Berkman Center for Internet & Society at Harvard Law School and the Research Center for Information Law at the University of St. Gallen in Switzerland. A collaborative research project is being run by Hivos, Netherlands and the Bangalore-based Centre for Internet and Society. The Net Generation Encountering eLearning at University Project funded by the UK research councils was completed in March 2010. More recently the Museum of Social Media, launched in 2012, has included an exhibition about "Digital Natives & Friends". Conflicts between generations The generational changes between digital natives, and their predecessors, the digital immigrant, have been astronomical. According to a study conducted by Tapscott, after interviewing and studying 11,000 young digital natives, he was able to determine eight different social norms between digital natives and the digital immigrants before them. Digital natives were offered the freedom, creativity to customize and ability to scrutinize, unlike their predecessors. For this generation, digital natives prioritized corporate integrity and openness when it was time for them to choose a career. These digital natives also began to seek entertaining and innovating career choices. Corporate integrity and openness also applied to consumer products, with digital natives more likely to choose products recommended by their friends. With playful mentalities, this new generation also brought with them a "need for speed". The eight social norms differentiated these digital natives, as well as the development of the need for approval from their peer groups. Because many digital immigrants are used to a life without digital technology or may be hesitant to adapt to it, they can sometimes be at variance with digital natives in their view of it. The everyday regimen of work-life is becoming more technologically dependent with advancements such as computers in offices, improved telecommunication, and more complex machinery in industry. This can make it difficult for digital immigrants to keep pace, which has the potential to create conflict between older supervisors and managers and an increasingly younger workforce. Similarly, parents of digital natives clash with their children at home over gaming, texting, YouTube, Facebook and other Internet technology issues. Much of the world's Millennials and Generation Z members are digital natives. According to law professor and educator John Palfrey, there may be substantial differences between digital natives and non digital natives, in terms of how people see relationships and institutions and how they access information. In spite of this, the timetable for training young and old on new technology is about the same. Prensky states that education is the single largest problem facing the digital world as digital immigrant instructors, who speak an outdated language (that of the pre-digital age), are struggling to teach a population that speaks an entirely new language. Digital natives have had an increased exposure to technology, which has changed the way they interact and respond to digital devices. In order to meet the unique learning needs of digital natives, teachers need to move away from traditional teaching methods that are disconnected with the way students now learn. For the last 20 years, technology training for teachers has been at the forefront of policy. However, immigrants suffer complications in teaching natives how to understand an environment which is "native" to them and foreign to immigrants. Teachers not only struggle with proficiency levels and their abilities to integrate technology into the classroom, but also, display resistance towards the integration of digital tools. Since technology can be frustrating and complicated at times, some teachers worry about maintaining their level or professionalism within the classroom. Teachers worry about appearing "unprofessional" in front of their students. Although technology presents challenges in the classroom, it is still very important for teachers to understand how natural and useful these digital tools are for students. To meet the unique learning needs of digital natives, Forzani and Leu suggest that digital tools are able to respond immediately to the natural, exploratory, and interactive learning style of students today. Learning how to use these digital tools not only provides unique learning opportunities for digital natives, but they also provide necessary skills that will define their future success in the digital age. One preference to this problem is to invent computer games to teach digital natives the lessons they need to learn, no matter how serious. This ideology has already been introduced to a number of serious practicalities. For example, piloting an unmanned aerial vehicle (UAV) in the army consists of someone sitting in front of a computer screen issuing commands to the UAV via a hand-held controller which resembles, in detail, the model of controllers that are used to play games on an Xbox 360 game console. (Jodie C Spreadbury, Army Recruiting and Training Division). Gamification as a teaching tool has sparked interest in education, and Gee suggests this is because games have special properties that books cannot offer for digital natives. For instance, gamification provides an interactive environment for students to engage and practice 21st century skills such as collaboration, critical thinking, problem solving, and digital literacy. Gee presents four reasons why gamification provides a distinct way of learning to promote 21st century skills. First, games are based on problem solving and not on ones ability to memorize content knowledge. Second, gamification promotes creativity in digital natives where they are encouraged to think like a designer or modify to redesign games. Third, digital natives are beginning to co-author their games through the choices they make to solve problems and face challenges. Therefore, students' thinking is stimulated to promote metacognition since they have to think about their choices and how they will alter the course and outcome of the game. Lastly, through online gaming, digital natives are able to collaborate and learn in a more social environment. Positive effects from gaming have been seen, one effect is expanding social relationships either preexisting or forming new social bonds. Based on the literature, one can see the potential and unique benefits digital tools have. For example, online games help digital natives meet their unique learning needs. Furthermore, online gaming seems to provide an interactive and engaging environment that promotes the necessary skills digital natives will need to be successful in their future. Implication of technological and media landscapes Digital natives vary in demographic based on their region's technological and media landscapes. Not everyone agrees with the language and underlying connotations of the digital native. The term, by definition, makes the assumption that all digital natives have the same base familiarity with technology. Similarly, the term digital immigrants implies that this entire age group struggles with technological advancements. For instance, those on the disadvantaged side of the digital divide lack access to technology. In its application, the concept of the digital native preferences those who grow up with technology as having a special status, ignoring the significant difference between familiarity and creative application. Digital Natives are determined based on their educational and cultural backgrounds as well as their access to technology. As the adoption of digital technology hasn't been a unified phenomenon worldwide, digital natives are not all in the same age group. Self-perception also plays a role: individuals who do not feel confident in their use of technology will not be considered a native regardless of the formally mentioned factors. Referring to some generations using terminologies such as "Digital Natives" is made because these groups can create their own culture and characteristics. Here are some of the cultures and characteristics of "Digital Natives" (according to self-description): They feel familiar with digital devices. 54% of them have a smartphone as their first personal mobile phone. These devices are used for entertainment and as a requirement in educational endeavours. They tend to be individualistic. They can multitask or focus on a single medium when needed. They are realistic. Raised in affluence, "Digital Natives" think their future is unclear due to the prolonged economic recession and the Fourth Industrial Revolution. This kind of thinking makes them focus more on their reality Teaching digital natives Prensky states that as digital natives grew up in the age of technology, they embraced new ways to gather information and communicate. This can be through social media, or through different computer programs. Digital natives have become comfortable with the use of technology, often possessing different levels of digital literacy. This does not mean all digital natives possess the same skill level or know how to use proper digital literacy. Digital natives developed these skills through the use of computers, phones, social media, research, etc. outside of formal education. Technology being a part of and introduced to classrooms meets the needs of digital natives. Educational technology in schools does not mean removing human relationships, as teacher-student relationships are essential to learning. Digital natives tend to be "masters" of multitasking; the ability to access and process large amounts of information at a time can allow digital natives to jump from one task to another or in parallel. There is a noted preference for visual and graphic learning rather than plain text. Concentration and attentiveness differs among digital natives compared to their predecessors; use of technology in the classroom increases student interactivity, and visuals such as slide shows allow for educators to share information easily while increasing student attentiveness. Digital natives must be interested in what they are learning; interactivity is important to aid in engagement, application of learned material, and in learning to connect various pieces of knowledge to each other. Applying skills, whether it be in a game, program, creating a blog, etc., provides digital natives with first-hand experiences of events, observations, and manipulation of natural processes. Digital natives like to be challenged in what they are learning as it is an opportunity to discover new information. Further learning is stimulated by the desire to know more and exploration of new information. This type of engagement gives natives a chance to be creative, to explore, research, and increases their ability to explain, elaborate, and evaluate what they have done in a meaningful way. Age discrimination in recruitment The popular adoption of the term "digital native" has caused concern to equality campaigners in various countries, including the United States and the UK, where the term has been used as a process for selection in recruitment. This has resulted in a number of successful legal claims. USA In 2017, in the USA, evidence had been provided to the Equal Employment Opportunity Commission that a test case needed to come before the courts to consider the lawfulness of recruiting for "digital natives". In January 2018, Maurice Anscombe, Lura Callahan, and the CWA filed complaints with the Equal Employment Opportunity Commission, claiming that the Target Corporation published job ads on a social media platform in 2017 that were directed to younger workers within certain age ranges only, and not to older workers. On 17th May 2023, in a voluntary agreement announced in a press release issued by AARP, the corporation confirmed that they would not exclude applicants based on "adjectives that describe people in relation to their age (such as “millennial” or “digital native”)." Germany On 18th January 2024, a 51-year-old in Germany brought a case in the ArbG Heilbronn 8th Chamber which ruled that the wording “digital native” in the job advertisement discriminated against them on the grounds of age and violated the prohibition set forth in Section 7 of the Equal Treatment Act (AGG), Germany's law created as a result of the Equality Directive 2000. UK In 2024, in the UK, a 54-year-old who had been rejected from roles within the Civil Service (United Kingdom) advertising both for "digital natives" and "social media natives" began litigation in the UK courts to force the British Government to stop using the terms. See also Digital addict Digital phobic Homo Ludens Information Age Information society Internet age Millennial pause Online identity Social Age Thumb tribe References Notes New Millennium Learners. Initial findings on the effects of digital technologies on school-age learners (PDF). OECD/CERI International Conference "Learning in the 21st Century: Research, Innovation and Policy" 15–16 May 2008 Paris. 2008. Paul Maunders (2007-11-04). "Public email to army about Xbox UAVs: Army fly UAV Spy Plane with Xbox 360 Controller | Paul Maunders | Web log". Pyrosoft.co.uk. Archived from the original on 2013-12-16. Retrieved 2013-12-10. Shah Nishant and Sunil Abraham, Digital Natives with a Cause? (2009) available online White, David S.; Le Cornu, Alison (23 August 2011). "Visitors and Residents: A new typology for online engagement". First Monday. doi:10.5210/fm.v16i9.3171. hdl:10818/12424. Rojas, Viviana. "Communities, Cultural Capital and the Digital Divide" Media Access: Social and Psychological Dimensions of New Technology Use. Mahwah: Lawrence Erlbaum Associates. Jenkins, Henry (4 December 2007). "Reconsidering Digital Immigrants". Retrieved 5 December 2007. Forzani, Elena; Leu, Donald J. (October 2012). "New Literacies for New Learners: The Need for Digital Technologies in Primary Classrooms". The Educational Forum. 76 (4): 421–424. doi:10.1080/00131725.2012.708623. S2CID 143840579. Hicks, Stephanie Diamond (August 2011). "Technology in Today's Classroom: Are You a Tech-Savvy Teacher?". The Clearing House: A Journal of Educational Strategies, Issues and Ideas. 84 (5): 188–191. doi:10.1080/00098655.2011.557406. S2CID 142593701. Morgan, Hani (1 March 2014). "Using Digital Story Projects to Help Students Improve in Reading and Writing". Reading Improvement. 51 (1): 20–26. Morgan, Hani (January 2014). "Maximizing Student Success with Differentiated Learning". The Clearing House: A Journal of Educational Strategies, Issues and Ideas. 87 (1): 34–38. doi:10.1080/00098655.2013.832130. S2CID 143716585. Parker, J; Lazaros, E J (2014). "Teaching 21st century skills and STEM concepts in the elementary classroom". Children's Technology and Engineering. 8 (4): 24–27. Further reading Aducci, Romina; et al. (2008), "The Hyperconnected: Here They Come!", An IDC Whitepaper Sponsored by Nortel (published May 2008), archived from the original on 2010-08-08 European Commission. Joint Research Centre. Institute for Prospective Technological Studies (2009). Young people and emerging digital services: An exploratory survey on motivations, perceptions and acceptance of risks. Publications Office. doi:10.2791/68925. ISBN 978-92-79-11330-7. Manafy, Michelle; Gautschi, Heidi (2011), Dancing With Digital Natives: Staying in Step With the Generation That's Transforming the Way Business is Done, Medford, NJ: CyberAge Books, p. 394, ISBN 978-0-910965-87-3 Palfrey, John; Gasser, Urs (2008), Born Digital: Understanding the First Generation of Digital Natives, Basic Books Position Papers for the Digital Natives With a Cause? Thinkathon: 6–8 December 2010, The Hague Museum for Communication (published December 2010), 2010, archived from the original on 2011-01-24 Thomas, Michael (2011), Deconstructing Digital Natives: Young People, Technology and the New Literacies, Routledge (published May 2011) External links Debate on Digital Native EDUCAUSE 2007 Podcast: Tomorrow's Students: Are We Ready for the New 21st-Century Learners? Commercial Media Viewing Habits: Digital Natives versus Digital Immigrants – Graduate Thesis Paper Video experience on a 20-month-old baby who discovers a touchscreen Ongoing research project 'Digital Natives with a cause' being conducted by www.cis-india.org in India Digital Learners in Higher Education research project Net Gen Skeptic – a blog that tracks the net generation discourse. Museum of Social Media – a museum that includes scholarly articles on digital natives and the impact of social media. Tarantism ( TERR-ən-tiz-əm) is a form of hysteric behaviour originating in Southern Italy, popularly believed to result from the bite of the wolf spider Lycosa tarantula (distinct from the broad class of spiders also called tarantulas). A better candidate cause is Latrodectus tredecimguttatus, commonly known as the Mediterranean black widow or steppe spider, although no link between such bites and the behaviour of tarantism has ever been demonstrated. However, the term historically is used to refer to a dancing mania – characteristic of Southern Italy – which likely had little to do with spider bites. The tarantella dance supposedly evolved from a therapy for tarantism. History It was originally described in the 11th century. The condition was common in Southern Italy, especially in the province of Taranto, during the 16th and 17th centuries. There were strong suggestions that there is no organic cause for the heightened excitability and restlessness that gripped the victims. The stated belief of the time was that victims needed to engage in frenzied dancing to prevent death from tarantism. Supposedly a particular kind of dance, called the tarantella, evolved from this therapy. A prime location for such outbursts was the church at Galatina, particularly at the time of the Feast of Saints Peter and Paul on 29 June. "The dancing is placed under the sign of Saint Paul, whose chapel serves as a "theatre" for the tarantulees' public meetings. The spider seems constantly interchangeable with Saint Paul; the female tarantulees dress as "brides of Saint Paul". As a climax, "the tarantulees, after having danced for a long time, meet together in the chapel of Saint Paul and communally attain the paroxysm of their trance, ... the general and desperate agitation was dominated by the stylised cry of the tarantulees, the 'crisis cry', an ahiii uttered with various modulations". Francesco Cancellieri, in his exhaustive treatise on Tarantism, takes note of semi-scientific, literary, and popular observations, both recent and ancient, giving each similar weight. He notes a report that in August 1693, a doctor in Naples had himself been bitten by two tarantulas, with six witnesses and a notary, but did not suffer the dancing illness. Cancellieri in part attributes this illness not only to the spiders but to the locale, since Tarantism was mainly seen in Basilicata, Apulia, Sicily, and Calabria. He states: Quando uno è punto da questa mal augurata bestia, si fanno cento diverse mosse in un momento. Si piange, si balla, si vomita, si trema, si ride, s'impallidisce, si grida, si sviene, si soffre gran dolore, e finalmente dopo qualche giorno si muore, se uno non è soccorso. Il sudore, e gli antidoti sollevano l'ammalato; ma il sovrano, ed unico rimedio è la Musica. When one is in the hold of this ill-wished beast, one has a hundred different feelings at a time. One cries, dances, vomits, trembles, laughs, pales, cries, faints, and one will suffer great pain, and finally after a few days, if unaided, you die. Sweat and antidotes relieve the sick, but the sovereign and the only remedy is Music. He goes on to describe some specific observations of the malady, typically afflicting peasants, alone or in groups. The malady typically affected peasants on hot summer days, causing indolence. Then he describes how only treatment through dancing music could restore them to vitality; for example: [...] e trovammo il misero contadino oppresso da difficile respirazione, ed osservammo inoltre, che la faccia, e le mani erano incominciate a divenir nere. E perchè il suo male era a tutti noto, si portò la Chitarra, la cui armonìa subito, che da lui fu intesa, cominciò a mover prima li piedi, poco dipoi le gambe. Si reggeva appresso sulle ginocchia. Indi a poco intervallo s'alzò passenggiando. Finalmente fra lo spazio di un quarto d'ora saltava si, che si sollevava ben tre palmi da terra. Sospirava, ma con empito sì grande, che portava terrore a' circostanti; e prima d'un'ora se gli tolse il nero dalle mani, e dal viso, riacquistando il suo natio colore. [...] and we found the poor peasant oppressed with difficult breathing, and we observed also that the face and hands had started to become black. And 'cause his illness was known to all, a guitar was brought, whose harmony immediately that he was understood, began first moving the feet, legs shortly afterwards. He stood on his knees. Soon after an interval he arose swaying. Finally, in the space of a quarter of an hour he was leaping, nearly three palms from the ground. Sighed, but with such great impetus, that it terrorised bystanders, and before an hour, the black was gone from his hands and face, and he regained his native colour. Interpretation and controversy John Crompton proposed that ancient Bacchanalian rites that had been suppressed by the Roman Senate in 186 BC went underground, reappearing under the guise of emergency therapy for bite victims. Although the popular belief persists that tarantism results from a spider bite, it remains scientifically unsubstantiated. Donaldson, Cavanagh, and Rankin (1997) conclude that the actual cause or causes of tarantism remain unknown. Modern times In recent years, tarantism has been defined by its connection to dance and music. In the 1990s and 2000s, people began rediscovering the genre of Tarantella, and in particular, the pizzica. In 1998, Salento began hosting an annual music festival, Notte della Taranta. Musicians tour throughout the region, and the festival culminates with a large late-night concert held in Melpignano. Composer and musician, Ludovico Einaudi directed the festival in 2010 and 2011, and released his album Taranta Project in 2015. Cultural references Many historical and cultural references are associated with this disease and the ensuing "cure" – the tarantella. It is, for example, a key image in Henrik Ibsen's A Doll's House and the spell "Tarantallegra" from the Harry Potter series. It was also mentioned in the novel 39 Clues: Superspecial Outbreak. The mention of the spider "tarantula" and description of its venom and the associated addiction has been depicted in the Indian television show "Byomkesh Bakshi" in episode 4 titled "Makdi ka Ras/makorshar rawsh". See also Dancing mania Ergotism Spider bite References Notes Sources Anon (1968). Tarantism: St. Paul and the Spider, in Essays and Reviews form the Times Literary Supplement. London: Oxford University Press, pp. 172–183. Originally published in the Times Literary Supplement, 27 April 1967. Cancellieri, Francesco (1817). Letters of Francesco Cancellieri to the ch. Signore Dottore Koreff, Professor of Medicine of the University of Berlin, about Tarantism, the airs of Roma, and of its countryside, and the Papal palaces inside, and outside, Rome: with the description of the Pontifical Castel Gandolfo, and surrounding countryside. (in Italian). Rome: Presso Francesco Bourlie. Crompton, John (1954). The Life of the Spider. Mentor Books. Donaldson, L.J.; Rankin, J. (July 1997). "The Dancing Plague: a public health conundrum". Public Health. 111 (4): 201–204. doi:10.1016/S0033-3506(97)00034-6. PMID 9242030. Hanna, Judith Lynne (2006) . Dancing for Health. Rowman Altamira. ISBN 0-7591-0859-5, ISBN 978-0-7591-0859-2. Rouget, Gilbert (1985) Music and Trance : a Theory of the Relations between Music and Possession. University of Chicago Press. ISBN 0-226-73006-9 Russell JF (October 1979). "Tarantism". Med Hist. 23 (4): 404–25. doi:10.1017/s0025727300052054. PMC 1082580. PMID 390267. Generation Z (often shortened to Gen Z), also known as zoomers, is the demographic cohort succeeding Millennials and preceding Generation Alpha. Researchers and popular media use the mid-to-late 1990s as starting birth years and the early 2010s as ending birth years, with the generation loosely being defined as people born around 1997 to 2012. Most members of Generation Z are the children of Generation X. As the first social generation to have grown up with access to the Internet and portable digital technology from a young age, members of Generation Z have been dubbed "digital natives" even if they are not necessarily digitally literate and may struggle in a digital workplace. Moreover, the negative effects of screen time are most pronounced in adolescents, as compared to younger children. Sexting became popular during Gen Z's adolescent years, although the long-term psychological effects are not yet fully understood. Generation Z has been described as "better behaved and less hedonistic" than previous generations. They have fewer teenage pregnancies, consume less alcohol (but not necessarily other psychoactive drugs), and are more focused on school and job prospects. They are also better at delaying gratification than teens from the 1960s. Youth subcultures have not disappeared, but they have been quieter. Nostalgia is a major theme of youth culture in the 2010s and 2020s. Globally, there is evidence that girls in Generation Z experienced puberty at considerably younger ages compared to previous generations, with implications for their welfare and their future. Furthermore, the prevalence of allergies among adolescents and young adults in this cohort is greater than the general population; there is greater awareness and diagnosis of mental health conditions, and sleep deprivation is more frequently reported. In many countries, Generation Z youth are more likely to be diagnosed with intellectual disabilities and psychiatric disorders than older generations. Generation Z generally hold left-wing political views, but has been moving towards the right since 2020. There is, however, a significant gender gap among the young around the world. A large percentage of Generation Z have positive views of socialism. East Asian and Singaporean students consistently earned the top spots in international standardized tests in the 2010s and 2020s. Globally, though, reading comprehension and numeracy have been on the decline. As of the 2020s, young women have outnumbered men in higher education across the developed world. Etymology and nomenclature The name Generation Z is a reference to the fact that it is the second generation after Generation X, continuing the alphabetical sequence from Generation Y (Millennials). Other proposed names for the generation included iGeneration, Homeland Generation, Net Gen, Digital Natives, Neo-Digital Natives, Pluralist Generation, Internet Generation, and Centennials. The rapper MC Lars used the term iGeneration as early as 2003, as a song title. Psychology professor and author Jean Twenge also used the term, intending it as the title of her 2006 book about Millennials but changing the title to Generation Me at the insistence of her publisher. Twenge later used the term for her 2017 book iGen. Others also claim to have coined the name. Author Neil Howe coined the term Homeland Generation in 2014, as a continuation of the Strauss–Howe generational theory with William Strauss. The term Homeland refers to being the first generation to enter childhood after protective surveillance state measures, like the Department of Homeland Security, were put into effect following the September 11 attacks. The Pew Research Center surveyed the various names for this cohort on Google Trends in 2019 and found that in the U.S., the term Generation Z was overwhelmingly the most popular, from then on calling it Gen Z in their research. The Merriam-Webster and Oxford dictionaries both have official entries for Generation Z. In Japan, the cohort is described as neo-digital natives, a step beyond the previous cohort described as digital natives. Digital natives primarily communicate by text or voice, while neo-digital natives use video, video-telephony, and movies. This emphasizes the shift from PC to mobile and text to video among the neo-digital population. Zoomer is an informal term used to refer to members of Generation Z. It combines the shorthand boomer, referring to baby boomers, with the "Z" from Generation Z. Zoomer in its current incarnation skyrocketed in popularity in 2018, when it was used in a 4chan internet meme mocking Gen Z adolescents via a Wojak caricature dubbed a "Zoomer". Merriam-Webster's records suggest the use of the term zoomer in the sense of Generation Z dates back at least as far as 2016. It was added to the Merriam-Webster dictionary in October 2021 and to Dictionary.com in January 2020. Prior to this, zoomer was occasionally used to describe particularly active baby boomers. Date and age range Researchers and popular media have used the mid-to-late 1990s as starting birth years and the early 2010s as ending birth years to define Generation Z. The Merriam-Webster Online Dictionary defines Generation Z as "the generation of people born in the late 1990s and early 2000s". The Oxford Dictionaries define Generation Z as "the group of people who were born between the late 1990s and the early 2010s, who are regarded as being very familiar with the internet". Encyclopedia Britannica defines Generation Z as "the term used to describe Americans born during the late 1990s and early 2000s. Some sources give the specific year range of 1997–2012, although the years spanned are sometimes contested or debated because generations and their zeitgeists are difficult to delineate." The Pew Research Center has defined 1997 as the starting birth year for Generation Z, basing this on "different formative experiences", such as new technological and socioeconomic developments, as well as growing up in a world after the September 11 attacks. Pew has not specified an endpoint for Generation Z, but used 2012 as a tentative endpoint for their 2019 report. Numerous news outlets use a starting birth year of 1997, often citing Pew Research Center. Various think tanks and analytics companies also have set a 1997 start date, as do various management and consulting firms. In a 2022 report, the U.S. Census designates Generation Z as "the youngest generation with adult members (born 1997 to 2013)". Statistics Canada used 1997 to 2012, citing Pew Research Center, in a 2022 publication analyzing their 2021 census. The Library of Congress uses 1997 to 2012, citing Pew Research as well. The Collins Dictionary define Generation Z as "members of the generation of people born between the mid-1990s and mid-2010s". In her book iGen (2017), psychologist Jean Twenge defines the "iGeneration" as the cohort born between 1995 and 2012. Other news outlets occasionally use 1995 as the starting birth year of Generation Z, as do various management and consulting firms. The Australian Bureau of Statistics have used 1996 to 2010 to define Generation Z in a 2021 Census report. Similarly, various management and consulting firms have used 1996 as a starting date for Generation Z. Individuals born in the Millennial and Generation Z cusp years have been sometimes identified as a "microgeneration" with characteristics of both generations. The most common name given for these cuspers is Zillennials. Individuals born on the cusp of Generation Z and Generation Alpha have been referred to as Zalphas. Arts and culture Happiness and personal values The Economist has described Generation Z as a more educated, well-behaved, stressed and depressed generation in comparison to previous generations. In 2016, the Varkey Foundation and Populus conducted an international study examining the attitudes of over 20,000 people aged 15 to 21 in twenty countries and that 59% of Gen Z youth were happy overall with the states of affairs in their personal lives. The most unhappy young people were from South Korea (29%) and Japan (28%) while the happiest were from Indonesia (90%) and Nigeria (78%). The best sources of happiness were being physically and mentally healthy (94%), having a good relationship with family (92%), and with friends (91%). In general, respondents who were younger and male tended to be happier. Religious faith was purportedly the least happiness-inducing. The top reasons for anxiety and stress were money (51%) and school (46%); social media and having access to basic resources (such as food and water) finished the list, both at 10%. Concerns over food and water were most serious in China (19%), India (16%), and Indonesia (16%); young Indians were also more likely than average to report stress due to social media (19%). Important personal values of Gen Z are their families and themselves get ahead in life (both 27%), followed by honesty (26%). Looking beyond their local communities came last at 6%. Familial values were especially strong in South America (34%) while individualism and the entrepreneurial spirit proved popular in Africa (37%). People who influenced youths the most were parents (89%), friends (79%), and teachers (70%). Celebrities (30%) and politicians (17%) came last. In general, young men were more likely to be influenced by athletes and politicians than young women, who preferred books and fictional characters. Celebrity culture was especially influential in China (60%) and Nigeria (71%) and particularly irrelevant in Argentina and Turkey (both 19%). For young people, the most important factors for their current or future careers were the possibility of honing their skills (24%), and income (23%) while the most unimportant factors were fame (3%) and whether or not the organization they worked for made a positive impact on the world (13%). The most important factors for young people when thinking about their futures were their families (47%) and their health (21%); the welfare of the world at large (4%) and their local communities (1%) bottomed the list. Common culture The COVID-19 pandemic struck when the oldest members of Generation Z were just joining the workforce and the rest were still in school. While Generation Z proved to be less resilient than older cohorts, their fundamental values did not change, and they remained open to change, such as the transition towards hybrid school and remote work. On average, Generation Z is more likely to value ambition, creativity, and curiosity than the general population, including Millennials. A 2020 survey conducted by the Center for Generational Kinetics, on 1,000 members of Generation Z and 1,000 Millennials, suggests that Generation Z still would like to travel, despite the COVID-19 pandemic and the recession it induced. However, Generation Z is more likely to look carefully for package deals that would bring them the most value for their money, as many of them are already saving money for buying a house and for retirement, and they prefer more physically active trips. Mobile-friendly websites and social-media engagements are both important. They take advantage of the Internet to market and sell their fresh produce. In Western countries like the United Kingdom, teenagers now prefer to get their news from social-media networks such as Instagram and TikTok and the video-sharing site YouTube rather than more traditional media, such as radio or television. Having a mobile device has become almost universal by the time the first wave of Generation Z reaches adolescence. Some even have their phones besides them in bed. But despite being digital natives, Generation Z also values in-person interactions and recognizes the limits of virtual communications. Among children and teenagers of the 2010s, much leisure time is spent watching television, reading, social networking, watching YouTube videos, and playing games on smartphones. Subcultures and nostalgia During the 2000s and especially the 2010s, youth subcultures that were as influential as what existed during the late 20th century became scarcer and quieter, at least in real life though not necessarily on the Internet, and more ridden with irony and self-consciousness due to the awareness of incessant peer surveillance. In Germany, for instance, youth appears more interested in a more mainstream lifestyle with goals such as finishing school, owning a home in the suburbs, maintaining friendships and family relationships, and stable employment, rather than popular culture, glamor, or consumerism. Boundaries between the different youth subcultures appear to have been blurred, and nostalgic sentiments have risen. Although nostalgia is normally associated with the elderly, this sentiment is now commonplace among those who came of age during the 2010s and 2020s. Struggling with present realities, Millennials and Generation Z long for the past, when life seemed simpler and less stressful, even if they have themselves never experienced it. For example, although an aesthetic dubbed 'cottagecore' in 2018 has been around for many years, it has become a subculture of Generation Z, especially on various social media networks in the wake of the mass lockdowns imposed to combat the spread of COVID-19. It is a form of escapism and aspirational nostalgia. Nostalgic sentiments surged during and after the COVID pandemic. Vintage fashion is growing in vogue among Millennial and Generation Z consumers. Nevertheless, large shares of Generation Z have never visited museums or heritage sites, preferring instead to watch television or browsing social media. Spotify consumer data from 2022 suggests that Generation Z is most nostalgic for the 1980s. The Netflix science-fiction horror series Stranger Things (2016–2025) is a major example of using and evoking nostalgia for the 1980s, enabling Generation Z to learn what their Generation X parents experienced in their youth during that decade. 1980s songs featured in the Stranger Things soundtracks that became popular among Generation Z included "Running Up That Hill" (1985) by Kate Bush, which has appeared in many TikTok videos. There is also evidence that Generation Z is also nostalgic for the Y2K era (the late 1990s and early 2000s), given the popularity of the Y2K aesthetic among this cohort. Other trends of fashion and lifestyles among Generation Z include VSCO girl, E-girl and E-boy, and Barbiecore, among many others, made popular by TikTok, Instagram, Pinterest, influencers and celebrities. In Japan, Generation Z has Shōwa nostalgia and the Shōwa-era music of Akina Nakamori, Seiko Matsuda and Yōko Oginome is popular with them. 1970s and 1980s city pop music, such as that of Mariya Takeuchi, is also popular with Generation Z, both in and outside of Japan. Television and streaming Figures from Nielsen and Magna Global revealed that the viewership of children's cable television channels such as Disney Channel, Cartoon Network, and Nickelodeon continued their steady decline from the early 2010s, with little to no alleviating effects due to the COVID-19 pandemic, which forced many parents and their children to stay at home. Disney Channel in particular lost a third of their viewers in 2020, leading to closures in Scandinavia, the United Kingdom, Australia, and Southeast Asia. On the other hand, streaming services saw healthy growth. In the United Kingdom, for instance, a majority of children and teenagers watched a film or series on Netflix rather than on television, according to a 2019 report by Childwise. By 2025, YouTube, Netflix, and Disney+ have become the most popular substitutes for traditional children's programming on television among young viewers. Generation Z continues to enjoy comfort television shows that first aired between the 1990s and early 2000s, such as The Office (2005–2013) and Friends (1994–2004). Meanwhile, the animated series Bluey (2018–present), though made for preschool children, has been surprisingly well-received among teenagers and young adults because it portrays family life positively and makes them feel nostalgic. It also helps many Millennials and members of Generation Z heal emotional wounds from their childhoods. Global demand for Japanese animations (anime) is projected to continue growing until at least 2030 due to interest among young people. Reading habits According to a 2019 OECD survey, members of Generation Z were spending more time on electronic devices and less time reading books than before, with implications for their attention spans, vocabulary, academic performance, and future economic contributions. In New Zealand, child development psychologist Tom Nicholson noted a marked decline in vocabulary usage and reading among schoolchildren, many of whom are reluctant to use the dictionary. According to a 2008 survey by the National Education Monitoring Project, about one in five four-year and eight-year pupils read books as a hobby, a ten-percent drop from 2000. In the United Kingdom, children and teenagers of the 2010s reportedly spent more time playing video games and watching YouTube videos but less time reading. By 2022, Generation Z accounted for the majority of book purchases in that country. However, teenage girls are much more likely than boys to read for pleasure. About one in three children struggle with finding something interesting to read. According to the Progress in International Reading Literacy Study (PIRLS), fourth graders in 2016, in 13 out of 20 countries and territories surveyed, were markedly less enthusiastic about reading than their predecessors in 2001 while their parents were even less keen on reading than they were. Among members of Generation Z who read, romantic fantasy and Japanese comics (manga), such as One Piece (1997–present) or Naruto (1999–2014) are some of the most popular. Unlike older cohorts, they are fond of fan fiction and escapism. In addition, BookTok, a community on Tiktok, has many members from Generation Z, especially teenage girls and young women. BookTok has stimulated a revival of volitional reading among the young and a surge in book sales for publishers. Fan fiction During the first two decades of the 21st century, writing and reading fan fiction and creating fandoms of fictional works became a prevalent activity worldwide. Demographic data from various depositories revealed that those who read and wrote fan fiction were overwhelmingly young, in their teens and twenties, and female. For example, an analysis published in 2019 by data scientists Cecilia Aragon and Katie Davis of the site FanFiction.Net showed that some 60 billion words of contents were added during the previous 20 years by 10 million English-speaking people whose median age was 151⁄2 years. Fan fiction writers base their work on various internationally popular cultural phenomena such as K-pop, Star Trek, Harry Potter, Twilight, Star Wars, Doctor Who, and My Little Pony, known as 'canon', as well as other things they considered important to their lives, like natural disasters. Much of fan fiction concerns the romantic pairing of fictional characters of interest, or 'shipping'. Aragon and Davis argued that writing fan fiction stories could help young people combat social isolation and hone their writing skills outside of school in an environment of like-minded people where they can receive (anonymous) constructive feedback, what they call 'distributed mentoring'. Informatics specialist Rebecca Black added that fan fiction writing could also be a useful resource for English-language learners. Indeed, the analysis of Aragon and Davis showed that for every 650 reviews a fan fiction writer receives, their vocabulary improved by one year of age, though this may not generalize to older cohorts. On the other hand, children browsing fan fiction contents might be exposed to cyberbullying, crude comments, and other inappropriate materials. Music Generation Z has a plethora of options when it comes to music consumption, allowing for a highly personalized experience. Spotify and terrestrial radio are the top choices for music listening while YouTube is the preferred platform for music discovery. In mid-2023, Spotify reported more growth than expected in the number of subscribers among Generation Z. Additional research showed that within the past few decades, popular music has gotten slower; that majorities of listeners young and old preferred older songs rather than keeping up with new ones; that the language of popular songs was becoming more negative psychologically; and that lyrics were becoming simpler and more repetitive, approaching one-word sheets, something measurable by observing how efficiently lossless compression algorithms (such as the LZ algorithm) handled them. On the other hand, texture and rhythm are becoming more complex. Streaming services have made it extremely easy for listeners to sample songs, creating pressure on musicians to compose songs that are as easy to process and have as many hooks as possible. Sad music is quite popular among adolescents, though it can dampen their moods, especially among girls. Demographics Although many countries have aging populations and declining birth rates, Generation Z is currently the largest generation alive. Bloomberg's analysis of United Nations data predicted that, in 2019, members of Generation Z accounted for 2.47 billion (32%) of the 7.7 billion inhabitants of Earth, surpassing the Millennial population of 2.43 billion. The generational cutoff of Generation Z and Millennials for this analysis was placed at 2000 to 2001. Africa Generation Z currently comprises the majority of the population of Africa. In 2017, 60% of the 1.2 billion people living in Africa fell below the age of 25. In 2019, 46% of the South African population, or 27.5 million people, are members of Generation Z. Statistical projections from the United Nations in 2019 suggest that, in 2020, the people of Niger had a median age of 15.2, Mali 16.3, Chad 16.6, Somalia, Uganda, and Angola all 16.7, the Democratic Republic of the Congo 17.0, Burundi 17.3, Mozambique and Zambia both 17.6. This means that more than half of their populations were born in the first two decades of the 21st century. These are the world's youngest countries by median age. Asia According to a 2022 McKinsey & Company insight, Generation Z will account for a quarter of the population of the Asia-Pacific region by 2025, and possess a global spending power of approximately US$140bn by 2030. As a result of cultural ideals, government policy, and modern medicine, there have been severe gender population imbalances in China and India. According to the United Nations, in 2018, there were 112 Chinese males for every hundred females ages 15 to 29; in India, there were 111 males for every hundred females in that age group. China had a total of 34 million excess males and India 37 million, more than the entire population of Malaysia. Together, China and India had a combined 50 million excess males under the age of 20. Such a discrepancy fuels loneliness epidemics, human trafficking (from elsewhere in Asia, such as Cambodia and Vietnam), and prostitution, among other societal problems. Population pyramids of China, India, Japan, and Singapore in 2016 Europe Out of the approximately 66.8 million people of the UK in 2019, there were approximately 12.6 million people (18.8%) in Generation Z, if defined as those born from 1997 to 2012. Generation Z is the most diverse generation in the European Union in regards to national origin. In Europe generally, 13.9% of those ages 14 and younger in 2019 (which includes older Generation Alpha) were born in another EU Member State, and 6.6% were born outside the EU. In Luxembourg, 20.5% were born in another country, largely within the EU (6.6% outside the EU compared to 13.9% in another member state); in Ireland, 12.0% were born in another country; in Sweden, 9.4% were born in another country, largely outside the EU (7.8% outside the EU compared to 1.6% in another member state). In Finland, 4.5% of people aged 14 and younger were born abroad and 10.6% had a foreign-background in 2021. However, Gen Z from eastern Europe is much more homogeneous: in Croatia, only 0.7% of those aged 14 and younger were foreign-born; in the Czech Republic, 1.1% aged 14 and younger were foreign-born. Higher portions of those ages 15 to 29 in 2019 (which includes younger Millennials) were foreign born in Europe. Luxembourg had the highest share of young people (41.9%) born in a foreign country. More than 20% of this age group were foreign-born in Cyprus, Malta, Austria and Sweden. The highest shares of non-EU born young adults were found in Sweden, Spain and Luxemburg. Like with those under age 14, countries in eastern Europe generally have much smaller populations of foreign-born young adults. Poland, Lithuania, Slovakia, Bulgaria and Latvia had the lowest shares of foreign-born young people, at 1.4 to 2.5% of the total age group. Population pyramids of France, Greece, and Russia in 2016 North America Data from Statistics Canada published in 2017 showed that Generation Z comprised 17.6% of the Canadian population.A report by demographer William Frey of the Brookings Institution stated that in the United States, the Millennials are a bridge between the largely white pre-Millennials (Generation X and their predecessors) and the more diverse post-Millennials (Generation Z and their successors). Frey's analysis of U.S. Census data suggests that as of 2019, 50.9% of Generation Z is white, 13.8% is black, 25.0% Hispanic, and 5.3% Asian. 29% of Generation Z are children of immigrants or immigrants themselves, compared to 23% of Millennials when they were at the same age. Members of Generation Z are slightly less likely to be foreign-born than Millennials; the fact that more American Latinos were born in the U.S. rather than abroad plays a role in making the first wave of Generation Z appear better educated than their predecessors. However, researchers warn that this trend could be altered by changing immigration patterns and the younger members of Generation Z choosing alternate educational paths. As a demographic cohort, Generation Z is smaller than the baby boomers and their children, the Millennials. According to the U.S. Census Bureau, Generation Z makes up about one quarter of the U.S. population, as of 2015. There was an 'echo boom' in the 2000s; this boom certainly increased the absolute number of future young adults, but did not significantly change the relative sizes of this cohort compared to their parents. According to a 2022 Gallup survey, 20.8%, or about one in five members, of Gen Z identify as LGBTQ+. Population pyramids of Canada, the United States, and Mexico in 2016 Economic trends Consumption As consumers, members of Generation Z are typically reliant on the Internet to research their options and to place orders. They tend to be skeptical and will shun firms whose actions and values are contradictory. Their purchases are heavily influenced by trends promoted by "influencers" on social media, as well as the fear of missing out (FOMO) and peer pressure. The need to be "trendy" is a prime motivator. Due to their relatively high income, Generation Z have higher spending habits. According to new research, they rely on social media to make purchasing decisions, with health and beauty products being the most consumed category on these platforms. In the West, while majorities might signal their support for certain ideals such as "environmental consciousness" to pollsters, actual purchases do not reflect their stated views, as can be seen from their high demand for cheap but not durable clothing ("fast fashion"), or preference for rapid delivery. Despite their socially progressive views, large numbers are still willing to purchase these items when human rights abuses in the developing countries that produce them are brought up. However, young Western consumers of this cohort are less likely to pay a premium for what they want compared to their counterparts from emerging economies. In China, young people have less disposable income than before due to a slowing economy. Even so, while they are saving money on basic necessities, they are willing to spend more money on hobbies or items that make them feel happy. In culturally modernizing Saudi Arabia, where 63% of the population was under the age of 30 as of 2024, luxury brands have seen growth in the market aimed at young consumers, most of whom make online purchases and prefer products that not only reflects their cultural heritage but are also modern. In the United Kingdom, Generation Z's general avoidance of alcohol and tobacco has noticeably reduced government revenue in the form of the 'sin tax'. Indeed, many young Britons remain dependent on their parents to pay their bills in a stagnant economy and about a quarter spends virtually nothing on luxuries. In much of Western Europe, Generation Z faces economic stagnation or even falling standards of living. But in the United States, the reverse is true. Food choices The food choices made by Generation Z reflect the generation's concerns about climate, sustainability, and animal welfare. A study by catering firm Aramark found 79% of members of the generation would go meatless between once and twice a week. The generation is considered the most interested in plant-based and vegan food choices, which they see as equal to other food types. As Generation Z's purchasing power grows, so does the amount of vegan and vegetarian food they eat. Generation Z sees dining out with friends and sharing small plates of food as exciting and interesting. According to 2022 Ernst & Young data, plant-based meat, cultured meat, and fermented meat are forecast to grow to 40% of the market by volume by 2040 in the United States. Plant-based meat is widely available in supermarkets and restaurants, but cultured and fermented meats (which are made without slaughtering animals) are not commercially available but are now being developed by companies. Transportation choices Across the developed world, young people are noticeably less likely to get a driver's license or to own a car than older generations. This new trend is driven by the possibility of making online purchases, economic constraints, concerns for the environment, viability of alternatives to driving (walking, biking, public transit, and ride sharing), and growing restrictions on driving within urban areas. In the United States, however, decades of auto-centric urban development have led to under-investment in walkable neighborhoods, bicycle lanes, and public transit, making it likely that most members of Generation Z will eventually become frequent drivers, like the Millennials before them, even if they dislike cars. Employment According to the International Labor Organization (ILA), the COVID-19 pandemic has amplified youth unemployment, but unevenly. By 2022, youth unemployment stood at 12.7% in Africa, 20.5% in Latin America, and 8.3% in North America. In the early 2020s, Chinese youths find themselves struggling with job hunting. University education offers little help. In fact, due to the mismatch between education and the job market, those with no university qualifications are less likely to be unemployed. By June 2023, China's unemployment rate for people aged 16 to 24 was about one fifth. In South Korea, people below the age of 40 are increasingly interested in relocating from the cities, especially Seoul, to the countryside and working on the farm. Working in a conglomerate like Samsung or Hyundai Group no longer appeals to young people, many of whom prefer to avoid becoming a workaholic or are pessimistic about their ability to be as successful as their fathers. In Germany, some public officials are recommending shorter work weeks at the same salary levels in spite of the struggling German economy. The situation is similar in other European countries. In the United Kingdom, Generation Z is facing a gig economy with precarious prospects and stagnant wages. Many young Europeans with high skills are leaving their home countries for places that offer more job opportunities, higher salaries, and lower taxes; they typically choose another country in Europe with a stronger economy or the United States. In Canada, people aged 15 to 24 faced an unemployment rate of 12.2%, or more than twice that of prime working-age adults, as of 2025. Among university students, that number was over one fifth, the highest since the Great Recession of the late 2000s. Young graduates face not only a tough labor market, but also global trade wars, persistent inflation, industrial automation and artificial intelligence. In the United States, the youth unemployment rate (16–24) was 7.5% in May 2023, the lowest in 70 years. American high-school graduates could join the job market right away, with employers offering them generous bonuses, high wages, and apprenticeship programs in order to offset the ongoing labor shortage. Generation Z in the United States is projected to be richer than previous generations at the same age thanks to higher wage growth and greater inheritance from their parents and grandparents, who have accumulated enormous wealth. As of 2023, members of Generation Z in North America and especially developing Asian nations were a much more optimistic about their economic prospects and more likely to believe in the value of hard work than their counterparts in developed Asia, Western Europe, or Latin America. As workers, Generation Z tends to prioritize a financial security, meaning, and their own well-being. They also value a work–life balance. Education Since the mid-20th century, enrollment rates in primary schools has increased significantly in developing countries. In 2019, the OECD completed a study showing that while education spending was up 15% over the previous decade, academic performance had stagnated. Results from Trends in International Mathematics and Science Study (TIMSS) in 2019 showed that the highest-scoring students in mathematics came from Asian polities and Russia. The OECD's Program for International Student Assessment (PISA) tests administered in 2022 unveiled the continuation of a long-term decline in reading and mathematical skills since the early 2010s. In other words, the COVID-19 pandemic was only one contributing factor. Even so, fifteen-year-old students (tenth graders) from Singapore, Macau, Hong Kong, Taiwan, South Korea, and Japan were largely unaffected or even saw an improvement. Once high-performing European countries—Iceland, Sweden, and Finland—continued their years-long decline. The U.S. national average remained behind those of other industrialized nations. By 2024, many places around the world have decided to ban the use of mobile phones in the classroom to help their students concentrate better. Different nations and territories approach the question of how to nurture gifted students differently. During the 2000s and 2010s, whereas the Middle East and East Asia (especially China, Hong Kong, and South Korea) and Singapore actively sought them out and steered them towards top programs, Europe and the United States had in mind the goal of inclusion and chose to focus on helping struggling students. In 2010, for example, China unveiled a decade-long National Talent Development Plan to identify able students and guide them into STEM fields and careers in high demand; that same year, England dismantled its National Academy for Gifted and Talented Youth and redirected the funds to help low-scoring students get admitted to elite universities. Developmental cognitive psychologist David Geary observed that Western educators remained "resistant" to the possibility that even the most talented of schoolchildren needed encouragement and support and tended to concentrate on low performers. In addition, even though it is commonly believed that past a certain IQ benchmark (typically 120), practice becomes much more important than cognitive abilities in mastering new knowledge, recently published research papers based on longitudinal studies, such as the Study of Mathematically Precocious Youth (SMPY) and the Duke University Talent Identification Program, suggest otherwise. Among developed nations, young women have been outnumbering men in tertiary education during the 2020s, reversing a historical trend. At the same time, the number of men in their 20s who are in neither education, employment, or training (NEET) has been rising. In France and the United Kingdom, this number has surpassed that of women. Since the early 2000s, the number of students from emerging economies going abroad for higher education has risen markedly. This was a golden age of growth for many Western universities admitting international students. In the late 2010s, around five million students traveled abroad each year for higher education, with the developed world being the most popular destinations and China the biggest source of international students. In 2019, the United States was the most popular destination for international students, with 30% of its international student body coming from mainland China, Australia, Canada, the United Kingdom, and Japan. Among children of the Chinese ruling class ("princelings"), attending elite institutions in the United States was commonplace and seen as a status symbol, but the deterioration of Sino-American relations as exemplified by President Donald Trump's entry restrictions on Chinese students in addition to the complications produced by the COVID-19 pandemic reduced the number of Chinese students enrolling in many American colleges and universities. But even before the pandemic, undergraduate and graduate enrollments of native-born American citizens have both been in decline, while trade schools continue to attract growing numbers of students due to a shortage of high-skilled blue-collar workers. Since the 2000s, numerous institutions of higher learning have permanently closed. These trends have led to the speculation that the higher-education bubble in the United States might deflate. But among the top colleges and universities, there is still growth in the number of applicants. This is due partly to students sending their applications to more schools for a chance of getting admitted and because these institutions have not significantly expanded their capacities. Although international enrollments rebounded post-pandemic, with a surge of students coming from India and sub-Saharan Africa, dependency on foreign students is a long-term liability for many American schools, which now face a political zeitgeist that has turned against immigration. Meanwhile, in Canada, the government has cut the number of international student visas granted each year in response to growing public disapproval of current levels of immigration. The same thing happened in Australia. Because China's expansion of higher education was done for political rather than economic reasons, the country is currently overproducing university graduates, who are struggling to find white-collar jobs that match their education. In 2023, as many as one in five Chinese graduates struggled to find gainful employment. Enrollment in higher education was just under 60% during the early 2020s, compared to around 40% in the United States. In response, the government has recommended that students and their families consider vocational training programs to fill factory jobs. Health issues Mental In general, teenagers and young adults are especially vulnerable to depression and anxiety due to the changes to the brain during adolescence. While materially well off, young people today commonly perceive the world in which they live to be highly precarious, complex, and ambiguous, which has a negative effect on their mental well-being. A 2020 meta-analysis found that the most common psychiatric disorders among adolescents were ADHD, anxiety disorders, behavioral disorders, and depression, consistent with a previous one from 2015. Data from the World Health Organization (WHO) and the Program for International Student Assessment (PISA) indicate that while the percentages of teenagers reporting mental-health issues (such as psychological distress and loneliness) remained approximately the same during the 2000s, they steadily increased during the 2010s. While the COVID-19 pandemic has damaged the mental health of people of all ages, the increase was most noticeable for people aged 15 to 24. A 2021 UNICEF report stated that 13% of ten- to nineteen-year-olds around the world had a diagnosed mental health disorder whilst suicide was the fourth most common cause of death among fifteen- to nineteen-year-olds. It commented that "disruption to routines, education, recreation, as well as concern for family income, health and increase in stress and anxiety, [caused by the COVID-19 pandemic] is leaving many children and young people feeling afraid, angry and concerned for their future." It also noted that the pandemic had widely disrupted mental health services. Anxiety over climate change has compounded the problem. Though males remain more likely than females to commit suicide, the prevalence of suicide among teenage girls has risen significantly during the 2010s in many countries. For example, data from the British National Health Service (NHS) showed that in England, hospitalizations for self-harm doubled among teenage girls between 1997 and 2018, but there was no parallel development among boys. In some Western countries—Australia, the Netherlands, Spain, the United Kingdom, and parts of the United States—intervention programs have been set up to prevent depression among teenagers. However, funding has been limited. Sleep deprivation Sleep deprivation is on the rise among contemporary youths, due to a combination of poor sleep hygiene, caffeine intake, beds that are too warm, a mismatch between biologically preferred sleep schedules at around puberty and social demands, insomnia, growing homework load, and having too many extracurricular activities. Consequences of sleep deprivation include low mood, worse emotional regulation, anxiety, depression, increased likelihood of self-harm, suicidal ideation, and impaired cognitive functioning. In addition, teenagers and young adults who prefer to stay up late tend to have high levels of anxiety, impulsivity, alcohol intake, and tobacco smoking. A study by Glasgow University found that the number of schoolchildren in Scotland reporting sleep difficulties increased from 23% in 2014 to 30% in 2018. 37% of teenagers were deemed to have low mood (33% males and 41% females), and 14% were at risk of depression (11% males and 17% females). Older girls faced high pressure from schoolwork, friendships, family, career preparation, maintaining a good body image and good health. In Canada, teenagers sleep on average between 6.5 and 7.5 hours each night, much less than what the Canadian Paediatric Society recommends, 10 hours. According to the Canadian Mental Health Association, only one out of five children who needed mental health services received it. In Ontario, for instance, the number of teenagers getting medical treatment for self-harm doubled in 2019 compared to ten years prior. The number of suicides has also gone up. Various factors that increased youth anxiety and depression include over-parenting, perfectionism (especially with regards to schoolwork), social isolation, social-media use, financial problems, housing worries, and concern over some global issues such as climate change. Cognitive abilities A 2010 meta-analysis by an international team of mental health experts found that the worldwide prevalence of intellectual disability (ID) was around one percent. But the share of individuals with such a condition in low- to middle-income countries were up to twice as high as their wealthier counterparts. The researchers also found that ID was more common among children and adolescents than adults. A 2020 literature review and meta-analysis confirmed that the incidence of ID was indeed more common than estimates from the early 2000s. In 2013, a team of neuroscientists from the University College London published a paper on how neurodevelopmental disorders can affect a child's educational outcome. They found that up to 10% of the human population have specific learning disabilities or about two to three children in a (Western) classroom. Such conditions include dyscalculia, dyslexia, attention deficit hyperactivity disorder (ADHD), and autism spectrum disorder. A 2017 study from the Dominican Republic suggests that students from all sectors of the educational system utilize the Internet for academic purposes, yet those from lower socioeconomic backgrounds tend to rank the lowest in terms of reading comprehension skills. A 2020 report by psychologist John Protzko analyzed over 30 studies and found that children have become better at delaying gratification over the previous 50 years, corresponding to an average increase of 0.18 standard deviations per decade on the IQ scale. This is contrary to the opinion of the majority of the 260 cognitive experts polled (84%), who thought this ability was deteriorating. Researchers test this ability using the Marshmallow Test. Children are offered treats: if they are willing to wait, they get two; if not, they only get one. The ability to delay gratification is associated with positive life outcomes, such as better academic performance, lower rates of substance use, and healthier body weights. Possible reasons for improvements in the delaying gratification include higher standards of living, better-educated parents, improved nutrition, higher preschool attendance rates, more test awareness, and environmental or genetic changes. Some other cognitive abilities, such as simple reaction time, color acuity, working memory, the complexity of vocabulary usage, and three-dimensional visuospatial reasoning have shown signs of secular decline. In a 2018 paper, cognitive scientists James R. Flynn and Michael Shayer argued that the observed gains in IQ during the 20th century—commonly known as the Flynn effect—had either stagnated or reversed, as can be seen from a combination of IQ and Piagetian tests. In the Nordic nations, there was a clear decline in general intelligence starting in the 1990s, an average of 6.85 IQ points if projected over 30 years. In Australia and France, the data remained ambiguous; more research was needed. In the United Kingdom, young children experienced a decline in the ability to perceive weight and heaviness, with heavy losses among top scorers. In German-speaking countries, young people saw a fall in spatial reasoning ability but an increase in verbal reasoning skills. In the Netherlands, preschoolers and perhaps schoolchildren stagnated (but seniors gained) in cognitive skills. What this means is that people were gradually moving away from abstraction to concrete thought. On the other hand, the United States continued its historic march towards higher IQ, a rate of 0.38 per decade, at least up until 2014. South Korea saw its IQ scores growing at twice the average U.S. rate. The secular decline of cognitive abilities observed in many developed countries might be caused by diminishing marginal returns due to industrialization and to intellectually stimulating environments for preschoolers, the cultural shifts that led to frequent use of electronic devices, the fall in cognitively demanding tasks in the job market in contrast to the 20th century, and possibly dysgenic fertility. Physical A 2015 study found that the frequency of nearsightedness has doubled in the United Kingdom within the last 50 years. Ophthalmologist Steve Schallhorn, chairman of the Optical Express International Medical Advisory Board, noted that research has pointed to a link between the regular use of handheld electronic devices and eyestrain. The American Optometric Association sounded the alarm in a similar vein. According to a spokeswoman, digital eyestrain, or computer vision syndrome, is "rampant, especially as we move toward smaller devices and the prominence of devices increase in our everyday lives." Symptoms include dry and irritated eyes, fatigue, eye strain, blurry vision, difficulty focusing, headaches. However, the syndrome does not cause vision loss or any other permanent damage. To alleviate or prevent eyestrain, the Vision Council recommends that people limit screen time, take frequent breaks, adjust the screen brightness, change the background from bright colors to gray, increase text sizes, and blinking more often. Parents should not only limit their children's screen time but should also lead by example. While food allergies have been observed by doctors since ancient times and virtually all foods can be allergens, research by the Mayo Clinic in Minnesota found they have been growing increasingly common since the early 2000s. Today, one in twelve American children has a food allergy, with peanut allergy being the most prevalent type. Reasons for this remain poorly understood. Nut allergies in general have quadrupled and shellfish allergies have increased 40% between 2004 and 2019. In all, about 36% of American children have some kind of allergy. By comparison, this number among the Amish in Indiana is 7%. Allergies have also risen ominously in other Western countries. In the United Kingdom, for example, the number of children hospitalized for allergic reactions increased by a factor of five between 1990 and the late 2010s, as did the number of British children allergic to peanuts. In general, the better developed the country, the higher the rates of allergies. Reasons for this remain poorly understood. One possible explanation, supported by the U.S. National Institute of Allergy and Infectious Diseases, is that parents keep their children "too clean for their own good". They recommend exposing newborn babies to a variety of potentially allergenic foods, such as peanut butter before they reach the age of six months. According to this "hygiene hypothesis", such exposures give the infant's immune system some exercise, making it less likely to overreact. Evidence for this includes the fact that children living on a farm are consistently less likely to be allergic than their counterparts who are raised in the city, and that children born in a developed country to parents who immigrated from developing nations are more likely to be allergic than their parents are. A research article published in 2019 in the journal The Lancet reported that the number of South Africans aged 15 to 19 being treated for HIV increased by a factor of ten between 2010 and 2019. This is partly due to improved detection and treatment programs. However, less than 50% of the people diagnosed with HIV went onto receive antiviral medication due to social stigma, concerns about clinical confidentiality, and domestic responsibilities. While the annual number of deaths worldwide due to HIV/AIDS has declined from its peak in the early 2000s, experts warned that this venereal disease could rebound if the world's booming adolescent population is left unprotected. Data from the Australian Bureau of Statistics reveal that 46% of Australians aged 18 to 24, about a million people, were overweight in 2017 and 2018. That number was 39% in 2014 and 2015. Obese individuals face higher risks of type II diabetes, heart disease, osteoarthritis, and stroke. The Australian Medical Associated and Obesity Coalition have urged the federal government to levy a tax on sugary drinks, to require health ratings, and to regulate the advertisement of fast foods. In all, the number of Australian adults who are overweight or obese rose from 63% in 2014–15 to 67% in 2017–18. Puberty In Europe and the United States, the average age of the onset of puberty among girls was around 13 in the early 21st century, down from about 16 a hundred years earlier. Early puberty is associated with a variety of mental health issues, such as anxiety and depression (as people at this age tend to strongly desire conformity with their peers), early sexual activity, substance use, tobacco smoking, eating disorders, and disruptive behavioral disorders. Girls who mature early also face higher risks of sexual harassment. Moreover, in some cultures, pubertal onset remains a marker of readiness for marriage, for, in their point of view, a girl who shows signs of puberty might engage in sexual intercourse or risk being assaulted, and marrying her off is how she might be 'protected'. To compound matters, factors known for prompting mental health problems are themselves linked to early pubertal onset; these are early childhood stress, absent fathers, domestic conflict, and low socioeconomic status. Possible causes of early puberty could be positive, namely improved nutrition, or negative, such as obesity and stress. Other triggers include genetic factors, high body-mass index (BMI), exposure to endocrine-disrupting substances that remain in use, such as Bisphenol A (found in some plastics) and dichlorobenzene (used in mothballs and air deodorants), and to banned but persistent chemicals, such as dichlorodiphenyl-trichloroethane (DDT) and dichlorodiphenyldichloroethylene (DDE), and perhaps a combination thereof (the 'cocktail effect'). A 2019 meta-analysis and review of the research literature from all inhabited continents found that between 1977 and 2013, the age of pubertal onset among girls has fallen by an average of almost three months per decade, but with significant regional variations, ranging from 10.1 to 13.2 years in Africa to 8.8 to 10.3 years in the United States. This investigation relies on measurements of thelarche (initiation of breast tissue development) using the Tanner scale rather than self-reported menarche (first menstruation) and MRI brain scans for signs of the hypothalamic-pituitary-gonadal axis being reactivated. Furthermore, there is evidence that sexual maturity and psychosocial maturity no longer coincide; 21st-century youth appears to be reaching the former before the latter. Neither adolescents nor societies are prepared for this mismatch. Political views and participation Among developed democracies, young people's faith in the institutions, including their own government, has declined compared to that of previous generations. Among respondents aged 15-29, trust in their national governments was the lowest in Greece, Italy, the United States, the United Kingdom, and South Korea, and highest in New Zealand, Ireland, Finland, Lithuania, and Switzerland. In Australia, where members of Generation Z as a group feel alienated by mainstream politics, about half vote only to avoid a fine. Voting is compulsory in that country. An early political movement primarily driven by Generation Z was School Strike for Climate of the late 2010s. The movement involved millions of young people around the world who followed the footsteps of Swedish activist Greta Thunberg to skip school in order to protest in favor of greater action on climate change. Around the world, large numbers of people from this cohort feel angry, anxious, guilty, helpless, and sad about climate change and are dissatisfied with how their governments have responded so far. However, their consumption choices (see above) reveal a gap between their stated values and their activism. Polling on immigration in various countries receives mixed responses from Generation Z. In tandem with more members of Generation Z being able to vote in elections during the late 2010s and early 2020s, the youth vote has increased in both Europe and the United States. In Australia, Millennials and Generation Z outnumbered the Baby Boomers as voters by the 2025 federal election. By the mid-2020s, young adults on both sides of the North Atlantic have demonstrated a willingness to vote for the populist right. In Europe, voters from Generation Z swung from favoring the Greens in the 2019 European Parliament elections to supporting parties of the (far) right in 2024. In the United States, while Generation Z might still support some left-wing causes like the Millennials, they have shifted noticeably towards the right since 2020 as their priorities change. Polls consistently show that the Democratic Party has been steadily hemorrhaging support among young adults during the late 2010s and early 2020s, even though they largely disapprove of the Republican Party. By the early 2020s, young voters in Europe have become increasingly concerned about the rising cost of living, violent crime, declining public services in rural areas, immigration, and the Russo-Ukrainian War. In Canada, voters under the age of 30 are most worried about the housing shortage, the cost of living, and crime rates; they, especially men, favored the Conservatives by a sizeable margin in 2025. In the United States, the single most important issue for Generation Z is the economy (including inflation; the costs of housing, healthcare, and higher education; income inequality; and taxes). Political scientist Jean-Yves Camus dismissed the stereotype of young people altruistically voting for green or left-wing parties as misguided and outdated. Living as young adults in what they perceive as a volatile world, they crave security. Compared to older cohorts, young voters of the 2020s have grown up with dimmer economic prospects and as such are more likely to think of life as a zero-sum competition for scarce resources and opportunities. Multinational polls conducted in the early 2020s reveal that with Generation Z, the age-old pattern of younger cohorts holding more liberal or progressive sociopolitical views than their elders is no longer true in general. Nevertheless, in Australia, not only does Generation Z start out as more liberal than their predecessors when they were at the same age, they also do not transition towards conservatism at the same rate as they get older. But these broad trends conceal a significant gender divide across the Western world, with young women (under 30) being left-leaning and young men being right-leaning on a variety of issues from immigration to sexual harassment. Both young men and young women are willing to vote for politically extreme parties or candidates. In the United Kingdom, young women are tilting heavily towards the Green Party whereas in the United States, both young men and young women have swung towards the nationalistic populist Donald Trump and his Republican Party. Some individuals who support gender equality are hesitant to identify as "feminist" because there are different interpretations of what the term represents in contemporary society. Furthermore, the backlash against feminism among young men is quite strong in many countries; older men tend to hold similar views to women across age groups on this topic. Significant numbers of Gen-Z men support traditional gender roles, believe that it is much harder to be a man today, and that women's emancipation has gone too far and has come at their expense. This political sex gap has been noticeable since the 2000s, but has widened since the mid-2010s. This growing difference has also been observed among young adults in China and South Korea. Across the Western world, young men's socioeconomic status has been on the decline relative to young women's, something certain online influencers such as Andrew Tate exploit in order to cultivate in their followers a zero-sum mindset and a deep resentment for women. Anti-feminist circles—the manosphere—have attracted large numbers of Gen-Z men in Australia and South Korea. This polarization of the sexes is exacerbated by social media. Politically engaged members of Generation Z are more likely than their elders to avoid buying from or working for companies that do not share their sociopolitical views, and they take full advantage of the Internet as activists. Consequently, maintaining a presence on social media networks, especially TikTok, is vital for politicians and political parties dependent upon the youth vote, such as the Left (Die Linke) and the Alternative for Germany (AfD), the two most popular German political parties among young voters in the 2025 federal election. Social media are platforms using which those on the margins of politics can directly address the public, eroding the advantages of establishment figures. Moreover, 2025 has been a turning point in Australian politics as the three major political parties—the Labor Party, the Liberal-National Coalition, and the Green Party—all spent considerable resources campaigning on TikTok, vying for youth support. For their part, members of Generation Z are also influenced by the political views of the people they follow on social media. Outside of Western countries, Generation Z has been politically active, too. In Iran, activists, most of whom women, took to the streets in 2022 to voice their disapproval of their government after 22-year-old Mahsa Amini died in morality police custody; she was arrested for allegedly violating the state's Islamic dress code. In Bangladesh, students overthrew the autocratic regime of Prime Minister Sheikh Hasina in the July Revolution of 2024, putting an end to what they deemed an unfair quota system of the Bangladeshi civil service and a massacre. In Kenya, young people, long faced with government corruption and economic precariousness despite being better educated that older generations, protested the 2024 tax hikes of President William Ruto. Religious tendencies In the Middle East and North Africa, young people were much more pious in the early 2020s compared to the late 2010s. Young Latin Americans of the 2020s are markedly more likely to be irreligious than the previous decade, making their region as a whole more secular. Those with higher education are especially likely to be religiously unaffiliated. Nevertheless, belief in astrology and spirituality remained common. In Western Europe and North America, Generation Z is the least religious generation in history. More members of Generation Z describe themselves as nonbelievers than any previous generation and reject religious affiliation, though many of them still describe themselves as spiritual. The 2016 British Social Attitudes Survey found that 71% of people between the ages of 18 and 24 had no religion, compared to 62% the year before. A 2018 ComRes survey found two-thirds of the same age group have never attended church; among the remaining third, 20% went a few times a year, and 2% multiple times per week. According to British Office for National Statistics (ONS), people under the age of 40 in England and Wales are more likely to consider themselves irreligious rather than Christian. In Canada, 43% of people aged 15 to 35 were religiously unaffiliated in 2021. Young Canadian adults, who are much more likely to have higher education than their counterparts in other countries of the OECD in the 2020s, tend to have a negative opinion of religion, viewing it as incompatible with modernity. In the United States, Millennials and Generation Z are driving the growth of secularism. In particular, young women are leaving religion at a faster pace than young men. Atheism is more common among Generation Z than in prior generations. Risky behaviors Adolescent pregnancy Adolescent pregnancy has been in decline during the early 21st century all across the industrialized world, due to the widespread availability of contraception and the growing avoidance of sexual intercourse among teenagers. In the European Union and the United Kingdom, teenage parenthood has fallen 58% and 69%, respectively, between the 1990s and the 2020s. In New Zealand, the pregnancy rate for females aged 15 to 19 dropped from 33 per 1,000 in 2008 to 16 in 2016. Highly urbanized regions had adolescent pregnancy rates well below the national average whereas Māori communities had much higher than average rates. In Australia, it was 15 per 1,000 in 2015. In the United States, teenage pregnancy rates continued to decline, reaching 13.5 in 2022, the lowest on record. Northern European countries, above all the Netherlands, have some of the world's lowest teenage pregnancy and abortion rates by implementing thorough sex education. Alcoholism and substance use 2020 data from the UK Office for National Statistics (ONS) showed on a per-capita basis, members of Generation Z binged on alcohol 20% less often than Millennials. However, 9.9% of people aged 16 to 24 consumed at least one drug in the past month, usually cannabis, or more than twice the share of the population between the ages of 16 and 59. "Cannabis has now taken over from the opiates in terms of the most people in treatment for addiction," psychopharmacologist Val Curran of the University College London (UCL) told The Telegraph. Moreover, the quality and affordability of various addictive drugs have improved in recent years, making them an appealing alternative to alcoholic beverages for many young people, who now have the ability to arrange a meeting with a dealer via social media. Addiction psychiatrist Adam Winstock of UCL found using his Global Drug Survey that young people rated cocaine more highly than alcohol on the basis of value for money, 4.8 compared to 4.7 out of 10. As of 2019, cannabis was legal for both medical and recreational use in Uruguay, Canada, and 33 states in the US. In the United States, Generation Z is the first to be born into a time when the legalization of marijuana at the federal level is being seriously considered. While adolescents (people aged 12 to 17) in the late 2010s were more likely to avoid both alcohol and marijuana compared to their predecessors from 20 years before, college-aged youths are more likely than their elders to consume marijuana. Marijuana use in Western democracies was three times the global average, as of 2012, and in the U.S., the typical age of first use is 16. This is despite the fact that marijuana use is linked to some risks for young people, such as in the impairment of cognitive abilities and school performance, though a causality has not been established in this case. Youth crime During the 2010s, when most of Generation Z experienced some or all of their adolescence, reductions in youth crime were seen in some Western countries. A report looking at statistics from 2018 to 2019 noted that the numbers of young people aged ten to seventeen in England and Wales being cautioned or sentenced for criminal activity had fallen by 83% over the previous decade, while those entering the youth justice system for the first time had fallen by 85%. In 2006, 3,000 youths in England and Wales were detained for criminal activity; ten years later, that number fell below 1,000. In Europe, teenagers were less likely to fight than before. Research from Australia suggested that crime rates among adolescents had consistently declined between 2010 and 2019. In a 2014 report, Statistics Canada stated that police-reported crimes committed by persons between the ages of 12 and 17 had been falling steadily since 2006 as part of a larger trend of decline from a peak in 1991. Between 2000 and 2014, youth crimes plummeted 42%, above the drop for overall crime of 34%. In fact, between the late 2000s and mid-2010s, the fall was especially rapid. This was primarily driven by a 51% drop in theft of items worth no more than CAN$5,000 and burglary. The most common types of crime committed by Canadian adolescents were theft and violence. At school, the most frequent offenses were possession of cannabis, common assault, and uttering threats. Overall, although they made up only 7% of the population, adolescents stood accused of 13% of all crimes in Canada. In addition, mid- to late-teens were more likely to be accused of crimes than any other age group in the country. Family and social life Upbringing Parents increasingly realize that in order to ensure their children have the best future attainable, they must have fewer of them and invest more resources per child. Sociologists Judith Treas and Giulia M. Dotti Sani analyzed the diaries of 122,271 parents (68,532 mothers and 53,739 fathers) aged 18 to 65 in households with at least one child below the age of 13 from 1965 to 2012 in eleven Western countries—Canada, the United Kingdom, the United States, Spain, Italy, France, the Netherlands, Germany, Denmark, Norway, and Slovenia—and discovered that in general, parents had been spending more and more time with their children. In 2012, the average mother spent twice as much time with her offspring than her counterpart in 1965. Among fathers, the average amount of time quadrupled. Nevertheless, women were still the primary caregivers. Parents of all education levels were represented, though those with higher education typically spent much more time with their children, especially university-educated mothers. France was the only exception. French mothers were spending less time with their children whereas fathers were spending more time. This overall trend reflected the dominant ideology of "intensive parenting"—the idea that the time parents spend with children is crucial for their development in various areas and the fact that fathers developed more egalitarian views with regards to gender roles over time and became more likely to want to play an active role in their children's lives. In the United Kingdom, there was a widespread belief in the early 21st century that rising parental, societal and state concern for the safety of children was leaving them increasingly mollycoddled and slowing the pace they took on responsibilities. The same period saw a rise in child-rearing's position in the public discourse with parenting manuals and reality TV programs focused on family life, such as Supernanny, providing specific guidelines for how children should be cared for and disciplined. According to Statistics Canada, the number of households with both grandparents and grandchildren remained rare but grew in the early 21st century. In 2011, five percent of Canadian children below the age of ten lived with a grandparent, up from 3.3% in the previous decade. This is in part because Canadian parents in the early 21st century could not (or believe they could not) afford childcare and often find themselves having to work long hours or irregular shifts. Meanwhile, many grandparents struggled to keep up with their highly active grandchildren on a regular basis due to their age. Because Millennials and members of Generation X tend to have fewer children than their parents the baby boomers, each child typically receives more attention from grandparents and parents compared to previous generations. Friendships and socialization According to the OECD PISA surveys, 15-year-olds in 2015 had a tougher time making friends at school than ten years prior. European teenagers were becoming more and more like their Japanese and South Korean counterparts in social isolation. This might be due to intrusive parenting, heavy use of electronic devices, and concerns over academic performance and job prospects. A study of social interaction among American teenagers found that the amount of time young people spent with their friends had been trending downwards since the 1970s but fallen into especially sharp decline after 2010. The percentage of students in the 12th grade (typically 17 to 18 years old) who said they met with their friends almost every day fell from 52% in 1976 to 28% in 2017. The percentage of that age group who said they often felt lonely (which had fallen during the early 2000s) increased from 26% in 2012 to 39% in 2017 whilst the percentage who often felt left out increased from 30% to 38% over the same period. Statistics for slightly younger teenagers suggested that parties had become significantly less common since the 1980s. Romance, marriage, and family According to a 2014 report from UNICEF, some 250 million females were forced into marriage before the age of 15, especially in South Asia and sub-Saharan Africa. Problems faced by child brides include loss of educational opportunity, less access to medical care, higher childbirth mortality rates, depression, and suicidal ideation. During the 2020s, young adults around the world are much more likely to be romantically unattached, either by choice or circumstance, than older generations. This trend is most pronounced among the poor. East Asia, the Middle East, North Africa, and Latin America saw the steepest declines compared to the 2000s. Many youths are also uninterested in having children. Some have pets instead. In Australia, growing numbers of older teenage boys and young men have been avoiding romantic relationships altogether, citing concerns over the traumatic experiences of older male family members, including false accusations of sexual misconduct or loss of assets and money after a divorce. This social trend—Men Going Their Own Way (MGTOW)—is an outgrowth of the men's rights movement, but one that emphasizes detachment from women as a way to deal with the issues men face. In China, young people nowadays are much more likely to deem marriage and children sources of stress rather than fulfillment, going against the Central Government's attempts to increase the birth rate. Women born between the mid-1990s to about 2010 are less interested in getting married than men their own age. In addition, the "lying flat" movement, popular among Chinese youths, also extends to the domain of marriage and child-rearing. Pluralities of young urban residents of the 2020s told pollsters they were not planning to get married due to having trouble finding the right person, the high costs of marriage, or skepticism of marriage. In line with a fall in adolescent pregnancy in the developed world, which is discussed in more detail elsewhere in this article, there has also been a reduction in the percentage of the youngest adults with children. The Office for National Statistics has reported that the number of babies being born in the United Kingdom to 18 year old mothers had fallen by 58% from 2000 to 2016 and the amount being born to 18 year old fathers had fallen by 41% over the same period. Pew Research reports that in 2016, 88% of American women aged 18 to 21 were childless as opposed to 80% of Generation X and 79% of millennial female youth at a similar age. Use of information and communications technologies (ICT) Use of ICT in general Generation Z is one of the first cohorts to have Internet technology readily available at a young age. With the Web 2.0 revolution that occurred throughout the mid-late 2000s and 2010s, they have been exposed to an unprecedented amount of technology in their upbringing, with the use of mobile devices growing exponentially over time. Anthony Turner characterizes Generation Z as having a "digital bond to the Internet", and argues that it may help youth to escape from emotional and mental struggles they face offline. According to U.S. consultants Sparks and Honey in 2014, 41% of Generation Z spend more than three hours per day using computers for purposes other than schoolwork, compared with 22% in 2004. In 2015, an estimated 150,000 apps, 10% of apps in Apple's App Store, were educational and aimed at children up to college level, though opinions are mixed as to whether the net result will be deeper involvement in learning and more individualized instruction, or impairment through greater technology dependence and a lack of self-regulation that may hinder child development. Parents who raise Gen Z children fear the overuse of the Internet, and dislike the ease of access to inappropriate information and images, as well as social networking sites where minors can gain access to people worldwide. Gen Z children, inversely, feel annoyed with their parents and complain about parents being overly controlling when it comes to their Internet usage. A 2015 study by Microsoft found that 77% of respondents aged 18 to 24 said yes to the statement, "When nothing is occupying my attention, the first thing I do is reach for my phone," compared to just 10% for those aged 65 and over. In a TEDxHouston talk, Jason Dorsey of the Center for Generational Kinetics stressed the notable differences in the way that Millennials and Generation Z consume technology, with 18% of Generation Z feeling that it is okay for a 13-year-old to have a smartphone, compared with just 4% for the previous generation. An online newspaper about texting, SMS and MMS writes that teens own cellphones without necessarily needing them; that receiving a phone is considered a rite of passage in some countries, allowing the owner to be further connected with their peers, and it is now a social norm to have one at an early age. An article from the Pew Research Center stated that "nearly three-quarters of teens have or have access to a smartphone and 30% have a basic phone, while just 12% of teens 13 to 15 say they have no cell phone of any type". These numbers are only on the rise and the fact that the majority own a cell phone has become one of this generation's defining characteristics. Consequently, "24% of teens go online 'almost constantly'." A survey of students from 79 countries by the OECD found that the amount of time spent using an electronic device has increased, from under two hours per weekday in 2012 to close to three in 2019, at the expense of extracurricular reading. Psychologists have observed that sexting, the transmission of sexually explicit content via electronic devices, has seen noticeable growth among contemporary adolescents. Older teenagers are more likely to participate in sexting. Besides some cultural and social factors such as the desire for acceptance and popularity among peers, the falling age at which a child receives a smartphone may contribute to the growth in this activity. However, while it is clear that sexting has an emotional impact on adolescents, it is still not clear how it precisely affects them. Some consider it a high-risk behavior because of the ease of dissemination to third parties leading to reputational damage and the link to various psychological conditions including depression and even suicidal ideation. Others defend youths' freedom of expression over the Internet. There is some evidence that at least in the short run, sexting brings positive feelings of liveliness or satisfaction. However, girls are more likely than boys to be receiving insults, social rejections, or reputational damage as a result of sexting. Digital literacy Despite being labeled as digital natives, the 2018 International Computer and Information Literacy Study (ICILS), conducted on 42,000 eighth-graders (or equivalents) from 14 countries and education systems, found that only two percent of these people were sufficiently proficient with information devices to justify that description, and only 19% could work independently with computers to gather information and to manage their work. ICILS assesses students on two main categories: Computer and Information Literacy (CIL), and Computational Thinking (CT). Countries or education systems whose students scored near or above the international average of 496 in CIL were, in increasing order, France, North Rhine-Westphalia, Portugal, Germany, the United States, Finland, South Korea, Moscow, and Denmark. Countries or education systems whose students scored near or above the international average of 500 were, in increasing order, the United States, France, Finland, Denmark, and South Korea. By the early 2020s, many members of Generation Z were entering the (digital) work place without some basic ICT skills, such as touch typing, though they can learn more quickly than older workers. Pornography viewing While pornography is made for entertainment, teenagers are increasingly turning to it as a source of information on sexuality, especially what to do during a sexual encounter, as teachers tend to focus on contraception. In fact, pornography is reaching an increasingly large youth audience — as young as people in their early teenage years – not only on social networks, but also on dedicated websites, thanks both to their access to electronic devices and the influence of their friends. Although parents generally believe adolescents who view pornography for pleasure tend to be boys, surveys and interviews reveal that this behavior is also common among girls. A 2020 report by the British Board of Film Classification (BBFC)—available only by request due to the presence of graphic materials—suggests that parents either are in denial or are completely oblivious to the prevalence of pornography viewership by adolescents, with three quarters telling researchers that they did not believe their children consumed such material. Over half of the teenagers interviewed told researchers they had viewed pornography, though the actual number is likely higher due to the sensitivity of this topic. Many interviewees told researchers they felt anxious about their body image and the expectations of their potential sexual partners as a result of viewing, and their concerns over sexual violence. About one-third of the British population watches these films, according to industry estimates. Use of social media networks Members of Generation Z live during a time of widespread access to social media platforms and have consequently integrated these into their daily lives, using them to not only communicate with friends and family but also interact with people they would otherwise never meet in the real world. Social media have become a tool for Generation Z to forge their personal identities. Indeed, an absolute majority have used social media and are frequently online. However, one side effect of this trend is that they interact "face to face" less often, causing them to feel more lonely and left out. Some also report online fatigue and want to spend less time on the Internet while others admit to having regrets about certain things they posted online. Speed and reliability are important factors in their choice of social networking platform, and they make frequent use of emojis. Unlike older generations, who prefer newspapers and television reports, Generation Z uses social media to access the news. Nevertheless, even though people aged 18 to 24 are heavily reliant upon social media networks, they have very little trust in them. Once the single most popular social media site among teenagers, Facebook has been on the decline since the early 2010s. The share of teenagers using Twitter has fallen as well. At the same time, YouTube has claimed the top spot while Snapchat and Instagram have also made significant gains among the young. During the late 2010s and early 2020s, TikTok exploded in usage among adolescents and has become the second most frequently used platform, surpassing Instagram in 2021. Generation Z finds Snapchat and Tiktok appealing because videos, pictures, and messages send much faster on it than in regular messaging. Another reason for the popularity of these platforms among Generation Z is that their parents do not typically use them. So popular is TikTok among people under the age of 30 in Europe and North America that they typically ignore their own governments' concerns over issues of user privacy and national security. As of 2022, TikTok has around 689 million active users, 43% of whom are from Gen Z. Based on current growth figures, it is predicted that by the end of 2023, TikTok audience will grow by 1.5 billion active users, 70% of whom will be members of Generation Z. Effects of screen time In his 2017 book Irresistible, professor of marketing Adam Alter explained that not only are children addicted to electronic gadgets, but their addiction jeopardizes their ability to read non-verbal social cues. A 2019 meta-analysis of thousands of studies from almost two dozen countries suggests that while as a whole, there is no association between screen time and academic performance, when the relation between individual screen-time activity and academic performance is examined, negative associations are found. Watching television is negatively correlated with overall school grades, language fluency, and mathematical ability while playing video games was negatively associated with overall school grades only. According to previous research, screen activities not only take away the time that could be spent on homework, physical activities, verbal communication, and sleep (the time-displacement hypothesis) but also diminish mental activities (the passivity hypothesis). Furthermore, excessive television viewing is known for harming the ability to pay attention as well as other cognitive functions; it also causes behavioral disorders, such as having unhealthy diets, which could damage academic performance. Excessively playing video games, on the other hand, is known for impairing social skills and mental health, and as such could also damage academic performance. However, depending on the nature of the game, playing it could be beneficial for the child; for instance, the child could be motivated to learn the language of the game in order to play it better. Among adolescents, excessive Internet surfing is well known for being negatively associated with school grades, though previous research does not distinguish between the various devices used. Nevertheless, one study indicates that Internet access, if used for schoolwork, is positively associated with school grades but if used for leisure, is negatively associated with it. Overall, the effects of screen time are stronger among adolescents than children. Research conducted in 2017 reports that the social media usage patterns of this generation may be associated with loneliness, anxiety, and fragility and that girls may be more affected than boys by social media. According to 2018 CDC reports, girls are disproportionately affected by the negative aspects of social media than boys. Researchers at the University of Essex analyzed data from 10,000 families, from 2010 to 2015, assessing their mental health utilizing two perspectives: Happiness and Well-being throughout social, familial, and educational perspectives. Within each family, they examined children who had grown from 10 to 15 during these years. At age 10, 10% of female subjects reported social media use, while this was only true for 7% of the male subjects. By age 15, this variation jumped to 53% for girls, and 41% for boys. This percentage influx may explain why more girls reported experiencing cyberbullying, decreased self-esteem, and emotional instability more than their male counterparts. Other researchers hypothesize that girls are more affected by social media usage because of how they use it. In a study conducted by the Pew Research Center in 2015, researchers discovered that while 78% of girls reported making a friend through social media, only 52% of boys could say the same. However, boys are not explicitly less affected by this statistic. They also found that 57% of boys claimed to make friends through video gaming, while this was only true for 13% of girls. Another Pew Research Center survey conducted in April 2015, reported that women are more likely to use Pinterest, Facebook, and Instagram than men, which are visual-heavy sites. In counterpoint, men were more likely to utilize online forums, e-chat groups, and Reddit than women. Cyberbullying, an act of bullying using technology, is more common now than among Millennials, the previous generation. It is more common among girls, 22% compared to 10% for boys. This results in young girls feeling more vulnerable to being excluded and undermined. According to a 2020 report by the British Board of Film Classification, "many young people felt that the way they viewed their overall body image was more likely the result of the kinds of body images they saw on Instagram." See also Glossary of Generation Z slang List of generations 9X Generation (Vietnam) Boomerang Generation Cusper Generation K, a demographic cohort defined by Noreena Hertz Generation Z in the United States Post-90s and Little emperor syndrome (China) Strawberry generation (Taiwan) Thumb tribe Zillennials Notes References Further reading Palfrey, John; Gasser, Urs (2008). Born Digital: Understanding the First Generation of Digital Natives. Basic Books. ISBN 978-0-465-00515-4. Arum, Richard; Roksa, Josipa (2011). Academically Adrift – Limited Learning on College Campuses. Chicago, Illinois: The University of Chicago Press. ISBN 978-0-226-02856-9. McCrindle, Mark; Wolfinger, Emily (2014). The ABC of XYZ: Understanding the Global Generations. McCrindle Research. Combi, Chloe (2015). Generation Z: Their Voices, Their Lives. London: Hutchinson. OCLC 91060. Greenspan, Louise; Deardorff, Julianna (2015). The New Puberty: How to Navigate Early Development in Today's Girls. Rodale Books. ISBN 978-1-62336-598-1. Katz, Roberta; Ogilvie, Sarah; Shaw, Jane; Wooodhead, Linda (2021). Gen Z, Explained: The art of living in a digital age. Chicago and London: University of Chicago Press. ISBN 978-0-226-79153-1. External links The Downside of Diversity. Michael Jonas. The New York Times. August 5, 2007. The Next America: Modern Family. Pew Research Center. April 30, 2014. (Video, 2:16) Meet Generation Z: Forget Everything You Learned About Millennials – 2014 presentation by Sparks and Honey Is a University Degree a Waste of Money? CBC News: The National. March 1, 2017. (Video, 14:39) A Generation Z Exploration. (Web version) Rubin Postaer and Associates (RPA). 2018. We asked teenagers what adults are missing about technology. This was the best response. Taylor Fang. MIT Technology Review. December 21, 2019. The Amish use tech differently than you think. We should emulate them. Jeff Smith. The Washington Post. February 17, 2020. The Strauss–Howe generational theory, devised by William Strauss and Neil Howe, is a psychohistorical theory which describes a theorized recurring generation cycle in American and Western history. According to the theory, historical events are associated with recurring generational personas (archetypes). Each generational persona unleashes a new era (called a turning) lasting around 21 years, in which a new social, political, and economic climate (mood) exists. They are part of a larger cyclical "saeculum" (a long human life, which usually spans around 85 years, although some saecula have lasted longer). The theory states that a crisis recurs in American history after every saeculum, which is followed by a recovery (high). During this recovery, institutions and communitarian values are strong. Ultimately, succeeding generational archetypes attack and weaken institutions in the name of autonomy and individualism, which eventually creates a tumultuous political environment that ripens conditions for another crisis. Academic response to the theory has been mixed, with some applauding Strauss and Howe for their "bold and imaginative thesis", while others have criticized the theory as being overly deterministic, unfalsifiable, and unsupported by rigorous evidence. The theory has been influential in the fields of generational studies, marketing, and business management literature. However, the theory has also been described by some historians and journalists as pseudoscientific, "kooky", and "an elaborate historical horoscope that will never withstand scholarly scrutiny". Academic criticism has focused on the lack of rigorous empirical evidence for their claims, as well as the authors' view that generational groupings are more powerful than other social groupings, such as economic class, race, sex, religion, and political parties. However, Strauss and Howe later suggested that there are no exact generational boundaries – the speed of their development cannot be predicted. The authors also compared the cycles with the seasons, which may come sooner or later. History William Strauss and Neil Howe's partnership began in the late 1980s when they began writing their first book Generations, which discusses the history of the United States as a succession of generational biographies. Each had written on generational topics previously. The authors' interest in generations as a broader topic emerged after they met in Washington, D.C., and began discussing the connections between each of their previous works. They attempted to find the reason why the Boomers and the G.I.s had developed such different ways of looking at the world, and what it was about these generations' experiences growing up that prompted their different outlooks. They also wanted to find patterns in previous generations, and their research discussed historical analogs to the current generations. They ultimately described a recurring pattern in the Anglo-American history of four generational types, each with a distinct collective persona, and a corresponding cycle of four different types of eras, each with a distinct "mood". Strauss and Howe laid the groundwork for their theory in their book Generations: The History of America's Future, 1584 to 2069 (1991), which discusses the history of the United States as a series of generational biographies going back to 1584. Strauss and Howe followed in 1993 with their second book 13th Gen: Abort, Retry, Ignore, Fail?, which was published while Gen Xers were teenagers and young adults. The book examines the generation born between 1961 and 1981, "Gen-Xers" (which they called "13ers", describing them as the thirteenth generation since the US became a nation). The book asserts that 13ers' location in history as under-protected children during the Consciousness Revolution explains their pragmatic attitude. They describe Gen Xers as growing up during a time when society was less focused on children and more focused on adults and their self-actualization. Strauss and Howe's theory made various predictions regarding the Millennial generation, a group consisting of young children at the time. These predictions lacked significant historical data. In Generations (1991) and The Fourth Turning (1997), the two authors discussed the generation gap between Baby Boomers and their parents and predicted there would be no such gap between Millennials and their elders. In 2000, they published Millennials Rising: The Next Great Generation. This work discussed the personality of the Millennial Generation, whose oldest members were described as the high school graduating class of the year 2000. In the 2000 book, Strauss and Howe asserted that Millennial teens and young adults were recasting the image of youth from "downbeat and alienated to upbeat and engaged", crediting increased parental attention and protection for these positive changes. They asserted that Millennials are held to higher standards than adults apply to themselves and that they are much less vulgar and violent than the teen culture older people produce for them. They described them as less sexually charged and as ushering in a new sexual modesty, with an increasing belief that sex should be saved for marriage and a return to conservative family values. The authors predicted that over the following decade, Millennials would transform what it means to be young, and could emerge as the next "Great Generation". The work was described as an optimistic, feel-good book for the parents of the Millennial Generation, predominantly the Baby Boomers. The theory was expanded in The Fourth Turning (1997), to focus on a fourfold cycle of generational types and recurring mood eras to describe the history of the United States, including the Thirteen Colonies and their British antecedents. However, the authors have also examined generational trends elsewhere in the world and described similar cycles in several developed countries.The terminology for generational archetypes were updated(e.g. "Civics" became "Heroes", which they applied to the Millennial Generation, "Adaptives" became "Artists"), and the terms "Turning" and "Saeculum" were introduced. In the mid-1990s, Strauss and Howe began receiving inquiries about how their research could be applied to strategic problems in organizations. They started speaking frequently about their work at events and conferences. In July 2023 Howe released a new book, titled The Fourth Turning Is Here. Steve Bannon, former Chief Strategist and Senior Counselor to president Donald Trump during his first term, is a prominent proponent of the theory. As a documentary filmmaker, Bannon discussed the details of Strauss–Howe generational theory in Generation Zero. According to historian David Kaiser, who was consulted for the film, Generation Zero "focused on the key aspect of their theory, the idea that every 80 years of American history has been marked by a crisis, or 'fourth turning', that destroyed an old order and created a new one". Kaiser said Bannon is "very familiar with Strauss and Howe's theory of crisis, and has been thinking about how to use it to achieve particular goals for quite a while." A February 2017 article from Business Insider titled: "Steve Bannon's obsession with a dark theory of history should be worrisome", commented: "Bannon seems to be trying to bring about the 'Fourth Turning'." Defining a generation Strauss and Howe describe the history of the U.S. as a succession of Anglo-American generational biographies from 1433 to the present, and theorized a recurring generational cycle in American history. The authors posit a pattern of four repeating phases, generational types, and a recurring cycle of spiritual awakenings and secular crises, from the founding colonials of America through the present day. Strauss and Howe define a social generation as the aggregate of all people born over a span of roughly 21 years or about the length of one phase of life: childhood, young adulthood, midlife, and old age. Generations are identified (from the first birthyear to last) by looking for cohort groups of this length that share three criteria. First, members of a generation share what the authors call an age location in history: they encounter key historical events and social trends while occupying the same phase of life. In this view, members of a generation are shaped in lasting ways by the eras they encounter as children and young adults and they share certain common beliefs and behaviors. Aware of the experiences and traits that they share with their peers, members of a generation would also share a sense of common perceived membership in that generation. They based their definition of a generation on the work of various writers and social thinkers, from ancient writers such as Polybius and Ibn Khaldun to modern social theorists such as José Ortega y Gasset, Karl Mannheim, John Stuart Mill, Émile Littré, Auguste Comte, and François Mentré. Turnings While writing Generations, Strauss and Howe described a theorized pattern in the historical generations they examined, which they say revolved around generational events which they call turnings. In Generations, and in greater detail in The Fourth Turning, they describe a four-stage cycle of social or mood eras which they call "turnings". The turnings include: "the high", "the awakening", "the unraveling", and "the crisis". High According to Strauss and Howe, the first turning is a high, which occurs after a crisis. During the high, institutions are strong and individualism is weak. Society is confident about where it wants to go collectively, though those outside the majoritarian center often feel stifled by conformity. According to the authors, the most recent first turning in the US was the post–World War II American high, beginning in 1946 and ending with the assassination of John F. Kennedy on November 22, 1963. Awakening According to the theory, the second turning is an awakening. This is an era when institutions are attacked in the name of personal and spiritual autonomy. Just when society is reaching its high tide of public progress, people suddenly tire of social discipline and want to recapture a sense of "self-awareness", "spirituality" and "personal authenticity". Young activists look back at the previous High as an era of cultural and spiritual poverty. Strauss and Howe say the U.S.'s most recent awakening was the "consciousness revolution", which spanned from the campus and inner-city revolts of the mid-1960s to the tax revolts of the early 1980s. Unraveling According to Strauss and Howe, the third turning is an unraveling. The mood of this era they say is in many ways the opposite of a high: Institutions are weak and distrusted, while individualism is strong and flourishing. The authors say highs come after crises when society wants to coalesce and build and avoid the death and destruction of the previous crisis. Unravelings come after awakenings when society wants to atomize and enjoy. They say the most recent unraveling in the US began in the 1980s and includes the long boom and the culture war. Crisis According to the authors, the fourth turning is a crisis. This is an era of destruction, often involving war or revolution, in which institutional life is destroyed and rebuilt in response to a perceived threat to the nation's survival. After the crisis, civic authority revives, cultural expression redirects toward community purpose, and people begin to locate themselves as members of a larger group. The authors say the previous fourth turning in the US began with the Wall Street Crash of 1929 and climaxed with the end of World War II. The G.I. generation (which they call a hero archetype, born 1901 to 1924) came of age during this era. They say their confidence, optimism, and collective outlook epitomized the mood of that era. The authors assert the millennial generation (which they also describe as a hero archetype, born 1982 to 2005) shows many similar traits to those of the G.I. youth, which they describe as including rising civic engagement, improving behavior, and collective confidence. Cycle The authors describe each turning as lasting circa 21 years. Four turnings make up a full cycle of circa 85 years, which the authors term a saeculum, after the Latin word meaning both "a long human life" and "a natural century". Generational change drives the cycle of turnings and determines its periodicity. As each generation ages into the next life phase (and a new social role) society's mood and behavior fundamentally change, giving rise to a new turning. Historical events shape generations in childhood and young adulthood; then, as parents and leaders in midlife and old age, generations in turn shape history. Each of the four turnings has a distinct "mood" that recurs every saeculum. Strauss and Howe describe these turnings as the "seasons of history". At one extreme is the Awakening, which is analogous to summer, and at the other extreme is the Crisis, which is analogous to winter. The High and the Unraveling are similar to spring and autumn, respectively. Strauss and Howe have discussed 26 theorized turnings over 7 saecula in Anglo-American history, from the year 1433 through today. The core of Strauss and Howe's ideas is a basic alternation between two different types of eras, Crises and Awakenings. Both of these are defining eras in which people observe that historic events are radically altering their social environment. Crises are periods marked by major secular upheaval, when society focuses on reorganizing the outer world of institutions and public behavior (they say the last American Crisis was the period spanning the Great Depression and World War II). Awakenings are periods marked by cultural or religious renewal when society focuses on changing the inner world of values and private behavior (the last American Awakening was the "Consciousness Revolution" of the 1960s and 1970s). During Crises, an ethic of collectivism emerges. During Awakenings, an ethic of individualism emerges, and the institutional order is attacked by new social ideals and spiritual agendas. According to the authors, about every 85 years—the length of a long human life—a national Crisis occurs in American society. Roughly halfway to the next Crisis, a cultural Awakening occurs (historically, these have often been called Great Awakenings). In describing this cycle of Crises and Awakenings, they draw from the work of other historians and social scientists who have also discussed long cycles in American and European history, which have grown to show a trend of economic downturns the more a society has industrialised. The cycle of Crises corresponds with long cycles of war identified by such scholars as Arnold J. Toynbee, Quincy Wright, and L. L. Ferrar Jr., and with geopolitical cycles identified by William R. Thompson and George Modelski. Strauss and Howe say their cycle of Awakenings corresponds with Anthony Wallace's work on revitalization movements; they also say recurring Crises and Awakenings correspond with two-stroke cycles in politics (Walter Dean Burnham, Arthur Schlesinger Sr. and Jr.), foreign affairs (Frank L. Klingberg), and the economy (Nikolai Kondratieff) as well as with long-term oscillations in crime and substance abuse. Archetypes The authors say two different types of eras and two formative age locations associated with them (childhood and young adulthood) produce four generational archetypes that repeat sequentially, in rhythm with the cycle of Crises and Awakenings. In Generations, they refer to these four archetypes as Idealist, Reactive, Civic, and Adaptive. In The Fourth Turning (1997) they change this terminology to Prophet, Nomad, Hero, and Artist. They say the generations in each archetype not only share a similar age-location in history, but they also share some basic attitudes towards family, risk, culture and values, and civic engagement. In essence, generations shaped by similar early-life experiences develop similar collective personas and follow similar life trajectories. To date, Strauss and Howe have described 25 generations in Anglo-American history, each with a corresponding archetype. The authors describe the archetypes as follows: Prophet Prophet (Idealist) generations enter childhood during a High, a time of rejuvenated community life and consensus around a new societal order. Examples: Transcendental Generation, Missionary Generation, Baby Boomers. Nomad Nomad (Reactive) generations enter childhood during an Awakening, a time of social ideals and spiritual agendas when young adults are passionately attacking the established institutional order. Examples: Gilded Generation, Lost Generation, Generation X. Hero Hero (Civic) generations enter childhood during an Unraveling, a time of individual pragmatism, self-reliance, and unrestrained. Examples: Republican Generation, G.I. Generation, Millennials. Artist Artist (Adaptive) generations enter childhood during a Crisis, a time when public consensus, aggressive institutions, and an ethic of personal sacrifice were favored. Examples: Progressive Generation, Silent Generation, Homeland Generation. Summary An average modern life is around 85 years and consists of four periods of ~21 years Childhood → Young adult → Midlife → Elderhood A generation is an aggregate of people born every ~21 years Baby Boomers → Gen X → Millennials → Homelanders Each generation experiences "four turnings" every ~85 years High → Awakening → Unraveling → Crisis A generation is considered "dominant" or "recessive" according to the turning experienced as young adults. But as a youth generation comes of age and defines its collective persona an opposing generational archetype is in its midlife peak of power. Dominant: independent behavior + attitudes in defining an era Recessive: dependent role in defining an era Dominant generations Prophet (idealist): Awakening as young adults. Awakening, defined: Institutions are attacked in the name of personal and spiritual autonomy Hero (civic): Crisis as young adults. Crisis, defined: Institutional life is destroyed and rebuilt in response to a perceived threat to the nation's survival Recessive generations Nomad (reactive): Unravelling as young adults. Unravelling, defined: Institutions are weak and distrusted, individualism is strong and flourishing Artist (adaptive): High as young adults. High, defined: Institutions are strong, and individualism is weak Generations Late Medieval Saeculum Arthurian Generation The arthurian generation was born between 1433 and 1460 and is of the hero archetype. Members of the generation grew up during England's retreat from the Hundred Years' War in France, during an era of rising civil unrest. Humanist Generation The humanist generation was born between 1461 and 1482 and is of the artist/adaptive archetype. This generation grew up at the height of the Middle Ages, just prior to the Reformation and Renaissance. The educated middle classes are influenced by Renaissance Humanist teaching and presented with a clear career path through the church or State bureaucracy. Humanist influences took hold across Europe, and in many ways prepared the intellectual landscape for the coming reformation. Their youth coincided with the development of the European Printing press allowing greater dissemination of knowledge. According to Strauss and Howe, they became Greek language tutors, international scholars, poets, prelates, and literate merchants and yeomen. They described their education produced by the humanist generation as being focused on the qualitative and the subjective, rather than the quantitative and the objective. Some of the notable people who influenced this generation include Thomas More, Erasmus, Thomas Linacre, John Colet, Cardinal Wolsey, Michelangelo, Copernicus, Francisco Pizarro and Cesare Borgia. King Edward V was also born into this generation, but as he died at only 15 years old, it is difficult to properly place him in this archetype. However, according to the historian Dominic Mancini Edward was very fascinated with science and philosophy, and was very well learned beyond his years. Reformation Saeculum Reformation Generation The reformation generation was born between 1483 and 1511 and is of the prophet archetype. This generation rebelled as youths, prompting the first colleges in the 1520s. Reprisal Generation The reprisal generation was born between 1512 and 1540 and is of the nomad/reactive archetype. They grew up during the wars of the Spanish Armada and saw the expansion of British territories and colonization in the New World overseas. Elizabethan Generation The Elizabethan generation was born between 1541 and 1565 and is of the hero archetype. They benefited as children from growth in academies. They grew up during the Anglo-Spanish War (1585–1604). They regulated commerce, explored overseas empires, built English country houses, pursued science, and wrote poetry that celebrated an orderly universe. Parliamentary Generation The parliamentary generation was born between 1566 and 1587 and is of the artist archetype. Their grew up during an era of foreign threats and war. They built credentials in law, scholarship, religion, and arts and crafts guilds. New World Saeculum Puritan Generation The puritan generation was born between 1588 and 1617 and is of the prophet archetype. Members of the generation were led through the Wars of the Three Kingdoms (1639–1651) by King Charles I and others led a large migration to the Americas. The generation was very religious. Cavalier Generation The cavalier generation was born from 1618 to 1647 and was of the nomad archetype. Members of this generation grew up in an era of religious upheaval and family collapse. Their generation was notoriously violent and uneducated, causing men to take great risks, and resulting in many young deaths. Their generation acted in many ways in reaction against the harsh piety and frugality of the puritans, with a more laissez-faire social attitude. This was the time of Merry Old England and the zeitgeist of this generation was possibly best displayed by king Charles II. Glorious Generation The glorious generation was born from 1648 to 1673 and was of the hero archetype. They had a protected childhood with tax-supported schools and new laws discouraging the kidnapping of young people. After participating in the Indian Wars and the Glorious Revolution, they became involved in the electoral office at a young age. As young adults, they took pride in the growing political, commercial, and scientific achievements of England. They designed insurance, paper money, and public works. Enlightenment Generation The enlightenment generation' was born between 1674 and 1700 and was of the artist archetype. They grew up as protected children when families were close, youth risk discouraged, and good educations and well-connected marriages highly prized. As adults, they provided America's first large cadre of credentialed professionals, political managers, and plantation administrators. Examples in Europe include George Frederic Handel, Antonio Vivaldi, Domenico Scarlatti, and Johann Sebastian Bach. Revolutionary Saeculum Awakening Generation The awakening generation was born between 1701 and 1723 and was of the prophet archetype. They were the first colonial generation to consist mostly of the offspring of native-born parents. As adults, they attacked their elders' moral complacency. Benjamin Franklin was born in this generation. Liberty Generation Strauss and Howe define the liberty generation (nomad archetype) as those born between 1724 and 1741. The first two U.S. Presidents, George Washington and John Adams, were born during this period. Also born in this era were 35 out of the 56 signatories of the United States Declaration of Independence. Republican Generation The republican generation (hero archetype) was born between 1742 and 1766. This generation is known for participating in several global revolutionary movements during the Age of Revolution. This generation witnessed political turmoil in response to the widespread expansion of European imperialism and the vast social inequalities exacerbated by ruthless competition between rival empires in Europe, the Americas and Asia. They came of age during a time when the viability of mercantilism and imperialism was being questioned both in Europe and the Americas. Relying on Enlightenment philosophy, they unleashed violent episodes of revolution, vilified Monarchy, and promoted Republicanism. In colonial America, they participated in the American Revolutionary War, secured America's independence from British rule, and established the American government. Many founding fathers and leading figures in the early years of the independent United States belong to this generation, including U.S. presidents Thomas Jefferson, James Madison, and James Monroe, as well as the leading figures of the French Revolution such as Maximilien Robespierre, Georges Danton, and Camille Desmoulins. Compromise Generation The compromise generation was born between 1767 and 1791 and was of the artist archetype. They "rocked in the cradle of the Revolution" as they watched brave adults struggle and triumph. Notable persons affiliated with this generation include Andrew Jackson, Napoleon Bonaparte and Simón Bolívar. Civil War Saeculum Transcendental Generation The transcendental generation was born between 1792 and 1821 and was of the prophet archetype. They started the Second Great Awakening across the United States. Gilded Generation Strauss and Howe define the gilded generation (nomad archetype) as those born from 1822 to 1842. They came of age during rising national tempers, torrential immigration, rampant commercialism and consumerism, declining college enrollment, and economic disputes. This led to a distrust of zealotry and institutional involvement, shifting focus to a life of materialism.. Most of the American Civil War soldiers were born during this period (the average age was 26). Progressive Generation The progressive generation (hero and artist archetypes) was born from 1843 to 1859 and grew up during or fought in the American Civil War. Great Power Saeculum Missionary Generation The missionary generation was born from 1860 to 1882 and is of the prophet/idealist archetype. Members of the missionary generation have been described as the "home-and-hearth children of the post-Civil War era". They were an idealist generation and as young adults; their leaders were famous preachers. Some were graduates of newly formed black and women's colleges. Their defining characteristics were missionary and social crusades: "muckraker" journalism, prohibitionism, workers' rights, trade unionism and women's suffrage. In midlife, they developed prohibition in the United States, immigration control, and organized vice squads. Because the lost generation were severely impacted by World War I, the leadership of the missionary generation lasted longer than previous generations and in the 1930s and 1940s, their elite became the "wise old men" who enacted a "New Deal" and Social Security, led the World War II, and reaffirmed America's highest ideals during this period. This generation is fully ancestral, with the last known member of the missionary generation, the American Sarah Knauss, having died on December 30, 1999, at 119 years of age. Lost Generation The lost generation (nomad archetype) is the generation that came of age during World War I. "Lost" in this context also means "disoriented, wandering, directionless"—a recognition that there was great confusion and aimlessness among the war's survivors in the early post-war years. Strauss and Howe define the cohort as individuals born between 1883 and 1900. Like the previous generation, the Lost Generation is fully ancestral, with the last known member of the lost generation, the Japanese Nabi Tajima, having died on April 21, 2018, at 117 years of age. G.I. Generation The greatest generation (hero archetype), also known as the G.I. generation and the World War II generation, is the demographic cohort following the lost generation and preceding the silent generation. Strauss and Howe define the cohort as individuals born between 1901 and 1924. They were shaped by the Great Depression and were the primary participants in World War II. Silent Generation The silent generation (artist archetype) is the demographic cohort following the greatest generation and preceding the baby boomers. Strauss and Howe define the cohort as individuals born between 1925 and 1942. Millennial Saeculum Baby Boom Generation Strauss and Howe define the baby boom generation (prophet archetype) as those born from 1943 to 1960. 13th Generation/Generation X Strauss and Howe define the 13th generation (nomad archetype) as those born from 1961 to 1981. Millennial Generation (As of 2023): Neil Howe defines the millennial generation (hero archetype) as those born from 1982 to 2005. Homeland Generation (As of 2023): Neil Howe defines the homeland generation (artist archetype) as those born from 2006 to 2029. Timing of generations and turnings Strauss and Howe argue that the basic length of both generations and turnings—approximately 21 years—derives from longstanding socially and biologically determined phases of life. This is the reason it has remained relatively constant over centuries. Some have argued that rapid increases in technology in recent decades are shortening the length of a generation. According to Strauss and Howe, this is not the case. As long as the transition to adulthood occurs around age 21, the transition to midlife around age 43, and the transition to old age around age 65, they say the basic length of both generations and turnings will remain the same. In their book, The Fourth Turning, however, Strauss and Howe say that the precise boundaries of generations and turnings are erratic. Strauss and Howe compare the saecular rhythm to the four seasons, which they say similarly occur in the same order, but with slightly varying timing, they claimed that the same is true for a Fourth Turning in any given saeculum. Critical reception The Strauss and Howe interpretation of history through a generational lens has received mixed reviews and described as a form of pseudoscience. Some reviewers have praised the authors for their ambition, erudition, and accessibility. For example, former U.S. Vice President Al Gore called Generations: The History of America's Future, 1584 to 2069 the most stimulating book on American history he'd ever read, and sent a copy to each member of Congress. The theory has been influential in the fields of generational studies, marketing, and business management literature. However, it has also been criticized by historians, political scientists, and journalists, as being overly deterministic, non-falsifiable, and unsupported by rigorous evidence. Generations: The History of America's Future, 1584 to 2069 After the publication of their first book Generations, Morton Keller, a professor of history at Brandeis University, said that the authors "had done their homework". He said that their theory could be seen as pop sociology and that it would "come in for a lot more criticism as history. But it's almost always true that the broader you cast your net, the more holes it's going to have. And I admire [the authors'] boldness." Sociologist David Riesman and political scientist Richard Neustadt offered strong, if qualified, praise. Riesman found in the work an "impressive grasp of a great many theoretical and historical bits and pieces" and Neustadt said Strauss and Howe "are asking damned important questions, and I honor them." The Times Literary Supplement called it "fascinating", but also "about as vague and plausible as astrological predictions". Publishers Weekly called it "as woolly as a newspaper horoscope". In 1991, Jonathan Alter wrote in Newsweek that Generations was a "provocative, erudite and engaging analysis of the rhythms of American life". However, he believed it was also "an elaborate historical horoscope that will never withstand scholarly scrutiny." He continued, "these sequential 'peer personalities' are often silly, but the book provides reams of fresh evidence that American history is indeed cyclical, as Arthur Schlesinger Jr. and others have long argued." But he complained, "The generational boundaries are plainly arbitrary. The authors lump together everyone born from 1943 through the end of 1960 (Baby Boomers), a group whose two extremes have little in common. And the predictions are facile and reckless." He concluded: "However fun and informative, the truth about generational generalizations is that they're generally unsatisfactory." Arthur E. Levine, a former president of the Teachers College of Columbia University said "Generational images are stereotypes. There are some differences that stand out, but there are more similarities between students of the past and the present. But if you wrote a book saying that, how interesting would it be?" In response to criticism that they stereotype or generalize all members of a generation, the authors have said, "We've never tried to say that any individual generation is going to be monochromatic. It'll obviously include all kinds of people. But as you look at generations as social units, we consider it to be at least as powerful and, in our view, far more powerful than other social groupings such as economic class, race, sex, religion, and political parties." Gerald Pershall wrote in 1991: "Generations is guaranteed to attract pop history and pop social science buffs. Among professional historians, it faces a tougher sell. Period specialists will resist the idea that their period is akin to several others. Sweeping theories of history are long out of fashion in the halls of ivy, and the authors' lack of academic standing won't help their cause. Their generational quartet is "just too wooden" and "too neat," says one Yale historian. "Prediction is for prophets," scoffed William McLoughlin (a former history professor at Brown), who said it is wrong to think that "if you put enough data together and have enough charts and graphs, you've made history into a science." He also said the book might get a friendlier reception in the sociology and political science departments than in the science department. In 1991, professor and New York Times writer Jay Dolan critiqued Generations for not talking more about class, race, and sex, to which Neil Howe replied that they "are probably generalizations not even as effective as a generation to say something about how people think and behave. One of the things to understand is that most historians never look at history in terms of generations. They prefer to tell history as a seamless row of 55-year-old leaders who always tend to think and behave the same way -- but they don't and they never have. If you look at the way America's 55-year-old leaders were acting in the 1960s -- you know, the ebullience and confidence of the JFKs and LBJs and Hubert Humphreys -- and compare them with today's leaders in Congress -- the indecision, the lack of sure-footedness -- I think you would have to agree that 55-year-olds do not always act the same way and you're dealing with powerful generational forces at work that explain why one generation of war veterans, war heroes, and another generation which came of age in very different circumstances tend to have very different instincts about acting in the world." Responding to criticisms in 1991, William Strauss accepted that some historians might not like their theory, responding with: "People are looking for a new way to connect themselves to the larger story of America. That is the problem. We've felt adrift over the past 10 years, and we think that the way history has been presented over the past couple of decades has been more in terms of the little pieces and people are not as interested in the little pieces now. They're looking for a unifying vision. We haven't had unifying visions of the story of America for decades now, and we're trying to provide it in this book. The kinds of historians who are drawn to our book -- and I'm sure it will be very controversial among academics because we are presenting something that is so new -- but the kinds who are drawn to it are the ones who themselves have focused on the human life cycle rather than just the sequential series of events. Some good examples of that are Morton Keller up at Brandeis and David Hackett Fischer. These are people who have noticed the power in not just generations, but the shifts that have happened over time in the way Americans have treated children and older people and have tried to link that to the broader currents of history." The Fourth Turning In his review for the Boston Globe, historian David Kaiser called The Fourth Turning "a provocative and immensely entertaining outline of American history, Strauss and Howe have taken a gamble". "If the United States calmly makes it to 2015, their work will end up in the ashcan of history, but if they are right, they will take their place among the great American prophets." Kaiser has since argued that Strauss and Howe's predictions of coming crisis seems to have occurred, citing events such as 9/11, the 2008 financial crisis, and the recent political gridlock. Kaiser has incorporated Strauss and Howe's theory in two historical works of his own, American Tragedy: Kennedy, Johnson, and the Origins of the Vietnam War (2000), and No End Save Victory: How FDR Led the Nation into War (2014). Michael Lind, a historian and co-founder of the New America Foundation, wrote that The Fourth Turning (1997) was vague and verged into the realm of "pseudoscience"; "most of the authors' predictions about the American future turn out to be as vague as those of fortune cookies". Lind said that the theory is essential "non-falsifiable" and "mystifying." For The New York Times in 2017, Pulitzer-winning journalist Jeremy Peters wrote that "many academic historians dismiss the book as about as scientific as astrology or a Nostradamus text." 13th Gen In 1993, Andrew Leonard reviewed the book 13th Gen: Abort, Retry, Ignore, Fail?. He wrote "as the authors (Strauss and Howe) relentlessly attack the iniquitous 'child-abusive culture' of the 1960s and '70s and exult in heaping insult after insult on their own generation -- they caricature Baby Boomers as countercultural, long-haired, sex-obsessed hedonists -- their real agenda begins to surface. That agenda becomes clear in part of their wish list for how the 13th generation may influence the future: "13ers will reverse the frenzied and centrifugal cultural directions of their younger years. They will clean up entertainment, de-diversify the culture, reinvent core symbols of national unity, reaffirm rituals of family and neighborhood bonding, and re-erect barriers to cushion communities from unwanted upheaval." Again in 1993, writing for The Globe and Mail, Jim Cormier reviewed the same book: "self-described boomers Howe and Strauss add no profound layer of analysis to previous pop press observations. But in cobbling together a more extensive overview of the problems and concerns of the group they call the 13ers, they've created a valuable primer for other fogeys who are feeling seriously out of touch." Cormier wrote that the authors "raised as many new questions as answers about the generation that doesn't want to be a generation. But at least they've made an honest, empathetic, and good-humoured effort to bridge the bitter gap between the twentysomethings and fortysomethings." In 1993, Charles Laurence at the London Daily Telegraph wrote that, in 13th Gen, Strauss and Howe offered this youth generation "a relatively neutral definition as the 13th American generation from the Founding Fathers,". According to Alexander Ferron's review in Eye Magazine, "13th Gen is best read as the work of two top-level historians. While its agenda is the 13th generation, it can also be seen as an incredibly well-written and exhaustive history of America from 1960 to 1981--examining the era through everything except the traditional historical subjects (war, politics, famine, etc)." In 2011, Jon D. Miller, at the Longitudinal Study of American Youth, funded by the National Science Foundation, wrote that the birth year definition (1961 to 1981) of "Generation X" ("13th Gen") has been widely used in popular and academic literature. Millennials Rising David Brooks reviewed the follow-up book about the next generation titled Millennials Rising (2000). "Millennials" is a term coined by Strauss and Howe. Brooks wrote: "This is not a good book, if by good you mean the kind of book in which the authors have rigorously sifted the evidence and carefully supported their assertions with data. But it is a very good bad book. It's stuffed with interesting nuggets. It's brightly written. And if you get away from the generational mumbo jumbo, it illuminates changes that really do seem to be taking place." Further, Brooks wrote that the generations aren't treated equally: "Basically, it sounds as if America has two greatest generations at either end of the age scale and two crummiest in the middle". A 2000 New York Times book review for this book titled: What's the Matter With Kids Today? Not a Thing, described the message of Millennials Rising as "we boomers are raising a cohort of kids who are smarter, more industrious and better behaved than any generation before", saying the book complimented the Baby Boomer cohort by way of their parenting skills. In 2001, reviewer Dina Gomez wrote in NEA Today that they make their case "convincingly," with "intriguing analysis of popular culture" but conceded that it "over-generalizes". Gomez argued that it is "hard to resist its hopeful vision for our children and future." Millennials Rising ascribes seven "core traits" to Millennials: special, sheltered, confident, team-oriented, conventional, pressured, and achieving. A 2009, Chronicle of Higher Education report commented Howe and Strauss based these core traits on a "hodgepodge of anecdotes, statistics, and pop-culture references" and on surveys of approximately 600 high-school seniors from Fairfax County, Virginia, an affluent county with median household income approximately twice the national average. The report described Millennials Rising as a "good-news revolution" making "sweeping predictions" and describing Millennials as "rule followers who were engaged, optimistic, and downright pleasant", commenting the "book gave educators and tens of millions of parents, a warm feeling, saying who wouldn't want to hear that their kids are special?" General In 2006, Frank Giancola wrote an article in Human Resource Planning that stated "the emphasis on generational differences is not generally borne out by empirical research, despite its popularity". In 2016 an article was published that explains the differences in generations, observed with the employer's position, through the development of working conditions, initiated by the employer. This development is due to the competition of firms on the job market for receiving more highly skilled workers. New working conditions as a product on the market have a classic product life-cycle and when they become widespread standard expectations of employees change accordingly. One criticism of Strauss and Howe's theory and generational studies is that conclusions are overly broad and do not reflect the reality of every person in each generation regardless of their race, color, national origin, religion, sex, age, disability, or genetic information. For example, Hoover cited the case of Millennials, noted that in 2009 commentators have tended to label white, affluent teenagers who accomplish great things as they grow up in the suburbs, who confront anxiety when applying to super-selective colleges, and who multitask with ease as their helicopter parents hover reassuringly above them as Millennials. The label tends not to appear in renderings of teenagers who happen to be minorities, poor, or who have never won a spelling bee. Nor does the term often refer to students from big cities and small towns that are nothing like Fairfax County, Virginia, or who lack technological know-how. Or who struggle to complete high school. Or who never even consider college. Or who commit crimes. Or who suffer from too little parental support. Or who drop out of college. Aren't they Millennials too?" In their 2000 book Millennials Rising they brought attention to the Millennial children of immigrants in the United States, "who face daunting challenges." They wrote "one-third have no health insurance, live below the poverty line and live in overcrowded housing". In a February 2017 article from Quartz two journalists commented on the theory saying: "it is too vague to be proven wrong, and has not been taken seriously by most professional historians. But it is superficially compelling, and plots out to some degree how America's history has unfolded since its founding". A May 2017 article from Quartz described the Strauss–Howe generational theory as "pseudoscience". In an April 2017 article from Politico, David Greenberg, a professor of history and media studies at Rutgers University, described Strauss–Howe generational theory as "crackpot theories". Peter Turchin, a scientist and specialist in the fields of cultural evolution, cliodynamics and structural-demographic theory, has criticized the theory, stating that it is not a scientific theory and that it is more akin to a prophecy since it "forces the historical record to fit a postulated cycle by stretching in some places and cutting off a bit here and there in others". In popular culture American electronic musician Oneohtrix Point Never was inspired by The Fourth Turning for the concept of his 2018 album Age Of and its accompanying performance installation MYRIAD. Will Arbery's play Heroes of the Fourth Turning, first produced at New York's Playwrights Horizons in 2019, is inspired by the theories of Strauss and Howe, and the character Teresa is a vocal proponent of them. The 2022 Netflix series The Watcher features a scene citing postulations from The Fourth Turning. See also Notes References Bibliography Strauss, William; Howe, Neil (1991). Generations: The History of America's Future, 1584 to 2069 (1 ed.). New York. ISBN 978-0-688-08133-1. OCLC 22306142.{{cite book}}: CS1 maint: location missing publisher (link) Howe, Neil; Strauss, William (1993). 13th Gen: Abort, Retry, Ignore, Fail? (1 ed.). New York: Vintage Books. ISBN 978-0-679-74365-1. OCLC 26632626. Strauss, William; Howe, Neil (1997). The Fourth Turning: An American Prophecy (1 ed.). New York. ISBN 978-0-553-06682-1. OCLC 35008291.{{cite book}}: CS1 maint: location missing publisher (link) Howe, Neil; Strauss, William (2000). Millennials Rising: The Next Great Generation. William Strauss. New York: Vintage Books. ISBN 978-0-375-70719-3. OCLC 44118080. Howe, Neil; Strauss, William (2007). Millennials Go to College: Strategies for a New Generation on Campus: Recruiting and Admissions, Campus Life, and the Classroom (2 ed.). Great Falls, Va.: LifeCourse Associates. ISBN 978-0-9712606-1-0. OCLC 123907203. Howe, Neil; Strauss, William; Nadler, Reena (2008). Millennials & K-12 Schools: Educational Strategies for a New Generation. Great Falls, Va.: LifeCourse Associates. ISBN 978-0-9712606-5-8. OCLC 311800406. External links Discussion forum of the Strauss and Howe generation theory Cultural imperialism (also cultural colonialism) comprises the cultural dimensions of imperialism. The word "imperialism" describes practices in which a country engages culture (language, tradition, ritual, politics, economics) to create and maintain unequal social and economic relationships among social groups. Cultural imperialism often uses wealth, media power and violence to implement the system of cultural hegemony that legitimizes imperialism. Cultural imperialism may take various forms, such as an attitude, a formal policy, or military action—insofar as each of these reinforces the empire's cultural hegemony. Research on the topic occurs in scholarly disciplines, and is especially prevalent in communication and media studies, education, foreign policy, history, international relations, linguistics, literature, post-colonialism, science, sociology, social theory, environmentalism, and sports. Cultural imperialism may be distinguished from the natural process of cultural diffusion. The spread of culture around the world is referred to as cultural globalization. Background and definitions Although the Oxford English Dictionary has a 1921 reference to the "cultural imperialism of the Russians", John Tomlinson, in his book on the subject, writes that the term emerged in the 1960s and has been a focus of research since at least the 1970s. Terms such as "media imperialism", "structural imperialism", "cultural dependency and domination", "cultural synchronization", "electronic colonialism", "ideological imperialism", and "economic imperialism" have all been used to describe the same basic notion of cultural imperialism. The term refers largely to the exercise of power in a cultural relationship in which the principles, ideas, practices, and values of a powerful, invading society are imposed upon indigenous cultures in the occupied areas. The process is often used to describe examples of when the compulsory practices of the cultural traditions of the imperial social group are implemented upon a conquered social group. The process is also present when powerful nations are able to flood the information and media space with their ideas, limiting countries and communities' ability to compete and expose people to locally created content. Cultural imperialism has been called a process that intends to transition the "cultural symbols of the invading communities from 'foreign' to 'natural,''domestic,'" comments Jeffrey Herlihy-Mera. He described the process as being carried out in three phases by merchants, then the military, then politicians. While the third phase continues "in perpetuity", cultural imperialism tends to be "gradual, contested (and continues to be contested), and is by nature incomplete. The partial and imperfect configuration of this ontology takes an implicit conceptualization of reality and attempts—and often fails—to elide other forms of collective existence." In order to achieve that end, cultural engineering projects strive to "isolate residents within constructed spheres of symbols" such that they (eventually, in some cases after several generations) abandon other cultures and identify with the new symbols. "The broader intended outcome of these interventions might be described as a common recognition of possession of the land itself (on behalf of the organizations publishing and financing the images)." For Herbert Schiller, cultural imperialism refers to the American Empire's "coercive and persuasive agencies, and their capacity to promote and universalize an American 'way of life' in other countries without any reciprocation of influence." According to Schiller, cultural imperialism "pressured, forced and bribed" societies to integrate with the U.S.'s expansive capitalist model but also incorporated them with attraction and persuasion by winning "the mutual consent, even solicitation of the indigenous rulers." He continues remarks that it is:the sum processes by which a society is brought into the modern [U.S.-centered] world system and how its dominating stratum is attracted, pressured, forced, and sometimes bribed into shaping social institutions to correspond to, or even promote, the values and structures of the dominating centres of the system. The public media are the foremost example of operating enterprises that are used in the penetrative process. For penetration on a significant scale the media themselves must be captured by the dominating/penetrating power. This occurs largely through the commercialization of broadcasting.The historical contexts, iterations, complexities, and politics of Schiller's foundational and substantive theorization of cultural imperialism in international communication and media studies are discussed in detail by political economy of communication researchers Richard Maxwell, Vincent Mosco, Graham Murdock, and Tanner Mirrlees. Downing and Sreberny-Mohammadi state: "Cultural imperialism signifies the dimensions of the process that go beyond economic exploitation or military force. In the history of colonialism, (i.e., the form of imperialism in which the government of the colony is run directly by foreigners), the educational and media systems of many Third World countries have been set up as replicas of those in Britain, France, or the United States and carry their values. Western advertising has made further inroads, as have architectural and fashion styles. Subtly but powerfully, the message has often been insinuated that Western cultures are superior to the cultures of the Third World." Poststructuralism In poststructuralist and postcolonial theory, cultural imperialism is often understood as the cultural legacy of Western colonialism, or forms of social action contributing to the continuation of Western hegemony. To some outside of the realm of this discourse, the term is critiqued as being unclear, unfocused, and/or contradictory in nature. The work of French philosopher and social theorist Michel Foucault has heavily influenced use of the term cultural imperialism, particularly his philosophical interpretation of power and his concept of governmentality. Following an interpretation of power similar to that of Machiavelli, Foucault defines power as immaterial, as a "certain type of relation between individuals" that has to do with complex strategic social positions that relate to the subject's ability to control its environment and influence those around itself. According to Foucault, power is intimately tied with his conception of truth. "Truth", as he defines it, is a "system of ordered procedures for the production, regulation, distribution, circulation, and operation of statements" which has a "circular relation" with systems of power. Therefore, inherent in systems of power, is always "truth", which is culturally specific, inseparable from ideology which often coincides with various forms of hegemony, including cultural imperialism. Foucault's interpretation of governance is also very important in constructing theories of transnational power structure. In his lectures at the Collège de France, Foucault often defines governmentality as the broad art of "governing", which goes beyond the traditional conception of governance in terms of state mandates, and into other realms such as governing "a household, souls, children, a province, a convent, a religious order, a family". This relates directly back to Machiavelli's treatise on how to retain political power at any cost, The Prince, and Foucault's aforementioned conceptions of truth and power. (i.e. various subjectivities are created through power relations that are culturally specific, which lead to various forms of culturally specific governmentality such as neoliberal governmentality.) Post-colonialism Edward Saïd is a founding figure of postcolonialism, established with the book Orientalism (1978), a humanist critique of The Enlightenment, which criticises Western knowledge of "The East"—specifically the English and the French constructions of what is and what is not "Oriental". Whereby said "knowledge" then led to cultural tendencies towards a binary opposition of the Orient vs. the Occident, wherein one concept is defined in opposition to the other concept, and from which they emerge as of unequal value. In Culture and Imperialism (1993), the sequel to Orientalism, Saïd proposes that, despite the formal end of the "age of empire" after the Second World War (1939–1945), colonial imperialism left a cultural legacy to the (previously) colonised peoples, which remains in their contemporary civilisations; and that said American cultural imperialism is very influential in the international systems of power. In "Can the Subaltern Speak?" Gayatri Chakravorty Spivak critiques common representations in the West of the Sati, as being controlled by authors other than the participants (specifically English colonizers and Hindu leaders). Because of this, Spivak argues that the subaltern, referring to the communities that participate in the Sati, are not able to represent themselves through their own voice. Spivak says that cultural imperialism has the power to disqualify or erase the knowledge and mode of education of certain populations that are low on the social and economic hierarchy. In A Critique of Postcolonial Reason, Spivak argues that Western philosophy has a history of not only exclusion of the subaltern from discourse, but also does not allow them to occupy the space of a fully human subject. Contemporary ideas and debate Cultural imperialism can refer to either the forced acculturation of a subject population, or to the voluntary embracing of a foreign culture by individuals who do so of their own free will. Since these are two very different referents, the validity of the term has been called into question. Cultural influence can be seen by the "receiving" culture as either a threat to or an enrichment of its cultural identity. It seems therefore useful to distinguish between cultural imperialism as an (active or passive) attitude of superiority, and the position of a culture or group that seeks to complement its own cultural production, considered partly deficient, with imported products. The imported products or services can themselves represent, or be associated with, certain values (such as consumerism). According to one argument, the "receiving" culture does not necessarily perceive this link, but instead absorbs the foreign culture passively through the use of the foreign goods and services. Due to its somewhat concealed, but very potent nature, this hypothetical idea is described by some experts as "banal imperialism". For example, it is argued that while "American companies are accused of wanting to control 95 percent of the world's consumers", "cultural imperialism involves much more than simple consumer goods; it involved the dissemination of American principles such as freedom and democracy", a process which "may sound appealing" but which "masks a frightening truth: many cultures around the world are disappearing due to the overwhelming influence of corporate and cultural America". Some believe that the newly globalised economy of the late 20th and early 21st century has facilitated this process through the use of new information technology. This kind of cultural imperialism is derived from what is called "soft power". The theory of electronic colonialism extends the issue to global cultural issues and the impact of major multi-media conglomerates, ranging from Paramount, WarnerMedia, AT&T, Disney, News Corp, to Google and Microsoft with the focus on the hegemonic power of these mainly United States–based communication giants. Cultural diversity One of the reasons often given for opposing any form of cultural imperialism, voluntary or otherwise, is the preservation of cultural diversity, a goal seen by some as analogous to the preservation of ecological diversity. Proponents of this idea argue either that such diversity is valuable in itself, to preserve human historical heritage and knowledge, or instrumentally valuable because it makes available more ways of solving problems and responding to catastrophes, natural or otherwise. Africa Of all the areas of the world that scholars have claimed to be adversely affected by imperialism, Africa is probably the most notable. In the expansive "age of imperialism" of the nineteenth century, scholars have argued that European colonisation in Africa has led to the elimination of many various cultures, worldviews, and epistemologies, particularly through neocolonisation of public education. This, arguably has led to uneven development, and further informal forms of social control having to do with culture and imperialism. A variety of factors, scholars argue, lead to the elimination of cultures, worldviews, and epistemologies, such as "de-linguicization" (replacing native African languages with European ones), devaluing ontologies that are not explicitly individualistic, and at times going as far as to not only define Western culture itself as science, but that non-Western approaches to science, the Arts, indigenous culture, etc. are not even knowledge. One scholar, Ali A. Abdi, claims that imperialism inherently "involve[s] extensively interactive regimes and heavy contexts of identity deformation, misrecognition, loss of self-esteem, and individual and social doubt in self-efficacy." Therefore, all imperialism would always, already be cultural. Neoliberalism Neoliberalism is often critiqued by sociologists, anthropologists, and cultural studies scholars as being culturally imperialistic. Critics of neoliberalism, at times, claim that it is the newly predominant form of imperialism. Other scholars, such as Elizabeth Dunn and Julia Elyachar have claimed that neoliberalism requires and creates its own form of governmentality. In Dunn's work, Privatizing Poland, she argues that the expansion of the multinational corporation, Gerber, into Poland in the 1990s imposed Western, neoliberal governmentality, ideologies, and epistemologies upon the post-Soviet people hired. Cultural conflicts occurred most notably the company's inherent individualistic policies, such as promoting competition among workers rather than cooperation, and in its strong opposition to what the company owners claimed was bribery. In Elyachar's work, Markets of Dispossession, she focuses on ways in which, in Cairo, NGOs along with INGOs and the state promoted neoliberal governmentality through schemas of economic development that relied upon "youth microentrepreneurs". Youth microentrepreneurs would receive small loans to build their own businesses, similar to the way that microfinance supposedly operates. Elyachar argues though, that these programs not only were a failure, but that they shifted cultural opinions of value (personal and cultural) in a way that favoured Western ways of thinking and being. Development studies Often, methods of promoting development and social justice are critiqued as being imperialistic in a cultural sense. For example, Chandra Mohanty has critiqued Western feminism, claiming that it has created a misrepresentation of the "third world woman" as being completely powerless, unable to resist male dominance. Thus, this leads to the often critiqued narrative of the "white man" saving the "brown woman" from the "brown man". Other, more radical critiques of development studies, have to do with the field of study itself. Some scholars even question the intentions of those developing the field of study, claiming that efforts to "develop" the Global South were never about the South itself. Instead, these efforts, it is argued, were made in order to advance Western development and reinforce Western hegemony. Media effects studies The core of cultural imperialism thesis is integrated with the political-economy traditional approach in media effects research. Critics of cultural imperialism commonly claim that non-Western cultures, particularly from the Third World, will forsake their traditional values and lose their cultural identities when they are solely exposed to Western media. Nonetheless, Michael B. Salwen, in his book Critical Studies in Mass Communication (1991), claims that cross-consideration and integration of empirical findings on cultural imperialist influences is very critical in terms of understanding mass media in the international sphere. He recognises both of contradictory contexts on cultural imperialist impacts. The first context is where cultural imperialism imposes socio-political disruptions on developing nations. Western media can distort images of foreign cultures and provoke personal and social conflicts to developing nations in some cases. Another context is that peoples in developing nations resist to foreign media and preserve their cultural attitudes. Although he admits that outward manifestations of Western culture may be adopted, but the fundamental values and behaviours remain still. Furthermore, positive effects might occur when male-dominated cultures adopt the "liberation" of women with exposure to Western media and it stimulates ample exchange of cultural exchange. Criticisms of "cultural imperialism theory" Critics of scholars who discuss cultural imperialism have a number of critiques. Cultural imperialism is a term that is only used in discussions where cultural relativism and constructivism are generally taken as true. (One cannot critique promoting Western values if one believes that said values are good. Similarly, one cannot argue that Western epistemology is unjustly promoted in non-Western societies if one believes that those epistemologies are good.) Therefore, those who disagree with cultural relativism and/or constructivism may critique the employment of the term, cultural imperialism on those terms. John Tomlinson provides a critique of cultural imperialism theory and reveals major problems in the way in which the idea of cultural, as opposed to economic or political, imperialism is formulated. In his book Cultural Imperialism: A Critical Introduction, he delves into the much debated "media imperialism" theory. Summarizing research on the Third World's reception of American television shows, he challenges the cultural imperialism argument, conveying his doubts about the degree to which US shows in developing nations actually carry US values and improve the profits of US companies. Tomlinson suggests that cultural imperialism is growing in some respects, but local transformation and interpretations of imported media products propose that cultural diversification is not at an end in global society. He explains that one of the fundamental conceptual mistakes of cultural imperialism is to take for granted that the distribution of cultural goods can be considered as cultural dominance. He thus supports his argument highly criticising the concept that Americanization is occurring through global overflow of American television products. He points to a myriad of examples of television networks who have managed to dominate their domestic markets and that domestic programs generally top the ratings. He also doubts the concept that cultural agents are passive receivers of information. He states that movement between cultural/geographical areas always involves translation, mutation, adaptation, and the creation of hybridity. Other key critiques are that the term is not defined well, and employs further terms that are not defined well, and therefore lacks explanatory power, that cultural imperialism is hard to measure, and that the theory of a legacy of colonialism is not always true. Dealing with cultural dominance David Rothkopf, managing director of Kissinger Associates and an adjunct professor of international affairs at Columbia University (who also served as a senior U.S. Commerce Department official in the Clinton Administration), wrote about cultural imperialism in his provocatively titled In Praise of Cultural Imperialism? in the summer 1997 issue of Foreign Policy magazine. Rothkopf says that the United States should embrace "cultural imperialism" as in its self-interest. But his definition of cultural imperialism stresses spreading the values of tolerance and openness to cultural change in order to avoid war and conflict between cultures as well as expanding accepted technological and legal standards to provide free traders with enough security to do business with more countries. Rothkopf's definition almost exclusively involves allowing individuals in other nations to accept or reject foreign cultural influences. He also mentions, but only in passing, the use of the English language and consumption of news and popular music and film as cultural dominance that he supports. Rothkopf additionally makes the point that globalisation and the Internet are accelerating the process of cultural influence. Culture is sometimes used by the organisers of society—politicians, theologians, academics, and families—to impose and ensure order, the rudiments of which change over time as need dictates. One need only look at the 20th century's genocides. In each one, leaders used culture as a political front to fuel the passions of their armies and other minions and to justify their actions among their people. Rothkopf then cites genocide and massacres in Armenia, Russia, the Holocaust, Cambodia, Bosnia and Herzegovina, Rwanda and East Timor as examples of culture (in some cases expressed in the ideology of "political culture" or religion) being misused to justify violence. He also acknowledges that cultural imperialism in the past has been guilty of forcefully eliminating the cultures of natives in the Americas and in Africa, or through use of the Inquisition, "and during the expansion of virtually every empire." The most important way to deal with cultural influence in any nation, according to Rothkopf, is to promote tolerance and allow, or even promote, cultural diversities that are compatible with tolerance and to eliminate those cultural differences that cause violent conflict: Successful multicultural societies, be they nations, federations, or other conglomerations of closely interrelated states, discern those aspects of culture that do not threaten union, stability, or prosperity (such as food, holidays, rituals, and music) and allow them to flourish. But they counteract or eradicate the more subversive elements of culture (exclusionary aspects of religion, language, and political/ideological beliefs). History shows that bridging cultural gaps successfully and serving as a home to diverse peoples requires certain social structures, laws, and institutions that transcend culture. Furthermore, the history of a number of ongoing experiments in multiculturalism, such as in the European Union, India, South Africa, Canada and the United States, suggests that workable, if not perfected, integrative models exist. Each is built on the idea that tolerance is crucial to social well-being, and each at times has been threatened by both intolerance and a heightened emphasis on cultural distinctions. The greater public good warrants eliminating those cultural characteristics that promote conflict or prevent harmony, even as less-divisive, more personally observed cultural distinctions are celebrated and preserved. Cultural dominance can also be seen in the 1930s in Australia where the Aboriginal Assimilation Policy acted as an attempt to wipe out the Native Australian people. The British settlers tried to biologically alter the skin colour of the Australian Aboriginal people through mixed breeding with white people. The policy also made attempts to forcefully conform the Aborigines to western ideas of dress and education. In history Although the term was popularised in the 1960s, and was used by its original proponents to refer to cultural hegemonies in a post-colonial world, cultural imperialism has also been used to refer to times further in the past. Antiquity The Ancient Greeks have a reputation for spreading their culture around the Mediterranean and Near East through trade and conquest. During the Archaic Period (c. 800 BCE to c. 480 BCE), the burgeoning Greek city-states established settlements and colonies across the Mediterranean Sea, especially in Sicily and southern Italy, influencing the Etruscan and Roman peoples of the region. Greek art affected the style of Scythian artworks through Greek trading colonies in the Black Sea region. In the late-fourth century BC, Alexander the Great conquered Persian and Indian territories all the way to the Indus River Valley and Punjab, spreading Greek religion, art, and science along the way. This resulted in the rise of Hellenistic kingdoms and cities across Egypt, the Near East, Central Asia, and Northwest India, where Greek culture fused with the cultures of the existing populations. The Greek influence prevailed even longer in science and literature: medieval Muslim scholars in the Middle East studied the writings of Aristotle for scientific insights. The Roman Empire also implemented cultural imperialism. Early Rome, in its conquest of Italy, assimilated the people of Etruria by replacing the Etruscan language with Latin, which led to the demise of that language and of many aspects of Etruscan civilisation. Cultural Romanization grew in many parts of Rome's empire: "many regions receiving Roman culture unwillingly, as a form of cultural imperialism." After Roman armies conquered Greece, Rome set about altering the culture of Greece to conform with Roman ideals. For instance, the Greek habit of stripping naked, in public, for exercise, was looked on askance by Roman writers, who considered the practice to be a cause of the Greeks' effeminacy and enslavement. The Roman example has been linked to modern instances of European imperialism in African countries, bridging the two instances with Slavoj Zizek's discussions of "empty signifiers". The Pax Romana was secured in the empire, in part, by the "forced acculturation of the culturally diverse populations that Rome had conquered." The first documented imperialist occupation of Britain dates from this period. British Empire British worldwide expansion in the 18th and 19th centuries was an economic and political phenomenon. However, "there was also a strong social and cultural dimension to it, which Rudyard Kipling termed the 'white man's burden'." One of the ways this was carried out was by religious proselytising, by, amongst others, the London Missionary Society, which was "an agent of British cultural imperialism." Another way, was by the imposition of educational material on the colonies for an "imperial curriculum". Robin A. Butlin writes, "The promotion of empire through books, illustrative materials, and educational syllabuses was widespread, part of an education policy geared to cultural imperialism". This was also true of science and technology in the empire. Douglas M. Peers and Nandini Gooptu note that "Most scholars of colonial science in India now prefer to stress the ways in which science and technology worked in the service of colonialism, as both a 'tool of empire' in the practical sense and as a vehicle for cultural imperialism. In other words, science developed in India in ways that reflected colonial priorities, tending to benefit Europeans at the expense of Indians, while remaining dependent on and subservient to scientific authorities in the colonial metropolis." British sports were spread across the Empire partially as a way of encouraging British values and cultural uniformity, though this was tempered by the fact that colonised peoples gained a sense of nationalistic pride by defeating the British in their own sports. The analysis of cultural imperialism carried out by Edward Said drew principally from a study of the British Empire. According to Danilo Raponi, the cultural imperialism of the British in the 19th century had a much wider effect than only in the British Empire. He writes, "To paraphrase Said, I see cultural imperialism as a complex cultural hegemony of a country, Great Britain, that in the 19th century had no rivals in terms of its ability to project its power across the world and to influence the cultural, political and commercial affairs of most countries. It is the 'cultural hegemony' of a country whose power to export the most fundamental ideas and concepts at the basis of its understanding of 'civilisation' knew practically no bounds." In this, for example, Raponi includes Italy. Other pre-Second World War examples The New Cambridge Modern History writes about the cultural imperialism of Napoleonic France. Napoleon used the Institut de France "as an instrument for transmuting French universalism into cultural imperialism." Members of the institute (who included Napoleon), descended upon Egypt in 1798. "Upon arrival they organised themselves into an Institute of Cairo. The Rosetta Stone is their most famous find. The science of Egyptology is their legacy." After the First World War, Germans were worried about the extent of French influence in the occupied Rhineland, which under the terms of the Treaty of Versailles was under Allied control from 1918 to 1930. An early use of the term appeared in an essay by Paul Ruhlmann (as "Peter Hartmann") at that date, entitled French Cultural Imperialism on the Rhine. North American colonisation Keeping in line with the trends of international imperialistic endeavours, the expansion of Canadian and American territory in the 19th century saw cultural imperialism employed as a means of control over indigenous populations. This, when used in conjunction of more traditional forms of ethnic cleansing and genocide in the United States, saw devastating, lasting effects on indigenous communities. In 2017 Canada celebrated its 150-year anniversary of the confederating of three British colonies. As Catherine Murton Stoehr points out in Origins, a publication organised by the history departments of Ohio State University and Miami University, the occasion came with remembrance of Canada's treatment of First Nations people. A mere 9 years after the 1867 signing of confederation Canada passed "The Indian Act", a separate and not equal form of government especially for First Nations. The Indian Act remains in place today, confining and constraining Indigenous jurisdiction in every area of life, in direct contravention of the nation's founding treaties with indigenous nations. Numerous policies focused on indigenous persons came into effect shortly thereafter. Most notable is the use of residential schools across Canada as a means to remove indigenous persons from their culture and instill in them the beliefs and values of the majorised colonial hegemony. The policies of these schools, as described by Ward Churchill in his book Kill the Indian, Save the Man, were to forcefully assimilate students who were often removed with force from their families. These schools forbid students from using their native languages and participating in their own cultural practices. Residential schools were largely run by Christian churches, operating in conjunction with Christian missions with minimal government oversight. The book, Stolen Lives: The Indigenous peoples of Canada and the Indian Residentials Schools, describes this form of operation: "The government provided little leadership, and the clergy in charge were left to decide what to teach and how to teach it. Their priority was to impart the teachings of their church or order—not to provide a good education that could help students in their post-graduation lives." In a New York Times op-ed, Gabrielle Scrimshaw describes her grandparents being forced to send her mother to one of these schools or risk imprisonment. After hiding her mother on "school pick up day" so as to avoid sending their daughter to institutions whose abuse was well known at the time (mid-20th century). Scrimshaw's mother was left with limited options for further education she says and is today illiterate as a result. Scrimshaw explains, "Seven generations of my ancestors went through these schools. Each new family member enrolled meant a compounding of abuse and a steady loss of identity, culture and hope. My mother was the last generation. the experience left her broken, and like so many, she turned to substances to numb these pains." A report, republished by CBC News, estimates nearly 6,000 children died in the care of these schools. The colonisation of native peoples in North America remains active today despite the closing of the majority of residential schools. This form of cultural imperialism continues in the use of Native Americans as mascots for schools and athletic teams. Jason Edward Black, a professor and chair in the Department of Communication Studies at the University of North Carolina at Charlotte, describes how the use of Native Americans as mascots furthers the colonial attitudes of the 18th and 19th centuries. Indigenous groups, along with cultural studies scholars, view the Native mascots as hegemonic devices–commodification tools–that advance a contemporary manifest destiny by marketing Native culture as Euromerican identity. In Deciphering Pocahontas, Kent Ono and Derek Buescher wrote: "Euro-American culture has made a habit of appropriating, and redefining what is 'distinctive' and constitutive of Native Americans." Nazi colonialism Cultural imperialism has also been used in connection with the expansion of German influence under the Nazis in the middle of the twentieth century. Alan Steinweis and Daniel Rogers note that even before the Nazis came to power, "Already in the Weimar Republic, German academic specialists on eastern Europe had contributed through their publications and teaching to the legitimization of German territorial revanchism and cultural imperialism. These scholars operated primarily in the disciplines of history, economics, geography, and literature." In the area of music, Michael Kater writes that during the WWII German occupation of France, Hans Rosbaud, a German conductor based by the Nazi regime in Strasbourg, became "at least nominally, a servant of Nazi cultural imperialism directed against the French." In Italy during the war, Germany pursued "a European cultural front that gravitates around German culture". The Nazi propaganda minister Joseph Goebbels set up the European Union of Writers, "one of Goebbels's most ambitious projects for Nazi cultural hegemony. Presumably a means of gathering authors from Germany, Italy, and the occupied countries to plan the literary life of the new Europe, the union soon emerged as a vehicle of German cultural imperialism." For other parts of Europe, Robert Gerwarth, writing about cultural imperialism and Reinhard Heydrich, states that the "Nazis' Germanization project was based on a historically unprecedented programme of racial stock-taking, theft, expulsion and murder." Also, "The full integration of the [Czech] Protectorate into this New Order required the complete Germanization of the Protectorate's cultural life and the eradication of indigenous Czech and Jewish culture." The actions by Nazi Germany reflect on the notion of race and culture playing a significant role in imperialism. The idea that there is a distinction between the Germans and the Jews has created the illusion of Germans believing they were superior to the Jewish inferiors, the notion of us/them and self/others. Western imperialism Cultural imperialism manifests in the Western world in the form legal system to include commodification and marketing of indigenous resources (example medicinal, spiritual or artistic) and genetic resources (example human DNA). Americanization The terms "McDonaldization", "Disneyization" and "Cocacolonization" have been coined to describe the spread of Western cultural influence, especially after the end of the Cold War. These Western influences often have personal, social, economical, and historical impact on people globally depending on the country and region. “Virtually all countries are moving discernibly toward the U.S. model, and the process is self reinforcing”, Herman, E. and McChesney, R. (n.d.). "Media Globalization: The US Experience and Influence". There are many countries affected by the US and their pop-culture. For example, the film industry in Nigeria referred to as "Nollywood" being the second largest as it produces more films annually than the United States, their films are shown across Africa. Another term that describes the spread of Western cultural influence is "Hollywoodization" it is when American culture is promoted through Hollywood films which can culturally affect the viewers of Hollywood films. See also Notes References Dunch, Ryan (2002). "Beyond Cultural Imperialism: Cultural Theory, Christian Missions, and Global Modernity". History and Theory. 41 (3): 301–325. doi:10.1111/1468-2303.00208. JSTOR 3590688. S2CID 143267711. Hamm, Bernd; Russell Charles Smandych (2005). Cultural imperialism: essays on the political economy of cultural domination. Reference, Information and Interdisciplinary Subjects Series. University of Toronto Press. ISBN 978-1-55111-707-2. Lechner, Frank; John Boli (2009). The Globalization Reader. Wiley-Blackwell. Lechner, Frank; John Boli (2012). The Globalization Reader. John Wiley & Sons. ISBN 978-0-470-65563-4. Tomlinson, John (1991). Cultural imperialism: a critical introduction (illustrated, reprint ed.). Continuum International Publishing Group. ISBN 978-0-8264-5013-5. White, Livingston A. (Spring–Summer 2001). "Reconsidering cultural imperialism theory". Transnational Broadcasting Studies (6). The Center for Electronic Journalism at the American University in Cairo and the Centre for Middle East Studies, St. Antony’s College, Oxford. External links "In Praise of Cultural Imperialism?", by David Rothkopf, Foreign Policy no. 107, Summer 1997, pp. 38–53, which argues that cultural imperialism is a positive thing. "Reconsidering cultural imperialism theory" by Livingston A. White, Transnational Broadcasting Studies no. 6, Spring/Summer 2001, which argues that the idea of media imperialism is outdated. Academic Web page from 24 February 2000, discussing the idea of cultural imperialism "Cultural Imperialism", BBC Radio 4 discussion with Linda Colley, Phillip Dodd and Mary Beard (In Our Time, 27 June 2002) The Eight Principles of Yong are used by calligraphers to practice how to write the eight most common strokes in regular script, using the fact that they are all present in the character 永; yǒng; 'forever', 'permanence'. It was believed that the frequent practice of these principles as such when beginning one's study could ensure beauty in the Chinese calligrapher's writing. The Eight Principles are influenced by the Eastern Jin-era Seven Powers (七勢) by Lady Wei Shuo. Publications on the principles include: The Tang-era Praise to the Eight Principles of "Yong" (永字八法頌) by Liu Zongyuan The Tang-era Praise to the Eight Principles of "Yong" (永字八法頌) by Yan Zhenqing The Yuan-era Eight Ways to Explain "Yong" (永字八法解) by Li Puguang, which provides two-character metaphorical names Table CJK strokes In addition to these eight common strokes in 永, there are at least two dozen strokes of combinations which enter in the composition of CJK strokes and by inclusion the CJK characters themselves. Most strokes are encoded in Unicode as symbols, to be used in ideographic description sequences (IDS). The standard characters names assigned in the UCS for these CJK strokes are based on initials of the modern Chinese names (romanized with Pinyin) of component principles with which they are recognized and drawn. Gallery See also CJK characters CJK strokes == References == The Invention of Art: A Cultural History (2001) is an art history book by Larry Shiner, Emeritus Professor of Philosophy, History, and Visual Arts at the University of Illinois Springfield. Shiner spent over a decade to finish the work of this book. Content The book sees fine art as a modern invention due to social transformations during the 18th century. David Clowney, writing for Contemporary Aesthetics summarizes the central thesis of this book. "In The Invention of Art: A Cultural History, Larry Shiner claims that Art with a capital A, Fine Art, was invented in the west in the eighteenth century. The claim is not original with him; he credits Paul Oskar Kristeller’s essay “The Modern System of the Arts” as the inspiration for his work. Others have made this claim as well, among them Pierre Bourdieu, Paul Mattick, and Terry Eagleton. What Shiner has added is a detailed proof of the Kristeller claim using the methods of intellectual, social, cultural, and art history. In brief, the thesis is this: There was a traditional “system of the arts” in the West before the eighteenth century. (Other traditional cultures still have a similar system.) In that system, an artist or artisan was a skilled maker or practitioner, a work of art was the useful product of skilled work, and the appreciation of the arts was integrally connected with their role in the rest of life. “Art,” in other words, meant approximately the same thing as the Greek word techne, or in English “skill”, a sense that has survived in phrases like “the art of war,” “the art of love,” and “the art of medicine.”" The book is divided into five parts I. Before Fine Art and Craft - Details the lack of any differentiation between skilled work of any type during three periods of history; classical antiquity, the middle ages, and the Renaissance. The lack of any social role separating artists from artisans, nor any concept of the aesthetic is also discussed. In the final chapter of Part I, the beginnings of some differentiation during the 17th century is described. II. Art Divided III. Countercurrents IV. The Apotheosis of Art V. Beyond Art and Craft. Quotes “The modern system of art is not an essence or a fate but something we have made. Art as we have generally understood it is a European invention barely two hundred years old.” (Shiner 2003, p. 3) Reviews In a book review published in The Journal of Aesthetics and Art Criticism, Mitch Avila wrote: Shiner aptly characterizes his narrative as one that aims to heal the unnecessarily fractured conceptions of art and art practice that mark the contemporary artworld... By showing that the essentialist conception of art, along with its normative and regulative implications, is the artifact of a particular historical and cultural world, Shiner invites us to freely respond to the manifold richness of human expression and embellishment. Reviewer Marc Spiegler called the book "a must-read for anyone active in the arts". == References == The legal aspects of surrogacy in any particular jurisdiction tend to hinge on a few central questions: Are surrogacy agreements enforceable, void, or prohibited? Does it make a difference whether the gestational carrier is paid (commercial) or simply reimbursed for expenses (altruistic)? What, if any, difference does it make whether the surrogacy is traditional or gestational surrogacy? Is there an alternative to post-birth adoption for the recognition of the intended parents as the legal parents, either before or after the birth? Laws differ widely from one jurisdiction to another. Of the countries which allow surrogacy, many have residency or citizenship requirements for the intended parent(s) and/or the surrogate. Countries without such requirements often attract persons from abroad, being destinations for fertility tourism. In some countries, such as the United States, Canada or Australia, laws vary by state/territory. Prohibition of commercial surrogacy Commercial surrogacy, that is, surrogacy where the individual is paid to carry the baby or surrender it to other person(s) is illegal in the European Union, as Article 3 of the Charter of Fundamental Rights of the European Union states that "In the fields of medicine and biology, the following must be respected in particular: [..] (c) the prohibition on making the human body and its parts as such a source of financial gain" The Oviedo Convention, ratified by 30 countries, also states at Article 21 "Prohibition of financial gain" that: "The human body and its parts shall not, as such, give rise to financial gain." Australia In Australia, all jurisdictions allow altruistic surrogacy; with commercial surrogacy being a criminal offense. In New South Wales, Queensland and the Australian Capital Territory it is an offence to enter into international commercial surrogacy arrangements with potential penalties extending to imprisonment for up to one year in Australian Capital Territory, up to two years imprisonment in New South Wales and up to three years imprisonment in Queensland. In 2004, the Australian Capital Territory made only altruistic surrogacy legal. In 2006, Australian senator Stephen Conroy and his wife Paula Benson announced that they had arranged for a child to be born through egg donation and gestational surrogacy. Unusually, Conroy was put on the birth certificate as the father of the child. Previously, couples who used to make surrogacy arrangements in Australia had to adopt the child after it was registered as born to the natural mother; rather than being recognized as birth parents, however now that surrogacy is more regular practice for childless parents; most states have switched to such arrangements to give the intended parents proper rights. After the announcement, Victoria passed the Assisted Reproductive Treatment Act 2008, effective since 1 January 2010 to make only altruistic surrogacy legal. In 2009, Western Australia passed a law to allow only altruistic surrogacy for couples of the opposite-sex only, and to prohibit it for single people and same-sex couples. In 2010, Queensland made only altruistic surrogacy legal, as did New South Wales, and Tasmania did the same in 2013 with the Surrogacy Act No 34 and the Surrogacy (Consequential Amendments) Act No 31 In 2017, South Australia passed a bill to allow gay couples equal access to both surrogacy and IVF. The bill received royal assent on 15 March 2017 and went into effect on 21 March 2017. In 2022, The Northern Territory passed legislation permitting altruistic surrogacy. A Medicare rebate is not available for IVF treatment for surrogacy. Brazil Commercial Surrogacy is banned by Brazilian law, but altruistic surrogacy is allowed. Nevertheless, the practice is performed illegally throughout the country. Canada The Assisted Human Reproduction Act (AHRC) permits only altruistic surrogacy: gestational carriers may be reimbursed for approved expenses but payment of any other consideration or fee is illegal. People's Republic of China Surrogacy is forbidden by Regulation of human assisted reproductive technology law in the People's Republic of China. The Ministry of Health has established "departmental rules" which prohibit medical professionals from performing surrogacy procedures, with violations punished by fines (but not criminal liabilities). In practice, surrogacy arrangements are common in mainland China with an underground market for commercial surrogacy estimated to encompass between 400 and 500 agencies in 2012. Underground surrogacy has grown in the 21st century and has led to significant complications. Colombia There are no clear rules in Colombia as of today regarding surrogacy and a loophole persists. The current laws applied are those from a natural childbirth. This means the child must be registered with the surnames of the gestational carrier and her partner or spouse, if she has any. Only through a challenging paternity lawsuit before a judge may the commissioning parents be recognized as legal parents, and it may include genetic tests. In July 2016, a right wing political party, Democratic Center, has introduced for the second time a bill in order to determine the concept of surrogacy and to forbid any types of it. Cuba Only altruistic surrogacy is allowed after referendum over a new family law passed and is available since September 2022. Czech Republic Surrogacy is not legally regulated in the Czech Republic and so is generally considered legal. The only mention of the phrase "surrogate motherhood" can be found in § 804 of the law n. 89/2012, where the law designates an exception to the ban of adoption by siblings for siblings carried by a surrogate mother. Finland All surrogacy arrangements (both commercial and altruistic) have been illegal since 2007. Commercial surrogacy arrangements were illegal even before 2007. France In France, since 1994, any surrogacy arrangement that is commercial or altruistic, is illegal or unlawful and is not sanctioned by the law (art 16-7 of the Code Civil). The French Court of Cassation already took this point of view in 1991. It held that if any couple makes an agreement or arranges with another person that she is to bear the husband's child and surrender it on birth to the couple, and that she is choosing that she will not keep the child, the couple making such an agreement or arrangement is not allowed to adopt the child. In its judgement the court held that such an agreement is illegal on the basis of articles 6, 353 and 1128 of the Code Civil. Germany All surrogacy arrangements (both commercial and altruistic) are illegal. The German Free Democratic Party wants to allow altruistic surrogacy. According to the German Civil Code, the legal mother is always the woman who gave birth to the child. Georgia Surrogacy is legally permitted in Georgia, making it one of the few countries in the region where intended parents can engage in both altruistic and commercial surrogacy agreements. The legislation, established in the early 1990s, explicitly allows surrogacy for both domestic and international intended parents. Since 2022, Georgia has absorbed some of the clients from Ukraine, which was the major surrogacy hub in Europe prior to the Russian invasion. Many Ukrainian clinics reopened in Georgia, while surrogates moved there to avoid conflict and due to the favorable legislation and costs. In 2023, the Georgian government announced some plans to reconsider its stance regarding commercial surrogacy for foreign citizens, however no further action was taken, and all forms of surrogacy remain fully legal, including programs involving egg donor programs, shipped embryos and self-cycles. Under Georgian law, surrogacy is available to couples that are married or in the domestic relationship for at least one year. Additionally, a surrogate cannot be an egg donor at the same time; however, donor materials can still be used in the surrogacy programs. The legislation stipulates that surrogates have no parental rights over the child born through surrogacy. Once the child is born, the intended parents are automatically recognized as the legal parents, and the surrogate has no claims to parental rights or responsibilities. Greece Law 3305–2005 ("Enforcement of Medically Assisted Reproduction") Surrogacy in Greece is fully legal and is only one of a handful of countries in the world to give legal protection to intended parents. Intended parents must meet certain qualifications, and will go before a family judge before starting their journey. As long as they meet the qualifications, the court appearance is procedural and will be granted their application. At present, intended parents must be in a heterosexual partnership or be a single female. Females must be able to prove there is a medical indication they cannot carry and be no older than 50 at the time of the contract. As in all jurisdictions, surrogates must pass medical and psychological tests so they can prove to the court that they are medically and mentally fit. What is unique about Greece is that it is the only country in Europe, and one of only countries in the world where the surrogate then has no rights over the child. The intended parents become the legal parents from conception and there is no mention of the surrogate mother anywhere on hospital or birth documents. The intended parent(s) are listed as the parents. This even applies if an egg or sperm donor is used by one of the partners. An added advantage for Europeans is that, due to the Schengen Treaty, they can freely travel home as soon as the baby is born and deal with citizenship issues at that time, as opposed to applying at their own embassy in Greece. The old regime (pursuant to art. 8 of Law 3089/2002), one of the prerequisites for granting the judicial permission for surrogacy was also the fact that the surrogate mother and the commissioning parents should be Greek citizens or permanent residents. However, the law has recently (in July 2014) changed and the new provisions of L. 4272/2014 foresee now that the surrogacy is allowed to applicants or surrogate mothers who have their permanent or temporary residence in Greece. With this new law Greece becomes the only EU country with a comprehensive framework to regulate, facilitate and enforce surrogacy, as according to the explanatory statement of the art. 17 of the L. 4272/2014: "The possibility is now extending also to applicants or surrogate mothers who have their permanent residence outside Greece". Hong Kong (S.A.R.) Commercial surrogacy is criminal under the Human Reproductive Technology Ordinance 2000. The law is phrased in a manner that no one can pay a surrogate, no surrogate can receive money, and no one can arrange a commercial surrogacy (the same applies to the supply of gametes), no matter within or outside Hong Kong. Normally only the gametes of the intended parents can be used. In October 2010, Peter Lee, the eldest son and one of the presumed heirs of billionaire Lee Shau Kee obtained three sons through a gestational carrier, reportedly from California. Since the junior Lee is single, the news attracted criticism on both moral and legal grounds. A vicar general of the territory's Roman Catholic diocese was critical. In December the case was reportedly referred to police after questions were asked in Legco. After a 10-month investigation,Department of Justice had been decided to end the probe as there was not enough evidence to show that a criminal offence had been committed in this case. Iceland All surrogacy arrangements (both commercial and altruistic) are illegal. India As per the Surrogacy (Regulation) Act, 2021, only a couple who has been married for 5 years can opt for surrogacy on medical grounds only. The law defines a couple as a married Indian “man and woman” and also prescribes an age-criteria with the woman being in the age of 23 years to 50 years and the man between 26 years to 55 years. The couple should not have a child of their own. Though the law allows single women to resort to surrogacy, she has to be a widow or a divorcee between the age of 35 and 45 years. Single men are not eligible. Starting 4 November 2015, commercial surrogacy for foreign intended parents is not legal in India. Those commissioned before this date are reviewed on case by case situation; however, no new surrogacies will be started. Before 2015, foreign commercial surrogacy was legal in India. India was a destination for surrogacy-related fertility tourism because of the relatively low cost. Including the costs of flight tickets, medical procedures, and hotels, it was roughly a third of the price compared with going through the procedure in the UK. In the case of Balaz v. Union of India the Supreme Court of India has given the verdict that the citizenship of the child born through this process will have the citizenship of its gestational carrier. Surrogacy was regulated by the Indian Council of Medical Research guidelines, 2005. Ireland There is no law in Ireland governing surrogacy. In 2005 a Government-appointed Commission published a report on Assisted Human Reproduction, which made recommendations on the broader area of assisted human reproduction. In response to Irish nationals engaging in surrogacy contracts in other nations, the Minister for Justice, Equality and Defence published guidelines for surrogacy on 21 February 2012. A Supreme Court ruling in May 2023 noted the lack of legislation around surrogacy in Ireland. Israel In March 1996, the Israeli government legalized gestational surrogacy under the Embryo Carrying Agreements Law. This law made Israel the first country in the world to implement a form of state-controlled surrogacy in which each and every contract must be approved directly by the state. A state-appointed committee permits surrogacy arrangements to be filed only by Israeli citizens who share the same religion. The numerous restrictions on surrogacy under Israeli law have prompted some intended parents to turn to surrogates outside of the country. In February 2020, the Israeli Supreme Court ruled the restriction on same-sex couples from entering surrogacy agreements as discriminatory, thus giving the state one year to change the law. In July 2021, the Supreme Court made a second ruling stating that legislation banning same-sex couples and single men from becoming parents via surrogacy would be null and void within six months. In January 2022, Health Minister Nitzan Horowitz announced that surrogacy would be allowed for same-sex couples, transgender people, and single men starting on 11 January. Iran All surrogacy arrangements (both commercial and altruistic) are legal and popular. Many couples from the Middle East do surrogacy in Iran for legal easiness. The legalization of surrogacy in Iran is an interplay between religious acceptance, legal interpretation, and societal norms. In a country where religious adherence plays a significant role in governance and daily life, permitting surrogacy was no doubt the same. Iran’s religious demographic is predominantly Shia Islam, with around 90-95% of its citizens practicing this sect of Islam. Initially, both Shia and Sunni religious scholars and authorities were aligned in the prohibition of third-party reproduction, including surrogacy. This was a stance rooted in traditional interpretations of Islamic law and ethics. However, the landscape began to shift in 1999 when Ayaollah Ali Hussein Al Khamene’j, the supreme leader of the Islamic Republic of Iran issued a fatwa addressing surrogacy. The fatwa was a turning point, providing detailed guidance on the permissibility of assisted reproduction techniques, including surrogacy, within the framework of Shia Islam. By issuing this fatwa, Ayatollah Khamene’j provided a sense of legitimacy and security for families considering or participating in surrogacy agreements. The weight of his authority as the supreme leader lent considerable credibility to the practice, reassuring both individuals and the broader society. Central to the acceptance of surrogacy in Iran was the principle of ijtihad, which emphasizes individual religious reasoning and interpretation. While, not codified as law, ijtihad holds significant sway in Shia Islam and allows a degree of flexibility and adaption to modern circumstances. In the case of surrogacy, Iranian citizens were able to exercise their right to ijtihad, engaging in personal religious reasoning that aligned with their beliefs and values. Italy According to the Law Decree no. 40 approved on February 19, 2004, by the Italian Parliament, titled "Regulations concerning medically assisted procreation", any form of selling of either gametes or embryos, as well as surrogacy, is prohibited and punished with jail time lasting between three months and two years and with a fine ranging between 600,000 and one million euros (article 12, subsection 6). This prohibition has been further underpinned by a sentence of the Italian Constitutional Court (i.e. sentence no. 272 of 2017), that stated that "the practice of surrogacy constitutes an unbearable attack on women's dignity and deeply undermines human relations". Its legitimacy was confirmed by the Italian Constitutional Court in 2019. In the same year, the partner of the biological father of a surrogated child asked for a birth certificate which had been registered outside of Italy to be legally recognised, but the joint session of the Civil Sections of the Italian Supreme Court of Cassation rejected his legal pursuit. In 2023, Prime Minister Giorgia Meloni ordered municipalities to follow a December 2021 court decision to stop certifying foreign birth certificates of children born to Italian same-sex couples through use of a surrogate in another country. On October 16th 2024, the Italian Parliament passed a law pursuant to which Italian citizens can be prosecuted if they have children through surrogacy abroad. Japan In March 2008, the Science Council of Japan proposed a ban on surrogacy and said that doctors, agents and their clients should be punished for commercial surrogacy arrangements. Other than that, it remains unregulated. Kenya There is no legal regulations/laws of surrogacy in Kenya. Kyrgyzstan In Kyrgyzstan, surrogacy is officially regulated by Article 57 of the Law "On the Protection of Citizens' Health in the Kyrgyz Republic" dated January 12, 2024, No. 14. This law establishes the primary conditions for legally conducted surrogacy. According to it, regardless of medical indications or marital status, every individual has the right to become a parent using the surrogacy method. There are no strict restrictions for prospective parents, making participation in the program maximally accessible for all aspiring to parenthood. Mexico Surrogacy in Mexico is unregulated in most states and Mexico City as they lack specific legal framework for surrogacy. There are two states that have civil codes that regulate surrogacy: the states of Sinaloa and Tabasco. The state of Sinaloa introduced a civil code pertaining to surrogacy in 2013, and it restricted surrogacy to married couples with proven infertility. The state of Tabasco introduced surrogacy in its civil code in 1997, and it amended its civil code to exclude international intended parents, LGBT individuals, and singles from accessing surrogacy. In 2021, the Supreme Court of Justice of the Nation found that the part of civil code of the state of Tabasco that restricted surrogacy to married Mexican couples violated Article 4 of the Constitution of Mexico and therefore is unconstitutional. The court ruled that surrogacy should be accessible to every individual regardless of sexuality, nationality, or marital status for the purpose of building a family, and that the intended parents who commissioned surrogacy should be deemed the legal parents of the child as procreational will should be the basis of determining the legal parentage, not gestational relationship. In regions of Mexico where surrogacy is unregulated, Amparo trials regularly take place when intended parents petition the federal court to order the local civil registry to register their baby born through surrogacy with a birth certificate showing only the name(s) of the intended parent(s), and in the majority of cases, a favorable ruling (Amparo) is issued. Malaysia Surrogacy is currently prohibited by fatwa issued by National Council of Islamic Religious Affairs in 2008. The Netherlands Altruistic surrogacy is legal in the Netherlands. Commercial surrogacy is illegal in the Netherlands. Although altruistic surrogacy is legal, there is only one hospital taking in couples and there are extremely strict rules to get in. This makes a lot of couples seek their treatment outside the Netherlands. Belgium In Belgium, surrogacy is not legally regulated, but it is often practiced, including by foreigners. Only altruistic surrogacy is allowed in the country. The future parents pay the surrogate mother only the expenses related to the pregnancy. For example, compensation for food, clothing, medical procedures, and transportation. Usually, parents find a surrogate mother through private contacts or through forums on the Internet. Since Belgium does not have any specific law regulating the process of surrogacy, in fact, a private agreement between a surrogate mother and future parents has no legal force, and there are no clear requirements for the surrogate mother's health (for example, there is no obligation to undergo medical and mental examinations before entering into the surrogacy process, no age requirements). And more, according to Belgian law, the legal mother of a child at birth is the person who gave birth to the child, even if there is no genetic relationship with the child. The transfer of parental rights to biological parents takes place in court, through adoption. The surrogate mother must voluntarily transfer parental rights to the child to the biological parents. If the surrogate mother who gave birth to the child does not want to hand over the child to the biological parents for any reason, the law will be on the side of the surrogate mother. There is no guarantee that the court will order the child's forced transfer to the biological parents. In such conditions, the surrogacy process can be complicated and unpredictable. In such countries as Poland, Italy, France, Germany, Austria, Sweden, Norway, Switzerland, surrogacy is prohibited by law. Italy has prohibited seeking surrogacy abroad, as there is no “right to a child” under international law. New Zealand Altruistic surrogacy is legal. Nigeria Surrogacy is unregulated in Nigeria, and surrogacy contracts may be enforceable in Nigerian courts on the basis of simple contracts. Gestational surrogacy is currently practiced in Nigeria by a few IVF clinics. The guidelines are as approved by the practice guidelines of the Association of Fertility and Reproductive Health (AFRH) of Nigeria. The ART regulation that is currently being considered by the Senate permits surrogacy and allows some inducement to be paid for transport and other expenses. Philippines There is no legal framework for surrogacy in the Philippines. Philippine laws on parenthood renders legal barriers on transferring the rights to the intended parents. The Philippine Society of Reproductive Medicine prohibits assisting in the practice of in vitro fertilization and surrogacy through donor sperm and egg cells among its members. Poland Surrogacy is mostly unregulated in Poland. A 2015 news report estimated there are likely dozens of surrogate mothers in Poland. According to the Family Code, the legal mother of a child is always the woman who gave birth to them. Portugal Heterosexual and lesbian couples could become parents via surrogacy in Portugal since 2016, under certain conditions. Yet, some parts of this law were considered unconstitutional in 2018 and it has since been suspended. Traditional surrogacy is illegal in Portugal except for some specific situations that give the right for a surrogate mother to be genetic (for example, if the future adoptive mother is completely barren). Before the first legal surrogacy law In 2006, a law was approved that explicitly considered any surrogacy contract to be null and void (Law 32/2006, article 8). A limited gestational surrogacy law proposed by the Left Bloc was approved in Parliament on the 13 May 2016, which went through considerable modifications during the five months that it took to get it from project to law (it was discussed in a work group with representatives from all parliamentary groups). The law received the favorable votes of the Socialist Party (minus two), Left Bloc (BE), The Greens (PEV), People Animals Nature (PAN), and 24 out of 89 members of the Social Democratic Party (PSD, including the former party leader Pedro Passos Coelho). Both CDS - People's Party (CDS) and the Communist Party (PCP) voted against the proposal. The communists justified their vote as being in line with the recommendations of the National Ethics Council. President Marcelo Rebelo de Sousa vetoed the law as soon as he received it, giving the same justification as the Communist Party. The law was once again sent to the Parliament and passed a second time after some changes, on 20 July 2016, with the favorable votes of BE, PS, PEV, PAN and 20 members of PSD. Eight members of PSD abstained, including Pedro Passos Coelho. The remaining members of PSD voted against, as did all members of CDS and PCP. Limited gestational surrogacy legalization On 22 August 2016, gestational surrogacy was legalized in Portugal (Law 25/2016) for the cases in which there is an absence of the uterus or, due to lesions or disease, the uterus is unable to successfully carry a pregnancy to term. The article 8 of the 2006 law was revoked. In such cases, the surrogate mother must willingly enter the contract and accept to renounce any motherhood rights and duties. The gestational surrogacy requires a written contract that explicitly deals with the possibility of abortion in certain cases, the Ordem dos Médicos' opinion (which does not need to be positive) and the National Council of Medically Assisted Reproduction's (CNPMA) permission. The only payment/donation that the genetic parents can make in favor of the surrogate mother is her medical expenses and the surrogate mother cannot be economically dependent on the genetic parents. No behavioral restrictions can be imposed on the surrogate mother. Single men and homosexual male couples cannot have access to gestational surrogacy. The law does not explicitly exclude single women, but requires that only one of the members of the couple has to contribute with genetic material, which may be interpreted as restricting the procedure to couples. The law does not restrict gestational surrogacy to residents, meaning that Portugal may become a fertility tourism destination. After the first legal surrogacy law Adoption of the law caused some debate within several Portuguese Christian Churches, though not among the once-dominant Roman Catholic Church. Representatives of Brazilian and US-based evangelical and Pentecostal churches condemn surrogacy and suggest that infertile couples can/must (depending on the Church) pursue conventional adoption (national or transnational even though the later is banned by law). After the 2016 law was approved, a regulatory decree (6/2017) was created on 31 July 2017 to materialize the 2016 law that included, for example, the requirement that the surrogacy mother is accompanied by a psychologist during and after the birth. In February 2017, a petition with more than 4000 signatures (the legal requirement for the Parliament to be forced to discuss the matter of the petition), was submitted to Parliament, asking for a referendum on gestational surrogacy. At the same time, a group of members of CDS and PSD requested that the law was subject to a constitutional inspection. In April 2018, the Constitutional Court deliberated on the matter and declared that some of the law's articles were in fact unconstitutional. The effects of the decision did not apply to any processes that had initiated the therapeutic procedure. This meant that a single process was allowed, while a total of eight processes that were in different stages of progress were considered null and void. In July 2019, new changes were made to the 2016 law in Parliament in an attempt to revert the Constitutional Court's decisions, but one of the issues raised by the Court (regarding the right of the surrogate mother to choose to keep the baby) failed to make it to the final version of the law. The final version had the positive votes of PS, BE (the author of the proposal), PEV, PAN and 21 PSD members, as well as five abstentions from PSD members. PCP, CDS, and 63 PSD members voted against it. In August 2019, president Marcelo Rebelo de Sousa sent the law to the Constitutional Court for review. In September 2019, the 2016 law, after the 2019 changes, was once again considered unconstitutional by the Constitutional Court. Russia Gestational surrogacy, even commercial, is legal in Russia, being available to practically all adults willing to be parents. There must be one of several medical indications for surrogacy: absence of uterus, deformity of the uterine cavity or cervix, uterine cavity synechia, somatic diseases contraindicating child bearing, or repeated failure of IVF despite high-quality embryos. The first surrogacy program in Russia was successfully implemented in 1995 at the IVF centre of the Obstetrics and Gynecology Institute in St. Petersburg. Public opinion in general is surrogacy-friendly; recent cases of a famous singer and a well-known businesswoman who openly used services of gestational surrogates received positive news coverage. Meanwhile, the Russian Orthodox Church has officially condemned surrogacy. As regards the baptism of the children born through surrogacy, the Russian Orthodox Church holds that a "child born with the assistance of “surrogate motherhood” can be Baptized according to the wishes of the party that will be raising it, if such are either its “biological parents” or its “surrogate mother,” only after they have recognized that, from the Christian point of view, such reproductive technology is morally reprehensible and have borne ecclesial repentance – regardless of whither they ignored the Church's position consciously or unconsciously". Registration of children born through surrogacy is regulated by the Family Code of Russia (art. 51–52) and the Law on Acts on Civil Status (art. 16). A surrogate's consent is needed for that. Apart from that consent, no adoption nor court decision is required. The surrogate's name is never listed on the birth certificate. There is no requirement for the child to be genetically related to at least one of the commissioning parents. Children born to heterosexual couples who are not officially married or single intended parents through gestational surrogacy are registered in accordance to analogy of jus (art. 5 of the Family Code). A court decision may be needed in that case. On 5 August 2009 a St. Petersburg court definitely resolved a dispute as to whether single women could apply for surrogacy and obliged the State Registration Authority to register a 35-year-old single intended mother, Nataliya Gorskaya, as the mother of her surrogate son. On 4 August 2010 a Moscow court ruled that a single man who applied for gestational surrogacy (using donor eggs) could be registered as the only parent of his son, becoming the first man in Russia to defend his right to become a father through court proceedings. The surrogate mother's name was not listed on the birth certificate; the father was listed as the only parent. After that, a few more identical decisions concerning single men who became fathers through surrogacy were issued by different courts in Russia, listing men as the only parents of their surrogate children and confirming that prospective single parents, regardless of their sex or sexual orientation, can exercise their right to parenthood through surrogacy in Russia. Liberal legislation makes Russia attractive for "reproductive tourists" looking for techniques not available in their countries. Intended parents go there for oocyte donation because of advanced age or marital status (single women and single men) and when surrogacy is considered. Foreigners have the same rights for assisted reproduction as Russian citizens. Within 3 days after the birth, the commissioning parents obtain a Russian birth certificate with both their names on it. Genetic relation to the child (in case of donation) does not matter. Saudi Arabia Religious authorities in Saudi Arabia do not allow the use of gestational carriers, instead suggesting medical procedures to restore fertility and ability to deliver. Serbia All surrogacy arrangements (both commercial and altruistic) are illegal. A draft of the new civil law is said to allow gestational carriers, but Serbian Assembly still did not adopt this law yet. On 21 April 2017, the Serbian Assembly started a discussion a legislation on assisted reproductive technology that bans all forms of surrogacy. (The legislation is being discussed.) South Africa The South Africa Children's Act of 2005 (which came fully into force in 2010) enabled the "commissioning parents" and the surrogate to have their surrogacy agreement validated by the High Court even before fertilization. This allows the commissioning parents to be recognized as legal parents from the outset of the process and helps prevent uncertainty – although if the surrogate mother is the genetic mother she has until 60 days after the birth of the child to change her mind. The law permits single people and gay couples to be commissioning parents. However, only those domiciled in South Africa benefit from the protection of the law, no non-validated agreements will be enforced, and agreements must be altruistic rather than commercial. If there is only one commissioning parent, s/he must be genetically related to the child. If there are two, they must both be genetically related to the child unless that is physically impossible due to infertility or sex (as in the case of a same-sex couple). The Commissioning parent or parents must be physically unable to birth a child independently. The surrogate mother must have had at least one pregnancy and viable delivery and have at least one living child. The surrogate mother has the right to unilaterally terminate the pregnancy, but she must consult with and inform the commissioning parents, and if she is terminating for a non-medical reason, may be obliged to refund any medical reimbursements she had received. South Korea As of mid-2010s, surrogacy was available and mostly unregulated in South Korea. The practice is often morally stigmatized. Surrogacy has declined since mid-2000s, as some aspects of commercial surrogacy became illegal. Spain Whereas surrogacy is not legal in Spain (the biological mother's renouncement contract is not legally valid), it is legal to perform the surrogacy in a country where it is legal, having the mother the nationality from that same country. Sweden Surrogacy is illegal in Swedish healthcare, but there are no legal regulations/laws of surrogacy for it and it remains unregulated. Switzerland Surrogacy is regulated in the "Bundesgesetz über die medizinisch unterstützte Fortpflanzung (Fortpflanzungsmedizingesetz, FMedG) vom 18. Dezember 1998" and illegal in Switzerland. Art. 4 forbids surrogacy, Art. 31 regulates the punishment of clinicians who apply in vitro fertilisation for surrogacy or persons who arrange surrogacy. The surrogate mother is not punished by law. She will be the legal mother of the child. On 24 August 2014, the Administrative Court of the Canton of St. Gallen granted parentship to two men of a child born in the USA. Thailand In response to the controversial Baby Gammy incident in 2014, Thailand since 30 July 2015, has banned foreign people travelling to Thailand, to have commercial surrogacy contract arrangement, under the Protection of Children Born from Assisted Reproductive Technologies Act. Only opposite-sex married couples as Thailand residents are allowed to have a commercial surrogacy contract arrangement. In the past Thailand was a popular destination for couples seeking surrogate mothers. Ukraine Ukraine is a major international surrogacy destination, given its very liberal laws, as well as the fact that prices are more affordable than in the United States. Since 2002, surrogacy and surrogacy in combination with egg/sperm donation has been absolutely legal in Ukraine. According to the law a donor or a surrogate mother has no parental rights over the child born and the child born is legally the child of the prospective parents. This is possible because in Ukraine a woman can give birth to a child without being considered the child's mother. In other countries a woman automatically becomes the mother of a child by giving birth, even if the surrogate mother is not genetically related to the child she carried. In Ukraine the start of the introduction of methods of supporting reproductive medicine was given in the eighties of the preceding century. It was Kharkiv where the extracorporeal fertilization method was for the first time successfully applied in Ukraine, and in 1991 a girl named Katy was born. Kharkiv was also the first city in the former Soviet Union to realize surrogacy. Many clinics dealing with surrogacy have been opened in Kyiv and Kharkiv. Ukrainian surrogacy laws are very favorable and fully support the individual's reproductive rights. Surrogacy is officially regulated by Clause 123 of the Family Code of Ukraine and the order of the Ministry of health of Ukraine "On approval of the application of assisted reproductive technologies in Ukraine" from 09.09.2013 No. 787. You can choose between Gestational Surrogacy, Egg/sperm Donation, special Embryo adoption programs and their combinations. No specific permission from any regulatory body is required for that. A written informed consent of all parties (intended parents and surrogate) participating in the surrogacy program is mandatory. The intended mother has to demonstrate that there is, in fact, a medical reason that makes impossible for her to get pregnant, either because there is a risk for her or for the baby. However, Ukraine does not support surrogacy for family models different from the heterosexual one, leaving apart single parent families, lesbian couples, etc. Ukrainian legislation allows intended parents to carry on a surrogacy program and their names will be on the birth certificate of the child born as a result of the surrogacy program from the very beginning. The child is considered to be legally "belonging" to the prospective parents from the very moment of conception. The surrogate's name is never listed on the birth certificate. The surrogate can't keep the child after the birth. Even if a donation program took place and there is no biological relation between the child and the intended mother, their names will be on the birth certificate (Clause 3 of article 123 of the Family Code of Ukraine). Embryo research is also allowed, gamete and embryo donation permitted on a commercial level. Single women can be treated by known or anonymous donor insemination. Gestational surrogacy is an option for officially married couples only (a man and a woman) if they are able to prove they cannot carry a baby themselves for medical reasons and at least one parent must have a genetic link to the newborn baby. Surrogacy in Ukraine is not regulated in law as commercial, it is more close to altruistic, as the so-called "payment" is not performed to the surrogate mother, it is called a compensation and is not under obligation to pay taxes. The ongoing war and Russian invasion of Ukraine has not stopped the surrogacy industry. From the beginning of the invasion in February 2022 to July 2023 more than 1,000 children have been born in Ukraine to surrogate mothers. Six hundred children were born at the BiotexCom clinic in Kyiv, one of Europe’s largest surrogacy clinics. Foreigners still traveled to Ukraine to collect the babies they had paid for despite the constant threat of Russian bombardments. In March 2022 Ukrainian MPs proposed prohibiting foreigners from using the services of Ukrainian surrogate mothers during martial law, but this proposal was voted down two months later. In July 2023 the BiotexCom clinic charged €40,000 for its services. The clinic stated that more than half of that sum goes to the surrogate mothers. In March 2022 Deutsche Welle reported that all-inclusive packages did cost between €30,000 ($33,000) and €40,000; and that the Ukrainian woman earned about €15,000 to €20,000 for her taking part in the surrogacy program — (that was) several times more than the average annual salary. United Kingdom Altruistic surrogacy is legal in the United Kingdom, however, commercial surrogacy arrangements are not: as they are prohibited by section 2 of the Surrogacy Arrangements Act 1985. Additionally, advertisement was made a criminal offense under section 3 of the Surrogacy Arrangement Act, however, the Human Fertilisation and Embryology Act 2008 adds an exception, allowing non-profit agency to advertise. Whilst it is illegal in the UK to pay more than reasonable expenses for a surrogacy, the relationship is recognised under section 30 of the Human Fertilisation and Embryology Act 1990. However, in the case of parental orders applied for as a result of international surrogacy arrangements, the court has retroactively authorised payment for commercial arrangements when it is in the child's best interest, see Re L (A Minor) [2010] E.W.H.C. 3146 (Fam), J v G [2013] E.W.H.C. 1432 (Fam), Re X and Y (Parental Order: Retrospective Authorisation of Payments) [2012] E.W.H.C. 3147 (Fam) and Re C (Parental Order) [2013] E.W.H.C. 2413 (Fam). ‘It will only be in the clearest case of the abuse of public policy that the court will be able to withhold an order if otherwise welfare considerations support its making.’ Regardless of contractual or financial consideration for expenses, surrogacy arrangements are not legally enforceable under section 1A of the Surrogacy Arrangements Act; therefore, a surrogate mother maintains the legal right of determination for the child, even if they are genetically unrelated. Unless a parental order or adoption order is made, the surrogate mother remains the legal mother of the child. This is because under section 33 of the Human Fertilisation and Embryology Act 2008 (HFEA 2008) ‘mother’ is the women who has, or is carrying the child. Additionally, if the surrogate is married, under section 42 HFEA 2008 their partner becomes the other legal parent, unless it can be shown they did not consent to the surrogate's insemination. Parental Orders Legal Parenthood through a parental order can be obtained under section 54 HFEA 2008, for two applicants, and section 54A HFEA 2008, for single applicants. In order to meet the criteria under section 54 HFEA 2008, an application must be made by two people in an enduring family relationship, and one of their gametes were used in the insemination, subject to section 54(1) and section 54(2). In X (A Child: Foreign Surrogacy) [2018] E.W.F.C. 15 the couple openly acknowledged they were in a “sham marriage” however, this was unproblematic under section 54: as the court still considered them to be in a lasting relationship. Under section 54(3) the application must be made no later than 6 months after the child's birth. Under section 54(4)(a) and 54(4)(b) at the time of the application the child must be living with the applicants, at least one of which is domiciled in the UK. Both applicants when applying must be over the age of 18, as required by section 54(5). The surrogate and any other individual with parental rights must give their full and informed consent to the parental order, as required under section 54(6)(a) and 54(6)(b) unless, under section 54(7) they lack the capacity to consent. Moreover, under section 54(8) the court must be satisfied the no payment has been given, other than reasonable expenses, unless authorised by the court. Under section 54A HFEA 2008, the requirements are the same as under 54, except 54A(1) allows for one applicant, provided the child was not carried by them and their gametes were used in the insemination. Best Interest of the Child In the first UK surrogacy case Re C (A Minor) (Wardship: Surrogacy) (Baby Cotton Case) [1985] F.L.R. 846. Latey J stated, ‘morals and ethics are irrelevant, what matters is what is in the child’s best interest.’ When considering if a parental order should be granted the primary consideration of the court is child welfare, pursuant to the Human Fertilisation and Embryology (Parental Orders) (Consequential, Transitional and Savings Provision) Order 2010, which imports the provisions of section 1 of the Adoption and Children Act 2002 into section 54 of the HFEA 2008. The primacy of child welfare has resulted in a more flexible application of section 54(3), to accommodate the child's best interest. In Re Re X (A Child) (Surrogacy: Time Limit) [2014] E.W.H.C. 3135 (Fam) Munby P states, excluding parental orders made even a day late is ‘…the very antithesis of sensible; it is almost nonsensical.” Munby P states this is because of the large impact on, not just the applicants, but ‘…the innocent child, whose welfare is the courts paramount concern.’ This flexible approach was followed in KB & RJ v RT (Rev 1) [2016] E.W.H.C. 760 (Fam) where a parental order was granted to a two-year-old child. United States Surrogacy and its attendant legal issues fall under state jurisdiction and the legal situation for surrogacy varies greatly from state to state. Some states have written legislation, while others have developed common law regimes for dealing with surrogacy issues. Some states facilitate surrogacy and surrogacy contracts, others simply refuse to enforce them, and some penalize commercial surrogacy. Surrogacy friendly states tend to enforce both commercial and altruistic surrogacy contracts and facilitate straightforward ways for the intended parents to be recognized as the child's legal parents. Some relatively surrogacy friendly states only offer support for married heterosexual couples. Generally, only gestational surrogacy is supported and traditional surrogacy finds little to no legal support. States generally considered to be surrogacy friendly include California, Illinois, Arkansas, Maryland, Washington D.C., Oregon, and New Hampshire among others. Both New Jersey and Washington state commercial surrogacy laws became effective from 1/1/2019. For legal purposes, key factors are where the contract is completed, where the gestational carrier resides, and where the birth takes place. Therefore, individuals living in a non-friendly state can still benefit from the policies of surrogacy friendly states by working with a surrogate who lives and will give birth in a friendly state. The variations in policy mean that employee surrogacy benefits, which an increasing number of employers offer, can only be enjoyed in certain jurisdictions. Arkansas Arkansas was one of the first states to enact surrogacy friendly laws. In 1989, under then-Governor Bill Clinton, it passed Act 647, which states that in a surrogacy arrangement, the biological father and his wife will be recognized as the child's legal parents from birth, even if his wife is not genetically related to the child (i.e., in a traditional surrogacy arrangement). If he is unmarried, he alone will be recognized as the legal parent. A woman may also be recognized as the legal mother of the surrogate's genetic child as long as that child was conceived with anonymous donor sperm. On the other hand, it is unclear how or whether same-sex couples could benefit these laws, since the 2008 ballot measure that made it illegal for unmarried, cohabiting individuals to adopt or provide foster care to minors. On 26 June 2015, the 2008 ballot issue is moot because of Obergefell v. Hodges. California California is known to be a surrogacy-friendly state. It permits commercial surrogacy, regularly enforces gestational surrogacy contracts, and makes it possible for all intended parents, regardless of marital status or sexual orientation, to establish their legal parentage prior to the birth and without adoption proceedings (pre-birth orders). Louisiana Louisiana restricts surrogacy to married, heterosexual couples who are using their own gametes. Surrogacy via egg and/or sperm donation is prohibited. No compensation of the surrogate is allowed, prohibiting commercial surrogacy statewide. All parties must have been Louisiana residents for at least six months at the time of entering the surrogacy contract. The surrogacy must be considered medically necessary, and court approval prior to embryo transfer is required. Michigan On April 1, 2024, Michigan legalized compensated and altruistic surrogacy agreements. Previously, Michigan forbade all surrogacy agreements, compensated or not, with it having been a felony to enter into such agreements, punishable by fines of up to $50,000 and up to five years in prison. New Hampshire Since 2014, New Hampshire is recognized as a surrogacy friendly state, with laws in place to protect all parties to a surrogacy arrangement. All intended parents, irrespective of marital status, sexual orientation, or a genetic connection to the child, are able to establish their legal parental rights through pre-birth orders placing their names directly on the child's initial birth certificate. Reasonable compensation to the surrogate is permitted by statute. New York As of February 15, 2021, New York law allows for compensated gestational surrogacy agreements and provides a simple path to establish legal parental rights for parents who rely on assisted reproductive technology to have children. The law, the Child-Parent Security Act, also created a first-in-the-nation "Surrogate's Bill of Rights" governing all of the surrogacy arrangements under its auspices. Oklahoma On 23 May 2019, Gov. Kevin Stitt signed into law HB2468, which legalizes and recognizes the validity of both compensated and uncompensated gestational surrogacy agreements. Under the bill, a comprehensive court procedure is created to validate all gestational agreements. The bill also allows a court to enter pre-birth orders establishing parentage prior to the birth of the child. The bill applies to gestational agreements entered into by single individuals, as well as heterosexual and homosexual couples, who wish to become parents. Rhode Island Rhode Island permits gestational surrogacy, as cited in the Uniform Parentage Act, which took effect January 1, 2021. This act allows couples, married or unmarried, or individuals, homosexual or heterosexual, to use their own gametes or those of an egg and/or sperm donor to conceive a child via surrogacy. It also allows pre-birth orders, as long as at least one intended parent is a US resident. Wisconsin There is no law prohibiting gestational surrogacy in Wisconsin. In July 2013 the Wisconsin Supreme Court ruled in the case of Rosecky v. Schissel that the surrogacy agreement between the parties was legal and enforceable, striking down an earlier Circuit Court ruling and thus setting a precedent in the state. Chief Justice Shirley Abrahamson and Justice Ann Walsh Bradley filing a strongly worded concurring opinion, stating that “A parenting agreement is a valid enforceable contract,” “unless enforcement is contrary to the best interests of the child.” Lawsuits Baby M: New Jersey 1988. The surrogate mother in a traditional surrogacy arrangement decided to keep the resulting child. The intended parents sued to have themselves recognized as the legal parents. The New Jersey Supreme Court found that the surrogacy contract was invalid as a matter of New Jersey public policy. However, the intended parents were given custody of the child because the courts thought they would provide a better home for the baby than the surrogate mother, who was instead given visitation rights. Vietnam Surrogacy for humanitarian purposes has been allowed in Vietnam from 2015 after the amended Family and Marriage Law passed with nearly 60% of votes from the National Assembly. Under the new law, surrogacy will only be allowed among married couples, who do not have any common child, after doctors confirm the wife can not give birth even with technical support. The surrogate must be a relative of either the husband or wife, and have already given birth successfully. A woman is only allowed to be a surrogate once in her life and must produce her husband's approval if she's married. The embryo must be created by the intended parents' sperm and ovum. The process must be voluntary and follow in-vitro fertilization regulations. == References == The Humanist Party (Icelandic: Húmanistaflokkurinn) is a political party in Iceland founded on 25 June 1984 as the Human Party (Icelandic: Flokkur mannsins). It adopted its current name in 1995. It has run candidates in the 1987 and 1999 parliamentary elections, but has never achieved representation. It is related to the International Humanist Party. They successfully applied for the list letter H to contest the 2013 Icelandic parliamentary election, and subsequently submitted an official candidate list on 12 April 2013. In the 2013 election, they chose to only run candidates in the Reykjavik North and Reykjavik South constituencies. History The party ran candidates in the 1987 and 1999 parliamentary elections. They also stood candidates for the Reykjavík City Council in 1986, 1990, 1998, and 2002. They never achieved representation in either body. Electoral results Parliament References External links Húmanistaflokkurinn Facebook page Clinical lycanthropy is a rare psychiatric syndrome that involves a delusion that the affected person can transform into, has transformed into, or is a non-human animal. Its name is associated with the mythical condition of lycanthropy, a supernatural affliction in which humans are said to physically shapeshift into wolves. The term is used by researchers mostly in the broader sense of transformation into animals in general, that, strictly speaking, is described as zoanthropy. Signs and symptoms Affected individuals believe that they are in the process of transforming into an animal, or have already transformed into an animal. Clinical lycanthropy has been associated with the altered states of mind that accompany psychosis, the mental state that typically involves delusions and hallucinations, with the transformation only seeming to happen in the mind and behavior of the affected person. A 1988 study on clinical lycanthropy from the McLean Hospital reported on a series of cases, and proposed some diagnostic criteria by which clinical lycanthropy could be recognised: A patient reports in a moment of lucidity or reminiscence that they sometimes feel as an animal or have felt like one. A patient behaves in a manner that resembles animal behavior, for example howling, growling, or crawling. According to these criteria, either a delusional belief in current or past transformation or behavior that suggests a person thinks of themselves as transformed is considered evidence of clinical lycanthropy. The authors note that, although the condition seems to be an expression of psychosis, there is no specific diagnosis of mental or neurological illness associated with its behavioral consequences. Clinical lycanthropy is not specific to an experience of human-to-wolf transformation. A wide variety of creatures have been reported as part of the shape-shifting experience. A 2004 review of the medical literature lists over thirty published cases of clinical lycanthropy, only the minority of which have wolf (Lycanthropy) or dog (Cynanthropy) themes. Canines are certainly not uncommon, although the experience of being transformed into other animals such as a hyena, cat, horse, bird, or tiger has been reported on more than one occasion. Transformation into frogs, and even bees, has been reported in some instances. The term ophidianthropy refers to the delusion that one has been transformed into a snake, of which two case studies have been reported. In Japan, transformation into foxes and dogs was common (ja:狐憑き, ja:犬神). A 1989 case study described how one individual reported a serial transformation, experiencing a change from human to dog, to a horse, and then to a cat, before returning to the reality of human existence after treatment. There are also reports of people who experienced transformation into an animal only listed as "unspecified." Proposed mechanisms Clinical lycanthropy is a very rare condition and is largely considered to be an idiosyncratic expression of a psychotic or dissociative episode caused by another condition such as dissociative identity disorder, schizophrenia, bipolar disorder, or clinical depression. It has also been associated with drug intoxication and withdrawal, cerebrovascular disease, traumatic brain injury, dementia, delirium, and seizures. However, there are suggestions that certain neurological conditions and cultural influences may result in the expression of the human-animal transformation theme that defines the condition. Neurological factors One important factor may be differences or changes in parts of the brain known to be involved in representing body shape (e.g., see proprioception and body image). A neuroimaging study of two people diagnosed with clinical lycanthropy showed that these areas display unusual activation, suggesting that when people report their bodies are changing shape, they may be genuinely perceiving those feelings. Treatment Because clinical lycanthropy is strongly associated with psychotic disorders, antipsychotic medication is often an effective treatment. It may also be treated with antidepressants or mood stabilizers, in cases in which it is a symptom of depression or bipolar disorder. Patients with clinical lycanthropy may also benefit from a cultural approach to treatment, as the syndrome is generally agreed to be culture-bound. Related disorders Clinical lycanthropy is a type of delusional misidentification syndrome of the self, and it often overlaps with other delusional misidentification syndromes. For example, there is a case study of a psychiatric patient who had both clinical lycanthropy and Cotard delusion. In rare cases, individuals may believe that other people have transformed into animals. This has been termed "lycanthropic intermetamorphosis" and "lycanthropy spectrum." A 2009 study reported that, after the consumption of the drug MDMA (Ecstasy), a man displayed symptoms of paranoid psychosis by claiming that his relatives had changed into various animals such as a boar, a donkey, and a horse. History Catherine Clark Kroeger has written that several parts of the Bible refer to King Nebuchadnezzar's behavior in the book of Daniel 4 as being a manifestation of clinical lycanthropy. Neurologist Andrew J. Larner has written that the fate of Odysseus's crew due to the magic of Circe may be one of the earliest examples of clinical lycanthropy. It is believed that the Armenian king Tiridates III also had this disorder. He was cured by Gregory the Illuminator. According to Persian tradition, the Buyid prince Majd al-Dawla was experiencing an illusion that he was a cow. He was cured by Ibn Sina. Notions that lycanthropy was due to a medical condition go back to the fourth century, when the Alexandrian physician Oribasius attributed lycanthropy to melancholia or an "excess of black bile". In 1563, a Lutheran physician named Johann Weyer wrote that werewolves had an imbalance in their melancholic humour and exhibited the physical symptoms of paleness, "a dry tongue and a great thirst" as well as sunken, dim and dry eyes. Even King James VI and I in his 1597 treatise Daemonologie does not blame werewolf behaviour on delusions created by the Devil but "an excess of melancholy as the culprit which causes some men to believe that they are wolves and to 'counterfeit' the actions of these animals". The perception of an association between mental illness and animalistic behaviour can be traced throughout the history of folklore from many different countries. Case examples On August 15, 2016, Martin County Florida Sheriff's Office deputies found a 19-year-old male on top of a bloodied 59-year-old man, gnawing on his face, eating pieces of flesh, and making growling sounds. Officers tased, repeatedly kicked, and ultimately required a police dog's assistance in subduing the 19-year-old. Inside the garage of the home on Southeast Kokomo Lane, just north of the Palm Beach County line, deputies found a 53-year-old woman, beaten, bloodied and unresponsive. Ultimately both the 59-year-old man and 53-year-old woman would die from their injuries. In the weeks ahead of this incident, the 19-year-old told family members he believed he was either half-man, half-horse or half-man, half-dog. Clinical psychologist Dr. Phillip Resnick later assessed the 19-year-old as having clinical lycanthropy. A 20-year-old man was admitted to a mental hospital due to his increasingly agitated and erratic behaviors. During his initial evaluation, he was guarded and preoccupied. He had no previous psychiatric history. Over the next few days, he displayed increasingly psychotic, animal-like behaviors. These behaviors included howling loudly, running abruptly, and crawling on all fours. He appeared to be internally stimulated. When asked about these behaviors, he was initially evasive but eventually admitted that he believed he was a werewolf and would periodically transform into a wolf. He started believing this after having visions of "the Devil" years before and reported hearing random voices. The patient was started on ziprasidone and his symptoms gradually responded and his animal-like behaviors eventually ceased altogether. A 25-year-old man was sent for treatment during a period of excessive hand-washing, irritable behavior, decreased sleep, and acting like a buffalo. The patient reported that he had engaged in sexual activity with his buffalo and believed that buffalo cells had entered his body and were transforming him into a buffalo. He began obsessively washing his hands and genitals in order to avoid the transition. He saw himself as having buffalo body parts and became preoccupied about his appearance. He then began to act as a buffalo by nodding his head, crawling on all fours, and seeking out hay and grass to eat. He was ultimately diagnosed with obsessive-compulsive disorder and body dysmorphic disorder with delusional beliefs. He was treated with fluoxetine and risperidone, and after 6 months of pharmacotherapy, his body dysmorphia and hand-washing were both reduced. See also References Citations Works cited Further reading Further reading External links "Real-Life Werewolves: Psychiatry Re-Examines Rare Delusion" Nazareth University (also known as Naz) is a private university in Pittsford, New York. It offers over 60 undergraduate majors and more than two dozen graduate programs. The college was previously Nazareth College of Rochester, or Nazareth College. History At the request of Thomas Francis Hickey, Bishop of Rochester, five Sisters of St. Joseph founded Nazareth College of Rochester in 1924 to provide undergraduate education to young women. The first class was composed of 25 young women who began their studies in a large mansion on Lake Avenue in Rochester, New York. The original mansion that housed the college was known as "the Glass House." At that time, the college offered Bachelor of Arts and Bachelor of Science degrees, each with a liberal arts core. In response to increasing enrollment, the college moved to a larger facility in 1928 at 402 Augustine Street. On June 1, 2023 Nazareth's status changed from a college to a university after the State changed the guidelines for being considered one. Campus The campus has 24 buildings, including 11 residence halls, a 2,200-seat stadium, and all-weather track, located on 150 acres (0.61 km2). The Golisano Academic Center was built in 1927. It is the oldest and largest building on campus, once serving as the "motherhouse" for the Sisters of St. Joseph who founded Nazareth. It was purchased from the Sisters of St. Joseph in 2003 and is now used for academic and administrative purposes. Features of the Center include the Linehan Chapel and Sorelle's gathering space. The Nazareth University Arts Center, which houses the departments of theatre arts, music, and art and design, opened in 1967. The Arts Center was renovated in September 2009 to become a dance and performance venue. The Golisano Training Center (opened in fall 2019), is a multi-use facility supporting varsity athletics, Special Olympics events, fitness, and a wide range of campus activities. The campus was listed as the Nazareth College census-designated place in 2020, with a population of 1,182. Gallery Academics Nazareth University is organized into four schools: College of Liberal Arts, Sciences, Business, and Education, which includes: School of Education School of Business and Leadership College of Interprofessional Health and Human Services College of Visual and Performing Arts and Design, which includes: School of Music Nazareth offers more than 60 bachelor's degree programs, more than 20 master's degree programs, a Doctorate of Physical Therapy, and three post-baccalaureate certificate programs. As of 2017, 88% of the students came from within New York State. Athletics Nazareth's men's and women's athletic teams are members of the National Collegiate Athletic Association's (NCAA) Division III with the exception of the men's Rugby team playing at the Division I level. The Golden Flyers are a member of the Empire 8 Athletic Conference (Empire 8). For men's volleyball, Nazareth is a member of the single-sport United Volleyball Conference and for men's ice hockey, a member of the United Collegiate Hockey Conference. Athletic facilities at Nazareth include the Robert A. Kidera Gymnasium (1,200), Golden Flyer Stadium (2,200), and the Golisano Training Center, a full size indoor field house with an indoor track, tennis courts, and turf field. The Nazareth men's lacrosse team is a three-time NCAA Division III National Champion (1992, 1996, and 1997). The team has also appeared in the NCAA postseason tournament nineteen times. In 2011, the Nazareth men's indoor volleyball team achieved a #1 national ranking and won the Molten Division III National Championship, while in 2013 they finished runner-up in the NCAA Division III championship to Springfield. Nazareth University's traditional rival is St. John Fisher University, just a mile north. The annual men's basketball game between the schools is known as "The Battle of the Beaks." Notable people Alumni Steve Anderson, director Catherine Cool Rumsey, former member of the Rhode Island Senate Malik Evans, mayor of Rochester, New York Gail Haines, former member of the Michigan House of Representatives Jim Jabir, basketball coach Rich Kilgour, former lacrosse player Judith A. McMorrow, professor Shannon B. Olsson, scientist Karen Osborne, author Michael Park, actor Herbie J Pilato, actor Neal Powless, lacrosse player and coach Manuel Rivera-Ortiz, photographer Jeff Van Gundy, basketball coach John Viavattine, musician angel Kyodo williams, writer Faculty John Beilein, basketball coach Craig Dahl, hockey coach Jared DeMichiel, hockey coach Ron Friedman, professor Richard Hirsch, professor Tom Parrotta, basketball coach Scott Perkins, professor George Roll, hockey coach Steve Scully, professor Robert Silverman, professor David Tacey, professor Andrea Talentino, former vice president of Academic Affairs Octavio Vázquez, professor Max Wickert, professor References External links Official website Athletics website Cultural policy is the government actions, laws and programs that regulate, protect, encourage and financially (or otherwise) support activities related to the arts and creative sectors, such as painting, sculpture, music, dance, literature, and filmmaking, among others and culture, which may involve activities related to language, heritage and diversity. The idea of cultural policy was developed at UNESCO in the 1960s. Generally, this involves governments setting in place processes, legal classifications, regulations, legislation and institutions (e.g., galleries, museums, libraries, opera houses, etc.) which promote and facilitate cultural diversity and creative expressions in a range of art forms and creative activities. Cultural policies vary from one country to another, but generally they aim to improve the accessibility of arts and creative activities to citizens and promote the artistic, musical, ethnic, sociolinguistic, literary and other expressions of all people in a country. In some countries, especially since the 1970s, there is an emphasis on supporting the culture of Indigenous peoples and marginalized communities and ensuring that cultural industries (e.g., filmmaking or TV production) are representative of a country's diverse cultural heritage and ethnic and linguistic demographics. Cultural policy can be done at a nation-state level, at a sub-national level (e.g., U.S. states or Canadian provinces), at a regional level or at a municipal level (e.g., a city government creating a museum or arts centre). Examples of cultural policy-making at the nation-state level could include anything from funding music education or theatre programs at little to no cost, to hosting corporate-sponsored art exhibitions in a government museum, to establishing legal codes (such as the U.S. Internal Revenue Service's 501(c)(3) tax designation for not-for-profit enterprises) and creating political institutions (such as the various ministries of culture and departments of culture and the National Endowment for the Humanities and the National Endowment for the Arts in the United States), arts granting councils, and cultural institutions such as galleries and museums. Similar significant organisations in the United Kingdom include the Department for Culture, Media and Sport (DCMS), and Arts Council England. Throughout much of the twentieth century, many of the activities that compose cultural policy in the 2010s were governed under the title of "arts policy". Arts policy includes direct funding to artists, creators and art institutions and indirect funding to artists and arts institutions through the tax system (e.g., by making donations to arts charities tax-deductible). However, as Kevin Mulcahy has observed, "cultural policy encompasses a much broader array of activities than were addressed under arts policy. Whereas arts policy was effectively limited to addressing aesthetic concerns (e.g., funding art galleries and opera houses), the significance of the transformation to cultural policy can be observed in its demonstrable emphases on cultural identity, valorization of indigineity [Indigenous people's culture] and analyses of historical dynamics (such as hegemony and colonialism)." A general trend in Western industrialized nations is a shift, since the 1970s and 1980s, away from solely supporting a small number of relatively elite, professionalized art forms and institutions (e.g., Classical music, painting, sculpture, art galleries) to also supporting amateur and community cultural and creative activities (e.g., community theatre) and cultural forms which were not considered part of the Western canon by previous generations (e.g., traditional music such as blues, World music, and so on). History Prior to the twentieth century, the arts were typically supported by the patronage of the church, aristocrats such as kings and queens, and wealthy merchants. During the nineteenth century, artists increased their use of the private marketplace to earn revenue. For example, the composer Beethoven put on public concerts in the 19th century for which admission was charged. During the twentieth century, governments began to take over some of the arts patronage roles. Governments' first efforts to support culture were typically the establishment of archives, museums and libraries. Over the twentieth century, governments established a range of other institutions, such as arts councils and departments of culture. The first departments of culture typically supported the major arts that are part of the Western canon, such as painting and sculpture, and the major performing arts (Classical music and theatre). Arts policy In the twentieth century, Western governments in the U.K., Canada, Australia, New Zealand and many European nations developed arts policy measures to promote, support and protect the arts, artists and arts institutions. These governments' arts policy initiatives generally had two aims: supporting excellence in the arts and broadening access to the arts by citizens. An example of an arts policy initiative that supports excellence would be a government grant program which provides funding to the highest-achieving artists in the country. A concrete example would be a literary prize of $100,000 for the best fiction authors from the country, as selected by a panel of top experts. An example of an arts policy initiative that aims at increasing access to the arts would be a music in the schools program funded by the government. A concrete example would be a program which funded an orchestra or jazz quartet and paid them to play free concerts in elementary schools. This would enable children from lower- and middle-income families to hear live music. The two goals, supporting excellence and broadening access, are often trade-offs, as any increase in emphasis on one policy objective typically has an adverse effect on the other goal. To give an example, if a hypothetical country has a $12 million per year grant program for orchestras in the country, if the government focuses on the goal of supporting musical excellence, it may decide to provide $4 million per year to the three top orchestras in the country, as determined by a panel of independent professional music critics, conductors and music professors. This decision would strongly support the goal of enhancing excellence, as funding would only go to the top musical groups. However, this approach would only enable citizens in three cities to have access to professional orchestras. On the other hand, if the government was focusing on broadening access to symphony concerts, it might direct the independent panel to pick 12 orchestras in the country, with the stipulation that only one orchestra per city be selected. By proving $1 million per year to 12 orchestras in 12 cities, this would enable citizens from 12 cities in the country to see live orchestra shows. However, by funding 12 orchestras, this would mean that funding would go to ensembles that do not meet the highest standards of excellence. Thus, excellence and broadening access are often trade-offs. Theoretical approaches Cultural policy, while a small part of the budgets of even the most generous of governments, governs a sector of immense complexity. It entails "a large, heterogeneous set of individuals and organizations engaged in the creation, production, presentation, distribution, and preservation of and education about aesthetic heritage, and entertainment activities, products and artifacts". A cultural policy necessarily encompasses a broad array of activities and typically involves public support for: Heritage, battlefield and historic preservation sites Zoos, botanical gardens, arboretums, aquariums, parks Libraries and museums (fine arts, scientific, historical) Visual arts (film, painting, sculpture, pottery, architecture) Performing arts (symphonic, chamber and choral music; jazz, hip-hop and folk music; ballet, ballroom and modern dance; opera and musical theatre; circus performances, rodeos and marching bands) Public humanities programs (public broadcasting, creative writing, poetry) Some governments may place policy areas from this list in other ministries or departments. For example, national parks may be assigned to an environment department, or public humanities may be delegated to an education department. Since culture is a public good (i.e., contributes a public value to society for which it is hard to exclude non-payers, as all of society benefits from arts and culture) and something that is generally viewed as a merit good, governments have pursued programs to promote greater accessibility. In this way of thinking, significant aesthetic works such as paintings and sculptures should be made broadly available to the public. In other words, "high culture" should not be the exclusive preserve of a particular social class or of a metropolitan location. Rather, the benefits of the highest reaches of cultural excellence should be made in an egalitarian manner; national cultural treasures should be accessible without regard to the impediments of class circumstances, educational attainment or place of habitation. A democratic state cannot be seen as simply indulging the aesthetic preferences of a few, however enlightened, or of overtly infusing art with political values. Consequently, a democratic cultural policy must articulate its purposes in ways that demonstrate how the public interest is being served. These purposes have often been expressed as involving either the creation of cultural democracy or the democratization of culture. The objective of cultural democratization is the aesthetic enlightenment, enhanced dignity, and educational development of the general citizenry. "Dissemination was the key concept with the aim of establishing equal opportunity for all citizens to participate in publicly organized and financed cultural activities". To further this goal, performances and exhibitions are low cost; public art education promotes equality of aesthetic opportunity; national institutions tour and perform in work places, retirement homes and housing complexes. As indicated earlier, the "democratization of culture" is a top-down approach that promulgates certain forms of cultural programming that are deemed to be a public good. Clearly, such an objective is open to criticism for what is termed cultural elitism; that is, the assumption that some aesthetic expressions are inherently superior - at least as determined by a cognoscenti concerned with the acquisition of cultural capital. "The problem with this policy [is] that, fundamentally, it intend[s] to create larger audiences for performances whose content [is] based on the experience of society's privileged groups. In sum, it has... taken for granted that the cultural needs of all society's members [are] alike". The objective of cultural democracy, on the other hand, is to provide for a more participatory (or populist) approach in the definition and provision of cultural opportunities. The coupling of the concept of democratization of culture to cultural democracy has a pragmatic as well as a philosophical component. Cultural patronage in democratic governments is markedly different from patronage by wealthy individuals or corporations. Private or politically paramount patrons are responsible only to themselves and are free to indulge in their tastes and preferences. Democratic governments, on the other hand, are responsible to the electorate and are held accountable for their policy decisions. The two objectives just discussed - dissemination of high culture and participation in a broader range of cultural activities - evoke a related debate about the content of public culture: "elitist" or "populist." Elitism Proponents of the elitist position argue that cultural policy should emphasize aesthetic quality as the determining criterion for public subvention. This view is typically supported by the major cultural organizations, creative artists in the traditionally defined field of the fine arts, cultural critics, and the well-educated, well-to-do audiences for these art forms. Ronald Dworkin terms this the "lofty approach," which "insists that art and culture must reach a certain degree of sophistication, richness, and excellence in order for human nature to flourish, and that the state must provide this excellence if the people will not or cannot provide it for themselves". Advocates of the elitist position generally focus on supporting the creation, preservation and performance of works of the Western canon, a group of artworks that are viewed as the best artistic and cultural products of Western society. Populism By contrast, the populist position advocates defining culture broadly and inclusively and making this culture broadly available. The populist approach emphasizes a less traditional and more pluralist notion of artistic merit and consciously seeks to create a policy of cultural diversity. With a focus on personal enhancement, the populist's position posits very limited boundaries between amateur and professional arts activities. Indeed, the goal is to provide opportunities for those outside the professional mainstream. To give an example, whereas an elite approach advocates support for professional musicians, particularly those from Classical music, a populist approach would advocate support for amateur, community singers and musicians. "Proponents of populism are frequently advocates of minority arts, folk arts, ethnic arts, or counter-cultural activities" as Kevin V. Mulcahy said. Cultural "elitists," on the other hand, argue in support of excellence over amateurism and favor an emphasis on aesthetic discipline over "culture as everything." There are "two key tensions for national cultural policy between the goals of excellence versus access, and between government roles as facilitator versus architect". Kevin V. Mulcahy argued that in effect, elitism is cultural democracy as populism is to the democratization of culture. Unfortunately, there has been a tendency to see these positions as mutually exclusive, rather than complementary. "Elitists" are denounced as "high brow snobs" advocating an esoteric culture which focuses on art music and the types of art seen in museums and galleries; populists are dismissed as "pandering philistines" promoting a trivialized and commercialized culture, as they endorse the value of popular music and folk art. However, these mutual stereotypes belie complementariness between two bookends of an artistically autonomous and politically accountable cultural policy. There is a synthesis that can be termed a "latitudinarian approach" to public culture; that is, one that is aesthetically inclusive and broadly accessible. Glocalization of arts Musicologists David Hebert and Mikolaj Rykowski write that when "music is recognized as invaluable cultural heritage, entailing unique artefacts of intellectual property, new developments in this field then become acknowledged as important forms of social innovation;" However, they caution policy-makers that with glocalization, the rise of "'big data' offers unprecedentedly powerful tools but also inevitably entails many risks for all kinds of artists (both musicians and their collaborators in other arts) as well as the sustainability of traditional cultural practices." Viewpoints Such a public-cultural policy would remain faithful to the highest standards of excellence from a broad range of aesthetic expressions while providing the widest possible access to people from different geographic locales, socio-economic strata, and educational background, as Dr. Mulcahy said. In conceiving of public policy as an opportunity to provide alternatives not readily available in the marketplace, public cultural agencies would be better positioned to complement the efforts of the private sector rather than duplicate their activities. Similarly, cultural agencies can promote community development by supporting artistic heritages that are at a competitive disadvantage in a cultural world that is increasingly profit-driven. In sum, excellence should be viewed as the achievements of greatness from a horizontal, rather than a vertical, perspective and a cultural policy as supporting the totality of these varieties of excellence. These attitudes about a public cultural responsibility stand in marked contrast to much of the rest of the world, where culture is a question of historic patrimony, or the national identities of peoples, whether in independent states or regions within more powerful states. Inevitably, sensitive issues are involved in any discussion of culture as a public policy. However, given the demands in a democratic system that public policies show a return to the taxpayer, cultural policy has frequently argued for support on the basis of utility. It can be argued that there is a parity between the state's responsibility for its citi' social-economic-physical needs and their access to culture and opportunities for artistic self-expression. However, the aesthetic dimension of public policy has never been widely perceived as intuitively obvious or politically imperative. Accordingly, the cultural sector has often argued its case from the secondary, ancillary benefits that result from public support for programs that are seemingly only aesthetic in nature. Cultural policy is not typically justified solely on the grounds that it is a good-in-itself, but rather that it yields other good results. The future of cultural policy would seem to predict an increasingly inexorable demand that the arts "carry their own weight" rather than rely on a public subsidy to pursue "art for art's sake". Kevin V. Mulcahy dubbed this "cultural Darwinism" is most pronounced in the United States where public subsidy is limited and publicly supported aesthetic activities are expected to demonstrate a direct public benefit. Non-American cultural institutions are less constrained by the need to maintain diversified revenue streams that demand high levels of earned income and individual and corporate donations to compensate for limited government appropriations. On the other hand, cultural institutions everywhere are increasingly market-driven in their need for supplementary funds and as a justification for continued public support. The American model of an essentially privatized culture is increasingly attractive to governments seeking to curtail their cultural subsidies. In a system of mixed funding, public culture can nurture the arts groups and cultural activities that contribute to individual self-worth and community definition even if counting for less in the economic bottom-line. At root, a cultural policy is about creating public spheres that are not dependent upon profit motives nor validated by commercial values. As political democracy is dependent upon the existence of civil society and socio-economic pluralism, cultural policy stands as an essential public commitment in realizing these fundamental preconditions. One of the available and yet underappreciated tools in cultural policy at the national level is the reduction of VAT rates for cultural goods and services. Economic theory can be used to explain how reduced fiscal rates are expected to decrease prices and increase quantities of consumed cultural goods and services. Fiscal policy can be an important part of cultural policy, in particular the VAT rate discounts on cultural consumption, yet it receives less attention than deserved. Scope At the international level UNESCO is in charge of cultural policy. Contact information for ministries of culture and national arts councils in 160 countries is available from the website of the International Federation of Arts Councils and Culture Agencies (IFACCA). On a local scale, subnational (e.g., state or provincial governments), city and local governments offer citizens and local authorities the opportunity to develop arts and culture with the Agenda 21 for Culture. Research Cultural Policy Research (or Cultural Policy Studies) is a field of academic inquiry that grew out of Cultural Studies in the 1990s. A quarter of a century later, by now both “Cultural Policy Research” and "Cultural Policy Studies" each match almost 100 million entries in the World Wide Web. Cultural Policy Research grew out of the idea that cultural studies should not only be critical, but also try to be useful. The Princeton University e.g. founded its Center for Arts and Cultural Policy Studies 1994 “to improve the clarity, accuracy and sophistication of discourse about the nation's artistic and cultural life.” The scientific approach is genuinely interdisciplinary, combining social sciences, a wide range of the humanities, jurisprudence and economics. As all political sciences do, the research focuses on the content dimension (policy), the formal-institutional dimension (polity) and the practical dimension (politics), particularly affecting decision processes and the results obtained. Cultural Policy Research asks: What do the actors and agents in the Cultural Policy sphere actually do when they do what they do? Which purposes they do pursue by that? What are their goals and which means do they use? What is the result of their action for society and for the citizens’ intellectual and artistic freedom? Among the many departments of Cultural Policy Studies around the world, there are several UNESCO Chairs in Cultural Policy from the programme launched in 1992 by the UNESCO to promote international inter-university cooperation: (412) Vilnius, Lithuania :UNESCO Chair in Cultural Policy and Cultural Management (1998), Vilnius Academy of Arts (436) Debrecen, Hungary: UNESCO Chair in Cultural Policy and Cultural Management (1999), Lajos Kossuth University of Arts and Sciences, Debrecen (454) Lomé, Togo: Chaire UNESCO sur les politiques culturelles pour le développement (1999), Centre régional d'action culturelle (CRAC) (527) Girona, Spain / Catalunya: Chaire UNESCO en matière de Politiques et de Coopération Culturelles (2001), Universitat de Girona (546) Barcelona, Spain / Catalunya: Chaire UNESCO d'études interculturelles (2001), Université Pompeu Febra de Barcelone (572) Barcelona, Spain / Catalunya: Chaire UNESCO de Diversité linguistique et culturelle (2002), Institut d'Etudes catalanes (654) Thessaloniki, Greece: Chaire UNESCO de politique interculturelle pour une citoyenneté active et solidaire (2004), Université de Macédoine (827) Kazan, Russia-Tatarstan: UNESCO Chair in Eurosian studies, Cultural Diversity and Cultural Policies (2008), Kazan State University (851) Buenos Aires, Argentine: Chaire UNESCO d’esthétique et sociologie de la différence et de la diversité culturelle en Argentine (2009), Universidad Nacional Tres de Febrero (978) Hildesheim, Germany: UNESCO Chair in Cultural Policy for the Arts in Development ”(2012), Department of Cultural Policy at University of Hildesheim. See also Arts council Cultural Institutions Studies Walden Two#Cultural engineering Cultural subsidy Cultural diplomacy Cultural policies of the European Union Cultural policy in Abu Dhabi Cultural policy of the United States Social policy References Bibliography Madden, C, 2009, 'The Independence of Government Arts Funding: A Review', D'Art Topics in Arts Policy, No. 9, International Federation of Arts Councils and Culture Agencies, Sydney, www.ifacca.org/themes Marcello Sorce Keller, "Why is Music so Ideological, Why Do Totalitarian States Take It So Seriously: A Personal View from History, and the Social Sciences", Journal of Musicological Research, XXVI(2007), no. 2–3, pp. 91–122; Marja Heimonen & David G. Hebert, "Pluralism and Minority Rights in Music Education: Implications of the Legal and Social Philosophical Dimensions," Visions of Research in Music Education, Vol.15 (2010). Mario d'Angelo, Paul Vesperini, Cultural Policies in Europe (a series in four volumes) : 1) A comparative Approach 2) Regions and Decentralization 3) Method and Practice of Evaluation 4) Local Issues, Council of Europe Publishing, Strasbourg, 1999–2001. Philippe Poirrier (Ed.), Pour une histoire des politiques culturelles dans le monde, 1945-2011, La Documentation française, Paris, 2011. Dave O'Brien, Cultural Policy: Management, Value and Modernity in the Creative Industries, Routledge, Abingdon, 2014. Tony Bennett, Culture, A reformer's Science, SAGE, London, 1998. Jim McGuigan, Rethinking Cultural Policy, Open University Press, Milton Keynes, 2004. External links International Federation of Arts Councils and Culture Agencies United Nations Educational, Scientific and Cultural Organization Les Rencontres - Association of European Cities and Regions for Culture Compendium - Cultural Policies and Trends in Europe Trompenaars's model of national culture differences is a framework for cross-cultural communication applied to general business and management, developed by Fons Trompenaars and Charles Hampden-Turner. This involved a large-scale survey of 8,841 managers and organization employees from 43 countries. This model of national culture differences has seven dimensions. There are five orientations covering the ways in which human beings deal with each other, one which deals with time, and one which deals with the environment. The first five of Trompenaars’ dimensions are Talcott Parsons's pattern variables. The other two of Trompenaars’ dimensions are taken from Kluckholn and Strodtbeck's dimensions of culture. Universalism vs particularism Universalism is the belief that ideas and practices can be applied everywhere without modification, while particularism is the belief that circumstances dictate how ideas and practices should be applied. It asks the question, What is more important, rules or relationships? Cultures with high universalism see one reality and focus on formal rules. Business meetings are characterized by rational, professional arguments with a "get down to business" attitude. Trompenaars research found there was high universalism in countries like the United States, Canada, UK, Australia, Germany, and Sweden. Cultures with high particularism see reality as more subjective and place a greater emphasis on relationships. It is important to get to know the people one is doing business with during meetings in a particularist environment. Someone from a universalist culture would be wise not to dismiss personal meanderings as irrelevancies or mere small talk during such business meetings. Countries that have high particularism include Venezuela, Indonesia, China, South Korea, and the former Soviet Union. Individualism vs communitarianism Individualism refers to people regarding themselves as individuals, while communitarianism refers to people regarding themselves as part of a group. Trompenaars research yielded some interesting results and suggested that cultures may change more quickly than many people realize. It may not be surprising to see a country like the United States with high individualism, but Mexico and the former communist countries of Czechoslovakia and the Soviet Union were also found to be individualistic in Trompenaars research. In Mexico, the shift from a previously communitarian culture could be explained with its membership in NAFTA and involvement in the global economy. This contrasts with Hofstede's earlier research, which found these countries to be collectivist, and shows the dynamic and complex nature of culture. Countries with high communitarianism include Germany, China, France, Japan, and Singapore. Neutral vs emotional A neutral culture is a culture in which emotions are held in check whereas an emotional culture is a culture in which emotions are expressed openly and naturally. Neutral cultures that come rapidly to mind are those of the Japanese and British. Some examples of high emotional cultures are the Netherlands, Mexico, Italy, Israel and Spain. In emotional cultures, people often smile, talk loudly when excited, and greet each other with enthusiasm. So, when people from neutral culture are doing business in an emotional culture they should be ready for a potentially animated and boisterous meeting and should try to respond warmly. As for those from an emotional culture doing business in a neutral culture, they should not be put off by a lack of emotion. Specific vs diffuse A specific culture is one in which individuals have a large public space they readily share with others and small private space they guard closely and share with only close friends and associates. A diffuse culture is one in which public space and private space are similar in size and individuals guard their public space carefully, because entry into public space affords entry into private space as well. It looks at how separate a culture keeps their personal and public lives. Fred Luthans and Jonathan Doh give the following example which explains this: An example of these specific and diffuse cultural dimensions is provided by the United States and Germany. A U.S. professor, such as Robert Smith, PhD, generally would be called “Doctor Smith” by students when at his U.S. university. When shopping, however, he might be referred to by the store clerk as “Bob,” and he might even ask the clerk’s advice regarding some of his intended purchases. When golfing, Bob might just be one of the guys, even to a golf partner who happens to be a graduate student in his department. The reason for these changes in status is that, with the specific U.S. cultural values, people have large public spaces and often conduct themselves differently depending on their public role. At the same time, however, Bob has private space that is off-limits to the students who must call him “Doctor Smith” in class. In high-diffuse cultures, on the other hand, a person’s public life and private life often are similar. Therefore, in Germany, Herr Professor Doktor Schmidt would be referred to that way at the university, local market, and bowling alley—and even his wife might address him formally in public. A great deal of formality is maintained, often giving the impression that Germans are stuffy or aloof. Achievement vs ascription In an achievement culture, people are accorded status based on how well they perform their functions. In an ascription culture, status is based on who or what a person is. Does one have to prove themself to receive status or is it given to them? Achievement cultures include the US, Austria, Israel, Switzerland and the UK. Some ascription cultures are Venezuela, Indonesia, and China. When people from an achievement culture do business in an ascription culture it is important to have older, senior members with formal titles and respect should be shown to their counterparts. However, for an ascription culture doing business in an achievement culture, it is important to bring knowledgeable members who can prove to be proficient to other group, and respect should be shown for the knowledge and information of their counterparts. Sequential vs synchronic A sequential time culture is the one in which the people like events to happen in a chronological order. Punctuality is very much appreciated and they base their lives in schedules, plannification and specific and clear deadlines; in this kind of cultures time is very important and they do not tolerate the waste of time. Instead in synchronic cultures, they see specific time periods as interwoven periods, the use to highlight the importance of punctuality and deadlines if these are key to meeting objectives and they often work in several things at a time, they are also more flexible with the distribution of time and commitments. Internal vs external control Do we control our environment or are we controlled by it? In inner directed culture, people believe in controlling outcomes and have a dominant attitude toward environment. In outer-directed culture, people believe in letting things take their own course and have a more flexible attitude, characterized by a willingness to compromise and maintain harmony with nature. See also Cross-cultural communication Fons Trompenaars Hofstede's cultural dimensions theory References External links THT Consulting Riding the Waves of Culture by Fons Trompenaars and Charles Hampden-Turner (book) Trompenaars' and Hampden-Turner's cultural factors Riding The waves of commerce: a test of Trompenaars' "model" of national culture differences Fons Trompenaars' Cultural Dimensions THE TROMPENAARS’ SEVEN-DIMENSION CULTURAL MODEL Culture Compass THT IAP Certification Presentation Day1 6Dec2010 Leverage Differences for Higher Performance on YouTube Cross-Cultural Management on YouTube Business with Bob: Culture and small talk on YouTube The Chinese zodiac is a traditional classification scheme based on the Chinese calendar that assigns an animal and its reputed attributes to each year in a repeating twelve-year (or duodenary) cycle. The zodiac is very important in traditional Chinese culture and exists as a reflection of Chinese philosophy and culture. Chinese folkways held that one's personality is related to the attributes of their zodiac animal. Originating from China, the zodiac and its variations remain popular in many East Asian and Southeast Asian countries, such as Japan, South Korea, Vietnam, Singapore, Nepal, Bhutan, Cambodia, and Thailand. Identifying this scheme as a "zodiac" reflects superficial similarities to the Western zodiac: both divide time cycles into twelve parts, label the majority of those parts with animals, and are used to ascribe a person's personality or events in their life to the person's particular relationship to the cycle. The 12 Chinese zodiac animals in a cycle are not only used to represent years in China but are also believed to influence people's personalities, careers, compatibility, marriages, and fortunes. For the starting date of a zodiac year, there are two schools of thought in Chinese astrology: Chinese New Year or the start of spring. History There are theories that suggest the twelve animals were chosen for their symbolic traits, based on their revered status in traditional Chinese culture. The selection process varied regionally before being standardized in the Han Dynasty (Cao, 2008). This standardization connected these animals into a cyclical timekeeping system, which is seen as a way to reflect personality traits and the broader society (Zhou, 2017). The Chinese zodiac, as an essential part of Chinese culture, started to take shape during the Han Dynasty. This era formalizes a twelve-year cycle, where each year is associated with a specific animal, as part of a timekeeping system. This system, known as the zodiac cycle, combined the twelve Earthly Branches (地支) with the ten Heavenly Stems (天干) to create a total of a 60-year cycle. Each Earthly Branch was linked to an animal, and to the twelve zodiac signs: Rat, Ox, Tiger, Rabbit, Dragon, Snake, Horse, Goat, Monkey, Rooster, Dog, and Pig. According to legend, the Jade Emperor held a contest to decide which animals would be lucky enough to be included in the calendar. The winner of the race – the rat – received the first year of the 12-year cycle, and so on. However, historical research suggests that the Chinese zodiac emerged after the establishment of the "Gangi Chronicle Law", with each of the twelve animals directly assigned to one of the twelve Earthly Branches. In this system, a person's birth year determines their associated animal, which is linked to a specific Earthly Branch and serves both a chronological function and a means of categorizing individuals into symbolic groups, akin to a genus. In the Eastern Han dynasty, Xu Shen said that the character si (巳) was the image of a snake, and the same was true for hai (亥) and shi (豕; 'pig'). Since the twelve Earthly Branches of the zodiac were easily confused, people replaced them with animals and borrowed the ordinal symbols to match them with the Earthly Branches to form a chronological symbol system. In "Totem and celestial combination theory", it is suggested that the zodiac is ancient animal totem worship combined with astronomical images in astronomy. Among them, the explanation of the totem and celestial combinations is more scientific. Signs The zodiac traditionally begins with the sign of the Rat. The following are the twelve zodiac signs in order, each with its associated characteristics (Heavenly Stems, Earthly Branch, yin/yang force, Trine, and nature element). The belief that everyone and every animal has a role to play in society conforms to Confucian beliefs in a hierarchical society. Just as Confucian beliefs persist in Asia today alongside more modern social views, so does zodiac use. In Chinese astrology the animal signs assigned by year represent self-presentation or perception by others. It is a common misconception that the animals assigned by year are the only signs, and many Western descriptions of Chinese astrology only reference this system. There are also animal signs assigned by month (called "inner animals"), by day (called "true animals"), and hours (called "secret animals"). The Earth is all twelve signs, with five seasons. Michel Ferlus (2013) notes that the Old Chinese names of the earthly branches are of Austroasiatic origin. Some of Ferlus's comparisons are given below, with Old Chinese reconstructions cited from Baxter & Sagart (2014). 丑: Old Chinese *[n̥]ruʔ (compare Proto-Viet-Muong *c.luː 'water buffalo') 午: Old Chinese *[m].qʰˤaʔ (compare Proto-Viet-Muong *m.ŋəːˀ) 亥: Old Chinese *[g]ˤəʔ (compare Northern Proto-Viet-Muong *kuːrˀ) There is also a lexical correspondence with Austronesian: 未: Old Chinese *m[ə]t-s (compare Atayal miːts) The terms for the earthly branches are attested from Shang dynasty inscriptions and were likely also used before Shang times. Ferlus (2013) suggests that the terms were ancient pre-Shang borrowings from Austroasiatic languages spoken in the Yangtze River region. Chinese calendar Years Within the Four Pillars, the year is the pillar representing information about the person's family background and society or relationship with their grandparents. The person's age can also be easily deduced from their sign, the current sign of the year, and the person's generational disposition (teens, mid-20s, and so on). For example, a person born a Tiger is 12, 24, 36, (etc.) years old in the year of the Tiger (2022); in the year of the Rabbit (2023), that person is one year older. The following table shows the 60-year cycle matched up to the Gregorian calendar for 1924–2043. The sexagenary cycle begins at lichun about February 4 according to some astrological sources. Animal Trines The Chinese zodiac's animal trines are deeply connected with ancient Chinese cosmology, reflecting the Five Elements (Wood, Fire, Earth, Metal, Water) and the natural order. Each animal trine has a specific elemental attribute and a celestial pattern, showing the combination of astronomy and philosophy in the system (Hui, n.d.). First The first Trine consists of the Rat, Dragon, and Monkey. These three signs are considered intense and powerful individuals capable of great good. They are associated with the element of water, seen as great leaders but are also known to be unpredictable. The three are intelligent, adaptive, generous, charismatic, charming, authoritative, confident, eloquent, and artistic. However, they can also exhibit traits such as being manipulative, jealous, selfish, aggressive, vindictive, and deceitful. Second The second Trine consists of the Ox, Snake, and Rooster. These three signs are said to possess endurance and application, with a slow accumulation of energy. They are associated with the element of metal, meticulous at planning but tend to hold fixed opinions. The three are described as intelligent, hard-working, modest, industrious, loyal, philosophical, patient, good-hearted, and morally upright. However, they can also exhibit traits such as being self-righteous, egotistical, vain, judgmental, narrow-minded, and petty. Third The third Trine consists of the Tiger, Horse, and Dog. These three signs are associated with the element of fire, said to seek true love, pursue humanitarian causes, and be idealistic and independent, but they tend to be impulsive. They are described as productive, enthusiastic, independent, engaging, dynamic, honorable, loyal, and protective. However, they can also display traits such as being rash, rebellious, quarrelsome, anxious, disagreeable, and stubborn. Fourth The fourth Trine consists of the Rabbit, Goat, and Pig. These three signs are associated with the element of wood, said to have a calm nature and a somewhat reasonable approach. They seek aesthetic beauty, are artistic, well-mannered, and compassionate, yet they can also be detached and resigned to their condition. The three are described as caring, self-sacrificing, obliging, sensible, creative, empathetic, tactful, and prudent. However, they can also exhibit traits such as being naive, pedantic, insecure, selfish, indecisive, and pessimistic. These associations extend beyond just symbolism, they reflect ancient people's deep understanding of the universe's cyclical nature. The trines are also linked to specific seasons and directions, as well as human activities with the Earth's patterns. For example, the Wood element's trine is more connected with spring and the east, meaning renewal and growth. This connection set a direction for agricultural practices and societal rituals, maintaining harmony between humanity and nature (Hui, n.d.). In addition, the connection of the lunar calendar with the zodiac signs also reflects the importance of celestial movements in day-to-day life. The lunar times dictated the timing of holidays and agricultural events, further linking astronomical observations to cultural traditions. This implication of astronomy, philosophy, and daily life reflected the open worldview of ancient China, where timekeeping was not just a practical tool but a way to achieve a balance between cosmic and societal harmony. Compatibility As the Chinese zodiac is derived according to the ancient Five Elements Theory, every Chinese sign is associated with five elements with relations, among those elements, of interpolation, interaction, over-action, and counter-action—believed to be the common law of motions and changes of creatures in the universe. Different people born under each animal sign supposedly have different personalities, and practitioners of Chinese astrology consult such traditional details and compatibilities to offer putative guidance in life or for love and marriage. A common way to explore zodiac compatibility is with a chart showing how each zodiac sign interacts other signs. For example, constellations that are considered compatible with each other may have similar values and interests, while incompatible constellations may have conflicting personalities and ways of communicating. Origin stories Many stories and fables explain the beginning of the zodiac. Since the Han dynasty, the twelve Earthly Branches have been used to record the time of day. However, for entertainment and convenience, they were replaced by the twelve animals, and a mnemonic refers to the behavior of the animals: Earthly Branches may refer to a double-hour period. In the latter case it is the center of the period; for instance, 馬 mǎ (Horse) means noon as well as a period from 11:00 to 13:00. Great Race An ancient folktale called "The Great Race" tells of the Jade Emperor's decree that the years on the calendar would be named for each animal in the order they reached him. To get there, the animals would have to cross a river. The Cat and the Rat were not good at swimming, but they were both quite intelligent. They decided that the best and fastest way to cross the river was to hop on the back of the Ox. The Ox, being kindhearted and naive, agreed to carry them both across. As the Ox was about to reach the other side of the river, the Rat pushed the Cat into the water, and then jumped off the Ox and rushed to the Jade Emperor. It was named as the first animal of the zodiac calendar. The Ox had to settle for second place. The third animal to come was the Tiger. Even though it was strong and powerful, it admitted to the Jade Emperor that the currents were pushing it downstream. Suddenly, a thump sound came from the distance, signaling the arrival of the Rabbit. It explained how it crossed the river: by jumping from one stone to another in a nimble fashion. Halfway through, it thought it might lose the race, but it was lucky enough to grab hold of a floating log that later washed it to shore. For that, it became the fourth animal in the zodiac cycle. In fifth place was the flying Dragon. The Jade Emperor wondered why a swift, airborne creature such as the Dragon did not come in first place. The Dragon explained that it had to stop by a village and bring rain for all the people, and therefore it was held back. Then, on its way to the finish, it saw the helpless Rabbit clinging onto a log, so it did a good deed and gave a puff of breath in the poor creature's direction so that it could land on the shore. The Jade Emperor was astonished by the Dragon's good nature, and it was named as the fifth animal of the zodiac. As soon as the Dragon arrived, there came a galloping sound, and the Horse appeared. Hidden on the Horse's hoof was the Snake, whose sudden appearance gave the Horse a fright, thus making it fall back and giving the Snake the sixth spot while the Horse placed seventh. After a while, the Goat, Monkey, and Rooster came to the river blocking the heavenly gate. The Rooster found a raft, and the Monkey and the Goat tugged and pulled, trying to get all the weeds out of the way. With combined efforts, they managed to arrive to the other side. The Jade Emperor was pleased with their teamwork and decided to name the Goat as the eighth animal, followed by the Monkey and then the Rooster. The eleventh animal placed in the zodiac cycle was the Dog. Although it should have been the best swimmer and runner, it spent its time playing in the river water. Its explanation for being late was that it needed a good bath after a long journey, but it almost did not make it to the finish line. Right when the Jade Emperor was going to end the race, an oink sound was heard: it was the Pig. The Pig felt hungry in the middle of the race, so it stopped, ate something, and then fell asleep. After it awoke, it finished the race in twelfth place, making it the last animal to arrive. The Cat eventually drowned and failed to become part of the zodiac. It is said that this is the reason why cats hate water. It is also the reason for the rivalry between the Cat and Rat, as it was the Rat's callous act to push the Cat into the river. Variations Another version of the folktale tells that the Rat deceived the Ox into letting it jump on its back by promising the Ox that it could hear the Rat sing, before jumping off at the finish line and finishing first. Another variant says that the Rat cheated the Cat out its place at the finish line, by hiding on the back of the Dog, who was too focused to notice that he had a stow-away. The Cat tried to attack the rat in retaliation, but hurt the Dog by accident. This is said to account for the antagonistic dynamic between cats and rats, beyond normal predator and prey behavior, and also why dogs and cats fight. In Chinese mythology, a story tells that the cat was tricked by the Rat so it could not go to the banquet. This is why the Cat is ultimately not part of the Chinese zodiac. In Buddhist legend Gautama Buddha summoned all animals of the Earth to come before him before his departure from this Earth, but only twelve animals came to bid him farewell. To reward these animals, he named a year after each of them in the order they had arrived. The twelve animals of the Chinese zodiac were developed in the early stages of Chinese civilization, so therefore it is difficult to investigate its real origins. Most historians agree that the Cat is not included, as cats had not yet been introduced to China from India with the arrival of Buddhism. However until recently, the Vietnamese moved away from their traditional texts and literature and, unlike all other countries who follow the Sino lunar calendar, include the Cat instead of the Rabbit as a zodiac animal. The most common explanation is that cats are worshipped by farmers in East Asia, believing that cats' luck and prosperity protects their crops. Another popular cultural reason is that the ancient word for rabbit (Mao) sounds like cat (Meo). Adaptations The Chinese zodiac signs are also used by cultures other than Chinese. For example, they usually appear on Korean New Year and Japanese New Year's cards and stamps. The United States Postal Service and several other countries' postal services issue a "Year of the ____" postage stamp each year to honor this Chinese heritage. The zodiac is widely used in commercial culture, for example, in the Chinese New Year market, and popular zodiac-related products, such as crafts, toys, books, accessories, and paintings and Chinese lunar coins. The coins depict zodiac animals, inspired the Canadian Silver Maple Leaf coins, as well as varieties from Australia, South Korea, and Mongolia. The Chinese zodiac is also used in some Asian countries that were under the cultural influence of China. However, some of the animals in the zodiac may differ by country. Asian The Korean zodiac includes the Sheep (yang) instead of the Goat (which would be yeomso), although the Chinese source of the loanword yang may refer to any goat-antelope. The Japanese zodiac includes the Sheep (hitsuji) instead of the Goat (which would be yagi), and the Wild boar (inoshishi, i) instead of the Pig (buta). Since 1873, the Japanese have celebrated the beginning of the new year on 1 January as per the Gregorian calendar. The Vietnamese zodiac varies from the Chinese zodiac with the second animal being the Water Buffalo instead of the Ox, and the fourth animal being the Cat instead of the Rabbit. The Cambodian zodiac is exactly identical to that of the Chinese although the dragon is interchangeable with the Neak (nāga) Cambodian sea snake. Sheep and Goat are interchangeable as well. The Cambodian New Year is celebrated in April, rather than in January or February as it is in China and most countries. The Cham zodiac uses the same order as the Chinese zodiac, but replaces the Monkey with the turtle (known locally as kra). Similarly the Malay zodiac replaces the Rabbit with the mousedeer (pelanduk) and the Pig with the tortoise (kura or kura-kura). The Dragon (Loong) is normally equated with the nāga but it is sometimes called Big Snake (ular besar) while the Snake sign is called Second Snake (ular sani). This is also recorded in a 19th-century manuscript compiled by John Leyden. The Thai zodiac includes a nāga in place of the Dragon and begins, not at the Chinese New Year, but either on the first day of the fifth month in the Thai lunar calendar, or during the Songkran New Year festival (now celebrated every 13–15 April), depending on the purpose of the use. Historically, Lan Na (Kingdom around Northern Thailand) also replaces pig with elephant. While modern Thai have returned to pig, its name is still กุน (gu̜n), retaining the actual word for elephant in the zodiac. The Gurung zodiac in Nepal includes a Cow instead of an Ox, a Cat instead of Rabbit, an Eagle instead of a Dragon (Loong), a Bird instead of a Rooster, and a Deer instead of a Pig. The Bulgar calendar used from the 2nd century and that has been only partially reconstructed uses a similar sixty-year cycle of twelve animal-named years groups. The Old Mongol calendar uses the Mouse, the Ox, the Leopard, the Hare, the Crocodile, the Serpent, the Horse, the Sheep, the Monkey, the Hen, the Dog and the Hog. The Tibetan calendar replaces the Rooster with the bird. The Volga Bulgars, Kazars and other Turkic peoples replaced some animals by local fauna: Leopard (instead of Tiger), Fish or Crocodile (instead of Dragon/Loong), Hedgehog (instead of Monkey), Elephant (instead of Pig), and Camel (instead of Rat/Mouse). In the Persian version of the Eastern zodiac brought by Mongols during the Middle Ages, the Chinese word lóng and Mongol word lū (Dragon) was translated as nahang meaning "water beast", and may refer to any dangerous aquatic animal both mythical and real (crocodiles, hippos, sharks, sea serpents, etc.). In the 20th century the term nahang is used almost exclusively as meaning Whale, thus switching the Loong for the Whale in the Persian variant. In the traditional Kazakh version of the twelve-year animal cycle (Kazakh: мүшел, müşel), the Dragon is replaced by a snail (Kazakh: ұлу, ulw), and the Tiger appears as a leopard (Kazakh: барыс, barıs). In the Kyrgyz version of the Chinese zodiac (Kyrgyz: мүчөл, müçöl) the words for the Dragon (Kyrgyz: улуу, uluu), Monkey (Kyrgyz: мечин, meçin) and Tiger (Kyrgyz: барс, bars) are only found in Chinese zodiac names, other animal names include Mouse, Cow, Rabbit, Snake, Horse, Sheep, Chicken, Dog and Wild boar. In the Turkish version of zodiac, the animals are almost the same, but it replaces Tiger with Leopard (Pars), Dragon with Fish (Balık) and Goat with Sheep (Koyun). Remarkably, the practise of zodiac persisted since the Ottoman Empire, including the presence of Pig (Domuz) despite contradicting Islamic rule. English translation Due to confusion with synonyms during translation, some of the animals depicted by the English words did not exist in ancient China. For example: The term 鼠 Rat can be translated as Mouse, as there are no distinctive words for the two genera in Chinese. However, Rat is the most commonly used one among all the synonyms. The term 牛 Ox, a castrated Bull, can be translated interchangeably with other terms related to Cattle (male Bull, female Cow) and Buffalo. However, Ox is the most commonly used one among all the synonyms. The term 卯 Rabbit can be translated as Hare, as 卯 (and 兔) do not distinguish between the two genera of leporids. As hares are native to China and most of Asia and rabbits are not, this would be more accurate. However, in colloquial English Rabbit can encompass hares as well. The term 蛇 Snake can be translated as Serpent, which refers to a large species of snake and has the same behavior, although this term is rarely used. The term 羊 Goat can be translated interchangeably with other terms related to Sheep (male Ram, female Ewe). However, Goat is the most commonly used one among all the synonyms. The term 雞 Rooster can be translated interchangeably with Chicken, as well as the female Hen. However, Rooster is the most commonly used one among all the synonyms in English-speaking countries. Gallery See also Astrology and science Chinese spiritual world concepts References Sources External links "The Year of the Rooster: On Seeing" "The Year of the Rooster: On Eating, Injecting, Imbibing & Speaking" "2016: The Golden Monkey: A Year to Remember" "The Dragon Raises its Head 龍抬頭" "2019 year of the Pig" "From the Year of the Ape to the Year of the Monkey"; Archived 2020-04-11 at the Wayback Machine (on use of Zodiac figures for political criticism) Scrupulosity is the pathological guilt and anxiety about moral issues. Although it can affect nonreligious people, it is usually related to religious beliefs. It is personally distressing, dysfunctional, and often accompanied by significant impairment in social functioning. It is typically conceptualized as a moral or religious form of obsessive–compulsive disorder (OCD). The term is derived from the Latin scrupus, a sharp stone, implying a stabbing pain on the conscience. Scrupulosity was formerly called scruples in religious contexts, but the word scruple now commonly refers to a troubling of the conscience rather than to the disorder. As a personality trait, scrupulosity is a recognized diagnostic criterion for obsessive–compulsive personality disorder. It is sometimes called "scrupulousness", but that word properly applies to the positive trait of having scruples. Presentation In scrupulosity, a person's obsessions focus on moral or religious fears, such as the fear of being an evil person or the fear of divine retribution for sin. Although it can affect nonreligious people, it is usually related to religious beliefs. Not all obsessive–compulsive behaviors related to religion are instances of scrupulosity: strictly speaking, for example, scrupulosity is not present in people who repeat religious requirements merely to be sure that they were done properly. Scrupulosity can be distinguished from normal religious beliefs through the four criteria established by Greenberg and Witzum (1991). These criteria include more intense than normative religious experiences, often distressing for the individual affected, associated with poor self-care/social functioning, and usually involves special messages from religious figures. In addition, while religiosity may affect how OCD is manifested, there is no proven causality between the severity of OCD and religiosity, and only small associations between the latter and scrupulosity. Some individuals afflicted with scrupulosity view their unwanted thoughts as morally equivalent to performing those thoughts or as evidence of a hidden desire to. This connection, known as moral thought-action fusion (moral TAF), creates significant distress for those experiencing it. An example of moral TAF is a mother who has an intrusive thought of hurting her child. The mother may feel she is a danger to the child; she considers her thoughts as evidence for her ostensible abuse. Some research indicates an increased likelihood of moral TAF with some religions and cultures that hold thoughts and actions morally equivalent. Treatment Treatment is similar to that for other forms of obsessive–compulsive disorder. Exposure and response prevention (ERP), a form of behavior therapy, is widely used for OCD in general and may be promising for scrupulosity in particular. ERP is based on the idea that deliberate repeated exposure to obsessional stimuli lessens anxiety, and that avoiding rituals lowers the urge to behave compulsively. For example, with ERP a person obsessed by blasphemous thoughts while reading the Bible would practice reading the Bible. However, ERP is considerably harder to implement than with other disorders, because scrupulosity often involves spiritual issues that are not specific situations and objects. For example, ERP is not appropriate for a man obsessed by feelings that God has rejected and is punishing him. Cognitive therapy may be appropriate when ERP is not feasible. Other therapy strategies include noting contradictions between the compulsive behaviors and moral or religious teachings, and informing individuals that for centuries religious figures have suggested strategies similar to ERP. Religious counseling may be an additional way to readjust beliefs associated with the disorder, though it may also stimulate greater anxiety. Little evidence is available on the use of medications to treat scrupulosity. Although serotonergic medications are often used to treat OCD, studies of pharmacologic treatment of scrupulosity in particular have produced so few results that even tentative recommendations cannot be made. Treatment of scrupulosity in children has not been investigated to the extent it has been studied in adults, and one of the factors that makes the treatment difficult is the fine line the therapist must walk between engaging and offending the client. Epidemiology The prevalence of scrupulosity is speculative. Available data do not permit reliable estimates, and available analyses mostly disregard associations with age or with gender, and have not reliably addressed associations with geography or ethnicity. Available data suggest that the prevalence of obsessive–compulsive disorder does not differ by culture, except where prevalence rates differ for all psychiatric disorders. Associations between OCD and the depth of religious beliefs have been difficult to demonstrate, and data are scarce. There are large regional differences in the percentage of OCD patients who have religious obsessions or compulsions, ranging from 0–7% in countries like the U.K. and Singapore, to 40–60% in traditional Muslim and orthodox Jewish populations. Characteristics of scrupulosity also tend to vary by religion in relation to traditional practices and beliefs. In Western Christian samples, increased levels of religiosity are associated with an increase in obsessions about controlling thoughts. This phenomenon is thought to be caused by the Biblical explanation that merely thinking of a sin is as bad as committing it. In Jewish communities, scrupulous compulsions tend to include washing, excessive prayer, and consultation with religious leaders, which are closely linked to Jewish customs of removing impurities through hand washing. Similarly, a study of a conservative Muslim population in Saudi Arabia revealed that obsessions about prayer, washing, and contamination dominate, seemingly stemming from the religious practice wuduʾ, which requires methodical cleansing of the body before prayer. Additionally, Muslims in Pakistan describe a concept called “Nepak” which is a “mix of unpleasant feelings of contamination with strong religious connotations of dirtiness and unholiness.” When suffering Nepak, an individual must cleanse himself thoroughly before participating in religious rituals again. History Scrupulosity is a modern-day psychological problem that echoes a traditional use of the term scruples in a religious context, e.g. by Catholics, to mean obsessive concern with one's own sins and compulsive performance of religious devotion. This use of the term dates to the 12th century. Several historical and religious figures suffered from doubts of sin, and expressed their pains. Ignatius of Loyola, founder of the Jesuits, wrote "After I have trodden upon a cross formed by two straws ... there comes to me from without a thought that I have sinned ... this is probably a scruple and temptation suggested by the enemy." Alphonsus Liguori, the Redemptorists' founder, wrote of it as "groundless fear of sinning that arises from 'erroneous ideas'". Although the condition was lifelong for Loyola and Liguori, Thérèse of Lisieux stated that she recovered from her condition after 18 months, writing "One would have to pass through this martyrdom to understand it well, and for me to express what I experienced for a year and a half would be impossible." Martin Luther also suffered from obsessive doubts; in his mind, his omitting the word enim ("for") during the Eucharist was as horrible as laziness, divorce, or murdering one's parent. Although historical religious figures such as Loyola, Luther and John Bunyan are commonly cited as examples of scrupulosity in modern self-help books, some of these retrospective diagnoses may be deeply ahistorical: these figures' obsession with salvation may have been excessive by modern standards, but that does not mean that it was pathological. Scrupulosity's first known public description as a disorder was in 1691, by John Moore, who called it "religious melancholy" and said it made people "fear, that what they do, is so defective and unfit to be presented unto God, that he will not accept it". Loyola, Liguori, the French confessor R.P. Duguet, and other religious authorities and figures attempted to develop solutions and coping mechanisms; the monthly newsletter Scrupulous Anonymous, published by the followers of Liguori, has been used as an adjunct to therapy.: 103–12 In the 19th century, Christian spiritual advisors in the U.S. and Britain became worried that scrupulosity was not only a sin in itself, but also led to sin, by attacking the virtues of faith, hope, and charity. Studies in the mid-20th century reported that scrupulosity was a major problem among American Catholics, with up to 25 per cent of high school students affected; commentators at the time asserted that this was an increase over previous levels. Starting in the 20th century, individuals with scrupulosity in the U.S. and Britain increasingly began looking to psychiatrists, rather than to religious advisors, for help with the condition. Resources International OCD Foundation (OCDF) . Non-profit organization dedicated to giving support to individuals with obsessive-compulsive disorder (OCDF), since 1986 raises funds for research; compiles and disseminates the latest treatment information, including scrupulosity Managing Scrupulosity . A service from Fr. Thomas M Santa, C.Ss.R., (A Roman Catholic priest). Fr. Santa has ministered to people with scrupulosity for more than 20 years. Further reading The Obsessive–Compulsive Disorder: Pastoral Care for the Road to Change ISBN 0-7890-0707-X Can Christianity Cure Obsessive–Compulsive Disorder?: A Psychiatrist Explores the Role of Faith in Treatment ISBN 1-58743-206-4 Beattie, Trent (2011). Scruples and Sainthood. Loreto Publications 2011. ISBN 978-1-930278-96-7 Fr. Thomas M. Santa, CSS. Understanding Scrupulosity (2017). William Van Ornum, A Thousand Frightening Fantasies: Understanding and Healing Scrupulosity and Obsessive Compulsive Disorder, Crossroad Pub., 1997, ISBN 978-0-8245-1605-5. Joseph W. Ciarrocchi, The Doubting Disease: Help for Scrupulosity and Religious Compulsions, Paulist Press, 1995, ISBN 978-0-8091-3553-0. == References == The popularity and worldwide scope of rock music resulted in a powerful impact on society in the 20th century, particularly among the baby boomer generation. Rock and roll influenced daily life, fashion, social attitudes, and language in a way few other social developments have equated to. As the original generation of rock and roll fans matured, the music became an accepted and deeply interwoven thread in popular culture. Beginning in the early 1950s, rock songs began to be used in a few television commercials; within a decade, this practice became widespread, and rock music also featured in film and television program soundtracks. By the 1980s, rock music culture had become the dominant form of popular music culture in the United States and other Western countries, before seeing a decline in subsequent years. Race In the crossover of African American "race music" to a growing white youth audience, the popularization of rock and roll involved both black performers reaching a white audience and white performers appropriating African-American music. Rock and roll appeared at a time when racial tensions in the United States were entering a new phase, with the beginnings of the civil rights movement for desegregation, leading to the Supreme Court ruling that abolished the policy of "separate but equal" in 1954, but leaving a policy which would be extremely difficult to enforce in parts of the United States. The coming together of white youth audiences and black music in rock and roll, inevitably provoked strong white racist reactions within the US, with many whites condemning its breaking down of barriers based on color. Many observers saw rock and roll as heralding the way for desegregation, in creating a new form of music that encouraged racial cooperation and shared experience. Many authors have argued that early rock and roll was instrumental in the way both white and black teenagers identified themselves. Sex and drugs The rock and roll lifestyle was popularly associated with sex and drugs. Many of rock and roll's early stars (as well as their jazz and blues counterparts) were known as hard-drinking, hard-living characters. During the 1960s the lifestyles of many stars became more publicly known, aided by the growth of the underground rock press. Musicians had always attracted attention of "groupies" (girls who followed musicians) who spent time with and often performed sexual favors for band members. As the stars' lifestyles became more public, the popularity and promotion of recreational drug use by musicians may have influenced use of drugs and the perception of acceptability of drug use among the youth of the period. For example, when in the late 1960s the Beatles, who had previously been marketed as clean-cut youths, started publicly acknowledging using LSD, many fans followed. Journalist Al Aronowitz wrote "...whatever the Beatles did was acceptable, especially for young people." Jerry Garcia, of the rock band Grateful Dead said, "For some people, taking LSD and going to Grateful Dead show functions like a rite of passage ... we don't have a product to sell; but we do have a mechanism that works." In the late 1960s and early 1970s, much of the rock and roll cachet associated with drug use dissipated as rock music suffered a series of drug-related deaths, including the 27 Club-member deaths of Jimi Hendrix, Janis Joplin and Jim Morrison. Although some amount of drug use remained common among rock musicians, a greater respect for the dangers of drug consumption was observed, and many anti-drug songs became part of the rock lexicon, notably "The Needle and the Damage Done" by Neil Young (1972). Many rock musicians, including John Lennon, Paul McCartney, Mick Jagger, Bob Dylan, Jerry Garcia, Stevie Nicks, Jimmy Page, Keith Richards, Bon Scott, Eric Clapton, Pete Townshend, Brian Wilson, Carl Wilson, Dennis Wilson, Steven Tyler, Scott Weiland, Sly Stone, Ozzy Osbourne, Mötley Crüe, Layne Staley, Kurt Cobain, Lemmy, Bobby Brown, Buffy Sainte Marie, Dave Matthews, David Crosby, Anthony Kiedis, Dave Mustaine, David Bowie, Richard Wright, Phil Rudd, Phil Anselmo, James Hetfield, Kirk Hammett, Joe Walsh, Julian Casablancas and others, have acknowledged battling addictions to many substances including alcohol, cocaine and heroin; many of these have successfully undergone drug rehabilitation programs, but others have died. In the early 1980s. along with the rise of the band Minor Threat, a straight edge lifestyle became popular. The straight edge philosophy of abstinence from recreational drugs, alcohol, tobacco and sex became associated with some hardcore punks through the years, and both remain popular with youth today. Fashion Rock music and fashion have been inextricably linked. In the mid-1960s of the UK, rivalry arose between "Mods" (who favoured 'modern' Italian-led fashion) and "Rockers" (who wore motorcycle leathers), each style had their own favored musical acts. (The controversy would form the backdrop for The Who's rock opera Quadrophenia). In the 1960s, The Beatles brought mop-top haircuts, collarless blazers, and Beatle Boots into fashion. Rock musicians were also early adopters of hippie fashion and popularised such styles as long hair and the Nehru jacket. As rock music genres became more segmented, what an artist wore became as important as the music itself in defining the artist's intent and relationship to the audience. In the early 1970s, glam rock became widely influential featuring glittery fashions, high heels and camp. In the late 1970s, disco acts helped bring flashy urban styles to the mainstream, while punk groups began wearing mock-conservative attire, (including suit jackets and skinny ties), in an attempt to be as unlike mainstream rock musicians, who still favored blue jeans and hippie-influenced clothes. Heavy Metal bands in the 1980s often favoured a strong visual image. For some bands, this consisted of leather or denim jackets and pants, spike/studs and long hair. Visual image was a strong component of the glam metal movement. In 1981, MTV was formed, marking a large shift in the music world. Because MTV would become such a cultural force, the young would look toward MTV. Fashion happened to be one of those cultural centers that the Television company would have a great effect on. With debuts like Madonna's Iconic underwear-as-outerwear look and the companies featuring heavy metal as well as new wave and other genera that would go to promote each artist's brand of fashion into the greater culture, because of the sheer amount of visibility that MTV gave these artist through music videos and other content that the television channel had. In the early 1990s, the popularity of grunge brought in a punk influenced fashion of its own, including torn jeans, old shoes, flannel shirts, backward baseball hats, and people grew their hair against the clean-cut image that was popular at the time in heavily commercialized pop music culture. Musicians continue to be fashion icons; pop-culture magazines such as Rolling Stone often include fashion layouts featuring musicians as models. Authenticity Rock musicians and fans have consistently struggled with the paradox of "selling out"—to be considered "authentic", rock music must keep a certain distance from the commercial world and its constructs; however it is widely believed that certain compromises must be made to become successful and to make music available to the public. This dilemma has created friction between musicians and fans, with some bands going to great lengths to avoid the appearance of "selling out" (while still finding ways to make a lucrative living). In some styles of rock, such as punk and heavy metal, a performer who is believed to have "sold out" to commercial interests may be labelled with the pejorative term "poseur". If a performer first comes to public attention with one style, any further stylistic development may be seen as selling out to long-time fans. On the other hand, managers and producers may progressively take more control of the artist, as happened, for instance, in Elvis Presley's swift transition in species from "The Hillbilly Cat" to "your teddy bear". It can be difficult to define the difference between seeking a wider audience and selling out. Ray Charles left behind his classic formulation of rhythm and blues to sing country music, pop songs and soft-drink commercials. In the process, he went from a niche audience to worldwide fame. Bob Dylan faced consternation from fans for embracing the electric guitar. In the end, it is a moral judgement made by the artist, the management, and the audience. Social activism Love and peace were very common themes in rock music during the 1960s and 1970s. Rock musicians have often attempted to address social issues directly as commentary or as calls to action. During the Vietnam War the first rock protest songs were heard, inspired by the songs of folk musicians such as Woody Guthrie and Bob Dylan, which ranged from abstract evocations of peace Peter, Paul and Mary's "If I Had a Hammer" to blunt anti-establishment diatribes Crosby, Stills, Nash & Young's "Ohio". Other musicians, notably John Lennon and Yoko Ono, were vocal in their anti-war sentiment both in their music and in public statements with songs such as "Imagine", and "Give Peace a Chance". Famous rock musicians have adopted causes ranging from the environment (Marvin Gaye's "Mercy Mercy Me (The Ecology)") and the Anti-Apartheid Movement (Peter Gabriel's "Biko"), to violence in Northern Ireland (U2's "Sunday Bloody Sunday") and worldwide economic policy (the Dead Kennedys' "Kill the Poor"). Another notable protest song is Patti Smith's recording "People Have the Power." On occasion this involvement would go beyond simple songwriting and take the form of sometimes-spectacular concerts or televised events, often raising money for charity and awareness of global issues. Live Aid concerts Rock and roll as social activism reached a milestone in the Live Aid concerts, held July 13, 1985, which were an outgrowth of the 1984 charity single "Do They Know It's Christmas?" and became the largest musical concert in history with performers on two main stages, one in London, England and the other in Philadelphia, USA (plus some other acts performing in other countries) and televised worldwide. The concert lasted 16 hours and featured nearly everybody who was in the forefront of rock and pop in 1985. The charity event raised millions of dollars for famine relief in Africa. Live Aid became a model for many other fund-raising and consciousness-raising efforts, including the Farm Aid concerts for family farmers in North America, and televised performances benefiting victims of the September 11 attacks. Live Aid itself was reprised in 2005 with the Live 8 concert, to raise awareness of global economic policy. Environmental issues have also been a common theme, one example being Live Earth. Religion The common usage of the term "rock god" acknowledges the quasi-religious quality of the adulation some rock stars receive. Songwriters like Pete Townshend have explored spirituality within their work. John Lennon became infamous, particularly in the United States, after he remarked in 1966 that The Beatles were "more popular than Jesus", with Beatles records being burned in public in some places in the South. However, he later said that this statement was misunderstood and not meant to be anti-Christian. Iron Maiden, Metallica, Ozzy Osbourne, King Diamond, Alice Cooper, Led Zeppelin, Marilyn Manson, Slayer and numerous others have also been accused of being satanists, immoral or otherwise having an "evil" influence on their listeners. Anti-religious sentiments also appear in punk and hardcore. There's the example of the song "Filler" by Minor Threat, the name and famous logo of the band Bad Religion and criticism of Christianity and all religions is an important theme in anarcho-punk and crust punk. Christianity Christian rock, alternative rock, metal, punk, and hardcore are specific, identifiable genres of rock music with strong Christian overtones and influence. Many groups and individuals who are not considered to be Christian rock artists have religious beliefs themselves. For example; The Edge and Bono of U2 are a Methodist and an Anglican, respectively; Bruce Springsteen is a Roman Catholic; and Brandon Flowers of The Killers is a Latter Day Saint. Carlos Santana, Ted Nugent, and John Mellencamp are all other examples of rock stars who profess some form of Christian faith. However, some conservative Christians single out the music genres of hip hop and rock as well as blues and jazz as containing jungle beats, or jungle music, and claim that it is a beat or musical style that is inherently evil, immoral, or sensual. Thus, according to them, any song in the rap, hip hop and rock genres is inherently evil because of the song's musical beat, regardless of the song's lyrics or message. A few even extend this analysis even to Christian rock songs. Christian conservative author David Noebel is one of the most notable opponents of the existence of jungle beats. In his writings and speeches, Noebel held that the use of such beats in music was a communist plot to subvert the morality of the youth of the United States. Pope Benedict XVI was quoted as saying, according to the British Broadcasting Corporation, that "Rock... is the expression of the elemental passions, and at rock festivals it assumes a sometimes cultic character, a form of worship, in fact, in opposition to Christian worship." Satanism Some metal bands use demonic imagery for artistic and/or entertainment purposes, though many do not worship or believe in Satan. Ozzy Osbourne is reported to be Anglican and Alice Cooper is a known born-again Christian. In some cases, though, metal performers have expressed satanic views. Numerous others in the early Norwegian black metal scene were Satanists. The most known example of this is Euronymous, who claimed that he worshiped Satan as a god. Varg Vikernes (back then called "the Count" or Grishnak) has also been called a Satanist, even though he has rejected that label. Even within this localized musical subgenre, however, the arson attacks against Christian churches and other centers of worship were condemned by some prominent figures within the Norwegian black metal scene, such as Kjetil Manheim. References Further reading Alain Dister, The Story Of Rock Smash Hits And Superstars (New York: Thames and Hudson, 1993), 40. Jeff Godwin, The Devil's Disciples: the Truth about Rock (Chino, Calif.: Chick Publications, 1985). ISBN 0-937958-23-9 Dan Peters, Steve Peters, and Cher Merrill. Why Knock Rock? (Minneapolis, Minn.: Bethany House Publishers, 1984). ISBN 0-87123-440-8 Perry F. Rockwood, Rock Music or Rock of Ages (Halifax, N.S.: People's Gospel Hour, [1980?]). Without ISBN External links Rock music museum San Francisco Rock and Roll Hall of Fame Cultural sustainability as it relates to sustainable development (or to sustainability), has to do with maintaining cultural beliefs, cultural practices, heritage conservation, culture as its own entity, and the question of whether or not any given cultures will exist in the future. From cultural heritage to cultural and creative industries, culture is both an enabler and a driver of the economic, social, and environmental dimensions of sustainable development. Culture is defined as a set of beliefs, morals, methods, institutions and a collection of human knowledge that is dependent on the transmission of these characteristics to younger generations. Cultural sustainability has been categorized under the social pillar of the three pillars of sustainability, but some argue that cultural sustainability should be its own pillar, due to its growing importance within social, political, environmental, and economic spheres. The importance of cultural sustainability lies within its influential power over the people, as decisions that are made within the context of society are heavily weighed by the beliefs of that society. Cultural sustainability can be regarded as a fundamental issue, even a precondition to be met on the path towards sustainable development. However, the theoretical and conceptual understanding of cultural sustainability within the general frames of sustainable development remains vague. Determining the impact of cultural sustainability is found by investigating the concept of culture in the context of sustainable development, through multidisciplinary approaches and analyses. This means examining the best practices for bringing culture into political and social policy as well as practical domains, and developing means and indicators for assessing the impacts of culture on sustainable development. Sociopolitical landscapes Culture has an overwhelming effect on social, economic and political planning, but as of yet, has failed to be incorporated into social, and political policy on a grand scale. However, certain policies regarding both policy and politics have managed to be implemented into some conventions that are implemented on a global scale. Culture is found everywhere within a society, from the relics of previous generations, to the accumulated values of a society. Culture within society can be divided into two, equally important subtopics that aid in the description of cultural specific characterizations. These categories, as defined by the United Nations Educational, Scientific and Cultural Organization (UNESCO) are "Material" and "Immaterial". Material objects such as shrines, paintings, buildings, landscapes and other humanistic formations act as a physical representation of the culture in that area. Although they have little social and political utility, they serve as physical landmarks and culturally dependent objects whose meaning is created and maintained within the context of that society. The accumulation of these cultural characteristics are what measures a society's cultural integrity, and these characteristics are inherently capable of transforming landscapes of political, social and environment nature via the influence that these values and historical remains have on the population. Little success has come with the implementations of cultural policy within the context of politics due to a lack of empirical information regarding the topic of cultural sustainability. The Immaterial category contains more socially and politically applicable characteristics such as practices, traditions, aesthetics, knowledge, expressions etc. These characteristics embody social and political utility through education of people, housing, social justice, human rights, employment and more. These values contribute to the well-being of a society through the use of collective thinking and ideals i.e. culture. Culture also presents more room for expansion on its effects on a society. Specifically, creativity, respect, empathy, and other practices are being used to create social integration and also to create a sense of "self" in the world. Convention implementation Implementation of policy on a global scale has had little success, but enough to show an increasing interest in the topic of Cultural Sustainability. The conventions that have been implemented, have done so on a large scale, involving multiple countries, across most continents. UNESCO has been responsible for the vast majority of these conventions, maintaining that cultural sustainability and cultural heritage are a strong cornerstone of society. One of the more relevant conventions created in 2003 is the "Convention for the Safeguarding of the Intangible Cultural Heritage" which proclaims that culture must be protected against all adversarial combatants. This safeguard was implemented as an understanding that culture guarantees sustainability. Implementing policy based on cultural history is in the process of becoming a widely talked about subject and holds that cultures will be able to thrive in the context of both present and future. Conventions made by UNESCO regarding cultural preservation and sustainability are surrounding the promotion of cultural diversity, which means multiple cultures and ideals within one grand culture. Cultural heritage Cultural memorabilia and artifacts from a cultures history maintain an important role in modern society as they are kept as relics and shrines in order to remember the stories, knowledge, skills and methods of ancestors and learn invaluable lessons from the past. Today, cultures use libraries, art exhibits and museums as a placeholder for these important objects and other culturally significant artifacts. Not only are these objects revered, but the buildings themselves are oftentimes a symbol of cultural integrity to the community which it belongs. Linking with the other pillars of sustainability, the biggest barrier to cultural sustainability is funding. Economic sustainability relies on a number of systems with goals to ensure economic prosperity by eliminating spending where it is not needed. Cultural buildings such as museums oftentimes fail to receive the funding it needs to continue the preservation of culturally significant artifacts. Human-centered design and cultural collaboration have been popular frameworks for sustainable development in marginalized communities. These frameworks involve open dialogue which entails sharing, debating, and discussing, as well as holistic evaluation of the site of development. Sustainable tourism Tourism is a traveling method for which people can venture to different areas of the globe and experience new ways of living, and explore landscapes not native to their country of origin. Tourism is constantly being criticized for its impact on the social, political and environmental landscapes due to its high volume of mass consumers. Within the realm of tourism exists more sustainable practices and ideals that are aligned with the idea of cultural sustainability. Geotourism Geotourism is a form of tourism which relies heavily upon the sustainability, or even the improvement of a selected geological location. Serving as an alternative to mass tourism, Geotourism was created with the purpose of aiding in the sustainable development movement. Geotourism is a method which focuses on Sustainable culture, ecological preservation and restoration, welfare of local populous, and the wildlife in the immediate area. The link between Geotourism and Cultural sustainability lies within their role in maintaining the natural state of the environment, including the social and cultural environment. Preservation of the local culture has been a key element of Geotourism from its inception, and due to this form of tourism, travelers are able to experience true local culture, lifestyles, and practices experienced by the people native to that region. The scope of Geotourism covers many geological features, from wider areas such as mountains or coasts to smaller rock formations. This form of tourism provides education regarding destinations they have traveled to via ethnographic methods, and also calls upon the traveler to become aware of the footprint they leave on the environment, as well as social changes that may be harmful to the indigenous peoples. Responsibility plays an important role in Geotourism by informing travelers of their duties to respect, and preserve the local culture. Many countries have adapted this method of tourism, going as far as to implement geotourism sites equipped with guides that discuss matters of importance in that area such as environmental or cultural concerns. Such countries include: Many states within the U.S. including California and Arizona Romania Norway Honduras Geotourism in Honduras includes The Bay Islands, a Caribbean archipelago made up of three main islands, Utila, Roatan, and Guanaja and a few lesser islands and cays located off the north coast of Honduras. These islands have been blessed with stunning natural scenery, highlighted by idyllic beaches, tropical hillsides, and mangrove forests. Mexico Geotourism in Mexico includes Puerto Peñasco. This region includes the protected Sea of Cortez, the Pinacate Crator which offers barren deserts, sacred tribal and Indian lands, fishing zones, estuaries, oyster beds, and vibrant farmland and wine country. Over the past decade, Puerto Peñasco, a former modest fishing village in Sonora situated just 65 kilometers away from the US border, has transformed into one of the most rapidly expanding urban areas in Mexico. Canada Geotourism in Canada includes Nova Scotia. It was visited by explorers and geologists from around the world for centuries, Nova Scotia's geological sites are now recognized for their beauty as much as for their rich history. Nova Scotia’s sites include the iconic lighthouses, which is built on rocky precipices that reach out into the sea. Portugal Geotourism in Portugal includes the Geopark of Arouca, which, in 2010, was officially recognized and joined the Global network of Geoparks, under the auspices of UNESCO. The park is known for its natural, gastronomical, and cultural heritage. This mountainous area, with rivers, natural parks, steep slopes and lush vegetation covers the entire municipality of Arouca. The granite used to build so many religious and historical monuments, Romanesque chapels and Baroque churches in the region came was extracted from its mountains – Freita and Montemuro. As the practice of sustainability in all forms (environmental, social, and economy) becomes a more revered topic and gains traction within political spheres, sociologists suggest refining the practices of tourism to fit a mold that is more conducive to the sustainability models. Tazim et al. suggests the key to sustainable tourism lies within the responsible practice of travelers, but also within the direct participation of the locals in tourism practices. Although Geotourism shows to be a viable alternative for mass tourism, reducing the footprint of travellers on different parts of the world, there have been criticisms made regarding its fairness to the local population. Issues of fair pay, and the rights of the local people are the basis of the ethical dilemmas this kind of tourism faces. Tourism has a direct effect on the culture of the local populous, and as such, the focus of sociologists has been how to maintain the local environment (physically, socioculturally, and economically) while at the same time, introduce people to a new culture. See also Circles of Sustainability Geotourism Human rights == References == Drinking is the act of ingesting water or other liquids into the body through the mouth, proboscis, or elsewhere. Humans drink by swallowing, completed by peristalsis in the esophagus. The physiological processes of drinking vary widely among other animals. Most animals drink water to maintain bodily hydration, although many can survive on the water gained from their food. Water is required for many physiological processes. Both inadequate and (less commonly) excessive water intake are associated with health problems. Methods of drinking In humans When a liquid enters a human mouth, the swallowing process is completed by peristalsis which delivers the liquid through the esophagus to the stomach; much of the activity is assisted by gravity. The liquid may be poured from the hands or drinkware may be used as vessels. Drinking can also be by sipping or sucking, typically when imbibing hot liquids or drinking from a spoon. Infants employ a method of suction wherein the lips are pressed tight around a source, as in breastfeeding: a combination of breath and tongue movement creates a vacuum which draws in liquid. In other land mammals By necessity, terrestrial animals in captivity become accustomed to drinking water, but most free-roaming animals stay hydrated through the fluids and moisture in fresh food, and learn to actively seek foods with high fluid content. When conditions impel them to drink from bodies of water, the methods and motions differ greatly among species. Cats, canines, and ruminants all lower the neck and lap in water with their powerful tongues. Cats and canines lap up water with the tongue in a spoon-like shape. Canines lap water by scooping it into their mouth with a tongue which has taken the shape of a ladle. However, with cats, only the tip of their tongue (which is smooth) touches the water, and then the cat quickly pulls its tongue back into its mouth which soon closes; this results in a column of liquid being pulled into the cat's mouth, which is then secured by its mouth closing. Ruminants and most other herbivores partially submerge the tip of the mouth in order to draw in water by means of a plunging action with the tongue held straight. Cats drink at a significantly slower pace than ruminants, who face greater natural predation hazards. Many desert animals do not drink even if water becomes available, but rely on eating succulent plants. In cold and frozen environments, some animals like hares, tree squirrels, and bighorn sheep resort to consuming snow and icicles. In savannas, the drinking method of giraffes has been a source of speculation for its apparent defiance of gravity; the most recent theory contemplates the animal's long neck functions like a plunger pump. Uniquely, elephants draw water into their trunks and squirt it into their mouths. In birds Most birds scoop or draw water into the buccal areas of their bills, raising and tilting their heads back to drink. An exception is the common pigeon, which can suck in water directly by inhalation. In insects Most insects obtain adequate water from their food: When dehydrated from a lack of moist food, however, many species will drink from standing water. Additionally, all terrestrial insects constantly absorb a certain amount of the air's humidity through their cuticles. Some desert insects, such as Onymacris unguicularis, have evolved to drink substantially from nighttime fog. In marine life Amphibians and aquatic animals which live in freshwater do not need to drink: they absorb water steadily through the skin by osmosis. Saltwater fish, however, drink through the mouth as they swim, and purge the excess salt through the gills. Saltwater fishes do drink plenty of water and excrete a small volume of concentrated urine. Hydration and dehydration Like nearly all other life forms, humans require water for tissue hydration. Lack of hydration causes thirst, a desire to drink which is regulated by the hypothalamus in response to subtle changes in the body's electrolyte levels and blood volume. A decline in total body water is called dehydration and will eventually lead to death by hypernatremia. Methods used in the management of dehydration include assisted drinking or oral rehydration therapy. An overconsumption of water can lead to water intoxication, which can dangerously dilute the concentration of salts in the body. Overhydration sometimes occurs among athletes and outdoor laborers, but it can also be a sign of disease or damage to the hypothalamus. A persistent desire to drink inordinate quantities of water is a psychological condition termed polydipsia. It is often accompanied by polyuria and may itself be a symptom of diabetes mellitus or diabetes insipidus. Human water requirements A daily intake of water is required for the normal physiological functioning of the human body. The USDA recommends a daily intake of total water: not necessarily by drinking but by consumption of water contained in other beverages and foods. The recommended intake is 3.7 liters (appx. 1 gallon) per day for an adult male, and 2.7 liters (appx. 0.75 gallon) for an adult female. Other sources, however, claim that a high intake of fresh drinking water, separate and distinct from other sources of moisture, is necessary for good health – eight servings per day of eight fluid ounces (1.8 liters, or 0.5 gallon) is the amount recommended by many nutritionists, although there is no scientific evidence supporting this recommendation. Evidence-based hydration experts say that the amount of drinking water needed depends on ambient temperature, activity level, body size, and sweat rate. Research shows drinking when thirsty will maintain hydration to within about 2% of the needed level. Drinking beyond thirst might be beneficial for people who need to perform tasks that require intense concentration, and those with kidney disease, kidney stones, urinary tract infections, and people with a weak sense of thirst (which may include more older people). Alcoholic beverages The term "drinking" is often used metonymically for the consumption of alcoholic beverages. Most cultures throughout history have incorporated some number of the wide variety of "strong drinks" into their meals, celebrations, ceremonies, toasts and other occasions. Evidence of fermented drinks in human culture goes back as early as the Neolithic Period, and the first pictorial evidence can be found in Egypt around 4,000 BC. Alcohol consumption has developed into a variety of well-established drinking cultures around the world. Despite its popularity, alcohol consumption poses significant health risks. Alcohol abuse and the addiction of alcoholism are common maladies in developed countries worldwide. A high rate of consumption can also lead to cirrhosis, gastritis, gout, pancreatitis, hypertension, various forms of cancer, and numerous other illnesses. See also Eating Hydration (disambiguation) References Bibliography Broom, Donald M. (1981). Biology of Behaviour: Mechanisms, Functions and Applications. Cambridge: Cambridge University Press. ISBN 0-521-29906-3. Retrieved 31 August 2013. Curtis, Helena; Barnes, N. Sue (1994). Invitation to Biology. Macmillan. ISBN 0879016795. Retrieved 31 August 2013. Fiebach, Nicholas H., ed. (2007). Principles of Ambulatory Medicine. Lippincott Williams & Wilkins. ISBN 978-0-7817-6227-4. Retrieved 31 August 2013. Flint, Austin (1875). The Physiology of Man. New York: D. Appleton and Co. OCLC 5357686. Retrieved 31 August 2013. Gately, Iain (2008). Drink: A Cultural History of Alcohol. New York: Penguin. pp. 1–14. ISBN 978-1-59240-464-3. Retrieved 31 August 2013. Mayer, William (2012). Physiological Mammalogy. Vol. II. Elsevier. ISBN 9780323155250. Retrieved 31 August 2013. Provan, Drew (2010). Oxford Handbook of Clinical and Laboratory Investigation. Oxford: Oxford University Press. ISBN 978-0-19-923371-7. Retrieved 31 August 2013. Smith, Robert Meade (1890). The Physiology of the Domestic Animals. Philadelphia, London: F.A. Davis. Retrieved 31 August 2013. External links "Are You Drinking Enough?", recommendations by the European Hydration Institute (Madrid) Superdiversity, or super-diversity, is a social science term and concept often said to have been coined by sociologist Steven Vertovec in a 2007 article in Ethnic and Racial Studies, but which he first used in a BBC article in 2005. Definition and usage The term superdiversity is used to refer to some current levels of population diversity that are significantly higher than before. Vertovec argues superdiversity in Britain "is distinguished by a dynamic interplay of variables among an increased number of new, small and scattered, multiple-origin, transnationally connected, socio-economically differentiated and legally stratified immigrants who have arrived over the last decade". It denotes increased diversity not only between immigrant and ethnic minority groups, but also within them. It has also been called the "diversification of diversity". Vertovec gives the example of Somalis in the United Kingdom, arguing that the Somali community includes British citizens, refugees and asylum seekers, people granted exceptional leave to remain, undocumented migrants, and secondary migrants from other European states. Parveen Akhtar, a sociologist at the University of Bradford, argues that the UK is no longer characterized by diversity but by superdiversity: "Post-1945 you had large waves of immigration from fewer places in the world, largely from the former colonies. Now, since the 1980s, you’ve got smaller waves of immigration from a wider range of places". According to Nasar Meer, "Super-diversity has emerged both as a description of empirical phenomena (the proliferation of diversities) and as a normative claim that increased pluralism (both associated with migration as well as wider changes in our understanding of identity categories) requires social scientists and policy makers to develop approaches to register this". According to Fran Meissner and Steven Vertovec, writing in 2015, the concept of superdiversity has been the subject of "considerable attention" since Vertovec introduced it in 2005. They note that Vertovec's 2007 article in Ethnic and Racial Studies is the most cited article in the history of the journal. The concept has started to influence the fields of sociolinguistics and linguistic anthropology. Criticism Some authors are critical of the concept of superdiversity. Sinfree B. Makoni argues that the concept "contains a powerful sense of social romanticism, creating an illusion of equality in a highly asymmetrical world, particularly in contexts characterized by a search for homogenization...I find it disconcerting, to say the least, to have an open celebration of diversity in societies marked by violent xenophobia, such as South Africa". Ana Deumert argues: "The use of 'superdiverse' as a descriptive adjective, is a theoretical cul-de-sac, because the complexities brought about by diversity in the social world ultimately defy numerical measurement". The claims of increased migrations and diversity have been challenged by Czajka and de Haas (2014). They observe that while globally the number of migrants has increased so has the world population so the proportion of migrants has actually decreased. In the Americas migration has increased but diversity has not. The fact that migrations have centered on a "shrinking pool of prime destination countries" (many of them small countries in Western Europe) led them to conclude that "the idea that immigration has become more diverse may partly reveal a Eurocentric worldview". Aneta Pavlenko argues that superdiversity is an exercise in academic branding which fails as an academic term: The uptake of the slippery slogan is not surprising. The aesthetic appeal of truthiness and the illusion of novelty, contemporaneity and relevance undoubtedly explain some of the attraction yet we cannot ignore the fact that the advent of superdiversity provided scholars of multilingualism with a new means to move up the academic ladder, distinguish their publications, and fund their work. Researchers and research institutes Key researchers working on superdiversity include Vertovec, Jan Blommaert and Jenny Phillimore. The University of Birmingham established the Institute for Research into Superdiversity in 2013. The Max Planck Institute for the Study of Religious and Ethnic Diversity in Göttingen is also an important centre for superdiversity research. New Zealand's "Superdiversity Stocktake: Impact on Business, Government and on New Zealand" was launched in November 2015 and sponsored by banks, companies, the Human Rights Commission, and the Ministry of Education. A study on "Implications of Superdiversity for NZ's Electoral Laws and Democracy" was also launched. Both projects were carried out by the Superdiversity Centre for Law, Policy and Business, which describes itself as "a multidisciplinary centre specialising in analysing the law, policy and business implications of New Zealand's superdiversity". Its patron is Sir Anand Satyanand and its chair is Mai Chen. See also Cultural diversity Multiculturalism References General references Karel Arnaut; Martha Sif Karrebæk; Massimiliano Spotti; Jan Blommaert (24 November 2016). Engaging Superdiversity: Recombining Spaces, Times and Language Practices. Channel View Publications. pp. 1979–. ISBN 978-1-78309-681-7. Budach, Gabriele; Saint-Georges, Ingrid de (2017). Superdiversity and language. Routledge Handbooks Online. p. Chapter 3. doi:10.4324/9781315754512. ISBN 9781138801981. External links Institute for Research into Superdiversity The Superdiversity Centre for Law, Policy and Business Chinoiserie in fashion refers to the any use of chinoiserie elements in fashion, especially in American and European fashion. Since the 17th century, Chinese arts and aesthetic were sources of inspiration to European artists, creators,: 52 and fashion designers when goods from oriental countries were widely seen for the first time in Western Europe.: 546 Western chinoiserie was also often mixed with other exotic elements which were not all indigenous to China.: 15 Throughout its history, chinoiserie in fashion was sometimes a display of cultural appreciation; but at times, it was also associated with exoticism, Orientalism, cultural appropriation, Western imperialism, and colonialism,: 16–19 and eroticism. The imagining of China was always more fanciful than real. Trade provided products, but even more importantly, the West copied the Oriental land that it had never conquered. It never possessed the dragons, butterflies, or pagodas that it admired and emulated. If it was an unrequited colonialism, the West's passion for China abides today in the continuing aesthetic fascination for that Far East land History Pre-17th century China traded luxury goods to Europe from ancient times,: 546 and early contacts of Europeans with China also directly influenced European fashion.: 4 Silk from China, as well as textiles from India and Turkey, were extremely popular among European royalty.: 4 The art of sericulture itself originated in China and was introduced to the West via the Byzantine Empire.: 90 The secret of sericulture was eventually smuggled out of China in the 6th century by the Byzantine Empire and then became an important component of Byzantine industry: 95 allowing the Byzantine empire to gain a monopoly of silk in Europe.: 122 From the eleventh century CE, the art of sericulture spread to Italy and to Southern France.: 90 However, the import of raw silk from China continued to remain significant.: 95 During the Italian Renaissance period (14th to 17th centuries), Italians saw Imperial China as a refined civilization which was equal to Europe (except for religion) and very advanced in terms of science, technology, architecture, and culture; as such, Italian élites would dress in Chinese fashion to show off their wealth. These Chinese influences in fashion were illusions created by Italian craftsmen who had started to produce in Lucca and had appropriated Chinese cultural symbols, such as lotus flowers, pomegranates, peonies, florets, phoenixes and dragons. Chinese silk which was manufactured in China to appeal to European taste continued to be imported into Europe; this import increased even more in the late-17th century as traders established direct maritime links between China and Europe.: 90 The introduction of items, such as painted silk, pearls, and umbrellas, from China was also sped up in the 1400s through the sea routes.: 437 In the 16th century, Europe imported Chinese brocades to make the vestments for priests in Roman Catholic cathedrals.: 618 According to British records dating to the late-19th century, gold foil was the ordinary form of precious metal which was used in embroidery and was a Chinese invention wherein Chinese people invented the process of laying a thin gold leaf on paper before rolling it around a silk thread. Chinese gold-thread technology was later introduced in the West and adopted by Italian weavers in their goldwork. 17th to 18th century The 17th to 18th centuries, Western fashion was greatly enriched by the various items which were imported from the East which led to the introduction of new patterns and new possibilities in Western dress and was immediately imitated by mills found in England and France. As China was considered as the greatest empire in the 17th and 18th century, China and chinoiserie became in vogue in Europe; chinoiserie in this period, however, was the result of a conscious attempt in making "oriental culture" acceptable to the taste of Europeans.: 116 : 152 17th century In the 17th century, Chinese luxury items, such as Chinese textiles and porcelain, were introduced in Italian port cities, Portugal, England, and Holland; these items were what Europeans used to informed themselves about the customs and cultures of the East.: 15 Imported porcelain from China depicted how clothing was worn in China while Imported Chinese textiles led to fascination in Europe due to the technical skills found in the weaving, hand-painting, and needlework of Chinese silk.: 15 Chinese textiles were readily tailored into Western-style garments.: 15 The large amounts of imported Chinese patterned silk textiles in the Western-sphere also influenced the Europeans' perception of Chinese designs; this became known as chinoiserie. Chinoiserie, however, was the result of the European's misunderstandings of authentic Chinese art and life. Not only did Europe imported Chinese textiles, but they also imitated Chinese textiles.: 16 Moreover, import of textiles from Asia by the East India companies in the late 17th and early 18th centuries influenced European designs creating a "bizarre style" as designs and motifs were blended into strange and familiar motifs and was influenced by chinoiserie and Japonisme.: 91 18th century In the 18th century, China was tremendously popular in France, leading to what was referred as the "Oriental Renaissance" by Edgar Quinet in 1848.: 301 From this period and throughout the 19th century, chinoiserie was especially celebrated in France, and the origin of most Chinese-inspired fashion was French during this period. French Chinese fashion, which involved the wearing of petticoats with frills, was also introduced in England where it became fashionable among British women; it is however unknown if British women were aware that they were wearing French Chinese fashion.: 544 This craze for China was also shared by England which also showed an obsession for Chinese culture objects in the 18th century.: 152 Chinoiserie was also a popular theme in masquerade balls, and King Gustav III of Sweden was even dressed in Chinese robes by the Swedish royal family at some point in his lifetime when they were at the summer palace in Drottningholm.: 113 The craze for chinoiserie however started to wane in England in the second half of the eighteenth century: 152 and further receded in Europe during the 19th century.: 301 19th century As a result of Europe being at the wake of industrialization, and due to Europeans' perception that Chinese civilization was almost outdated following the first and the second Opium Wars lead to the decrease of chinoiserie popularity in Europe.: 301 However, this period was marked by an era of universal colonial exchanges and exposure to various categories found in Orient, such as textiles (e.g. silk) from China and Chinese dress elements (e.g. the precursor of the cheongsam). Many items were looted from China and brought back to Europe during this period. The Old Summer Palace, known as Yuanmingyuan (traditional Chinese: 圓明園; simplified Chinese: 圆明园; pinyin: Yuánmíng Yuán; lit. 'Gardens of Perfect Brightness') in Chinese, in particular, which was sacked by Anglo-French forces in 1860s gained the "mythical status as a source of Chinese objects in the West".: 239 From the looting of the Old Summer Palace, the French not only looted the imperial treasures, but also forced open the imperial warehouses stealing shiploads of clothing, jewellery, hats, and rolls of fabrics, amongst many other items.: 260 : 19 Looted items from the Old Summer Palace also flooded the markets of Britain; a cap which was said to have belonged to the Chinese emperor was presented to Queen Victoria, along with a pekingese dog, which became known as Looty.: 19 In Europe, these looted items were sometimes cut into a western-style clothing.: 239 At the end of the 19th century, British chinoiserie fashion had incorporated key elements from the construction design of Chinese clothing, including the use of wide sleeves and side closure.: 239 However, their passion of the British for chinoiserie had vanished.: 152 On the other hand, the 19th century was when chinoiserie was fully developed in America as a kind of "aesthetic colonialism" associating China with exoticism and fantasy, perceiving it as "a fantastic, uncivilized nation"; the upper classes, especially those in New England and the Middle Colonies, imitated chinoiseriee fashion; following their independence from Britain, they eventually ventured to China where they directly imported Chinese items.: 38 The late 1800s was thus marked with Westerner's fascination to the Far East, especially China and Japan, including in Canada. In the 1850s, there was a deliberate and self-conscious usage of Chinese materials and symbols in the design of dresses.: 16 Floral medallions, for example, were used on dresses as they were characteristics of China.: 17 A second wave of looted items from the suppression of the Boxer uprising (1899–1901) also made its way to Britain. During the suppression of the Boxer Uprising, many places were looted, including many pawnshops in Beijing.: 54 Clothing items by far were the largest-volume trade in these pawnshops, but they also had other items of value, such as jewellery, watches, furniture, rickshaws, and musical instrument; these items were personal items of Beijing commoners who had exchanged their personal items for a small sum of money and intended to redeem their items later when they would be in better financial times.: 54 Wearing Chinese clothing at home in the West was not deemed as being done out of frivolity or fancy, but was itself an imperial act which signified having worldly knowledge.: 17 20th century In the early 20th century, European and fashion designers would use China and other countries outside of the Eurocentric-fashion world to seek inspiration; Vogue Magazine also acknowledged that China had contributed to the aesthetic inspiration to global fashion. Chinese motifs regrew popular in European fashion during this period.: 239 China and the Chinese people also supplied the materials and aesthetics to American fashion and influenced global fashion; however, they remained perceived as being fashion-less and did not fit the criteria of modern status. For example, in the early 1900s, Vogue magazine encouraged people to buy beautifully embroidered Chinese garments made of high quality silk in Chinatowns, which were sold as cheap items in America; however, many of these items were actually looted items from Beijing during the suppression of the Boxer Uprising. From the 1910s in the United Kingdoms, Chinese robes, which were perceived as being only suitable as a fancy or luxurious dress or a source of embroidery pieces, started to be worn by British women as a form of loose coats. 1920s to 1930s The 1920s was marked by the return of a great craze for chinoiserie. Genuine embroidered Chinese jackets and coats were worn as evening wear. The loose fitting cut of British women garments in the 1920s also reflects the influence of Chinese clothing. The cheongsam was created in the 1920s and was turned into a high-style evening wear when it was appropriated by the West.: 19 By the 1930s, the cheongsam was associated with Chinese dress and was used in Hollywood movies as the identifying clothing of Chinese women.: 19 When worn by Asian Hollywood stars, such as Anna May Wong, the sexualized version cheongsam was turned into a symbol of the exotic and erotic nightlife in Shanghai.: 269 1940s to end of 20th century In the mid-20th century, chinoiserie influenced the designs of great designers and/or couturiers, such as Christian Dior and Yves Saint-Laurent.: 187 On 23 February 1981, Princess Diana wore a red coloured silk, midi Chinese skirt known as mamianqun when she posed with Prince Charles at Clarence House prior to their official engagement announcement. This Chinese skirt was in the Qing dynasty langan style and was embroidered with chrysanthemum embroidery motifs. and had a red waistband. The use of auspicious red colour was in line with Chinese wedding tradition; however, the skirt was not considered fully auspicious according to Chinese beliefs as it lacked a white waistband instead of a red one. A mamianqun with white waistband was usually worn by Chinese bride to symbolize: "to grow old together", which Princess Diana lacked; and thus, Princess Diana's mamianqun was did not conform to the guiju (Chinese: 规矩; lit. 'established rules') and was instead considered buxiangde yuzhao (Chinese: 不祥的预兆; pinyin: bùxiángde yùzhào; lit. 'inauspicious omen'), a sign of bad omen. 21st century Chinoiserie fashion continues to appears in the work of fashion designers and directive creators of luxury brands in the 21st century. For instances, chinoiserie appeared have been a key seasonal influence to Louis Vuitton Spring/ Summer 2011 collections; for example, with the use of brisé fan by Marc Jacobs, etc. The Valentino Fall/Winter 2015–2016 depicted the use of colourful Chinese motifs, such as lion's heads, flowers, plants, in the embroidery work on their clothing and handbags, which were described as "reinterpretations of symbols representing human qualities and spiritual values" by the Magazine Vogue. Designers Some famous fashion designers and/or creative directors, who are known to have adopted or incorporated chinoiserie aesthetics at some point in their fashion collection, include Mariano Fortuny, the Callot Soeurs who were known for their usage of Chinese silks, Chinese-style embroideries, had Orientalism as their favourite theme, Jean Paquin,: 4 Paul Poiret, Jeanne Lanvin,: 17–19 Christian Dior, Yves Saint-Laurent, Alexander McQueen,: 113 John Galliano,: 9 Tom Ford,: 270 and Maria Grazia Chiuri. Chinoiserie continues to appears in fashion creation in present-days. Luxury fashion brands such as, Louis Vuitton, Dior, and Chanel, etc., were also inspired by Chinese art and aesthetics, these influences are sometimes reflected in their creation of colours and the patterns found on their fabrics.: 52 Christian Dior Christian Dior, who had never travelled to China, especially celebrated Chinese aesthetics since the 1947; Chinese aesthetics in his design collections were influenced by Chinese overcoats and have been inspired by the "exotic" (chinoiserie) home decor of his childhood; throughout the 1960s, Dior used various cultural references to China, such as Chinese calligraphy, the silhouette of the cheongsam, and the Tang dynasty blue and white porcelain in various of his collections. Yves Saint-Laurent Like Christian Dior, Yves Saint-Laurent was very inspired by Chinese culture although he never visited China; this is also reflected in his 1977's collection "Les Chinoises":: 187 Beijing, however, remains a dazzling memory. The China that I had so often interpreted in my designs was exactly as I had imagined it. All I need for my imagine to blend into a place or a landscape is a picture book. … I don't feel any need to go there. I have already dreamt about it so much. Sources of fashion inspiration Chinese auspicious ornaments and textile The most visible form of chinoiserie is through the appropriation of Chinese decorative (and auspicious) motifs and styles.: 55 During the Italian Renaissance, Italian craftsmen appropriated Chinese cultural and auspicious symbols, such as the lotus flowers, pomegranates, peonies, florets, phoenixes and dragons in their textiles which were then used in fashionable dressmaking for the wealthy Italian social class. Chinese motifs also grew in popularity in European fashion in the 20th century.: 239 Textile obtained through imperialistic appropriation Dragon robes (and python robes) of the Qing dynasty were highly regulated by the Qing dynasty's Sumptuary laws and court and the workshops and storehouses were managed by the Qing Imperial Household Department.: 243–245 They were also typically bestowed by the Qing dynasty court to important people within the Qing Empire boundaries, such as Mongolia and Tibet as diplomatic gifts, who were allowed to cut and adapt to fit their own customs.: 245 In chinoiserie fashion of the early 20th century, the dragon robes (and python robes) were at times cut and converted into Western-style attire, such as banyan and waistcoat; however, the direct alterations of Chinese garments for the use of Westerners are sometimes regarded as "imperialistic appropriation".: 243–247 Some of these adapted dragon robe clothing were possibly fabric rolls and/or clothing looted from the Old Summer Palace contrary to what museum donors sometimes wish explain about their origins.: 243–245 During the Opium wars, the use of Chinese dragons robes by Europeans in the late Victorian Europe were sometimes used to mock Chinese masculinity; for example, George Smith in the painting The Rightful Heir, exhibited in 1874 in the Royal Academy, would paint the villain found in the painting wearing a Chinese dragon robe tied with a belt around the waist with slippers on his feet.: 246 In similar instances, Liberty in 1898 offered evening capes which were advertised as being made of "Mandarin robes" (i.e. Qing dynasty court dress); however, these capes were actually made of Han Chinese women's traditional skirts.: 248 In 1981, Blue and white porcelain The combination of blue and white colour is one of the most popular colour palette combination in history and originated from Asian ceramics of the 9th century. Chinese blue and white porcelain, which was developed since the Tang dynasty and fully matured in Yuan dynasty, and are one of the most nationalistic arts of China, often appears in modern fashion shows.: 513–514 This colour palette found in ceramics later spread in Europe and influenced the Delftware in the 16th century and Willow pattern created by British manufacturers in the later 18th century; the 18th century was also the era when printed fabrics such as blue and white Toile de Jouy gained popularity and inspired fashion designers to use the blue and white as a prominent colour palette in the coming year. It was thus adopted in fashion designs of garments and shoes of famous fashion designers, such as Christian Dior, Valentino, Dr Martens. Some modern fashion designers, such as Roberto Cavalli, Guo Pei, were also directly inspired by Chinese blue and white porcelain. Adoption of Chinese garments, clothing elements, and construction British chinoiserie fashion had incorporated key elements from the construction design of Chinese clothing, including the use of wide sleeves and side closure; these designs were then adapted to meet the aesthetic tastes of Europeans.: 239 Chinese fashion also influenced various designs and styles of déshabillé. The design of wrap-style closure or neckline, known as jiaoling (Chinese: 交領; lit. 'intersecting collar') in China, in European garments was the results of the heavy influences of Orientalism which was popular in the 19th century. Chinese jackets with wrap closure also influenced American fashion in the early 1900s; an example of such jacket is the San toy (#4777), which appeared in American women's magazine, The Delineator, in 1901. In volume 57, The Delineator described it as being "Ladies' Chinese dressing" or as a "Lounging sack", and as having "a strong suggestion of the Orient".: 216–217 The San toy was designed to be loose-fitting, a wrap closure on the left side (known as jiaoling zuoren in China) which closes with satin ribbon ties; it also featured deep side vents, which was considered as being a "novel effect", and was trimmed with a single band creating a fancy outline.: 206, 217 The San toy of Volume 57 (#4777) reappeared in Volume 58 of The Delineator along with another Chinese-style inspired wrap top (#3920), one of which closed on the right side (known as jiaoling youren in China) with a single ribbon.: 152 The Ladies' Chinese dressing sac #3920 appeared at least a year earlier and was published in Volume 56 of The Delineator of 1900. In the 1910s, Euro-American women showed women in Chinese robes used as loose evening coats over dresses.: 248 Among the items which were advertised by Vogue in its 15 December 1911 publication, there was the aoqun, which composed of the ao a type of Chinese jackets, and the Qing dynasty-style mamianqun, a traditional skirt of the Han Chinese. There was also a fashion trend for day-wear jackets and coats to be cut in styles which would suggest various Chinese items as was published the Ladies’ Home Journal in June 1913. According to the Ladies’ Home Journal of June 1913, volume 30, issue 6:Interest in the political and civic activities of the new China, which is more or less world-wide at this time, led the designers of this page [p.26] and the succeeding one [p.27] to look to that country for inspiration for clothes that would be unique and new and yet fit in with present-day modes and the needs and environments of American women [...] Garments displayed from The Chinese Summer Dress published in the Ladies’ Home Journal of June 1913, volume 30, issue 6, show influences of the Qing dynasty mandarin court gown, especially the bufu (a mandarin court dress with a mandarin square badge), the jiaoling ruqun, kanjia, mamianqun, yunjian, yaoqun (a short waist-length overskirt), piling (collar in Qing dynasty court dress), chenyi and changyi (Manchu women dresses), ao and gua, as well as traditional Chinese embroideries, and traditional Chinese lào zi, pankou, Mandarin collars, etc. There are also photographic evidences of Chinese robes being used outside its wearer's home as fashion items with little or no adaption from the 1920s.: 248 The loosening of women's fashion found in the 1920s loose-fitting fashion, especially the disappearance of nipped-in corset, appears to have also been influenced by the loose lines and roomy armholes of the traditional Chinese robes and jackets along with other factors, such as the experience of freedoms of elite women at that time, the sportswear-designs of Chanel, and the garment designs by Paul Poiret who designed Middle-Eastern inspired garments. Cheongsam The cheongsam was created in the 1920s and was originally a symbol of women emancipation in China; when it was appropriated by the West, it was turned into a high-style evening wear.: 19 In the 21st century, some evening dresses designed by Tom Ford showed the influences of the sexualized version cheongsam in terms of cut and the imperial five-clawed Chinese dragon robes in terms of use of colour (e.g. imperial yellow) and Chinese motifs (such as xiangyun clouds, Lishui, and the Twelve Ornaments), as well as the Manchu's horsehoof cuffs.: 269–270 Chinese shawls Chinese shawls were popular among European elite style leaders in the early 20th century. However, in a report dating to 1921 written by Vogue, it was referred as Spanish shawls, and readers were informed that these shawls were imported from Venice, Spain, Persia, and the Philippines, while omitting the initial Chinese importation of these shawls when earlier importers of Chinese goods and other travellers to China were key sources for these shawls twenty years prior to the publication of the report. The Spanish shawls, also known as Manila shawls and mantón de Manila, have become traditional accessory for women in Spain and Latin America and is also a crucial feature in Flamenco dance costume. The term Manila shawl itself is a misnomer, which appeared when the America-European people got confused concerning the origins and provenance of the shawl, thus leading to a misattribution to the Philippines.: 252 These shawls of Chinese origins then became identified with Spanish ladies.: 137 The Chinese shawls were manufactured in Guangdong province, China and were then introduced in Mexico and Spain from the seaport of Manila, which was where goods from Asia (including various forms of items manufactured in Guangdong) could be exported to Mexico and Europe. These shawls became a popular fashion accessory for women in Spain and Latin America after the year 1821.: 137 : 252 The demand for these Chinese shawls grew so much that it led to an increase in production from Chinese factories; and simultaneously, local embroiderers from Spain started to embroider their own. Despite the emerging local production in Spain, a large amount of Manila shawls continued to be manufactured in China for the sole purpose of the export market. The popularity of these shawls (which were actually still being produced in China) in the 19th century Europe eventually resulted in the adoption of the Chinese shawls in the traditional Spanish clothing attire. With time and through various form cultural exchanges with other cultures, the Spanish shawls developed into its current style through the exposure and interaction of different cultures. Chinese shoes Chinese shoes have influenced the design of European slippers with turned-up toes and with small low heels of the late 1880s.: 284 In the early 20th century, Chinese slippers, which were manufactured in China for American trade, were exported and sold in American stores; however, the fine grade Chinese slippers were never sold to Chinese people in America instead they were sold to American women as boudoir shoes.: 21 On the other hand, local Chinese shoe companies in America would mainly sell shoes to Chinese people.: 21 Controversies Lack of fashion myth, Western Imperialism, and Orientalism Though Chinese fashions had a global influence, the Chinese themselves were still perceived as being fashion-less when they did not fit the criteria of fashionable modernity. Europeans had visited imperial China since the 1500s at the times of the Ming dynasty and the difference of fashion was one of the first thing that they noticed.: 204 "Clothing never changed in China" became a myth constructed by early European writers and foreign sojourners who visited Imperial China but lacked knowledge on Chinese fashion of the previous decades.: 204 European writers at least since the 18th century, such as Jean-Baptiste Du Halde, Fernand Braudel, had held opinions that China had a static fashion.: 80 However, the descriptions of Chinese fashion by Europeans from the 16th to the 18th centuries were mainly based on their perceptions of the Chinese clothing that they saw, instead of describing Chinese garments itself.: 9 In the 18th century, Jean-Baptiste Du Halde, for example, had identified fashion as being a key difference between Europe and ancient China is the lack of changing fashion in China in his publications:: 80 As for what is here called Fashion, it has nothing at all in it like what we call so in Europe, where the manner of Dress is subject to many changesDu Halde's claims of the static fashion of China was later circulated along with his publications and consolidated the belief that Chinese people dressed in fashion-less robes in the imagination of the Europeans.: 80 Ironically, Du Halde actually never went to Imperial China; however, to strengthen the veracity of his claims, Du Halde paired these images of engravings of Chinese with exhaustive descriptions of Chinese customs and relied on the accounts of other Jesuit missionaries.: 80 Similar accounts continued to appear at different point of time. Western Imperialism also often accompanied Orientalism, and European imperialism was especially at its highest in the 19th century.: 10 In the 19th century time, Europeans described China in binary opposition to Europe, describing China as "lacking in fashion" among many other things, while Europeans deliberately placed themselves in a superior position when they would compare themselves to the Chinese: 10 as well as to other countries in Asia:: 166 Latent orientalism is an unconscious, untouchable certainty about what the Orient is, static and unanimous, separate, eccentric, backward, silently different, sensual, and passive. It has a tendency towards despotism and away from progress. [...] Its progress and value are judged in comparison to the West, so it is the Other. Many rigorous scholars [...] saw the Orient as a locale requiring Western attention, reconstruction, even redemption.Works by Europeans writers which were influenced by Orientalist ideas would depict China as lacking fashion and by extension construct China as a static and unchanging nation.: 238 Compared to the Chinese, the Europeans would therefore describe themselves as "not superstitious, backwards, unhygienic, effeminate, or slavish".: 10 Foot binding, in particular, fuelled the imaginations of the Europeans and the Americans who perceived China as being "a mysterious, exotic, and barbaric Orient" where bound feet of the Chinese women became a representative of the "Chinese barbarity" and as signs of female oppression. Similar ideas were also applied to other countries in the East Asia, in India, and Middle East, where the perceived lack of fashion were associated with offensive remarks on the Asian social and political systems:: 187 I confess that the unchanging fashions of the Turks and other Eastern peoples do not attract me. It seems that their fashions tend to preserve their stupid despotism. Accusation of cultural appropriation and plagiarism 2022 Mamianqun and new Dior skirt from fall 2022 collection: In July 2022, Dior first was accused of cultural appropriation and design plagiarism of the traditional Han Chinese skirt, mamianqun. Dior was accused of cultural appropriation for a second time in July 2022 for due to its usage of pattern print which looks like the huaniaotu (Chinese: 花鳥圖; lit. 'bird-and-flower painting'), into its 2022 autumn and winter ready-to-wear collection and has been introduced as being Dior's signature motif Jardin d'Hiver which was inspired by Christian Dior's wall murals. The huaniaotu is a traditional Chinese painting theme which belong to the Chinese scholar-artist style in Chinese painting and originated in the Tang dynasty. Related content Korea: Chinese influences on Korean clothing Japan: Kimono, Ryusou See also Fashion Chinese clothing: Hanfu, Qizhuang, cheongsam Major historical events in Chinese fashion history: Tifayifu; Hanfu Movement Gallery Chinoiserie fashion Sources of inspiration and materials in chinoiserie fashion Notes == References == Liberal arts education (from Latin liberalis 'free' and ars 'art or principled practice') is a traditional academic course in Western higher education. Liberal arts takes the term art in the sense of a learned skill rather than specifically the fine arts. Liberal arts education can refer to studies in a liberal arts degree course or to a university education more generally. Such a course of study contrasts with those that are principally vocational, professional, or technical, as well as religiously based courses. The term liberal arts for an educational curriculum dates back to classical antiquity in the West, but has changed its meaning considerably, mostly expanding it. The seven subjects in the ancient and medieval meaning came to be divided into the trivium of rhetoric, grammar, and logic, and the quadrivium of astronomy, arithmetic, geometry, and music. Since the late 1990s, major universities have gradually dropped the term liberal arts from their curriculum or created schools for liberal art disciplines to categorize programs outside of science and technology. Common rebrandings for liberal arts colleges and schools include: arts and social sciences, arts and sciences and humanities. The name changing at American institutions comes as the result of modern statistics suggesting a Liberal Arts degree offers graduates a considerably lower income when compared to science and technology graduates. Despite the rebranding, liberal arts degrees from today's universities and colleges traditionally include the following disciplines: Anthropology, English, Literature, Fine arts, Foreign languages, Philosophy, Psychology, Sociology, Music, Journalism, Economics, Law, Communications, Architecture, Creative arts, Art, and History. Degrees in Liberal studies are often confused with those in a liberal arts discipline. Liberal studies refers to degrees with a broad curriculum, across multiple liberal arts disciplines or sciences and technologies. History Before they became known by their Latin variations (artes liberales, septem artes liberales, studia liberalia), the liberal arts were the continuation of Ancient Greek methods of inquiry that began with a "desire for a universal understanding." Pythagoras argued that there was a mathematical (and geometric) harmony to the cosmos or the universe; his followers linked the four arts of astronomy, arithmetic, geometry, and music into one area of study to form the "disciplines of the mediaeval quadrivium". In the 4th-century-BC Athens, the government of the polis, or city-state, respected the ability of rhetoric or public speaking above almost everything else. Eventually rhetoric, grammar, and dialectic (logic) became the educational programme of the trivium. Together they came to be known as the seven liberal arts. Originally these subjects or skills were held by classical antiquity to be essential for a free person (liberalis, "worthy of a free person") to acquire in order to take an active part in civic life, something that included among other things participating in public debate, defending oneself in court, serving on juries, and participating in military service. While the arts of the quadrivium might have appeared prior to the arts of the trivium, by the Middle Ages educational programmes taught the trivium (grammar, logic, and rhetoric) first while the quadrivium (arithmetic, geometry, music, astronomy) were the following stage of education. Rooted in the basic curriculum – the eukuklios paideia or "well-rounded education" – of late Classical and Hellenistic Greece, the "liberal arts" or "liberal pursuits" (Latin liberalia studia) were already called so in formal education during the Roman Empire. The first recorded use of the term "liberal arts" (artes liberales) occurs in De Inventione by Marcus Tullius Cicero, but it is unclear if he created the term. Seneca the Younger discusses liberal arts in education from a critical Stoic point of view in Moral Epistles. The exact classification of the liberal arts varied however in Roman times, and it was only after Martianus Capella in the 5th century influentially brought the seven liberal arts as bridesmaids to the Marriage of Mercury and Philology, that they took on canonical form. The four "scientific" artes – music, arithmetic, geometry, and astronomy – were known from the time of Boethius onwards as the quadrivium. After the 9th century, the remaining three arts of the "humanities" – grammar, logic, and rhetoric – were grouped as the trivium. It was in that two-fold form that the seven liberal arts were studied in the medieval Western university. During the Middle Ages, logic gradually came to take predominance over the other parts of the trivium. In the 12th century the iconic image – Philosophia et septem artes liberales (Philosophy and seven liberal arts) – was produced by an Alsatian nun and abbess Herrad of Landsberg with her community of women as part of the Hortus deliciarum. Their encyclopedia compiled ideas drawn from philosophy, theology, literature, music, arts, and sciences and was intended as a teaching tool for women of the abbey. The image Philosophy and seven liberal arts represents the circle of philosophy, and is presented as a rosette of a cathedral: a central circle and a series of semicircles arranged all around. It shows learning and knowledge organised into seven relations, the Septem Artes Liberales or Seven Liberal Arts. Each of these arts find their source in the Greek φιλοσοφία, philosophia, literally "love of wisdom". St. Albert the Great, a doctor of the Catholic Church, asserted that the seven liberal arts were referred to in Sacred Scripture, saying: "It is written, 'Wisdom hath built herself a house, she hath hewn her out seven pillars' (Proverbs 9:1). This house is the Blessed Virgin; the seven pillars are the seven liberal arts." In the Renaissance, the Italian humanists and their Northern counterparts, despite in many respects continuing the traditions of the Middle Ages, reversed that process. Re-christening the old trivium with a new and more ambitious name: Studia humanitatis, and also increasing its scope, they downplayed logic as opposed to the traditional Latin grammar and rhetoric, and added to them history, Greek, and moral philosophy (ethics), with a new emphasis on poetry as well. The educational curriculum of humanism spread throughout Europe during the sixteenth century and became the educational foundation for the schooling of European elites, the functionaries of political administration, the clergy of the various legally recognized churches, and the learned professions of law and medicine. The ideal of a liberal arts, or humanistic education grounded in classical languages and literature, persisted in Europe until the middle of the twentieth century; in the United States, it had come under increasingly successful attack in the late 19th century by academics interested in reshaping American higher education around the natural and social sciences. Similarly, Wilhelm von Humboldt's educational model in Prussia (now Germany), which later became the role model for higher education also in North America, went beyond vocational training. In a letter to the Prussian king, he wrote: There are undeniably certain kinds of knowledge that must be of a general nature and, more importantly, a certain cultivation of the mind and character that nobody can afford to be without. People obviously cannot be good craftworkers, merchants, soldiers or businessmen unless, regardless of their occupation, they are good, upstanding and – according to their condition – well-informed human beings and citizens. If this basis is laid through schooling, vocational skills are easily acquired later on, and a person is always free to move from one occupation to another, as so often happens in life. The philosopher Julian Nida-Rümelin has criticized discrepancies between Humboldt's ideals and the contemporary European education policy, which narrowly understands education as a preparation for the labor market, arguing that we need to decide between "McKinsey and Humboldt". Modern usage The modern use of the term liberal arts consists of four areas: the natural sciences, social sciences, arts, and humanities. Academic areas that are associated with the term liberal arts include: Life science (biology, neuroscience) Physical science (physics, astronomy, physical geography, chemistry, earth science) Formal science (logic, mathematics, statistics) Humanities (philosophy, history, english literature, the arts) Social science (economics, political science, human geography, linguistics, anthropology, psychology, sociology) For example, the core courses for Georgetown University's Doctor of Liberal Studies program cover philosophy, theology, history, art, literature, and the social sciences. Wesleyan University's Master of Arts in Liberal Studies program includes courses in visual arts, art history, creative and professional writing, literature, history, mathematics, film, government, education, biology, psychology, and astronomy. Secondary school Liberal arts education at the secondary school level prepares students for higher education at a university. Curricula differ from school to school, but generally include language, chemistry, biology, geography, art, mathematics, music, history, philosophy, civics, social sciences, and foreign languages. In the United States In the United States, liberal arts colleges are schools emphasizing undergraduate study in the liberal arts. The teaching at liberal arts colleges is often Socratic, typically with small classes; professors are often allowed to concentrate more on their teaching responsibilities than are professors at research universities. In addition, most four-year colleges are not devoted exclusively or primarily to liberal arts degrees, and offer STEM programs. In fact, STEM graduates at liberal arts colleges have been demonstrated to be more likely to apply to graduate school in STEM than their peers and make up a higher proportion of National Academy of Science members than would usually be expected for the number of STEM graduates produced by an institution. Traditionally, a bachelor's degree in one particular area within liberal arts, with substantial study outside that main area, is earned over four years of full-time study. However, some universities such as Saint Leo University, Pennsylvania State University, Florida Institute of Technology, and New England College have begun to offer an associate degree in liberal arts. Colleges like the Thomas More College of Liberal Arts offer a unique program with only one degree offering, a Bachelor of Arts in Liberal Studies, while the Harvard Extension School offers both a Bachelor of Liberal Arts and a Master of Liberal Arts. Additionally, colleges like the University of Oklahoma College of Liberal Studies and the Harvard Extension School offer an online, part-time option for adult and nontraditional students. Most students earn either a Bachelor of Arts degree or a Bachelor of Science degree. Great Books movement In 1937 St. John's College radically shifted its curriculum to focus on the Great Books of the Western World, aiming to provide a form of liberal arts education that stood apart from increasingly specialized nature of higher education. This new approach emphasized a broad-based education rooted in classical texts from philosophy, literature, science, and other disciplines, in contrast to the growing trend toward technical and vocational training in universities. In 1952, Encyclopædia Britannica published a 54 volume set titled the Great Books of the Western World under the direction of Robert Hutchins and Mortimer Adler. This monumental work was designed to serve as a comprehensive anthology of the foundational texts of Western civilization, spanning authors from Homer and Plato to Shakespeare and Newton. The collection aimed to promote critical thinking and engagement with the ideas that have shaped Western thought. In 1990, a second edition was released, expanding the collection to 60 volumes and updating its content to reflect more contemporary works and scholarship. In recent years, there has been a revival of interest in the Great Books and the broader Liberal Arts tradition within some contemporary Muslim educational institutions. Notably, neo-traditional Muslim scholars like Shaykh Hamza Yusuf and Shaykh Abdal Hakim Murad (also known as Tim Winter) have advocated for incorporating the study of these classical works into the curriculum. These scholars emphasize that Muslims historically engaged deeply with the classical liberal arts, particularly the trivium (grammar, rhetoric, and logic) and quadrivium (arithmetic, geometry, music, and astronomy), both of which are foundational to the Western liberal arts tradition. Institutions like Zaytuna College in the United States and the Cambridge Muslim College in the United Kingdom have integrated elements of the Great Books and the liberal arts into their educational models, fostering a greater appreciation for the interconnectedness of intellectual traditions across cultures. These colleges encourage students to study classical Islamic texts alongside Western works, fostering a holistic education that draws on both Islamic and Western intellectual heritage. The engagement with the trivium and quadrivium in these institutions highlights the enduring value of liberal arts education, not only in Western contexts but also within broader, more global educational traditions. In Europe In most parts of Europe, liberal arts education is deeply rooted. In Germany, Austria and countries influenced by their education system it is called 'humanistische Bildung' (humanistic education). The term is not to be confused with some modern educational concepts that use a similar wording. Educational institutions that see themselves in that tradition are often a Gymnasium (high school, grammar school). They aim at providing their pupils with comprehensive education (Bildung) to form personality with regard to a pupil's own humanity as well as their innate intellectual skills. Going back to the long tradition of the liberal arts in Europe, education in the above sense was freed from scholastic thinking and re-shaped by the theorists of the Enlightenment; in particular, Wilhelm von Humboldt. Since students are considered to have received a comprehensive liberal arts education at gymnasia, very often the role of liberal arts education in undergraduate programs at universities is reduced compared to the US educational system. Students are expected to use their skills received at the gymnasium to further develop their personality in their own responsibility, e.g. in universities' music clubs, theatre groups, language clubs, etc. Universities encourage students to do so and offer respective opportunities but do not make such activities part of the university's curriculum. Thus, on the level of higher education, despite the European origin of the liberal arts college, the term liberal arts college usually denotes liberal arts colleges in the United States. With the exception of pioneering institutions such as Franklin University Switzerland (formerly known as Franklin College), established as a Europe-based, US-style liberal arts college in 1969, only recently some efforts have been undertaken to systematically "re-import" liberal arts education to continental Europe, as with Leiden University College The Hague, University College Utrecht, University College Maastricht, Amsterdam University College, Roosevelt Academy (now University College Roosevelt), University College Twente (ATLAS), Erasmus University College, the University of Groningen, Bratislava International School of Liberal Arts, Leuphana University of Lüneburg, Central European University, and Bard College Berlin, formerly known as the European College of Liberal Arts. Central European University launched a liberal arts undergraduate degree in Culture, Politics, and Society in 2020 as part of its move to Vienna and accreditation in Austria. As well as the colleges listed above, some universities in the Netherlands offer bachelors programs in Liberal Arts and Sciences (Tilburg University). Liberal arts (as a degree program) is just beginning to establish itself in Europe. For example, University College Dublin offers the degree, as does St. Marys University College Belfast, both institutions coincidentally on the island of Ireland. In the Netherlands, universities have opened constituent liberal arts colleges under the terminology university college since the late 1990s. The four-year bachelor's degree in Liberal Arts and Sciences at University College Freiburg is the first of its kind in Germany. It started in October 2012 with 78 students. The first Liberal Arts degree program in Sweden was established at Gothenburg University in 2011, followed by a Liberal Arts Bachelor Programme at Uppsala University's Campus Gotland in the autumn of 2013. The first Liberal Arts program in Georgia was introduced in 2005 by American-Georgian Initiative for Liberal Education (AGILE), an NGO. Thanks to their collaboration, Ilia State University became the first higher education institution in Georgia to establish a liberal arts program. In France, Chavagnes Studium, a Liberal Arts Study Centre in partnership with the Institut Catholique d'études supérieures, and based in a former Catholic seminary, is launching a two-year intensive BA in the Liberal Arts, with a distinctively Catholic outlook. It has been suggested that the liberal arts degree may become part of mainstream education provision in the United Kingdom, Ireland and other European countries. In 1999, the European College of Liberal Arts (now Bard College Berlin) was founded in Berlin and in 2009 it introduced a four-year Bachelor of Arts program in Value Studies taught in English, leading to an interdisciplinary degree in the humanities. In England, the first institution to retrieve and update a liberal arts education at the undergraduate level was the University of Winchester with their BA (Hons) Modern Liberal Arts program which launched in 2010. In 2012, University College London began its interdisciplinary Arts and Sciences BASc degree (which has kinship with the liberal arts model) with 80 students. In 2013, the University of Birmingham created the School of Liberal Arts and Natural Sciences, home of a suite of flexible 4-year programs in which students study a broad range of subjects drawn from across the university, and gain qualifications including both traditional Liberal Arts and Natural Sciences, but also novel thematic combinations linking both areas. King's College London launched the BA Liberal Arts, which has a slant towards arts, humanities and social sciences subjects. The New College of the Humanities also launched a new liberal education programme. Richmond American University London is a private liberal arts university where all undergraduate degrees are taught with a US liberal arts approach over a four-year programme. Durham University has both a popular BA Liberal Arts and a BA Combined Honours in Social Sciences programme, both of which allow for interdisciplinary approaches to education. The University of Nottingham also has a Liberal Arts BA with study abroad options and links with its Natural Sciences degrees. In 2016, the University of Warwick launched a three/four-year liberal arts BA degree, which focuses on transdisciplinary approaches and problem-based learning techniques in addition to providing structured disciplinary routes and bespoke pathways. And for 2017 entry UCAS lists 20 providers of liberal arts programmes. In Scotland, the four-year undergraduate Honours degree, specifically the Master of Arts, has historically demonstrated considerable breadth in focus. In the first two years of Scottish MA and BA degrees students typically study a number of different subjects before specialising in their Honours years (third and fourth year). The Bratislava International School of Liberal Arts (BISLA), a private institution located in the Old Town of Bratislava, Slovakia, is the first liberal arts college in Central Europe and has been granting three-year degrees since its opening in September 2006. In Asia The Commission on Higher Education of the Philippines mandates a General Education curriculum required of all higher education institutions; it includes a number of liberal arts subjects, including history, art appreciation, and ethics, plus interdisciplinary electives. Many universities have much more robust liberal arts core curricula; most notably, the Jesuit universities such as Ateneo de Manila University have a strong liberal arts core curriculum that includes philosophy, theology, literature, history, and the social sciences. Forman Christian College is a liberal arts university in Lahore, Pakistan. It is one of the oldest institutions in the Indian subcontinent. It is a chartered university recognized by the Higher Education Commission of Pakistan. Aga Khan University offers a worldclass liberal arts education in the arts and sciences in Karachi, Pakistan, and Habib University in Karachi, Pakistan offers a holistic liberal arts and sciences experience to its students through its uniquely tailored liberal core program which is compulsory for all undergraduate degree students. In India, there are many institutions that offer undergraduate UG or bachelor's degree/diploma and postgraduate PG or master's degree/diploma as well as doctoral PhD and postdoctoral studies and research, in this academic discipline. The highly ranked IIT Guwahati offers a "Master's Degree in Liberal Arts". Manipal Academy of Higher Education – MAHE, an Institution of Eminence as recognised by MHRD of Govt of India in 2018, houses a Faculty of Liberal Arts, Humanities and Social Sciences, and also others like Symbiosis & FLAME University in Pune, Ahmedabad University, and Pandit Deendayal Energy University (PDEU) in Ahmedabad, Ashoka University, and Azim Premji University in Bangalore. Lingnan University, Asian University for Women and University of Liberal Arts- Bangladesh (ULAB) are also a few such liberal arts colleges in Asia. International Christian University in Tokyo is the first and one of the very few liberal arts universities in Japan. Fulbright University Vietnam is the first liberal arts institution in Vietnam. In Australia Campion College is a Roman Catholic dedicated liberal arts college, located in the western suburbs of Sydney. Founded in 2006, it is the first tertiary educational liberal arts college of its type in Australia. Campion offers a Bachelor of Arts in the Liberal Arts as its sole undergraduate degree. The key disciplines studied are history, literature, philosophy, and theology. The Millis Institute is the School of Liberal Arts at Christian Heritage College located in Brisbane. Founded by Dr. Ryan Messmore, former President of Campion College, the Millis Institute offers a Bachelor of Arts in the Liberal Arts in which students can choose to major in philosophy, theology, history or literature. It also endorses a 'Study Abroad' program whereby students can earn credit towards their degree by undertaking two units over a five-week program at the University of Oxford. As of 2022, Elizabeth Hillman is currently the President of the Millis Institute. A new school of Liberal Arts has been formed in the University of Wollongong; the new Arts course entitled 'Western Civilisation' was first offered in 2020. The interdisciplinary curriculum focuses on the classic intellectual and artistic literature of the Western tradition. Courses in the liberal arts have recently been developed at the University of Sydney and the University of Notre Dame. See also Citations General and cited references Castle, E.B. (1969). Ancient Education and Today. Curtius, Ernst Robert (1973) [1948]. European Literature and the Latin Middle Ages. Translated by Trask, Willard R. Princeton: Princeton University Press. ISBN 9780691097398. Griffiths, Fiona J. (2011). The Garden of Delights: Reform and Renaissance for Women in the Twelfth Century. University of Pennsylvania Press. ISBN 9780812202113. Kimball, Bruce A. Orators and Philosophers: A History of the Idea of Liberal Education. College Board, 1995. Lausberg, H. (1998). Handbook of Literary Rhetoric. Michael, William (2020). "The Virgin Mary and the Classical Liberal Arts". Classical Liberal Arts Academy. Tidbury, Iain (5 August 2019). "Liberal Arts Education by and for Women". Liberal Arts. Retrieved 5 August 2019. Tubbs, Nigel (2014). Philosophy and Modern Liberal Arts Education: Freedom is to Learn. Houndmills, Basingstoke, Hampshire: Palgrave Macmillan. ISBN 978-1-137-35891-2. OCLC 882530818. Waddell, Helen (1968). The Wandering Scholars. Wagner, David Leslie (1983). The Seven liberal arts in the Middle Ages. Indiana University Press. ISBN 978-0-253-35185-2. Further reading Anders, George (2019). You Can Do Anything: The Surprising Power of a "Useless" Liberal Arts Education. Back Bay Books. ISBN 978-0316548885. Barzun, Jacques. The House of Intellect, Reprint Harper Perennial, 2002. Blaich, Charles, Anne Bost, Ed Chan, and Richard Lynch. "Defining Liberal Arts Education." Center of Inquiry in the Liberal Arts, 2004. Blanshard, Brand. The Uses of a Liberal Education: And Other Talks to Students. (Open Court, 1973. ISBN 0-8126-9429-5) Friedlander, Jack. Measuring the Benefits of Liberal Arts Education in Washington's Community Colleges. Los Angeles: Center for the Study of Community Colleges, 1982a. (ED 217 918) Grafton Anthony and Lisa Jardine. From Humanism to the Humanities: The Institutionalizing of the Liberal Arts in Fifteenth- and Sixteenth-century Europe, Harvard University Press, 1987. Guitton, Jean. A Student's Guide to Intellectual Work, The University of Notre Dame Press, 1964. Highet, Gilbert. The Art of Teaching, Vintage Books, 1950. Joseph, Sister Miriam. The Trivium: The Liberal Arts of Logic, Grammar, and Rhetoric. Paul Dry Books Inc, 2002. Kimball, Bruce A. The Liberal Arts Tradition: A Documentary History. University Press Of America, 2010. T. Kaori Kitao; William R. Kenan, Jr. (27 March 1999). The Usefulness Of Uselessness (PDF). Keynote Address, The 1999 Institute for the Academic Advancement of Youth's Odyssey at Swarthmore College. Archived from the original (PDF) on 2 October 2008.{{cite book}}: CS1 maint: location (link) CS1 maint: location missing publisher (link) McGrath, Charles. "What Every Student Should Know", New York Times, 8 January 2006. Parker, H. "The Seven Liberal Arts," The English Historical Review, Vol. V, 1890. Pfnister, Allan O. (1984). "The Role of the Liberal Arts College: A Historical Overview of the Debates". The Journal of Higher Education. 55 (2). Ohio State University Press: 145–70. doi:10.2307/1981183. ISSN 1538-4640. JSTOR 1981183. Reeves, Floyd W. (1930). "The Liberal-Arts College". The Journal of Higher Education. 1 (7). Ohio State University Press: 373–80. doi:10.2307/1974170. ISSN 1538-4640. JSTOR 1974170. Ruckdeschel, Christopher. On the Nature of the Classical Liberal Arts, Bookbaby, 2019. Saint-Victor, Hugh of. The Didascalicon, Columbia University Press, 1961. Schall, James V. Another Sort of Learning, Ignatius Press, 1988. Seidel, George J. (1968). "Saving the Small College". The Journal of Higher Education. 39 (6). Ohio State University Press: 339–42. doi:10.2307/1979916. ISSN 1538-4640. JSTOR 1979916. Sertillanges, A. G. The Intellectual Life, The Catholic University of America Press, 1998. Tubbs, N. (2011) "Know Thyself: Macrocosm and Microcosm" in Studies in Philosophy and Education Volume 30 no.1 Winterer, Caroline. The Culture of Classicism: Ancient Greece and Rome in American Intellectual Life, 1780–1910. Baltimore: Johns Hopkins University Press, 2002. Wriston, Henry M. The Nature of a Liberal College. Lawrence University Press, 1937. Zakaria, Fareed. In Defense of a Liberal Education. New York: W.W. Norton & Company, 2015. External links "Arts, Liberal" . New International Encyclopedia. 1905. Definition and short history of the Seven Liberal Arts from 1905. Fr. Herve de la Tour, "The Seven Liberal Arts", Edocere, a Resource for Catholic Education, February 2002. Thomas Aquinas's definition of and justification for a liberal arts education. Otto Willmann. "The Seven Liberal Arts". In The Catholic Encyclopedia. New York: Robert Appleton Company, 1907. Retrieved 13 August 2012. "[Renaissance] Humanists, over-fond of change, unjustly condemned the system of the seven liberal arts as barbarous. It is no more barbarous than the Gothic style, a name intended to be a reproach. The Gothic, built up on the conception of the old basilica, ancient in origin, yet Christian in character, was misjudged by the Renaissance on account of some excrescences, and obscured by the additions engrafted upon it by modern lack of taste… That the achievements of our forefathers should be understood, recognized, and adapted to our own needs, is surely to be desired." Andrew Chrucky (1 September 2003). "The Aim of Liberal Education". "The content of a liberal education should be moral problems as provided by history, anthropology, sociology, economics, and politics. And these should be discussed along with a reflection on the nature of morality and the nature of discussions, i.e., through a study of rhetoric and logic. Since discussion takes place in language, an effort should be made to develop a facility with language." "Philosophy of Liberal Education" A bibliography, compiled by Andrew Chrucky, with links to essays offering different points of view on the meaning of a liberal education. Mark Peltz, "The Liberal Arts and Leadership", College News (The Annapolis Group), 14 May 2012. A defense of liberal education by the Associate Dean of Grinnell College (first appeared in Inside Higher Ed). "Liberal Arts at the Community College" Archived 28 November 2020 at the Wayback Machine, an ERIC Fact Sheet. ERIC Clearinghouse for Junior Colleges Los Angeles "A Descriptive Analysis of the Community College Liberal Arts Curriculum" Archived 28 November 2020 at the Wayback Machine. ERIC Clearinghouse for Junior Colleges Los Angeles The Center of Inquiry in the Liberal Arts. Website about The Wabash Study (for improving liberal education). Sponsored by the Center of Inquiry in the Liberal Arts at Wabash College (Indiana), the Wabash Study began in the fall of 2010 – scheduled to end in 2013. Participants include 29 prominent colleges and universities. Academic Commons. An online platform in support of the liberal education community. It is a forum for sharing practices, outcomes, and lessons learned of online learning. Formerly sponsored by the Center of Inquiry in the Liberal Arts, The Academic Commons is hosted by the National Institute for Technology in Liberal Education ("NITLE". Archived 2 March 2018 at the Wayback Machine). The Liberal Arts Advantage – for Business. Website dedicated to "Bridging the gap between business and the liberal arts". "A liberal arts education is aimed at developing the ability to think, reason, analyze, decide, discern, and evaluate. That's in contrast to a professional or technical education (business, engineering, computer science, etc.) which develops specific abilities aimed at preparing students for vocations." Video explanation by Professor Nigel Tubbs of liberal arts curriculum and degree requirements of Winchester University, UK.. "Liberal arts education (Latin: liberalis, free, and ars, art or principled practice) involves us in thinking philosophically across many subject boundaries in the humanities, the social and natural sciences, and fine arts. The degree combines compulsory modules covering art, religion, literature, science and the history of ideas with a wide range of optional modules. This enables students to have flexibility and control over their programme of study and the content of their assessments." Sue Miles (born Susan Crane, 20 March 1944 – 8 October 2010) was an Anglo-American counter-culture activist and restaurateur. With her husband Barry and the support of celebrities such as Paul McCartney, she started the Indica Gallery and the underground newspaper International Times (IT). She started her cooking career by running the cafe at the Arts Lab and then worked at other prominent restaurants in central London such as Food for Thought and L'Escargot. == References == The 1989 Loma Prieta earthquake is featured in films and television shows. Films and TV shows The events in the TV sitcom Full House episode "Aftershocks" (December 8, 1989) take place following the Loma Prieta earthquake; it centers on 7-year-old Stephanie Tanner having a hard time telling her father Danny about how she fears another aftershock might happen and kill him (since he is a widowed single father) as well as other members of the family. After the Shock (1990), a made-for-TV movie that was aired on the USA Network, with stories of rescue after the disaster. Miracle on Interstate 880 (1993), a TV movie fictionalization and re-enactment of events at the collapsed Cypress Structure. Journeyman (2007), TV show on NBC in which the main character travels back in time to save a person who died during the earthquake. Occurs in the third episode, "Game Three," in reference to the 1989 World Series. Midnight Caller (1990), TV show set in San Francisco in the aftermath of the earthquake. In the show, a caller jokes about Candlestick Park being renamed "Wiggly Field". The San Francisco-based punk music group Loma Prieta derived their name from the 1989 earthquake. Medium (2005), TV show on NBC in which a character uses her coincidental presence in Oakland during the earthquake as an opportunity to fake her own death and disappear. Occurs in the episode "Sweet Dreams". Fringe (2008), TV show on Fox in which a character's parents are killed in the Oakland Bay Bridge collapse, while the Observer is watching her and her parents. Occurs in the episode "August". In a later episode ("Amber 31422", November 4, 2010), the alternate Walter Bishop refers to October 17, 1989, as the first time he used his amber-based protocol to heal breaches between the two universes. The Grateful Dead performed the Rodney Crowell song "California Earthquake" at their concert in Philadelphia, Pennsylvania, on October 20, 1989, then again three nights later in Charlotte, North Carolina – the only times the band ever performed the song. On December 6, 1989, the band played an earthquake benefit concert at the Oakland-Alameda County Coliseum Arena. Gillian Welch in the song "Wrecking Ball" on her Soul Journey album. The 2004 video game Grand Theft Auto: San Andreas takes place in 1992. In it, fictional San Fierro, the game's version of San Francisco, has a few quake-damaged areas courtesy of a major shaker occurring three years before the game begins, and another earthquake being the reason the player is locked from visiting San Fierro and Las Venturas in early parts of the game. One location includes a damaged overpass resembling the collapsed Cypress Street Viaduct. Faultline.mov – An unlisted 2022 YouTube video from Kane Pixels' Backrooms creepypasta series that details the events following the earthquake. In the video, the earthquake occurs at the same time as a scientific experiment conducted by the fictional Async research institute. Episode 6, Season 1 of the 2022 revial of Quantum Leap takes place during the earthquake. Documentaries Some earthquake and disaster-themed television documentaries that feature the earthquake include: The Quake of 89: The Final Warning? – a 12th episode of the 26th series of the BBC television documentary programme Horizon, broadcast on April 2, 1990. Nature's Fury – produced by National Geographic in 1994 . Earth's Fury (also known as Anatomy of Disaster outside the United States) – produced by GRB Entertainment and aired on television networks around the world such as The Learning Channel in the United States, feature the earthquake. Raging Planet – aired on Discovery Channel in 1997, features the earthquake. Mega Disaster – produced by NHNZ and aired internationally on National Geographic Channel in 2006. Surviving Disaster – aired on the BBC in the United Kingdom features the earthquake. World's Deadliest Earthquakes – aired on ABC television network in 1999 features the earthquake. Minute by Minute – aired on A&E in 2002, features the earthquake. Critical Rescue – aired on Discovery Channel in 2003, features the rescuers of the earthquake. San Francisco Quake: A Matter of Seconds – aired on TLC in 1999, a special focus on the earthquake. Loma Prieta Earthquake, 30 Years Later – aired on NBC owned-and-operated television station KNTV in San Francisco Bay Area for its 30th anniversary in 2019. The Day the Series Stopped – 2014 documentary film from ESPN's 30 for 30 series that focuses on the disruption of the 1989 World Series by the earthquake. I Alive – The first episode of the series, broadcast in 2019 by Cottage Life Network, titled "San Francisco Freeway Collapse" that focuses on the collapse of the Cypress Street Viaduct during the earthquake. The comic book Smile by Raina Telgemeier gives her account of how the earthquake felt, as well as the aftermath of it. == References == An intellectual is a person who engages in critical thinking, research, and reflection about the nature of reality, especially the nature of society and proposed solutions for its normative problems. Coming from the world of culture, either as a creator or as a mediator, the intellectual participates in politics, either to defend a concrete proposition or to denounce an injustice, usually by either rejecting, producing or extending an ideology, and by defending a system of values. Etymological background "Man of letters" The term "man of letters" derives from the French term belletrist or homme de lettres but is not synonymous with "an academic". A "man of letters" was a literate man, able to read and write, and thus highly valued in the upper strata of society in a time when literacy was rare. In the 17th and 18th centuries, the term Belletrist(s) came to be applied to the literati: the French participants in—sometimes referred to as "citizens" of—the Republic of Letters, which evolved into the salon, a social institution, usually run by a hostess, meant for the edification, education, and cultural refinement of the participants. In the late 19th century, when literacy was relatively common in European countries such as the United Kingdom, the "Man of Letters" (littérateur) denotation broadened to mean "specialized", a man who earned his living writing intellectually (not creatively) about literature: the essayist, the journalist, the critic, et al. Examples include Samuel Johnson, Walter Scott and Thomas Carlyle. In the 20th century, such an approach was gradually superseded by the academic method, and the term "Man of Letters" became disused, replaced by the generic term "intellectual", describing the intellectual person. The archaic term is the basis of the names of several academic institutions which call themselves Colleges of Letters and Science. "Intellectual" The earliest record of the English noun "intellectual" is found in the 19th century, where in 1813, Byron reports that 'I wish I may be well enough to listen to these intellectuals'.: 18 Over the course of the 19th century, other variants of the already established adjective 'intellectual' as a noun appeared in English and in French, where in the 1890s the noun (intellectuels) formed from the adjective intellectuel appeared with higher frequency in the literature.: 20 Collini writes about this time that "[a]mong this cluster of linguistic experiments there occurred ... the occasional usage of 'intellectuals' as a plural noun to refer, usually with a figurative or ironic intent, to a collection of people who might be identified in terms of their intellectual inclinations or pretensions.": 20 In early 19th-century Britain, Samuel Taylor Coleridge coined the term clerisy, the intellectual class responsible for upholding and maintaining the national culture, the secular equivalent of the Anglican clergy. Likewise, in Tsarist Russia, there arose the intelligentsia (1860s–1870s), who were the status class of white-collar workers. For Germany, the theologian Alister McGrath said that "the emergence of a socially alienated, theologically literate, antiestablishment lay intelligentsia is one of the more significant phenomena of the social history of Germany in the 1830s".: 53 An intellectual class in Europe was socially important, especially to self-styled intellectuals, whose participation in society's arts, politics, journalism, and education—of either nationalist, internationalist, or ethnic sentiment—constitute "vocation of the intellectual". Moreover, some intellectuals were anti-academic, despite universities (the academy) being synonymous with intellectualism. In France, the Dreyfus affair (1894–1906), an identity crisis of antisemitic nationalism for the French Third Republic (1870–1940), marked the full emergence of the "intellectual in public life", especially Émile Zola, Octave Mirbeau and Anatole France directly addressing the matter of French antisemitism to the public; thenceforward, "intellectual" became common, yet initially derogatory, usage; its French noun usage is attributed to Georges Clemenceau in 1898. Nevertheless, by 1930 the term "intellectual" passed from its earlier pejorative associations and restricted usages to a widely accepted term and it was because of the Dreyfus Affair that the term also acquired generally accepted use in English.: 21 In the 20th century, the term intellectual acquired positive connotations of social prestige, derived from possessing intellect and intelligence, especially when the intellectual's activities exerted positive consequences in the public sphere and so increased the intellectual understanding of the public, by means of moral responsibility, altruism, and solidarity, without resorting to the manipulations of demagoguery, paternalism and incivility (condescension).: 169 The sociologist Frank Furedi said that "Intellectuals are not defined according to the jobs they do, but [by] the manner in which they act, the way they see themselves, and the [social and political] values that they uphold. According to Thomas Sowell, as a descriptive term of person, personality, and profession, the word intellectual identifies three traits: Educated; erudition for developing theories; Productive; creates cultural capital in the fields of philosophy, literary criticism, and sociology, law, medicine, and science, etc.; and Artistic; creates art in literature, music, painting, sculpture, etc. Historical uses In Latin, at least starting from the Carolingian Empire, intellectuals could be called litterati, a term which is sometimes applied today. The word intellectual is found in Indian scripture Mahabharata in the Bachelorette meeting (Swayamvara Sava) of Draupadi. Immediately after Arjuna and Raja-Maharaja (kings-emperors) came to the meeting, Nipuna Buddhijibina (perfect intellectuals) appeared at the meeting. In Imperial China in the period from 206 BC until AD 1912, the intellectuals were the Scholar-officials ("Scholar-gentlemen"), who were civil servants appointed by the Emperor of China to perform the tasks of daily governance. Such civil servants earned academic degrees by means of imperial examination, and were often also skilled calligraphers or Confucian philosophers. Historian Wing-Tsit Chan concludes that: Generally speaking, the record of these scholar-gentlemen has been a worthy one. It was good enough to be praised and imitated in 18th century Europe. Nevertheless, it has given China a tremendous handicap in their transition from government by men to government by law, and personal considerations in Chinese government have been a curse.: 22 In Joseon Korea (1392–1910), the intellectuals were the literati, who knew how to read and write, and had been designated, as the chungin (the "middle people"), in accordance with the Confucian system. Socially, they constituted the petite bourgeoisie, composed of scholar-bureaucrats (scholars, professionals, and technicians) who administered the dynastic rule of the Joseon dynasty.: 73–4 Public intellectual The term public intellectual describes the intellectual participating in the public-affairs discourse of society, in addition to an academic career. Regardless of their academic fields or professional expertise, public intellectuals address and respond to the normative problems of society, and, as such, are expected to be impartial critics who can "rise above the partial preoccupation of one's own profession—and engage with the global issues of truth, judgment, and taste of the time".: 32 In Representations of the Intellectual (1994), Edward Saïd said that the "true intellectual is, therefore, always an outsider, living in self-imposed exile, and on the margins of society".: 1–2 Public intellectuals usually arise from the educated élite of a society, although the North American usage of the term intellectual includes the university academics. The difference between intellectual and academic is participation in the realm of public affairs. Jürgen Habermas' The Structural Transformation of the Public Sphere (1963) made significant contribution to the notion of public intellectual by historically and conceptually delineating the idea of private and public. Controversial, in the same year, was Ralf Dahrendorf's definition: "As the court-jesters of modern society, all intellectuals have the duty to doubt everything that is obvious, to make relative all authority, to ask all those questions that no one else dares to ask".: 51 An intellectual usually is associated with an ideology or with a philosophy. The Czech intellectual Václav Havel said that politics and intellectuals can be linked, but that moral responsibility for the intellectual's ideas, even when advocated by a politician, remains with the intellectual. Therefore, it is best to avoid utopian intellectuals who offer 'universal insights' to resolve the problems of political economy with public policies that might harm and that have harmed civil society; that intellectuals be mindful of the social and cultural ties created with their words, insights and ideas; and should be heard as social critics of politics and power.: 13 Public engagement The determining factor for a "thinker" (historian, philosopher, scientist, writer, artist) to be considered a public intellectual is the degree to which the individual is implicated and engaged with the vital reality of the contemporary world, i.e. participation in the public affairs of society. Consequently, being designated as a public intellectual is determined by the degree of influence of the designator's motivations, opinions, and options of action (social, political, ideological), and by affinity with the given thinker. After the failure of the large-scale May 68 movement in France, intellectuals within the country were often maligned for having specific areas of expertise while discussing general subjects like democracy. Intellectuals increasingly claimed to be within marginalized groups rather than their spokespeople, and centered their activism on the social problems relevant to their areas of expertise (such as gender relations in the case of psychologists). A similar shift occurred in China after the Tiananmen Square Massacre from the "universal intellectual" (who plans better futures from within academia) to minjian ("grassroots") intellectuals, the latter group represented by such figures as Wang Xiaobo, social scientist Yu Jianrong, and Yanhuang Chunqiu editor Ding Dong (丁東). Public policy In the matters of public policy, the public intellectual connects scholarly research to the practical matters of solving societal problems. The British sociologist Michael Burawoy, an exponent of public sociology, said that professional sociology has failed by giving insufficient attention to resolving social problems, and that a dialogue between the academic and the layman would bridge the gap. An example is how Chilean intellectuals worked to reestablish democracy within the right-wing, neoliberal governments of the military dictatorship of 1973–1990, the Pinochet régime allowed professional opportunities for some liberal and left-wing social scientists to work as politicians and as consultants in effort to realize the theoretical economics of the Chicago Boys, but their access to power was contingent upon political pragmatism, abandoning the political neutrality of the academic intellectual. In The Sociological Imagination (1959), C. Wright Mills said that academics had become ill-equipped for participating in public discourse, and that journalists usually are "more politically alert and knowledgeable than sociologists, economists, and especially ... political scientists".: 99 That, because the universities of the U.S. are bureaucratic, private businesses, they "do not teach critical reasoning to the student", who then does not know "how to gauge what is going on in the general struggle for power in modern society". Likewise, Richard Rorty criticized the quality of participation of intellectuals in public discourse as an example of the "civic irresponsibility of intellect, especially academic intellect".: 142 The American legal scholar Richard Posner said that the participation of academic public intellectuals in the public life of society is characterized by logically untidy and politically biased statements of the kind that would be unacceptable to academia. He concluded that there are few ideologically and politically independent public intellectuals, and disapproved public intellectuals who limit themselves to practical matters of public policy, and not with values or public philosophy, or public ethics, or public theology, nor with matters of moral and spiritual outrage. Intellectual status class Socially, intellectuals constitute the intelligentsia, a status class organised either by ideology (e.g., conservatism, fascism, socialism, liberalism, reactionary, revolutionary, democratic, communism), or by nationality (American intellectuals, French intellectuals, Ibero–American intellectuals, et al.). The term intelligentsiya originated from Tsarist Russia (c. 1860s–1870s), where it denotes the social stratum of those possessing intellectual formation (schooling, education), and who were Russian society's counterpart to the German Bildungsbürgertum and to the French bourgeoisie éclairée, the enlightened middle classes of those realms.: 169–71 In Marxist philosophy, the social class function of the intellectuals (the intelligentsia) is to be the source of progressive ideas for the transformation of society: providing advice and counsel to the political leaders, interpreting the country's politics to the mass of the population (urban workers and peasants). In the pamphlet What Is to Be Done? (1902), Vladimir Lenin (1870–1924) said that vanguard-party revolution required the participation of the intellectuals to explain the complexities of socialist ideology to the uneducated proletariat and the urban industrial workers in order to integrate them to the revolution because "the history of all countries shows that the working class, exclusively by its own efforts, is able to develop only trade-union consciousness" and will settle for the limited, socio-economic gains so achieved. In Russia as in Continental Europe, socialist theory was the product of the "educated representatives of the propertied classes", of "revolutionary socialist intellectuals", such as were Karl Marx and Friedrich Engels.: 31, 137–8 The Hungarian Marxist philosopher György Lukács (1885–1971) identified the intelligentsia as the privileged social class who provide revolutionary leadership. By means of intelligible and accessible interpretation, the intellectuals explain to the workers and peasants the "Who?", the "How?" and the "Why?" of the social, economic and political status quo—the ideological totality of society—and its practical, revolutionary application to the transformation of their society. The Italian communist theoretician Antonio Gramsci (1891–1937) developed Karl Marx's conception of the intelligentsia to include political leadership in the public sphere. That because "all knowledge is existentially-based", the intellectuals, who create and preserve knowledge, are "spokesmen for different social groups, and articulate particular social interests". That intellectuals occur in each social class and throughout the right-wing, the centre and the left-wing of the political spectrum and that as a social class the "intellectuals view themselves as autonomous from the ruling class" of their society. Addressing their role as a social class, Jean-Paul Sartre said that intellectuals are the moral conscience of their age; that their moral and ethical responsibilities are to observe the socio-political moment, and to freely speak to their society, in accordance with their consciences.: 119 The British historian Norman Stone said that the intellectual social class misunderstand the reality of society and so are doomed to the errors of logical fallacy, ideological stupidity, and poor planning hampered by ideology. In her memoirs, the Conservative politician Margaret Thatcher wrote that the anti-monarchical French Revolution (1789–1799) was "a utopian attempt to overthrow a traditional order [...] in the name of abstract ideas, formulated by vain intellectuals".: 753 Latin America The American academic Peter H. Smith describes the intellectuals of Latin America as people from an identifiable social class, who have been conditioned by that common experience and thus are inclined to share a set of common assumptions (values and ethics); that ninety-four per cent of intellectuals come either from the middle class or from the upper class and that only six per cent come from the working class. Philosopher Steven Fuller said that because cultural capital confers power and social status as a status group they must be autonomous in order to be credible as intellectuals: It is relatively easy to demonstrate autonomy, if you come from a wealthy or [an] aristocratic background. You simply need to disown your status and champion the poor and [the] downtrodden [...]. [A]utonomy is much harder to demonstrate if you come from a poor or proletarian background [...], [thus] calls to join the wealthy in common cause appear to betray one's class origins. : 113–4 United States The 19th-century U.S. Congregational theologian Edwards Amasa Park said: "We do wrong to our own minds, when we carry out scientific difficulties down to the arena of popular dissension".: 12 In his view, it was necessary for the sake of social, economic and political stability "to separate the serious, technical role of professionals from their responsibility [for] supplying usable philosophies for the general public". This expresses a dichotomy, derived from Plato, between public knowledge and private knowledge, "civic culture" and "professional culture", the intellectual sphere of life and the life of ordinary people in society.: 12 In the United States, members of the intellectual status class have been demographically characterized as people who hold liberal-to-leftist political perspectives about guns-or-butter fiscal policy. In "The Intellectuals and Socialism" (1949), Friedrich Hayek wrote that "journalists, teachers, ministers, lecturers, publicists, radio commentators, writers of fiction, cartoonists, and artists" form an intellectual social class whose function is to communicate the complex and specialized knowledge of the scientist to the general public. He argued that intellectuals were attracted to socialism or social democracy because the socialists offered "broad visions; the spacious comprehension of the social order, as a whole, which a planned system promises" and that such broad-vision philosophies "succeeded in inspiring the imagination of the intellectuals" to change and improve their societies. According to Hayek, intellectuals disproportionately support socialism for idealistic and utopian reasons that cannot be realized in practice. Criticism The French philosopher Jean-Paul Sartre noted that "the Intellectual is someone who meddles in what does not concern them" (L'intellectuel est quelqu'un qui se mêle de ce qui ne le regarde pas).: 588–9 Noam Chomsky expressed the view that "intellectuals are specialists in defamation, they are basically political commissars, they are the ideological administrators, the most threatened by dissidence." In his 1967 article "The Responsibility of Intellectuals", Chomsky analyzes the intellectual culture in the U.S., and argues that it is largely subservient to power. He is particularly critical of social scientists and technocrats, who provide a pseudo-scientific justification for the crimes of the state. In "An Interview with Milton Friedman" (1974), the American economist Milton Friedman said that businessmen and intellectuals are enemies of capitalism: most intellectuals believed in socialism while businessmen expected economic privileges. In his essay "Why Do Intellectuals Oppose Capitalism?" (1998), the American libertarian philosopher Robert Nozick of the Cato Institute argued that intellectuals become embittered leftists because their superior intellectual work, much rewarded at school and at university, are undervalued and underpaid in the capitalist market economy. Thus, intellectuals turn against capitalism despite enjoying more socioeconomic status than the average person. The conservative economist Thomas Sowell wrote in his book Intellectuals and Society (2010) that intellectuals, who are producers of knowledge, not material goods, tend to speak outside their own areas of expertise, and yet expect social and professional benefits from the halo effect derived from possessing professional expertise. In relation to other professions, public intellectuals are socially detached from the negative and unintended consequences of public policy derived from their ideas. Sowell gives the example of Bertrand Russell (1872–1970), who advised the British government against national rearmament in the years before the Second World War.: 218–276 References Bibliography Further reading External links The Responsibility of Intellectuals, by Noam Chomsky, 23 February 1967. "Richard Posner's table of 600+ public intellectuals" (PDF). Archived from the original (PDF) on 22 September 2015. Retrieved 1 August 2006. (105 KB) classified by profession, discipline, scholastic citations, media affiliation, number of web hits and sex. Barton, Laura (2 July 2004). "Here's a Few You Missed". The Guardian. London. "The Optimist's Book Club", The New Haven Advocate—discussion of public intellectuals in the 21st century. Anorexia athletica (sports anorexia), also referred to as hyper-gymnasia, is an eating disorder characterized by excessive and compulsive exercise. An athlete with sports anorexia tends to overexercise, to give themselves a sense of having control over their body. Most often, people with the disorder tend to feel they have no control over their lives, other than their control of food and exercise. In actuality, they have no control; they cannot stop exercising or regulating food intake without feeling guilty. Generally, once the activity is started, it is difficult to stop, because the person is seen as being addicted to the method adopted. Anorexia athletica is used to refer to "a disorder for athletes who engage in at least one unhealthy method of weight control". Unlike anorexia nervosa, anorexia athletica does not have as much to do with body image as it does with performance. Athletes usually begin by eating more 'healthy' foods, as well as increasing their training. People feel like that is not enough and start working out excessively and cutting back their caloric intake, until it becomes a psychological disorder. Hypergymnasia and anorexia athletica are not recognized as mental disorders in medical manuals such as the ICD-11 or the DSM-5. There are limited studies on the exact prevalence of anorexia athletica, but it has been found to be more common among elite athletes than the general population. Signs and symptoms Someone with anorexia athletica can experience numerous signs and symptoms, a few of which are listed below. The seriousness of the symptoms is dependent on the individual, and more symptoms come with the length the athlete excessively exercises. If anorexia athletica persists for long enough, the individual can become malnourished, which eventually leads to further complications in major organs such as the liver, kidney, heart and brain. Excessive exercise Obsessive behavior with calories, fat, and weight Self-worth is based on physical performance Enjoyment of sports is diminished or gone Denying that the over-exercising is a problem Causes There is not one single cause of anorexia athletica, but many factors that are involved in the disorder. Research has shown that an area on chromosome 1 is linked to anorexia nervosa-sports anorexia. Thus, a person is more likely to have anorexia athletica if someone in their immediate family has had the disorder. Besides genetics, the environment a person is in, has a major impact on the disorder. Coaches and parents often suggest to their athlete/child to lose weight in order to perform better. Sports such as figure skating, ballet, and gymnastics promote both male and female athletes to have a thin figure. There has been research into the prevalence in certain types of sports. For example, the pursuit of a certain body aesthetic in gymnastics, the need to be in a certain weight categorisation in order to compete in judo, or endurance sports such as running where weight and performance are closely linked. Other sports with a high possibility of anorexia athletica are rowing, dancing and ski jumping. Females who partake in sports can develop a syndrome known as the triad. The female athlete triad was recognized in 1992 and is defined as a spectrum disorder of three interrelated components: (1) low energy availability due to disordered eating, eating disorder, or lack of nutrition relative to caloric expenditure; (2) menstrual dysfunction; and (3) low bone mineral density (BMD). The media play a very significant role in pressuring athletes to have the perfect body and to be thin, which can also trigger sports anorexia. Treatment According to the National Eating Disorder Information Centre (NEDIC), the first step for someone going through anorexia athletica is to realize their eating and exercise habits are hurting them. Once an individual has realized they have a disorder, an appointment should be made with the family doctor. A family doctor can advise further medical attention if needed. With sports anorexia, it is important to go to a dietitian as well as a personal trainer. People with sports anorexia need to learn the balance between exercise and caloric intake. See also Eating disorders Exercise addiction Exercise bulimia Female athlete triad syndrome Overtraining == References == New 7 Wonders Cities (2011–2014) was the third in a series of Internet-based polls operated by the New 7 Wonders Foundation. It followed New 7 Wonders of the World and New 7 Wonders of Nature. The poll began in 2007 with more than 1200 nominees from 220 countries. A longlist of 77, limited to a maximum of one city per country, was considered by a panel headed by Federico Mayor Zaragoza, former director-general of UNESCO, which shortlisted 28 suggestions. The shortlist was then opened to a public vote. Announced in 2011, it ended in 2014 with the selection of Beirut, Doha, Durban, Havana, Kuala Lumpur, La Paz and Vigan as the winning cities. Winners References External links New7Wonders official website New7Wonders Cities Bernard Weber Project Founder The Strauss–Howe generational theory, devised by William Strauss and Neil Howe, is a psychohistorical theory which describes a theorized recurring generation cycle in American and Western history. According to the theory, historical events are associated with recurring generational personas (archetypes). Each generational persona unleashes a new era (called a turning) lasting around 21 years, in which a new social, political, and economic climate (mood) exists. They are part of a larger cyclical "saeculum" (a long human life, which usually spans around 85 years, although some saecula have lasted longer). The theory states that a crisis recurs in American history after every saeculum, which is followed by a recovery (high). During this recovery, institutions and communitarian values are strong. Ultimately, succeeding generational archetypes attack and weaken institutions in the name of autonomy and individualism, which eventually creates a tumultuous political environment that ripens conditions for another crisis. Academic response to the theory has been mixed, with some applauding Strauss and Howe for their "bold and imaginative thesis", while others have criticized the theory as being overly deterministic, unfalsifiable, and unsupported by rigorous evidence. The theory has been influential in the fields of generational studies, marketing, and business management literature. However, the theory has also been described by some historians and journalists as pseudoscientific, "kooky", and "an elaborate historical horoscope that will never withstand scholarly scrutiny". Academic criticism has focused on the lack of rigorous empirical evidence for their claims, as well as the authors' view that generational groupings are more powerful than other social groupings, such as economic class, race, sex, religion, and political parties. However, Strauss and Howe later suggested that there are no exact generational boundaries – the speed of their development cannot be predicted. The authors also compared the cycles with the seasons, which may come sooner or later. History William Strauss and Neil Howe's partnership began in the late 1980s when they began writing their first book Generations, which discusses the history of the United States as a succession of generational biographies. Each had written on generational topics previously. The authors' interest in generations as a broader topic emerged after they met in Washington, D.C., and began discussing the connections between each of their previous works. They attempted to find the reason why the Boomers and the G.I.s had developed such different ways of looking at the world, and what it was about these generations' experiences growing up that prompted their different outlooks. They also wanted to find patterns in previous generations, and their research discussed historical analogs to the current generations. They ultimately described a recurring pattern in the Anglo-American history of four generational types, each with a distinct collective persona, and a corresponding cycle of four different types of eras, each with a distinct "mood". Strauss and Howe laid the groundwork for their theory in their book Generations: The History of America's Future, 1584 to 2069 (1991), which discusses the history of the United States as a series of generational biographies going back to 1584. Strauss and Howe followed in 1993 with their second book 13th Gen: Abort, Retry, Ignore, Fail?, which was published while Gen Xers were teenagers and young adults. The book examines the generation born between 1961 and 1981, "Gen-Xers" (which they called "13ers", describing them as the thirteenth generation since the US became a nation). The book asserts that 13ers' location in history as under-protected children during the Consciousness Revolution explains their pragmatic attitude. They describe Gen Xers as growing up during a time when society was less focused on children and more focused on adults and their self-actualization. Strauss and Howe's theory made various predictions regarding the Millennial generation, a group consisting of young children at the time. These predictions lacked significant historical data. In Generations (1991) and The Fourth Turning (1997), the two authors discussed the generation gap between Baby Boomers and their parents and predicted there would be no such gap between Millennials and their elders. In 2000, they published Millennials Rising: The Next Great Generation. This work discussed the personality of the Millennial Generation, whose oldest members were described as the high school graduating class of the year 2000. In the 2000 book, Strauss and Howe asserted that Millennial teens and young adults were recasting the image of youth from "downbeat and alienated to upbeat and engaged", crediting increased parental attention and protection for these positive changes. They asserted that Millennials are held to higher standards than adults apply to themselves and that they are much less vulgar and violent than the teen culture older people produce for them. They described them as less sexually charged and as ushering in a new sexual modesty, with an increasing belief that sex should be saved for marriage and a return to conservative family values. The authors predicted that over the following decade, Millennials would transform what it means to be young, and could emerge as the next "Great Generation". The work was described as an optimistic, feel-good book for the parents of the Millennial Generation, predominantly the Baby Boomers. The theory was expanded in The Fourth Turning (1997), to focus on a fourfold cycle of generational types and recurring mood eras to describe the history of the United States, including the Thirteen Colonies and their British antecedents. However, the authors have also examined generational trends elsewhere in the world and described similar cycles in several developed countries.The terminology for generational archetypes were updated(e.g. "Civics" became "Heroes", which they applied to the Millennial Generation, "Adaptives" became "Artists"), and the terms "Turning" and "Saeculum" were introduced. In the mid-1990s, Strauss and Howe began receiving inquiries about how their research could be applied to strategic problems in organizations. They started speaking frequently about their work at events and conferences. In July 2023 Howe released a new book, titled The Fourth Turning Is Here. Steve Bannon, former Chief Strategist and Senior Counselor to president Donald Trump during his first term, is a prominent proponent of the theory. As a documentary filmmaker, Bannon discussed the details of Strauss–Howe generational theory in Generation Zero. According to historian David Kaiser, who was consulted for the film, Generation Zero "focused on the key aspect of their theory, the idea that every 80 years of American history has been marked by a crisis, or 'fourth turning', that destroyed an old order and created a new one". Kaiser said Bannon is "very familiar with Strauss and Howe's theory of crisis, and has been thinking about how to use it to achieve particular goals for quite a while." A February 2017 article from Business Insider titled: "Steve Bannon's obsession with a dark theory of history should be worrisome", commented: "Bannon seems to be trying to bring about the 'Fourth Turning'." Defining a generation Strauss and Howe describe the history of the U.S. as a succession of Anglo-American generational biographies from 1433 to the present, and theorized a recurring generational cycle in American history. The authors posit a pattern of four repeating phases, generational types, and a recurring cycle of spiritual awakenings and secular crises, from the founding colonials of America through the present day. Strauss and Howe define a social generation as the aggregate of all people born over a span of roughly 21 years or about the length of one phase of life: childhood, young adulthood, midlife, and old age. Generations are identified (from the first birthyear to last) by looking for cohort groups of this length that share three criteria. First, members of a generation share what the authors call an age location in history: they encounter key historical events and social trends while occupying the same phase of life. In this view, members of a generation are shaped in lasting ways by the eras they encounter as children and young adults and they share certain common beliefs and behaviors. Aware of the experiences and traits that they share with their peers, members of a generation would also share a sense of common perceived membership in that generation. They based their definition of a generation on the work of various writers and social thinkers, from ancient writers such as Polybius and Ibn Khaldun to modern social theorists such as José Ortega y Gasset, Karl Mannheim, John Stuart Mill, Émile Littré, Auguste Comte, and François Mentré. Turnings While writing Generations, Strauss and Howe described a theorized pattern in the historical generations they examined, which they say revolved around generational events which they call turnings. In Generations, and in greater detail in The Fourth Turning, they describe a four-stage cycle of social or mood eras which they call "turnings". The turnings include: "the high", "the awakening", "the unraveling", and "the crisis". High According to Strauss and Howe, the first turning is a high, which occurs after a crisis. During the high, institutions are strong and individualism is weak. Society is confident about where it wants to go collectively, though those outside the majoritarian center often feel stifled by conformity. According to the authors, the most recent first turning in the US was the post–World War II American high, beginning in 1946 and ending with the assassination of John F. Kennedy on November 22, 1963. Awakening According to the theory, the second turning is an awakening. This is an era when institutions are attacked in the name of personal and spiritual autonomy. Just when society is reaching its high tide of public progress, people suddenly tire of social discipline and want to recapture a sense of "self-awareness", "spirituality" and "personal authenticity". Young activists look back at the previous High as an era of cultural and spiritual poverty. Strauss and Howe say the U.S.'s most recent awakening was the "consciousness revolution", which spanned from the campus and inner-city revolts of the mid-1960s to the tax revolts of the early 1980s. Unraveling According to Strauss and Howe, the third turning is an unraveling. The mood of this era they say is in many ways the opposite of a high: Institutions are weak and distrusted, while individualism is strong and flourishing. The authors say highs come after crises when society wants to coalesce and build and avoid the death and destruction of the previous crisis. Unravelings come after awakenings when society wants to atomize and enjoy. They say the most recent unraveling in the US began in the 1980s and includes the long boom and the culture war. Crisis According to the authors, the fourth turning is a crisis. This is an era of destruction, often involving war or revolution, in which institutional life is destroyed and rebuilt in response to a perceived threat to the nation's survival. After the crisis, civic authority revives, cultural expression redirects toward community purpose, and people begin to locate themselves as members of a larger group. The authors say the previous fourth turning in the US began with the Wall Street Crash of 1929 and climaxed with the end of World War II. The G.I. generation (which they call a hero archetype, born 1901 to 1924) came of age during this era. They say their confidence, optimism, and collective outlook epitomized the mood of that era. The authors assert the millennial generation (which they also describe as a hero archetype, born 1982 to 2005) shows many similar traits to those of the G.I. youth, which they describe as including rising civic engagement, improving behavior, and collective confidence. Cycle The authors describe each turning as lasting circa 21 years. Four turnings make up a full cycle of circa 85 years, which the authors term a saeculum, after the Latin word meaning both "a long human life" and "a natural century". Generational change drives the cycle of turnings and determines its periodicity. As each generation ages into the next life phase (and a new social role) society's mood and behavior fundamentally change, giving rise to a new turning. Historical events shape generations in childhood and young adulthood; then, as parents and leaders in midlife and old age, generations in turn shape history. Each of the four turnings has a distinct "mood" that recurs every saeculum. Strauss and Howe describe these turnings as the "seasons of history". At one extreme is the Awakening, which is analogous to summer, and at the other extreme is the Crisis, which is analogous to winter. The High and the Unraveling are similar to spring and autumn, respectively. Strauss and Howe have discussed 26 theorized turnings over 7 saecula in Anglo-American history, from the year 1433 through today. The core of Strauss and Howe's ideas is a basic alternation between two different types of eras, Crises and Awakenings. Both of these are defining eras in which people observe that historic events are radically altering their social environment. Crises are periods marked by major secular upheaval, when society focuses on reorganizing the outer world of institutions and public behavior (they say the last American Crisis was the period spanning the Great Depression and World War II). Awakenings are periods marked by cultural or religious renewal when society focuses on changing the inner world of values and private behavior (the last American Awakening was the "Consciousness Revolution" of the 1960s and 1970s). During Crises, an ethic of collectivism emerges. During Awakenings, an ethic of individualism emerges, and the institutional order is attacked by new social ideals and spiritual agendas. According to the authors, about every 85 years—the length of a long human life—a national Crisis occurs in American society. Roughly halfway to the next Crisis, a cultural Awakening occurs (historically, these have often been called Great Awakenings). In describing this cycle of Crises and Awakenings, they draw from the work of other historians and social scientists who have also discussed long cycles in American and European history, which have grown to show a trend of economic downturns the more a society has industrialised. The cycle of Crises corresponds with long cycles of war identified by such scholars as Arnold J. Toynbee, Quincy Wright, and L. L. Ferrar Jr., and with geopolitical cycles identified by William R. Thompson and George Modelski. Strauss and Howe say their cycle of Awakenings corresponds with Anthony Wallace's work on revitalization movements; they also say recurring Crises and Awakenings correspond with two-stroke cycles in politics (Walter Dean Burnham, Arthur Schlesinger Sr. and Jr.), foreign affairs (Frank L. Klingberg), and the economy (Nikolai Kondratieff) as well as with long-term oscillations in crime and substance abuse. Archetypes The authors say two different types of eras and two formative age locations associated with them (childhood and young adulthood) produce four generational archetypes that repeat sequentially, in rhythm with the cycle of Crises and Awakenings. In Generations, they refer to these four archetypes as Idealist, Reactive, Civic, and Adaptive. In The Fourth Turning (1997) they change this terminology to Prophet, Nomad, Hero, and Artist. They say the generations in each archetype not only share a similar age-location in history, but they also share some basic attitudes towards family, risk, culture and values, and civic engagement. In essence, generations shaped by similar early-life experiences develop similar collective personas and follow similar life trajectories. To date, Strauss and Howe have described 25 generations in Anglo-American history, each with a corresponding archetype. The authors describe the archetypes as follows: Prophet Prophet (Idealist) generations enter childhood during a High, a time of rejuvenated community life and consensus around a new societal order. Examples: Transcendental Generation, Missionary Generation, Baby Boomers. Nomad Nomad (Reactive) generations enter childhood during an Awakening, a time of social ideals and spiritual agendas when young adults are passionately attacking the established institutional order. Examples: Gilded Generation, Lost Generation, Generation X. Hero Hero (Civic) generations enter childhood during an Unraveling, a time of individual pragmatism, self-reliance, and unrestrained. Examples: Republican Generation, G.I. Generation, Millennials. Artist Artist (Adaptive) generations enter childhood during a Crisis, a time when public consensus, aggressive institutions, and an ethic of personal sacrifice were favored. Examples: Progressive Generation, Silent Generation, Homeland Generation. Summary An average modern life is around 85 years and consists of four periods of ~21 years Childhood → Young adult → Midlife → Elderhood A generation is an aggregate of people born every ~21 years Baby Boomers → Gen X → Millennials → Homelanders Each generation experiences "four turnings" every ~85 years High → Awakening → Unraveling → Crisis A generation is considered "dominant" or "recessive" according to the turning experienced as young adults. But as a youth generation comes of age and defines its collective persona an opposing generational archetype is in its midlife peak of power. Dominant: independent behavior + attitudes in defining an era Recessive: dependent role in defining an era Dominant generations Prophet (idealist): Awakening as young adults. Awakening, defined: Institutions are attacked in the name of personal and spiritual autonomy Hero (civic): Crisis as young adults. Crisis, defined: Institutional life is destroyed and rebuilt in response to a perceived threat to the nation's survival Recessive generations Nomad (reactive): Unravelling as young adults. Unravelling, defined: Institutions are weak and distrusted, individualism is strong and flourishing Artist (adaptive): High as young adults. High, defined: Institutions are strong, and individualism is weak Generations Late Medieval Saeculum Arthurian Generation The arthurian generation was born between 1433 and 1460 and is of the hero archetype. Members of the generation grew up during England's retreat from the Hundred Years' War in France, during an era of rising civil unrest. Humanist Generation The humanist generation was born between 1461 and 1482 and is of the artist/adaptive archetype. This generation grew up at the height of the Middle Ages, just prior to the Reformation and Renaissance. The educated middle classes are influenced by Renaissance Humanist teaching and presented with a clear career path through the church or State bureaucracy. Humanist influences took hold across Europe, and in many ways prepared the intellectual landscape for the coming reformation. Their youth coincided with the development of the European Printing press allowing greater dissemination of knowledge. According to Strauss and Howe, they became Greek language tutors, international scholars, poets, prelates, and literate merchants and yeomen. They described their education produced by the humanist generation as being focused on the qualitative and the subjective, rather than the quantitative and the objective. Some of the notable people who influenced this generation include Thomas More, Erasmus, Thomas Linacre, John Colet, Cardinal Wolsey, Michelangelo, Copernicus, Francisco Pizarro and Cesare Borgia. King Edward V was also born into this generation, but as he died at only 15 years old, it is difficult to properly place him in this archetype. However, according to the historian Dominic Mancini Edward was very fascinated with science and philosophy, and was very well learned beyond his years. Reformation Saeculum Reformation Generation The reformation generation was born between 1483 and 1511 and is of the prophet archetype. This generation rebelled as youths, prompting the first colleges in the 1520s. Reprisal Generation The reprisal generation was born between 1512 and 1540 and is of the nomad/reactive archetype. They grew up during the wars of the Spanish Armada and saw the expansion of British territories and colonization in the New World overseas. Elizabethan Generation The Elizabethan generation was born between 1541 and 1565 and is of the hero archetype. They benefited as children from growth in academies. They grew up during the Anglo-Spanish War (1585–1604). They regulated commerce, explored overseas empires, built English country houses, pursued science, and wrote poetry that celebrated an orderly universe. Parliamentary Generation The parliamentary generation was born between 1566 and 1587 and is of the artist archetype. Their grew up during an era of foreign threats and war. They built credentials in law, scholarship, religion, and arts and crafts guilds. New World Saeculum Puritan Generation The puritan generation was born between 1588 and 1617 and is of the prophet archetype. Members of the generation were led through the Wars of the Three Kingdoms (1639–1651) by King Charles I and others led a large migration to the Americas. The generation was very religious. Cavalier Generation The cavalier generation was born from 1618 to 1647 and was of the nomad archetype. Members of this generation grew up in an era of religious upheaval and family collapse. Their generation was notoriously violent and uneducated, causing men to take great risks, and resulting in many young deaths. Their generation acted in many ways in reaction against the harsh piety and frugality of the puritans, with a more laissez-faire social attitude. This was the time of Merry Old England and the zeitgeist of this generation was possibly best displayed by king Charles II. Glorious Generation The glorious generation was born from 1648 to 1673 and was of the hero archetype. They had a protected childhood with tax-supported schools and new laws discouraging the kidnapping of young people. After participating in the Indian Wars and the Glorious Revolution, they became involved in the electoral office at a young age. As young adults, they took pride in the growing political, commercial, and scientific achievements of England. They designed insurance, paper money, and public works. Enlightenment Generation The enlightenment generation' was born between 1674 and 1700 and was of the artist archetype. They grew up as protected children when families were close, youth risk discouraged, and good educations and well-connected marriages highly prized. As adults, they provided America's first large cadre of credentialed professionals, political managers, and plantation administrators. Examples in Europe include George Frederic Handel, Antonio Vivaldi, Domenico Scarlatti, and Johann Sebastian Bach. Revolutionary Saeculum Awakening Generation The awakening generation was born between 1701 and 1723 and was of the prophet archetype. They were the first colonial generation to consist mostly of the offspring of native-born parents. As adults, they attacked their elders' moral complacency. Benjamin Franklin was born in this generation. Liberty Generation Strauss and Howe define the liberty generation (nomad archetype) as those born between 1724 and 1741. The first two U.S. Presidents, George Washington and John Adams, were born during this period. Also born in this era were 35 out of the 56 signatories of the United States Declaration of Independence. Republican Generation The republican generation (hero archetype) was born between 1742 and 1766. This generation is known for participating in several global revolutionary movements during the Age of Revolution. This generation witnessed political turmoil in response to the widespread expansion of European imperialism and the vast social inequalities exacerbated by ruthless competition between rival empires in Europe, the Americas and Asia. They came of age during a time when the viability of mercantilism and imperialism was being questioned both in Europe and the Americas. Relying on Enlightenment philosophy, they unleashed violent episodes of revolution, vilified Monarchy, and promoted Republicanism. In colonial America, they participated in the American Revolutionary War, secured America's independence from British rule, and established the American government. Many founding fathers and leading figures in the early years of the independent United States belong to this generation, including U.S. presidents Thomas Jefferson, James Madison, and James Monroe, as well as the leading figures of the French Revolution such as Maximilien Robespierre, Georges Danton, and Camille Desmoulins. Compromise Generation The compromise generation was born between 1767 and 1791 and was of the artist archetype. They "rocked in the cradle of the Revolution" as they watched brave adults struggle and triumph. Notable persons affiliated with this generation include Andrew Jackson, Napoleon Bonaparte and Simón Bolívar. Civil War Saeculum Transcendental Generation The transcendental generation was born between 1792 and 1821 and was of the prophet archetype. They started the Second Great Awakening across the United States. Gilded Generation Strauss and Howe define the gilded generation (nomad archetype) as those born from 1822 to 1842. They came of age during rising national tempers, torrential immigration, rampant commercialism and consumerism, declining college enrollment, and economic disputes. This led to a distrust of zealotry and institutional involvement, shifting focus to a life of materialism.. Most of the American Civil War soldiers were born during this period (the average age was 26). Progressive Generation The progressive generation (hero and artist archetypes) was born from 1843 to 1859 and grew up during or fought in the American Civil War. Great Power Saeculum Missionary Generation The missionary generation was born from 1860 to 1882 and is of the prophet/idealist archetype. Members of the missionary generation have been described as the "home-and-hearth children of the post-Civil War era". They were an idealist generation and as young adults; their leaders were famous preachers. Some were graduates of newly formed black and women's colleges. Their defining characteristics were missionary and social crusades: "muckraker" journalism, prohibitionism, workers' rights, trade unionism and women's suffrage. In midlife, they developed prohibition in the United States, immigration control, and organized vice squads. Because the lost generation were severely impacted by World War I, the leadership of the missionary generation lasted longer than previous generations and in the 1930s and 1940s, their elite became the "wise old men" who enacted a "New Deal" and Social Security, led the World War II, and reaffirmed America's highest ideals during this period. This generation is fully ancestral, with the last known member of the missionary generation, the American Sarah Knauss, having died on December 30, 1999, at 119 years of age. Lost Generation The lost generation (nomad archetype) is the generation that came of age during World War I. "Lost" in this context also means "disoriented, wandering, directionless"—a recognition that there was great confusion and aimlessness among the war's survivors in the early post-war years. Strauss and Howe define the cohort as individuals born between 1883 and 1900. Like the previous generation, the Lost Generation is fully ancestral, with the last known member of the lost generation, the Japanese Nabi Tajima, having died on April 21, 2018, at 117 years of age. G.I. Generation The greatest generation (hero archetype), also known as the G.I. generation and the World War II generation, is the demographic cohort following the lost generation and preceding the silent generation. Strauss and Howe define the cohort as individuals born between 1901 and 1924. They were shaped by the Great Depression and were the primary participants in World War II. Silent Generation The silent generation (artist archetype) is the demographic cohort following the greatest generation and preceding the baby boomers. Strauss and Howe define the cohort as individuals born between 1925 and 1942. Millennial Saeculum Baby Boom Generation Strauss and Howe define the baby boom generation (prophet archetype) as those born from 1943 to 1960. 13th Generation/Generation X Strauss and Howe define the 13th generation (nomad archetype) as those born from 1961 to 1981. Millennial Generation (As of 2023): Neil Howe defines the millennial generation (hero archetype) as those born from 1982 to 2005. Homeland Generation (As of 2023): Neil Howe defines the homeland generation (artist archetype) as those born from 2006 to 2029. Timing of generations and turnings Strauss and Howe argue that the basic length of both generations and turnings—approximately 21 years—derives from longstanding socially and biologically determined phases of life. This is the reason it has remained relatively constant over centuries. Some have argued that rapid increases in technology in recent decades are shortening the length of a generation. According to Strauss and Howe, this is not the case. As long as the transition to adulthood occurs around age 21, the transition to midlife around age 43, and the transition to old age around age 65, they say the basic length of both generations and turnings will remain the same. In their book, The Fourth Turning, however, Strauss and Howe say that the precise boundaries of generations and turnings are erratic. Strauss and Howe compare the saecular rhythm to the four seasons, which they say similarly occur in the same order, but with slightly varying timing, they claimed that the same is true for a Fourth Turning in any given saeculum. Critical reception The Strauss and Howe interpretation of history through a generational lens has received mixed reviews and described as a form of pseudoscience. Some reviewers have praised the authors for their ambition, erudition, and accessibility. For example, former U.S. Vice President Al Gore called Generations: The History of America's Future, 1584 to 2069 the most stimulating book on American history he'd ever read, and sent a copy to each member of Congress. The theory has been influential in the fields of generational studies, marketing, and business management literature. However, it has also been criticized by historians, political scientists, and journalists, as being overly deterministic, non-falsifiable, and unsupported by rigorous evidence. Generations: The History of America's Future, 1584 to 2069 After the publication of their first book Generations, Morton Keller, a professor of history at Brandeis University, said that the authors "had done their homework". He said that their theory could be seen as pop sociology and that it would "come in for a lot more criticism as history. But it's almost always true that the broader you cast your net, the more holes it's going to have. And I admire [the authors'] boldness." Sociologist David Riesman and political scientist Richard Neustadt offered strong, if qualified, praise. Riesman found in the work an "impressive grasp of a great many theoretical and historical bits and pieces" and Neustadt said Strauss and Howe "are asking damned important questions, and I honor them." The Times Literary Supplement called it "fascinating", but also "about as vague and plausible as astrological predictions". Publishers Weekly called it "as woolly as a newspaper horoscope". In 1991, Jonathan Alter wrote in Newsweek that Generations was a "provocative, erudite and engaging analysis of the rhythms of American life". However, he believed it was also "an elaborate historical horoscope that will never withstand scholarly scrutiny." He continued, "these sequential 'peer personalities' are often silly, but the book provides reams of fresh evidence that American history is indeed cyclical, as Arthur Schlesinger Jr. and others have long argued." But he complained, "The generational boundaries are plainly arbitrary. The authors lump together everyone born from 1943 through the end of 1960 (Baby Boomers), a group whose two extremes have little in common. And the predictions are facile and reckless." He concluded: "However fun and informative, the truth about generational generalizations is that they're generally unsatisfactory." Arthur E. Levine, a former president of the Teachers College of Columbia University said "Generational images are stereotypes. There are some differences that stand out, but there are more similarities between students of the past and the present. But if you wrote a book saying that, how interesting would it be?" In response to criticism that they stereotype or generalize all members of a generation, the authors have said, "We've never tried to say that any individual generation is going to be monochromatic. It'll obviously include all kinds of people. But as you look at generations as social units, we consider it to be at least as powerful and, in our view, far more powerful than other social groupings such as economic class, race, sex, religion, and political parties." Gerald Pershall wrote in 1991: "Generations is guaranteed to attract pop history and pop social science buffs. Among professional historians, it faces a tougher sell. Period specialists will resist the idea that their period is akin to several others. Sweeping theories of history are long out of fashion in the halls of ivy, and the authors' lack of academic standing won't help their cause. Their generational quartet is "just too wooden" and "too neat," says one Yale historian. "Prediction is for prophets," scoffed William McLoughlin (a former history professor at Brown), who said it is wrong to think that "if you put enough data together and have enough charts and graphs, you've made history into a science." He also said the book might get a friendlier reception in the sociology and political science departments than in the science department. In 1991, professor and New York Times writer Jay Dolan critiqued Generations for not talking more about class, race, and sex, to which Neil Howe replied that they "are probably generalizations not even as effective as a generation to say something about how people think and behave. One of the things to understand is that most historians never look at history in terms of generations. They prefer to tell history as a seamless row of 55-year-old leaders who always tend to think and behave the same way -- but they don't and they never have. If you look at the way America's 55-year-old leaders were acting in the 1960s -- you know, the ebullience and confidence of the JFKs and LBJs and Hubert Humphreys -- and compare them with today's leaders in Congress -- the indecision, the lack of sure-footedness -- I think you would have to agree that 55-year-olds do not always act the same way and you're dealing with powerful generational forces at work that explain why one generation of war veterans, war heroes, and another generation which came of age in very different circumstances tend to have very different instincts about acting in the world." Responding to criticisms in 1991, William Strauss accepted that some historians might not like their theory, responding with: "People are looking for a new way to connect themselves to the larger story of America. That is the problem. We've felt adrift over the past 10 years, and we think that the way history has been presented over the past couple of decades has been more in terms of the little pieces and people are not as interested in the little pieces now. They're looking for a unifying vision. We haven't had unifying visions of the story of America for decades now, and we're trying to provide it in this book. The kinds of historians who are drawn to our book -- and I'm sure it will be very controversial among academics because we are presenting something that is so new -- but the kinds who are drawn to it are the ones who themselves have focused on the human life cycle rather than just the sequential series of events. Some good examples of that are Morton Keller up at Brandeis and David Hackett Fischer. These are people who have noticed the power in not just generations, but the shifts that have happened over time in the way Americans have treated children and older people and have tried to link that to the broader currents of history." The Fourth Turning In his review for the Boston Globe, historian David Kaiser called The Fourth Turning "a provocative and immensely entertaining outline of American history, Strauss and Howe have taken a gamble". "If the United States calmly makes it to 2015, their work will end up in the ashcan of history, but if they are right, they will take their place among the great American prophets." Kaiser has since argued that Strauss and Howe's predictions of coming crisis seems to have occurred, citing events such as 9/11, the 2008 financial crisis, and the recent political gridlock. Kaiser has incorporated Strauss and Howe's theory in two historical works of his own, American Tragedy: Kennedy, Johnson, and the Origins of the Vietnam War (2000), and No End Save Victory: How FDR Led the Nation into War (2014). Michael Lind, a historian and co-founder of the New America Foundation, wrote that The Fourth Turning (1997) was vague and verged into the realm of "pseudoscience"; "most of the authors' predictions about the American future turn out to be as vague as those of fortune cookies". Lind said that the theory is essential "non-falsifiable" and "mystifying." For The New York Times in 2017, Pulitzer-winning journalist Jeremy Peters wrote that "many academic historians dismiss the book as about as scientific as astrology or a Nostradamus text." 13th Gen In 1993, Andrew Leonard reviewed the book 13th Gen: Abort, Retry, Ignore, Fail?. He wrote "as the authors (Strauss and Howe) relentlessly attack the iniquitous 'child-abusive culture' of the 1960s and '70s and exult in heaping insult after insult on their own generation -- they caricature Baby Boomers as countercultural, long-haired, sex-obsessed hedonists -- their real agenda begins to surface. That agenda becomes clear in part of their wish list for how the 13th generation may influence the future: "13ers will reverse the frenzied and centrifugal cultural directions of their younger years. They will clean up entertainment, de-diversify the culture, reinvent core symbols of national unity, reaffirm rituals of family and neighborhood bonding, and re-erect barriers to cushion communities from unwanted upheaval." Again in 1993, writing for The Globe and Mail, Jim Cormier reviewed the same book: "self-described boomers Howe and Strauss add no profound layer of analysis to previous pop press observations. But in cobbling together a more extensive overview of the problems and concerns of the group they call the 13ers, they've created a valuable primer for other fogeys who are feeling seriously out of touch." Cormier wrote that the authors "raised as many new questions as answers about the generation that doesn't want to be a generation. But at least they've made an honest, empathetic, and good-humoured effort to bridge the bitter gap between the twentysomethings and fortysomethings." In 1993, Charles Laurence at the London Daily Telegraph wrote that, in 13th Gen, Strauss and Howe offered this youth generation "a relatively neutral definition as the 13th American generation from the Founding Fathers,". According to Alexander Ferron's review in Eye Magazine, "13th Gen is best read as the work of two top-level historians. While its agenda is the 13th generation, it can also be seen as an incredibly well-written and exhaustive history of America from 1960 to 1981--examining the era through everything except the traditional historical subjects (war, politics, famine, etc)." In 2011, Jon D. Miller, at the Longitudinal Study of American Youth, funded by the National Science Foundation, wrote that the birth year definition (1961 to 1981) of "Generation X" ("13th Gen") has been widely used in popular and academic literature. Millennials Rising David Brooks reviewed the follow-up book about the next generation titled Millennials Rising (2000). "Millennials" is a term coined by Strauss and Howe. Brooks wrote: "This is not a good book, if by good you mean the kind of book in which the authors have rigorously sifted the evidence and carefully supported their assertions with data. But it is a very good bad book. It's stuffed with interesting nuggets. It's brightly written. And if you get away from the generational mumbo jumbo, it illuminates changes that really do seem to be taking place." Further, Brooks wrote that the generations aren't treated equally: "Basically, it sounds as if America has two greatest generations at either end of the age scale and two crummiest in the middle". A 2000 New York Times book review for this book titled: What's the Matter With Kids Today? Not a Thing, described the message of Millennials Rising as "we boomers are raising a cohort of kids who are smarter, more industrious and better behaved than any generation before", saying the book complimented the Baby Boomer cohort by way of their parenting skills. In 2001, reviewer Dina Gomez wrote in NEA Today that they make their case "convincingly," with "intriguing analysis of popular culture" but conceded that it "over-generalizes". Gomez argued that it is "hard to resist its hopeful vision for our children and future." Millennials Rising ascribes seven "core traits" to Millennials: special, sheltered, confident, team-oriented, conventional, pressured, and achieving. A 2009, Chronicle of Higher Education report commented Howe and Strauss based these core traits on a "hodgepodge of anecdotes, statistics, and pop-culture references" and on surveys of approximately 600 high-school seniors from Fairfax County, Virginia, an affluent county with median household income approximately twice the national average. The report described Millennials Rising as a "good-news revolution" making "sweeping predictions" and describing Millennials as "rule followers who were engaged, optimistic, and downright pleasant", commenting the "book gave educators and tens of millions of parents, a warm feeling, saying who wouldn't want to hear that their kids are special?" General In 2006, Frank Giancola wrote an article in Human Resource Planning that stated "the emphasis on generational differences is not generally borne out by empirical research, despite its popularity". In 2016 an article was published that explains the differences in generations, observed with the employer's position, through the development of working conditions, initiated by the employer. This development is due to the competition of firms on the job market for receiving more highly skilled workers. New working conditions as a product on the market have a classic product life-cycle and when they become widespread standard expectations of employees change accordingly. One criticism of Strauss and Howe's theory and generational studies is that conclusions are overly broad and do not reflect the reality of every person in each generation regardless of their race, color, national origin, religion, sex, age, disability, or genetic information. For example, Hoover cited the case of Millennials, noted that in 2009 commentators have tended to label white, affluent teenagers who accomplish great things as they grow up in the suburbs, who confront anxiety when applying to super-selective colleges, and who multitask with ease as their helicopter parents hover reassuringly above them as Millennials. The label tends not to appear in renderings of teenagers who happen to be minorities, poor, or who have never won a spelling bee. Nor does the term often refer to students from big cities and small towns that are nothing like Fairfax County, Virginia, or who lack technological know-how. Or who struggle to complete high school. Or who never even consider college. Or who commit crimes. Or who suffer from too little parental support. Or who drop out of college. Aren't they Millennials too?" In their 2000 book Millennials Rising they brought attention to the Millennial children of immigrants in the United States, "who face daunting challenges." They wrote "one-third have no health insurance, live below the poverty line and live in overcrowded housing". In a February 2017 article from Quartz two journalists commented on the theory saying: "it is too vague to be proven wrong, and has not been taken seriously by most professional historians. But it is superficially compelling, and plots out to some degree how America's history has unfolded since its founding". A May 2017 article from Quartz described the Strauss–Howe generational theory as "pseudoscience". In an April 2017 article from Politico, David Greenberg, a professor of history and media studies at Rutgers University, described Strauss–Howe generational theory as "crackpot theories". Peter Turchin, a scientist and specialist in the fields of cultural evolution, cliodynamics and structural-demographic theory, has criticized the theory, stating that it is not a scientific theory and that it is more akin to a prophecy since it "forces the historical record to fit a postulated cycle by stretching in some places and cutting off a bit here and there in others". In popular culture American electronic musician Oneohtrix Point Never was inspired by The Fourth Turning for the concept of his 2018 album Age Of and its accompanying performance installation MYRIAD. Will Arbery's play Heroes of the Fourth Turning, first produced at New York's Playwrights Horizons in 2019, is inspired by the theories of Strauss and Howe, and the character Teresa is a vocal proponent of them. The 2022 Netflix series The Watcher features a scene citing postulations from The Fourth Turning. See also Notes References Bibliography Strauss, William; Howe, Neil (1991). Generations: The History of America's Future, 1584 to 2069 (1 ed.). New York. ISBN 978-0-688-08133-1. OCLC 22306142.{{cite book}}: CS1 maint: location missing publisher (link) Howe, Neil; Strauss, William (1993). 13th Gen: Abort, Retry, Ignore, Fail? (1 ed.). New York: Vintage Books. ISBN 978-0-679-74365-1. OCLC 26632626. Strauss, William; Howe, Neil (1997). The Fourth Turning: An American Prophecy (1 ed.). New York. ISBN 978-0-553-06682-1. OCLC 35008291.{{cite book}}: CS1 maint: location missing publisher (link) Howe, Neil; Strauss, William (2000). Millennials Rising: The Next Great Generation. William Strauss. New York: Vintage Books. ISBN 978-0-375-70719-3. OCLC 44118080. Howe, Neil; Strauss, William (2007). Millennials Go to College: Strategies for a New Generation on Campus: Recruiting and Admissions, Campus Life, and the Classroom (2 ed.). Great Falls, Va.: LifeCourse Associates. ISBN 978-0-9712606-1-0. OCLC 123907203. Howe, Neil; Strauss, William; Nadler, Reena (2008). Millennials & K-12 Schools: Educational Strategies for a New Generation. Great Falls, Va.: LifeCourse Associates. ISBN 978-0-9712606-5-8. OCLC 311800406. External links Discussion forum of the Strauss and Howe generation theory A religious reform (from Latin re-: "back, again", and formare: "to form"; i.e. put together: "to restore, reconstruct, rebuild") aims at the reform of religious teachings. It is not to be confused with an organizational reform of a religious community, though mostly this is a consequence of a reform of religious teachings. Definition Religious reforms are performed when a religious community reaches the conclusion that it deviated from its - assumed - true faith. Mostly religious reforms are started by parts of a religious community and meet resistance in other parts of the same religious community. Religious reforms usually lead to a reformulation of the religious teachings held for true, and to the condemnation resp. rejection of teachings held for wrong. Mostly the deviation from the assumed true faith which gives reason for a religious reform crept in over a longer period of time, sometimes over centuries. A religious reform is always a reorientation at the historical beginnings of a religion (therefore: re-formare, reconstruct) under the perspective of the present time and with the knowledge of the present time. A typical example for deviations from an assumed true faith are social changes within society which lead a loss purpose for ethical prescriptions, so they have to be replaced by other ethical prescriptions in order to protect the underlying, unchanged value for the future. Another typical example is the factual falsification of traditional views, e.g. by better insights into historical events or into natural science, by which the traditional views are falsified. The eternally continuing change of society and the progress of human knowledge are the reasons why a "final" reform of religious teachings is not possible. Religious teachings have to be reformed again and again. This realization was formulated in a concise Latin sentence, allegedly deriving from a saying of St. Augustine, and popularized by the Swiss Reformed theologian Karl Barth in 1947: Ecclesia semper reformanda est, which means "the Church must always be reformed". Religious reforms do not aim at an adjustment to the spirit of the time in the first place, yet they naturally bring about certain adjustments to the present time, since the religious tradition is reconsidered and reformed under the perspective of the present time and with the knowledge of the present time. A full adjustment of a religious teaching to the spirit of the present time cannot be expected from a credible religious reform. Religious reforms which do not aim at the reestablishment of an assumed true faith in the first place, yet at a mere adjustment of religious teaching to the spirit of the time without respect to an assumed true faith are no religious reforms, strictly speaking. Their purpose is questionable since those reforms are not based on the faith of the believers. Reforms of this nature are often based on compulsion and are usually not long-lasting but are reversed in the next generations. An example is the attempt of the Roman emperor Julian the Apostate to restore paganism as state religion. The opponents of justified religious reforms are called traditionalists, their ideology is traditionalism. The adherents of reforms to adjust to the spirit of the time in the first place without respect for an assumed true faith are called modernists, their ideology is modernism. Both concepts were coined by Christian-Catholic historical developments, yet today they are applied to all religions. Famous examples of religious reforms The Buddhist councils (from the 5th century BCE onwards), for the clarification on the fundamental teachings of Gautama Buddha among the early Buddhist schools. The First Council of Nicaea (325 CE), for the clarification on the doctrine of the Trinity among the early Christian theologians. The Muʿtazila school of Islamic theology, which existed between the 8th and 10th centuries. The Protestant Reformation led by Martin Luther, John Calvin, and other Protestant Reformers between the 16th and 17th centuries. Religious liberalism Liberal Christianity Fundamentalist–Modernist controversy The introduction of the historical-critical method in Biblical scholarship in the 19th and 20th centuries. Liberalism and progressivism within Islam Islamic modernism Reform Judaism, otherwise referred to as Liberal Judaism. The Second Vatican Council of the Catholic Church (1962–1965). See also Aggiornamento Bid'ah Criticism of Christianity Criticism of Islam Criticism of Judaism Continuous revelation Personal revelation in the Latter Day Saint movement Progressive revelation in the Baháʼí Faith Gamaliel's principle Heresy Religious studies Sociology of religion References Bibliography Ronald L. Johnstone: Religion in Society: A Sociology of Religion, Pearson/Prentice Hall, 2006. Armin W. Geertz, Jeppe Sinding Jensen: Religion, Tradition, and Renewal, Aarhus Universitetsforlag, 1991. Michael Molloy, Richard C. Trussell: Experiencing the World's Religions: Tradition, Challenge and Change, McGraw-Hill Higher Education, 1998. John P. Bradbury: Perpetually Reforming: A Theology of Church Reform and Renewal, 2013. External links Religious Reforms, General thoughts on reform on Almuslih.org. The arts or creative arts are a vast range of human practices involving creative expression, storytelling, and cultural participation. The arts encompass diverse and plural modes of thought, deeds, and existence in an extensive range of media. Both a dynamic and characteristically constant feature of human life, the arts have developed into increasingly stylized and intricate forms. This is achieved through sustained and deliberate study, training, or theorizing within a particular tradition, generations, and even between civilizations. The arts are a medium through which humans cultivate distinct social, cultural, and individual identities while transmitting values, impressions, judgments, ideas, visions, spiritual meanings, patterns of life, and experiences across time and space. The arts are divided into three main branches. Examples of visual arts include architecture, ceramic art, drawing, filmmaking, painting, photography, and sculpture. Examples of literature include fiction, drama, poetry, and prose. Examples of performing arts include dance, music, and theatre. The arts can employ skill and imagination to produce physical objects and performances, convey insights and experiences, and construct new natural environments and spaces. The arts can refer to common, popular, or everyday practices, as well as more sophisticated, systematic, or institutionalized ones. They can be discrete and self-contained or combine and interweave with other art forms, such as combining artwork with the written word in comics. Art forms can also develop or contribute to aspects of more complex art forms, as in cinematography. By definition, the arts themselves are open to being continually redefined. The practice of modern art, for example, is a testament to the shifting boundaries, improvisation and experimentation, reflexive nature, and self-criticism or questioning that art and its conditions of production, reception, and possibility can undergo. As both a means of developing capacities of attention and sensitivity and ends in themselves (art for art's sake), the arts can be a form of response to the world. It is a way to transform human responses and what humans deem worthwhile goals or pursuits. From prehistoric cave paintings during the Upper Palaeolithic, to ancient and contemporary forms of rituals, to modern-day films, the arts have registered, embodied, and preserved the ever-shifting relationships of humans with each other and the world. Definition The arts are considered various practices or objects done by people with skill, creativity, and imagination across cultures and history. These activities include painting, sculpting, music, theatre, literature, and more. Art refers to the way of doing or applying human creative skills, typically, but not necessarily, in visual form. However, there have been disputes on whether or not to classify something as a work of art, referred to as classificatory disputes about art. For example, classificatory disputes in the 20th century have included Cubist and Impressionist paintings, Marcel Duchamp's Fountain, the movies, J. S. G. Boggs' superlative imitations of banknotes, conceptual art, and video games. History and classifications In Ancient Greece, art and craft were referred to by the word techne. Ancient Greek art introduced veneration of the animal form and the development of equivalent skills to show musculature, poise, beauty, and anatomically correct proportions. Ancient Roman art depicted gods as idealized humans, shown with characteristically distinguishing features, such as Zeus' thunderbolt. In Byzantine and Gothic art of the Middle Ages, the dominant church insisted on the expression of Christian themes due to the overlap of church and state in medieval Europe. Asian art has generally worked in style akin to Western medieval art, namely a concentration on surface patterning and local colour. A characteristic of this style is that local colour is defined by an outline, the cartoon being a contemporary equivalent. This is evident in the art of India, Tibet, and Japan. Islamic art avoids the representation of living beings, particularly humans and other animals, in religious contexts. It instead expresses religious ideas through calligraphy and geometrical designs. Classifications In the Middle Ages, liberal arts were taught in European medieval universities as part of the trivium, an introductory curriculum involving grammar, rhetoric, and logic, and of the quadrivium, a curriculum involving the "mathematical arts" of arithmetic, geometry, music, and astronomy. In modern academia, the arts can be grouped with, or as a subset of, the humanities. The arts have been classified into seven forms: painting, architecture, sculpture, literature, music, theatre, and filmmaking. Some arts may be derived from others; for example, drama is literature with acting, dance is music expressed through motion, and songs are music with literature and human voice. Film is sometimes called the "eighth" and comics the "ninth art" in Francophone scholarship, adding to the traditional "Seven Arts". Cultural fields like gastronomy are only sometimes considered as arts. Visual arts Visual art forms include architecture, ceramic art, crafts, design, drawing, filmmaking, image, painting, photography, printmaking, sculpture, and video. Many artistic disciplines such as performing arts, conceptual art, and textile arts, also involve aspects of the visual arts, as well as arts of other types. Within the visual arts, the applied arts, such as industrial design, graphic design, fashion design, interior design, and decorative arts are also included. Architecture Architecture is the art and science of designing buildings and structures. A wider definition would include the design of the built environment, from the macro level of urban planning, urban design, and landscape architecture, to the micro level of creating furniture. Architectural design usually must address feasibility and cost for the builder, as well as function and aesthetics for the user. In modern usage, architecture is the art and discipline of creating or inferring an implied or apparent plan for a complex object or system. Some types of architecture manipulate space, volume, texture, light, shadow, or abstract elements, to achieve pleasing aesthetics. Architectural works may be seen as cultural and political symbols or works of art. The role of architects, though changing, has been central to the design and implementation of pleasingly built environments in which people live. Ceramic art Ceramic art is art made from ceramic materials, which may take forms such as pottery, tiles, figurines, sculptures, and tableware. While some ceramic products are considered fine art, others are considered decorative, industrial, or applied art objects. Ceramics may also be considered artefacts in archaeology. People design, manufacture, and decorate pottery in pottery or ceramic factories. Some pottery is regarded as art pottery. In one-person pottery studios, ceramists or potters produce studio pottery. Ceramics exclude glass and mosaics made from glass tesserae. Conceptual art Conceptual art is art where the concepts or ideas involved in the work take precedence over traditional aesthetic and material concerns. The inception of the term in the 1960s referred to a strict and focused practice of idea-based art that defied traditional visual criteria associated with the visual arts in its presentation as text. Through its association with the Young British Artists and the Turner Prize during the 1990s, the popular usage of conceptual art, particularly in the United Kingdom, developed into a synonym for all contemporary art that does not practice the traditional skills of painting and sculpture. Drawing Drawing is a means of making an image using various tools and techniques. It generally involves making marks on a surface by applying pressure from a tool, or moving a tool across a surface. Common tools are graphite pencils, pen and ink, inked brushes, wax coloured pencils, crayons, charcoals, pastels, and marker pens. Digital tools with similar effects are also used. The main techniques used in drawing are line drawing, hatching, cross-hatching, random hatching, scribbling, stippling, and blending. An artist who excels in drawing is referred to as a drafter, draftswoman, or draughtsman. Drawing can be used to create art used in cultural industries such as illustrations, comics, and animation. Comics are often called the "ninth art" (le neuvième art) in Francophone scholarship, adding to the traditional "Seven Arts". Painting Painting is considered to be a form of self-expression. Drawing, gesture (as in action painting), composition, narration (as in narrative art), or abstraction (as in abstract art), among other aesthetic modes, may serve to manifest the expressive and conceptual intention of the practitioner. Paintings can be on a wide variety of topics, such as photographic, abstract, narrative, symbolistic (symbolism), emotive (Expressionism), or political in nature (artivism). Some modern painters, such as Jean Dubuffet or Anselm Kiefer, incorporate different materials, such as sand, cement, straw, wood, or strands of hair, for their artwork texture. Photography Photography as an art form refers to photographs that are created in accordance with the creative vision of the photographer. Art photography stands in contrast to photojournalism, which provides a visual account of news events, and commercial photography, the primary focus of which is to advertise products or services. Sculpture Sculpture is the branch of the visual arts that operates in three dimensions. It is one of the plastic arts. Durable sculptural processes originally used carving (the removal of material) and modelling (the addition of material, such as clay), in stone, metal, ceramic, wood, and other materials, but shifts in sculptural processes have led to almost complete freedom of materials and processes following modernism. A wide variety of materials may be worked by removal such as carving, assembled by welding or modelling, or moulded or cast. Applied arts The applied arts are the application of design and decoration to everyday, functional objects to make them aesthetically pleasing. The applied arts include fields such as industrial design, illustration, and commercial art. The term "applied art" is used in distinction to fine art, where the latter is defined as arts that aim to produce objects that are beautiful or provide intellectual stimulation but have no primary everyday function. In practice, the two often overlap. Literary arts Literature (also known as literary arts or language arts) is generally identified as a collection of writings, which in Western culture are mainly prose (both fiction and non-fiction), drama, and poetry. In much, if not all, of the world, artistic linguistic expression can be oral as well and include such genres as epic, legend, myth, ballad, other forms of oral poetry, and folktales. Comics, the combination of drawings or other visual arts with narrating literature, are called the "ninth art" (le neuvième art) in Francophone scholarship. Performing arts Performing arts comprise dance, music, theatre, opera, mime, and other art forms in which human performance is the principal product. Performing arts are distinguished by this performance element in contrast with disciplines such as visual and literary arts, where the product is an object that does not require a performance to be observed and experienced. Each discipline in the performing arts is temporal in nature, meaning the product is performed over a period of time. Products are broadly categorized as being either repeatable (for example, by script or score) or improvised for each performance. Artists who participate in these arts in front of an audience are called performers, including actors, magicians, comedians, dancers, musicians, and singers. Performing arts are also supported by the services of other artists or essential workers, such as songwriters and those involved with stagecraft. Performers adapt their physical appearance with tools such as costumes and theatrical makeup. Dance Dance generally refers to human movement, either used as a form of expression or presented in a social, spiritual, or performance setting. Choreography is the art of making dances, and the person who does this is called a choreographer. Definitions of what constitutes dance are dependent on social, cultural, aesthetic, artistic, and moral constraints, ranging from functional movement (such as folk dance) to codified virtuoso techniques such as ballet. Dance disciplines in sports include gymnastics, figure skating, and synchronized swimming. In martial arts, kata is compared to dance. Music Music is defined as an art form which medium is a combination of sounds. Though scholars agree that music generally consists of a few core elements, their exact definitions are debated. Commonly identified aspects include pitch (which governs melody and harmony), duration (including rhythm and tempo), intensity (including dynamics), and timbre. Though considered a cultural universal, the definition of music varies throughout the world as it is based on diverse views of nature, the supernatural, and humanity. Music is differentiated into composition and performance, while musical improvisation may be regarded as an intermediary tradition. Music can be divided into genres and subgenres, although the dividing lines and relationships between genres are subtle, open to individual interpretation, and controversial. Theatre Theatre is the branch of the performing arts concerned with acting out stories in front of an audience using combinations of speech, gesture, music, dance, sound, and spectacle. In addition to the standard narrative dialogue style, theatre takes such forms as opera (including Chinese opera), ballet, mime, kabuki, and Indian classical dance. Multidisciplinary artistic works Areas exist in which artistic works incorporate multiple artistic fields, such as film, opera, and performance art. While opera is often categorized as the performing arts of music, the word itself is Italian for "works", because opera combines artistic disciplines into a singular artistic experience. In a traditional opera, the work uses the following: sets, costumes, acting, a libretto, singers, and an orchestra. The composer Richard Wagner recognized the fusion of many disciplines into a single work of opera, exemplified by his cycle Der Ring des Nibelungen ("The Ring of the Nibelung"). He did not use the term opera for his works, but instead Gesamtkunstwerk ("synthesis of the arts" or sometimes "music drama"), emphasizing the literary and theatrical components, which were as important as the music. Classical ballet is another form that emerged in the 17th century in which orchestral music is combined with dance. Other works in the late 19th, 20th, and 21st centuries have fused other disciplines in creative ways, such as performance art. Performance art is a performance over time that combines any number of instruments, objects, and art within a predefined or less well-defined structure, some of which can be improvised. Performance art may be scripted, unscripted, random, or carefully organized—even audience participation may occur. John Cage is regarded by many as a performance artist rather than a composer, although he preferred the latter term. He did not compose for traditional ensembles. For example, Cage's composition Living Room Music, composed in 1940, is a quartet for unspecified instruments, really non-melodic objects, that can be found in the living room of a typical house, hence the title. Video games Video games are multidisciplinary works that include uncontroversial artistic elements such as visuals and sound, as well as an emergent experience from the nature of their interactivity. Within video game culture, debates surround whether video games should be classified as an art form and whether video game developers—AAA or indie—should be classified as artists. Hideo Kojima, a video game designer considered a gaming auteur, argued in 2006 that video games are a type of service rather than an art form. In the social sciences, cultural economists show how playing video games is conducive to involvement in more traditional art forms. In 2011, the National Endowment for the Arts included video games in its definition of a "work of art", and the Smithsonian American Art Museum presented an exhibit titled The Art of the Video Game in 2012. Criticism Art criticism is the discussion or evaluation of art. Art critics usually criticize art in the context of aesthetics or the theory of beauty. A goal of art criticism is the pursuit of a rational basis for art appreciation but it is questionable whether such criticism can transcend prevailing sociopolitical circumstances. The variety of art movements has resulted in a division of art criticism into different disciplines, which may each use different criteria for their judgements. The most common division in the field of criticism is between historical criticism and evaluation, a form of art history, and contemporary criticism of work by living artists. Despite perceptions that criticism is a lower-risk activity than making art, opinions of current art are liable to corrections with the passage of time. Critics of the past can be ridiculed for dismissing artists now venerated (like the early work of the Impressionists). Some art movements themselves were named disparagingly by critics, with the name later adopted as a badge of honour by the artists of the style with the original negative meaning forgotten, e.g. Impressionism and Cubism. Artists have had an uneasy relationship with their critics. Artists usually need positive opinions from critics for their work to be viewed and purchased. Many variables determine judgement of art, such as aesthetics, cognition, or perception. Aesthetic, pragmatic, expressive, formalist, relativist, processional, imitation, ritual, cognition, mimetic, and postmodern theories are some of the many theories to criticize and appreciate art. Art criticism and appreciation can be subjective based on personal preference toward aesthetics and form, or on the elements and principles of design and by social and cultural acceptance. Education Arts in education is a field of educational research and practice informed by investigations into learning through arts experiences. In this context, the arts can include performing arts education (dance, drama, and music), literature and poetry, storytelling, visual arts education in film, craft, design, digital art, media, and photography. Politics A strong relationship between the arts and politics, particularly between various kinds of art and power, occurs across history and archaeological cultures. As the arts respond to news and politics, they take on political as well as social dimensions, becoming a focus of controversy and a force of political and social change. Some artists have been observed to have free spirits. For instance, Alexander Pushkin, a well-regarded writer, attracted the irritation of Russian officialdom, particularly Emperor Alexander I, since he "instead of being a good servant of the state in the rank and file of the administration and extolling conventional virtues in his vocational writings (if write he must), composed extremely arrogant, independent, and wicked verse in which dangerous freedom of thought was evident in the novelty of his versification, in the audacity of his sensual fancy, and in his propensity for making fun of major and minor tyrants." In more recent times, Banksy, an England-based graffiti artist who constantly conflicted with the authorities, has also been considered a "free spirit" due to his work. Artists use their work to express their political views and promote social change, from negatively influencing through hate speech to positively influencing through artivism. Governments use art, or propaganda, to promote their own agendas. Notes References Bibliography Further reading External links Topic Dictionaries at Oxford Learner's Dictionaries Definition of Art by Lexico Meitei headgears, or headwears, or headdresses, are traditional headdresses originating from the Meitei community of Manipur, India. These headpieces are used in a variety of cultural, religious, and ceremonial contexts, including festivals, dance performances, weddings, and official functions. Each type of headgear is characterized by distinct structural and symbolic features, often associated with specific roles, gender, or occasions. Notable examples include the Kokyet, Kajenglei, and Samjin, each of which holds particular significance within the cultural and religious framework of Meitei society, including its indigenous belief system, Sanamahism. Kajenglei Kajenglei (ꯀꯖꯦꯡꯂꯩ), also known as Leitreng (ꯂꯩꯇ꯭ꯔꯦꯡ), is a traditional headdress worn by Meitei women. It is typically used during marriage ceremonies by brides and in classical dance performances by artists. The headdress is composed of approximately eighty to one hundred brass strips, which are affixed to red flannel bands measuring about one centimeter in width and arranged around a circular metal ring. Kokyet Kokyet (ꯀꯣꯛꯌꯦꯠ), also rendered as Koyet, Koyyet, or Koiyet, is a traditional male headdress in Meitei culture. It is produced in twelve distinct designs and is worn during various cultural ceremonies, festivals, and formal occasions. The Kokyet is used by individuals across social strata, including both commoners and members of the royal family. Its design is derived from the two horns of the ancient Meitei deity Pakhangba. The Kokyet also functions as a symbolic representation of Sidaba, a central deity in the traditional Meitei religion, Sanamahism. Samjin Samjin (ꯁꯝꯖꯤꯟ) is a traditional male headgear associated with Meitei cultural attire, notably worn by performers of the Khamba Thoibi Jagoi dance. It is attributed to the reign of King Meidingu Khuyoi Tompok. The headgear features intricate needlework with tassel-ended designs hanging from the front and back. Its stylistic elements are traditionally believed to be derived from the horn and head structure of the Meitei deity, Pakhangba. National level recognition Since 2023, the Kokyet has been incorporated into the official uniform of marshals serving in the Indian Parliament, encompassing both the Lok Sabha and the Rajya Sabha. The adoption of the headdress was authorized by the Union Government of India. Usage by notable people In 2017, during a visit to election-bound Manipur, Prime Minister Narendra Modi wore a traditional Meitei attire, including a headgear featuring a peacock plume and a cloak. In 2022, Manipur Chief Minister N. Biren Singh presented Prime Minister Narendra Modi with a traditional Meitei headgear, white in color with saffron embroidery, and a matching block-printed shawl. The headgear included a golden brooch, and the shawl contrasted with the Prime Minister’s tan-colored attire. See also Meitei clothing Meitei clothing in Bangladesh Meitei clothing in Myanmar Meitei royal etiquette Women in Meitei civilisation Manipur and Bollywood == References == The Gold effect is the phenomenon in which a scientific idea, particularly in medicine, is developed to the status of an accepted position within a professional body or association by the social process itself of scientific conferences, committees, and consensus building, despite not being supported by conclusive evidence. The Gold effect is used to analyze errors in public health policy and practice, such as the widespread use of cholesterol screening in the prevention of cardiovascular disease. The effect was described by Thomas Gold in 1979. The effect was reviewed by Petr Skrabanek and James McCormick in their book Follies and Fallacies in Medicine. In their book, Skrabanek and McCormick describe the Gold effect as: "At the beginning a few people arrive at a state of near belief in some idea. A meeting is held to discuss the pros and cons of the idea. More people favouring the idea than those disinterested will be present. A representative committee will be nominated to prepare a collective volume to propagate and foster interest in the idea. The totality of resulting articles based on the idea will appear to show an increasing consensus. A specialized journal will be launched. Only orthodox or near orthodox articles will pass the referees and the editor." The progression of the Gold effect was described by mathematician Raymond Lyttleton. According to Lyttleton, the area under the curve of a gaussian curve represents the number of people concerned with a particular subject. As the Gold effect progresses and more and more people start believing a particular idea, the gaussian curve starts to concentrate more around the center. By the end, the gaussian function will have become a delta function, representing everyone as a believer (infinite value at 0), with no non-believers. == References == The Four Perils (Chinese: 四凶; pinyin: Sì Xiōng) are four malevolent beings that exist in Chinese mythology. Book of Documents In the Book of Documents, they are defined as the "Four Criminals" (四罪; Sì Zuì): Gonggong (Chinese: 共工; pinyin: Gònggōng; lit. 'join(t) works'), the disastrous god; Huandou (驩兜; Huāndōu; 'happy helmet', a.k.a. 驩頭, 讙頭; Huāntóu; 'happy head'), a chimeric minister and/or nation from the south who conspired with Gonggong against Emperor Yao Gun (鯀; Gǔn; 'big fish'), whose poorly built dam released a destructive flood and disrupted the Wuxing, and whose son was Yu the Great; Sanmiao (三苗; Sān Miáo; 'Three Miao'), the tribes that attacked Emperor Yao's tribe. Zuo Zhuan, Shanhaijing, and Shenyijing In Zuo Zhuan, Shanhaijing, and Shenyijing, the Four Perils (Sì Xiōng) are defined as: the Hundun (渾敦, 混沌; Hùndùn; 'chaotic torrent'), a yellow winged creature of chaos with six legs and no face; the Qiongqi (窮奇; Qióngqí; 'distressingly strange', 'thoroughly odd'), a monstrous creature that eats people, the Taowu (檮杌; Táowù; 'block stump'), a reckless, stubborn creature; The Taowu is said to appear with "a human face, a tiger's feet, a pig's tusks and a tail 18 feet long." the Taotie (饕餮; Tāotiè; 'greedy glutton'), a gluttonous beast. Identification Zhang Shoujie's Correct Meanings of the Record of the Grand Historian (史記正義; Shǐjì Zhèngyì) identifies Huandou (讙兠) with Hundun (渾沌), Gonggong with Qiongqi (窮竒), Gun with Taowu (檮杌), and the Sanmiao (三苗; 'Three Miao') with Taotie (饕餮). See also Four Barbarians Four Horsemen of the Apocalypse Notes == References == Husák's Children (Czech: Husákovy děti, Slovak: Husákove deti) is a term commonly used for a generation of people born in Czechoslovakia during the baby boom which started in the early 1970s, during the period of "normalization". The generation was named after the President and a long-term Communist leader of Czechoslovakia, Gustáv Husák. State pro-population policy in the 1970s The most significant post-war baby boom in Czechoslovakia culminated in 1974. After the events of the Prague Spring in 1968 and subsequent restoration of the conditions prevailing before the reform period, many Czechoslovaks resigned themselves to their fate and became unconcerned with the political situation in the country. In reaction, the Czechoslovak communist régime came with its own version of Goulash Communism, presented in a new concept: the state pro-population policy. The duration of maternity leave was extended, the child allowance increased, and newlyweds were subsidized with attractive state loans. However, the "generous" state policy soon reached its limits. In the late 1970s, the state finance reserves intended for the support of the pro-population policy were limited and the baby boom ended. The influence of the state pro-population policy has been questioned in the past. The population growth began to increase in 1969. The state financial subsidy was approved in 1973 and the baby boom culminated in 1974. After the initial sharp increase, the birth rate began to decline. The roots of the unusual population explosion could be associated with the post-war period rather than the "social engineering" of the communist regime. 21st century As of the 00s, the generation of Husák's children represent an important element of Czech society. The 1970s generation of baby boomers caused another significant population increase in the later years of the aughts. Husák's children also became a target of a marketing and economic "retro-invasion" on the Czech market. After the fall of the Iron Curtain, following a period of fascination with almost any Western product, Czech society gradually turned its attention back to the traditional Czech brands. The marketing campaigns of the companies began to focus on the productive generation and its nostalgia for its childhood. Retro designs of products and brands known in the 1970s and 1980s appeared on the Czech market once again. The term Husák's children appeared in the words of the song 1970 by the Czech band Chinaski: See also Mánička Generation X References Further reading Roberts, Andrew Lawrence (2005). From Good King Wenceslas to the Good Soldier Švejk: a Dictionary of Czech Popular Culture. Central European University Press. ISBN 978-963-7326-26-4. External links Pedro chewing gum makes a comeback to Czech Republic (Zimbio.com, originally published by Prague Daily Monitor) HomeExchange is a network that facilitates home exchanges. Born of the merger between GuesttoGuest and HomeExchange.com, is a platform for the exchange of houses and apartments between private individuals. The platform is owned by Tukazza. Concept HomeExchange brings together private individuals who wish to exchange their house or apartment for short or long stays. Each house is assigned a value in points, called GuestPoints. Members can make reciprocal or non-reciprocal exchanges with GuestPoints. GuestPoints are earned by hosting guests in your home, and used by visiting other members' homes. To finalize an exchange, an annual membership is required (175 EUR or 235 USD per year for unlimited exchanges). History Ed Kushins first came up with the idea of starting a home exchange service back in 1992, after he and his family made their first home exchange from Hermosa Beach, California, to Washington D.C. On this trip, he learned that staying in a home, as opposed to a hotel, was not only more comfortable but offered a more enriching "living like a local" experience and was a lot more affordable. The HomeExchange club would later be launched that same year and was able to be navigated using a mail-ordered, printed directory. However, it would not become a web-based platform until 1996, when e-commerce was gaining popularity. It is currently unknown what the site originally looked like on its launch date since web.archive.org could only trace it to 1998, despite the site being founded in 1996. In 2006, HomeExchange.com was featured and promoted in the romantic comedy "The Holiday", produced and directed by Nancy Meyers. In March 2017, GuestToGuest merged with HomeExchange.com, and since 2019 continued under that name. GuestToGuest's founder, Emmanuel Arnaud, then became the CEO of HomeExchange.com. In March 2023, HomeExchange.com purchased its competitor, Love Home Swap. How it works HomeExchange works on the principle of collaborative exchange. Unlike traditional rentals, the platform allows you to travel without paying for accommodation. All you have to do is make your accommodation available to other members. In exchange, you can stay in a member's home, either directly or using a points system. There are two types of exchange on HomeExchange: Reciprocal exchange: two members exchange houses or apartments at the same time or on different dates. Non-reciprocal exchange: If a direct exchange is not possible, HomeExchange uses a points system called GuestPoints. To acquire them, all you have to do is welcome members into your home. These GuestPoints can then be used to stay with another member. The system works on an annual membership model which allows members to organize as many exchanges as they like at no additional cost. HomeExchange also emphasizes trust, with a profile verification system, a review system and guarantees to ensure the security of exchanges. HomeExchange around the world In 2025, HomeExchange is said to be present in 155 countries. Since 2022, the platform has grown from 100,000 to 200,000 members and now counts 360,000 houses and apartments worldwide. The countries where HomeExchange has the most members are France, the United States and Spain. HomeExchange's tourism model Collaborative tourism The HomeExchange concept is based on the desire to offer an authentic travel experience, favoring immersion in a local's home. A key player in collaborative tourism, HomeExchange encourages people to meet and share with each other. In addition to exchanging houses and apartments for vacations, HomeExchange also offers to exchange other amenities such as cars and bicycles, although these are not covered by the platform's guarantees. Increasingly popular, pet-sitting between private individuals is also possible on the platform, where some members offer their services when they go on vacation and in turn benefit from pet-sitting when they need it. Responsible tourism HomeExchange advocates a more sustainable and authentic travel model, combating mass tourism. Through its various communications, the brand encourages local, environmentally-friendly experiences, far from destinations saturated by traditional tourism. To meet its objective of promoting an inclusive, fair and regenerative economy, HomeExchange obtained B Corp certification in September 2022, attesting to its respect for strict social and environmental standards. References External links Official website Cultural literacy is a term coined by American educator and literary critic E. D. Hirsch, referring to the ability to understand and participate fluently in a given culture. Cultural literacy is an analogy to literacy proper (the ability to read and write letters). A literate reader knows the object-language's alphabet, grammar, and a sufficient set of vocabulary; a culturally literate person knows a given culture's signs and symbols, including its language, particular dialectic, stories, entertainment, idioms, idiosyncrasies, and so on. The culturally literate person is able to talk to and understand others of that culture with fluency. Causes Children of a given culture typically become culturally literate there via the process of enculturation. Enculturation seems to occur naturally, being intertwined with education, play, family relationships, friendships, etc. The cause of cultural literacy is a more difficult question when considering acculturation of immigrants, outsiders, cultural minorities, strangers, guests, etc. Literacy of a given culture seems to arise over time with consistent exposure to and participation in that culture, especially certain key cultural strongholds, like business, story, arts, education, history, religion, and family. One could become literate for an oral culture (with no written language or recorded media) only by extended conversation. Alternatively, one could become literate for a written culture through conversation as well as reading culturally relevant books or exposure to culturally relevant films, plays, monuments, television shows, etc. Western culture in general and Anglo-American culture in particular is a bibliocentric culture. It often trades in allusions to the Christian Bible, the influential works of Early Modern English such as works of William Shakespeare, the Thomas Cranmer Book of Common Prayer, Geoffrey Chaucer's poetry, and many others. Knowledge of these books (among others) contributes largely to cultural literacy in the west. However, also essential are exposure to the art, history, and the lived experience of members of that culture. Examples For example, in 1908 British author G. K. Chesterton wrote, "Complete self-confidence is a weakness... the man who has [self-confidence] has 'Hanwell' written on his face as plain as it is written on that omnibus". This statement, especially the latter half, might be opaque to a reader from outside the United Kingdom, who does not know that "omnibus" is a less common British word for "bus" and "Hanwell" was the name of a (now defunct) insane asylum. Consequences The benefits and detriments of cultural literacy are debated. For example, social mobility increases when one is able to comfortably participate in conversation with gatekeepers like employers and teachers. Non-native members of a culture, such as missionaries to a foreign land or refugees from a native land, may experience negative consequences due to cultural illiteracy. However, the achievement of cultural literacy may seem to come at a cost to one's own native culture. Research and questions Discussions of cultural literacy have given rise to several controversial questions: The Literature Question: How important are books to cultural literacy in the west? And which books? The Content Question: What kinds of knowledge are important for cultural literacy? Knowing such and such facts, names, dates or more ethereal experiences like having heard such and such a song? The Minority Question: Is cultural literacy part of the hegemony of the dominant culture? The Multicultural Question: Which culture are we talking about when we say "cultural" literacy? Should we be talking about one or several—and which one(s)? The Education Question: Should advancing cultural literacy be one of the goals of education? If so, what is the best means of doing so? The Assessment Question: How do we evaluate cultural literacy? Is there a best way to test someone's cultural literacy? See also Bildung Cultural competence Cultural sensitivity Educational essentialism Great books Intercultural communication References Further reading E. D. Hirsch Jr. (1987). Cultural Literacy: What Every American Needs to Know. Boston: Houghton Mifflin. ISBN 0-395-43095-X. Hirsch, Eric Donald; Kett, Joseph F.; Trefil, James S. (2002). The New Dictionary of Cultural Literacy (3rd ed.). Houghton Mifflin Harcourt. ISBN 0-618-22647-8. Christenbury, Leila "Cultural Literacy: A Terrible Idea Whose Time Has Come" The English Journal 78.1 (January 1989), pp. 14–17. Broudy, Harry S. (Spring 1990). "Cultural Literacy and General Education". Journal of Aesthetic Education. 24 (1, Special Issue: Cultural Literacy and Arts Education): 7–16. doi:10.2307/3332851. JSTOR 3332851. Anson, Chris M. "Book Lists, Cultural Literacy, and the Stagnation of Discourse" The English Journal 77.2 (February 1988), pp. 14–18. Zurmuehlen, Marilyn "Serious Pursuit of Cultural Trivialization" Art Education 42.6 (November 1989), pp. 46–49. Simpson, Alan "The Uses of "Cultural Literacy": A British View" Journal of Aesthetic Education 25.4, 25th Anniversary Issue Winter, 1991), pp. 65–73. Reedy, Jeremiah "Cultural Literacy and the Classics" The Classical Journal 84.1 (October 1988), pp. 41–46. Murray, Denise E. Diversity as Resource. Redefining Cultural Literacy (Alexandria, Virginia) 1994. Bernard Schweizer. "Cultural Literacy: Is It Time to Revisit the Debate?" Thought & Action 25 (Fall 2009). "Cultural Literacy Tests - Challenge Yourself With Our Free Online Cultural Literacy Tests". The Literacy Company. Archived from the original on 2021-07-29. Retrieved 2014-02-26. Family honor (or honour) is an abstract concept involving the perceived quality of worthiness and respectability that affects the social standing and the self-evaluation of a group of related people, both corporately and individually. The family is viewed as the main source of honor, and the community highly values the relationship between honor and the family. The conduct of family members reflects upon family honor and the way the family perceives itself and is perceived by others. Family honor can be dependent upon many factors. Areas that are affected by family honor include multiple aspects of lifestyle such as social status, religion, clothing, eating, education, job or career, ownership such as real estate, and marriage. People who live in cultures of honor perceive family as the central institution in their society, and a person's social identity depends largely on their family. Therefore, it is important for these individuals to fulfill expectations of family and society in order to be accepted by their family and experience feelings of belonging to this central institution that they are tied to through birth or marriage. In some cultures, maintaining family honor is perceived as more important than either individual freedom or individual achievement. The ideology and practice of family honor varies from country to country. Individuals of certain cultures are often unaware or discerning in their understanding of differing cultural traditions. Many fail to grasp the concept of honor as the basis for traditions such as defending one's honor or their family's. Some cultures value family honor more than others. Many times a family's honor may overpower the actions or beliefs of the individual. However, a theme that is common within many traditions is the respecting of elders. Children of the family are to respect their elders who have earned what some call a "badge of 'honor'" representative of their age. Once an individual has lived many years, they have earned this badge of honor and should be shown respect, teaching their young the cultural traditions that have deemed them honorable. History An individual is considered as honorable based on his/her behaviors and characteristics he or she displays that the society deems to be worthy of honor. In addition, honor also entails the aspect of how high of a position an individual holds in relation to the group and how much he or she is respected by others. One of the ideals of family honor is social class. Social class can be defined as a group of people categorized into a hierarchy based on the amount of money they have accumulated, how much education they have received, and the amount of power they hold within society, amongst other variables. People who play similar roles within society tend to have similar outlooks. Social standing affects the way in which families form. It determines how and who a person mates with, how they raise their children, and how people relate to one another. Historically, honor is a quality ascribed to an individual in two ways: either by obtaining it through his or her birth into an honorable family or being assigned as honorable by powerful people who hold higher status in the society. An individual's parental lineage, is the traditional source for his or her honor. Because honor is passed through paternal lineage in most patrilineal cultures, these societies historically considered having sons as a source of pride and honor. For example, in the Moroccan culture, it is still a preference among women to have sons instead of daughters. Morocco is a typical patrilineal society in which the son has a more important function for the family such as the son supports his parents once they have aged compared to the daughter who will marry into a different group becoming a loss to the family. In such societies men also hold more sexual rights compared to women besides the supportive role men obtain from caring for their aging parents. Females in these societies are perceived as threats to family honor. Within cultures, honor is an important and highly esteemed theme. It can be maintained through living up to one's word and promises, providing for the family, and keeping a certain social status. Honor can be affected by both men and women through ways in which a man heightens his family's honorable status, and a woman can shame her family through disapproved actions. Ensuing constant pressure to uphold her family's honor, a woman can suffer psychological and social damage. Gender roles The different effects honor instills upon men and women can be seen in the ancient world, where women and men played contrasting roles in society. Men displayed their honorable roles in public while women were restricted to the limitations of their households. While in public, women were required to avoid conversations with estranged men while only visiting places frequented by women. In the present Islamic culture, men hold a higher social status, but they also carry more responsibility in caring and providing for their family. If a man is single or childless his place in society does not waver or become lower. A woman should have a family whom she stays faithful to and respectful towards at all times. A man's actions will not greatly affect or hurt his family's standing like a woman's actions would. Women are perceived as vulnerable individuals who must be kept safe at all times. This aspect of a woman's characterization comes from her fertility and role within her family. Societies in which "family honor" is considered highly important generally place a correspondingly high degree of restriction of the freedom of women. In these cultures, a family may defend its honor, or may seek reparation or revenge if the family honor is perceived to have been abused or treated with disrespect. In Ancient Rome, sexual activity of married women outside of their marriage was seen as dishonor to the family and it was legal for men to kill their wives or married daughters that shamed the family through adultery. Reasoning for the designation of women to private and/or nonmale areas comes from the ancient tradition of a woman's place in the world. Women are not seen as independent individuals, but rather extensions of their male counterparts' identity and honor. Family honor within society Because the approval of honor is dependent upon the recognition of others, during ancient times individuals worked hard for the approval of their peers within their societal cultures. This meant that individuals were more likely to behave like their honorable counterparts. Groups reinforced what it meant to be honorable through various expected behaviors and goals placed amongst individual members. This discouraged members from any negative activity that may have adversely affected a group or family's honor. In the event that the leader of the group promoted actions that did not appear to be honorable on a larger societal scale, the leader then offered some explanation defending their actions which lead to the preservation of what they defined as honor and their traditions forever. In Ancient Rome, chastity and loyalty of members of a family was an important factor contributing to family honor in addition to social standing and accomplishments of that family. For example, if a married woman committed adultery, her father had the legal right to kill her whereas her husband was required to divorce her. If the husband chose not to divorce his wife, he would jeopardize his honor and be labeled as a pimp. In opposition to freedom In some cultures with strong principles of family honor, offspring are not free to choose a partner for themselves but may instead be expected to enter into an arranged marriage or if the offspring resists, into a forced marriage or child marriage. The use of violence may be collective in its character, where many relatives act together. Males and females in this type of honor culture may act as either persecutors and the oppressed, for instance a son in a family may be forced to enter into an arranged marriage by his older relatives while controlling his sisters. Family honor within different cultures Honor cultures exist throughout the world but are more common among peoples from regions stretching from North Africa via the Middle East, Central Asia and to the Indian subcontinent. Examples of these countries are Afghanistan, Albania, Eritrea, Iraq, Kurdistan, Libya, Palestine, Pakistan and Somalia. Family honour is also an important part of Latin American culture. Middle East and Africa The aspects of family honor mentioned above differ throughout varying cultures and countries. Family honor in the Bedouin and other Middle Eastern cultures consists of interdependent forms of ird and sharaf. Ird is the honor of a woman she is born with which involves her chastity and continence whereas sharaf is the honor code for men which depends on ird of women in the family. Due to its connection to ird, sharaf includes protecting ird of the family members. The adherence to these gender-specific honor codes is important to keep the respectability and sexual honors of the family, particularly men, known as namus. For example, the sexual relationships of a girl are seen in these societies to make her impure and of lesser value, which affects her eligibility for marriage. Public knowledge and gossip about sexual impurity or adultery are proposed to be the main reasons that loss of family honor can bring shame to the family. Preservation of family honor is not only important for respectability of family members in society but also affects the fate of all family members. To protect family honor and dignity, families may resort in killing the woman who is involved in the dishonorable act. In an associated context, to protect the purity and chastity of young girls, families may decide to practice female genital mutilation, a practice that removes or damages female genital organs to assure that sex will not be pleasurable. A mutilated woman is changed so that she has no desire to engage in sexual activity that may undermine the family's honor. An example of this can be seen in Sierra Leone, Africa where young girls are mutilated every year. The number of girls mutilated in Africa per year has risen to 3 million. Rugiatu Turay, founder of the Amazonian Initiative Movement, protects young girls from being circumcised by other women in secret societies like Sande and other female practitioners who still engage in the ceremonial tradition today. Girls as young as the age of five assist in the mutilation of other young girls in the country. At the age of 12, Turay was snatched by female family members, held down and had her clitoris cut off with a knife. She was beaten, forced to walk, and had hot pepper water poured in her eyes. As she was mutilated, the women sang, danced, and clap ceremoniously. According to these women, Turay had become a woman. However, females are generally mutilated under the age of 15. Girls who are trained to assist in this ceremony are trained as young as five years old. Turay has convinced 400 practitioners to stop the practice of female mutilation, but 97 million females have been mutilated within the country and the numbers remain constant, even increasing. The practice has been enforced by politicians within the country and locals refer to the practice as a ceremony that initiates womanhood, prepares females for marriage, and restricts their sexual conduct. Turkey Family honor is a highly valued concept known as namus in Turkey and it is linked to female modesty, chastity, and family reputation among other families, as well as loyalty. An ideology of preserving family honor is deeply rooted is society and is not affected by high levels of education. Many women in Turkey are well educated but still are expected to be modest and sexually chaste in order to preserve the perceived honor of their families. If a family's honor is perceived to be breached, it can bring social shame to the entire family. In such cases, traditionally the family would decide the fate of the woman accused of dishonoring her family. This might involve forcing the young woman into a shotgun wedding, while in extreme cases a young male in the family was given the duty to cleanse the family name through an honor killing. With recent changes to criminal law that removed reduced sentences for honor killings, women that are accused of bringing shame to the family are sometimes forced to commit honor suicides by their families, especially in the predominantly Kurdish South-East regions, a region that greatly values traditions. Families who do not want their sons to face possible incrimination have encouraged their daughters to commit suicide. The number of female suicides over the years has increased greatly. Stories have surfaced revealing girls who are given tools with which to kill themselves such as rope to hang themselves, poisons to drink, or a gun to shoot themselves. Some murders have also been disguised as suicides in order to protect family members. Female family members are not the only ones that may be punished to preserve family honor. It is proposed that the death of a male homosexual physics student, Ahmet Yildiz, was an honor killing. Members of the Republican People Party have stated that in the six months preceding Hatice Firat's death a woman had been killed every day because of domestic violence. South Asia Similar to the South African and Turkish culture, family is a central value for Asian societies. Asian families are usually multi-generational, patriarchal and self-sufficient structures strongly bound to traditions. For instance, in India touching feet of relatives and elderly is a ritual to express respect and submission. However, family honor entails different components depending on the continental region. For example, family honor is strongly linked to female chastity in Afghanistan, Pakistan, and India in a similar fashion to Middle Eastern and Mediterranean societies. Whereas men threaten family honor by extreme actions such as murder and addiction, females may dishonor family by leaving the house too often or having unnecessary conversations with men. Sikh women need to be modest and reserved to be considered honorable; even rape is seen as a major insult to family honor. Cultures near the Middle East also consider involuntary sexual assaults dishonorable and shameful. East Asia Contrastingly in other Continental Asian culture, East Asian family honor depends on other factors such as success in education. Academic success of a student is seen as a source of pride and honor for traditional East Asian families. Therefore, both the East Asian student and the parents had to marshal resources and work diligently to curtail the likelihood of academic failure in order to avoid bringing shame to the family. In the past, honor in East Asia was also linked to success in the military and the battlefield. In Feudal Japan, for example, ritualized suicide practices such as harakiri were committed by Japanese samurai for centuries in the event of a defeat in battle. Instead of being captured or living with the shame of defeat, harakiri was committed in order to preserve the honor of their families. Europe In Europe, similar to traditional practices in South Africa, honor relates to different concepts depending on the geographic area; whereas honor is strongly linked to family reputation in the Mediterranean countries, in Northern Europe it has a more individualized meaning that is focused on personal accomplishments and qualities. Similar to gender-specific family honor codes in the Middle East, Mediterranean nations also traditionally exhibit such codes; women are seen as honorable by their chastity whereas men are seen as honorable by their productivity, toughness, and by protecting the honor of the women. In Italy, infidelity of women was seen dishonorable, thus crimes of passion were classified as second-degree murders until the 1970s. Although Western and Northern European nations traditionally have a more individualistic approach to honor, there have been an increase in honor killings or crimes of passion within their borders by immigrant populations residing in these countries. The European culture also served as a basis for American traditions of honor as well. In mainly Muslim Kosovo, the impact on family honor of admitting one has been raped, has discouraged some women from applying for compensation as victims of atrocities committed during the 1998–99 war with Serbia. Denmark In Denmark, the migration authority published that 24% of immigrants of non-Western heritage and their offspring of ages 18–29 were limited in their choice of partner by their relatives. The report showed a higher incidence for women than men and in areas with high concentrations of migrants, 59% were limited in choice of partner. Of migrant women who lived in high-concentration areas, fewer of those in education or employment were limited. Of migrant women who live in areas with a low share of migrants, 22% were limited in partner choice. It was also shown that migrant youth are more limited if their social circles only comprises other migrants when it came to choosing a partner. Norway In 2018, an investigation into court cases involving domestic violence against children showed that 47% of the cases involved parents who were both born abroad. According to a researcher at Norwegian Police University College the over-representation was due to cultural (honor culture) and legal differences in Norway and foreign countries. Sweden The Swedish National Police Board and the Swedish Prosecution Authority define honor related crime as crimes against a relative who, according to the perpetrator and his family's point of view has dishonoured the family honor. These crimes are intended to prevent the family honor being damaged or to restore damaged or lost family honor. The most serious honor related crime is often organised and deliberate. Incidents include torture, forced suicides, forced marriages, rapes, kidnapping, assault, mortal threats, extortion and protecting a criminal. In a 2009 study by the Swedish Agency for Youth and Civil Society (MUCF), about 70,000 individuals of ages 16–25 stated they could not freely choose whom to marry. The MUCF report is limited to people aged 16–25 and for instance excludes adult women who wish to divorce but are threatened with violence due to family honor. Women in such situations face a greater threat as they are persecuted by both their own extended family and that of the husband. Some women's shelters report that nearly all women who seek refuge with them are fleeing honor based violence. In 2012, the county administrative board of Östergötland got a mandate to coordinate efforts against honor culture-based violence and persecution. According to an investigation of 3,000 cases by newspaper Göteborgsposten, the most common scenario are girls being supervised and banned from being outside the home after school hours, who are forced to wear an Islamic veil and girls risking a forced marriage. About 80% of the child victims have been physically abused, most frequently with bare hands but also being beaten with belts or cables. In several cases the children have been burned with kitchen utensils or metal objects. According to John Åberg, the exact number of people who experience honor-related oppression in Sweden is unknown. State statistics put the number around 70,000. Researcher Astrid Schlytter claims, based on research conducted on British and other European populations, that the number of people who experience honor culture (Swedish: hedersförtryck) could be as high as 240,000. United Kingdom In the UK, honor crimes include forced marriage and female genital mutilation and honour based crimes disproportionally affect women from ethnic minorities. The number of honor crimes reported to police increased from 3,335 in 2014 to 5,595 in 2015, an increase of 68%, before a slight drop to 5,105 in 2016. Figures published by the Crown Prosecution Service showed that 256 crimes were referred to the CPS by police in 2016–17, about 5% of the cases reported. Of the 256 referrals, 215 lead to prosecutions which resulted in 122 convictions. North America United States According to the United States Department of Justice in 2015 there were about 23–27 honor killings per year in the previous decade in the country. The Old South The Old South took honor particularly seriously. Southerners in the Old South held themselves to their own sets of social codes. An affront to a Southerner's honor, if serious enough, was resolved by a duel. Dueling was originally a European custom, later adopted by the United States. Dueling was not technically legal in the United States, but it was difficult to enforce the laws written against it, especially in the Old South. Usually only men engaged in duels; their opponents were men they perceived to be equals. Public opinion for dueling varied: some thought it was a barbaric and backwards custom, while others believed it was a perfectly legitimate way of dealing with affronts to honor. Hispanic countries and culture Looking at the Hispanic community, similar to many of the countries mentioned above, elders are seen as wise and are to be shown respect from other family members. Family members turn to elders for help regularly, and when a family member falls ill wisdom on what should be done to care for the ill family member is searched for within the elders of the family. Men are the dominant figures within their households and embody a decisive, authoritative role. Contrary to popular belief, women hold as much weight within their homes as their husbands. They are the matriarchs of the family, and the family's health and stability relies on the mother. Although they need to be protected, women are cherished within their families as important figures. Hispanic families strongly display their emotions towards one another. Family members show that they care and love one another through taking care of each other. They search for reinforcement and support within their own homes more often than they would in today's society. The Hispanic culture has what is called Curanderismo. This is a system in which families consult the help of a religious figure called a curandero who gives medical, psychological, and social advice. Families make offerings unto the curandero such as money, candle lighting, creating metal or wooden offerings (shaped in the form of the body part in need of healing), etc. Many families believe that all personal or familial issues should be kept within the home. The Hispanic culture values modesty for all individuals including males and children in addition to females who are historically expected to behave in a modest fashion. Family members that suffer from mental illnesses are reluctant to inform their family members of this information in fear that their family members will criticize them. In the aspect of childbirth, men are required to wait until after the mother has given birth and dressed in a decent manner to visit his wife and newborn child. Mothers typically accompany the new mother during birth. Much like the American culture, Hispanic women take time to rest after childbirth, but traditionally they returned to heavier labor jobs as opposed to jobs normally held by women within society. Brazil In one country in particular Brazil, the community attempts to divide the upper-class families who are considered honorable from the lower-class families who are seen as a threat to society in Rio de Janeiro. The country defends family honor through the creation of different policies and laws, but this has caused great opposition. Politics has played a major in role in determining the status and meaning of what is considered to be honorable within Brazils' society. Honor of women in Brazil In the late 1800s Viveiros de Castro noted an increase in violations of female honor within Brazil at the turn of the century. Women were expanding their roles throughout society which many believed opened doors for women to be taken advantage of and seduced. Viveiros de Castro believed that women's working in factories was a threat to society and morality. Men believed that the new change in opinion amongst women contributed to this new susceptibility. Many men believed that women believed they were freer than they actually were, and because of this the actions of women led them to lose honor because of their lack of dependency on the male gender. The new century had changed the image of how women were historically perceived and portrayed. Certain individuals, a judge by the name of Nelson Hungria (quoted in Sueann Caulfield's book) specifically, labeled a woman's reserved role as the source of her honor which was lost once she branched out into society losing this reserved quality attributed to women. Because women chose to leave the traditional role of being the homemaker they lost the characterization of being innocent and some assumed they were engaging sexual activities. Michael Herzfeld, an anthropologist, argued that the idea of women losing their chastity derived from those who wished to explain a woman's new role within society. The idea of women expanding their roles outside of the home in society went against the ideas and morals that were upheld centuries before. In order to defend this morality, many highlighted the customs of the past as something that should have been continuously practiced and enforced even in a new modern era. Many women were subjected to sexual trials in which they were criticized for being dishonorable. Women during this time were similar to their ancestors. They engaged in practices such as sex before marriage, consenting to unions, and taking on the head role within their homes. However, these acts were perceived in an extremely contrasting manner during this period (after World War I). Because of this, many were uncertain of how they should approach the act of preserving sexual honor. Politics and honor within Brazil Turning to a different aspect of society that intertwined itself with idea of honor, politics played a major role in defining honor within Brazil. The country attempted to define honor while displaying this view of honor along with the country's traditions to others around the world. In September 1920, King Albert and Queen Elisabeth made a trip to Brazil. This trip sparked major controversy from different parties in the country. Many believed the trip was an attempt to "Europeanize" the country. Some viewed the trip as a positive way to show off Brazil the country and its civilization. Others viewed the trip as a negative opportunity in which the culture would attempt to conform to European standards. In doing so, attempts by the upper class would be made to hide Brazil's struggle of poverty which was a main part of its society and culture at the time. With this trip the importance of honor intensified. Before the King and Queen arrived, preparing for the couple included exposing and exercising honor in a manner many believed would promote the division of social classes and international affairs. Those who were responsible for posing as advocates for their country concealed their social class under what was perceived as honorable and behaved as though they themselves were a part of an "honorable" class. One of the aspects they displayed were gender ideologies that played a crucial role in differentiating social classes from one another. Natives desired to portray the country the best way possible. In doing so, they defended their families' traditions and morals and sexual honor. Towards the end of the 1930s, the definition of honor within Brazilian society has transformed completely. In result, in 1940, a penal code derived definitions of the term honor. Sexual crimes became a breaking of not family honor but "social customs". With the Vargas regime (Dictator Getúlio Vargas who ruled from 1937–1945), came a new form or definition of honor. Many attribute the change in the meaning of honor and a devaluing of its meaning to Vargas' rule. Vargas attributed the aspect of authority to the meaning of honor. Vargas closely tied traditional Brazilian family honor with the aspect of the nation's honor. Through his regime, Vargas intended to create a hierarchy of social, authoritative classes. However, debates over class, and gender in relation to honor and the nation of Brazil continued to take place. Women continued to transform their traditional roles and these changes could not be neglected. See also Blood money Duel Feud Honour Honor codes of the Bedouin Honor killing Izzat (honor) Namus == References == Glass delusion is an external manifestation of a psychiatric disorder recorded in Europe mainly in the late Middle Ages and early modern period (15th to 17th centuries). People feared that they were made of glass "and therefore likely to shatter into pieces". Delusion In the 16th and 17th centuries of Europe, glass became a valuable commodity. It was regarded as a magical, alchemical object. Associated with fragility and luxury, glass influenced the way noblemen of early Europe perceived their esteemed positions in society. This fixation on a novel material contributed to the manifestation of the delusion. Edward Shorter, a historian of psychiatry from the University of Toronto, attributes the rise of the delusion in 17th century Europe to the novelty of glass, stating that "throughout history, the inventive unconscious mind has pegged its delusions on to new materials and the technological advances of the age." Concentration of the glass delusion among the wealthy and educated classes allowed modern scholars to associate it with a wider and better described disorder of melancholy. Contemporary accounts Robert Burton's The Anatomy of Melancholy (1621) touches on the subject in the commentary as one of many related manifestations of the same anxiety: Fear of devils, death, that they shall be so sick, of some such or such disease, ready to tremble at every object, they shall die themselves forthwith, or that some of their dear friends or near allies are certainly dead; imminent danger, loss, disgrace still torment others; that they are all glass, and therefore will suffer no man to come near them; that they are all cork, as light as feathers; others as heavy as lead; some are afraid their heads will fall off their shoulders, that they have frogs in their bellies, Etc. Miguel de Cervantes based one of his short Exemplary Novels, The Glass Graduate (Spanish: El licenciado Vidriera, 1613), on the delusion of the title subject, an aspiring young lawyer. The protagonist of the story falls into a grave depression after being bedridden for six months subsequent to being poisoned with a purportedly aphrodisiac potion. He claims that, being of glass, his perceptions are clearer than those of men of flesh and demonstrates by offering witty comments. After two years of illness, Rodaja is cured by a monk; no details of the cure are provided except that the monk is allegedly a miracle-maker. The Dutch poet Constantijn Huygens wrote a Costly Folly (1622) centered on a subject who "fears everything that moves in his vicinity... the chair will be the death for him; he trembles at the bed, fearful that one will break his bum, the other smash his head". His Dutch contemporary Caspar Barlaeus experienced the glass delusion. French philosopher René Descartes wrote Meditations on First Philosophy (1641), using the glass delusion as an example of an insane person whose perceived knowledge of the world differs from the majority. In An Essay Concerning Human Understanding (Book II, Chapter XI, 13) when proposing his celebrated model of madness, John Locke also refers to the glass delusion. In modern times, the glass delusion has not completely disappeared, and there are still isolated cases today. "Surveys of modern psychiatric institutions have only revealed two specific (uncorroborated) cases of the glass delusion. Foulché-Delbosc reports finding one Glass Man in a Paris asylum, and a woman who thought she was a potsherd was recorded at an asylum in Merenberg." Andy Lameijn, a psychiatrist from the Netherlands, reports that he has a male patient suffering from the delusion in Leiden. German alchemist Johann Joachim Becher had a fascination with glass delusion. In Physica Subterranea (1669), he wrote that he discovered a way of turning dead human bodies into glass. However, Becher's claim was not true. Historical cases King Charles VI King Charles VI of France was famously afflicted by the glass delusion. He wore clothing that was reinforced with iron rods and did not allow his advisors to come near him due to his fear that his body would accidentally "shatter." He may have been the first known case of glass delusion. Princess Alexandra of Bavaria Princess Alexandra of Bavaria believed that she had swallowed a glass piano as a child. She was convinced that the object remained inside her body from that point on, fearful that it might shatter and puncture her organs. Georgios Hatzianestis Georgios Hatzianestis, a Greek military officer, was commander of the Army of Asia Minor during the Greco-Turkish War in 1922. He failed to adequately respond to the Great Offensive that turned the war in the Turks' favour because he believed that his legs were made of glass and could shatter if he moved. For his failure, he was tried as an anti-Venizelist in the Trial of the Six (the only military leader to be so prosecuted) and was executed for high treason. See also Charles VI of France El licenciado Vidriera Princess Alexandra of Bavaria Georgios Hatzianestis Notes References Speak, Gill (October 1990). "'El licenciado Vidriera' and the Glass Men of Early Modern Europe". The Modern Language Review. 85 (4): 850–865. doi:10.2307/3732644. JSTOR 3732644. "Music for a glass man". BBC. December 5, 2005. Brown, David (April 1992). Tchaikovsky: The Final Years 1885-1893. W. W. Norton & Company. pp. 97–98. ISBN 978-0-393-33757-0. Ritualcide is the systematic destruction or alteration of traditional ritual practices and their sequencing across five ritual domains akin to most cultures (birth, illness, courtship, marriage and death, which include ancestor obligations). Rituals have a prescribed form, source, and sequence that include sacred objects, places, times and seasons, music, dance, texts, songs and words, and mediators (such as monks, spirit mediums, and traditional healers and nature-infused sources, such as trees, birds, water and so on). Ritualcide primes genocide. In particular, when regimes tamper with collective tradition, inhabitants become vulnerable and/or susceptible to spirit-based harm. As Indigenous people loose access to sources of spirit protection, their angst increases and compliance may increase due to fear of animist harm. The term ritualcide was coined by Peg LeVine in Love and Dread in Cambodia: Weddings, Births and Ritual Harm Under the Khmer Rouge, which emerged from an eight-year ethnographic study into Khmer Rouge weddings and Cambodian rituals (2010). LeVine meticulously studied ritual history before, during and after Democratic Kampuchea. The definition was expanded in 2015 when LeVine continued research at the Shoah Foundation, Center for Advanced Genocide Research. In October, 2016, ritualcide was introduced at the Extraordinary Chambers in the Courts of Cambodia (ECCC) in conjunction with Khmer Rouge activity between 1975 and 1979. LeVine described how ritual loss complicates the aftermath of trauma for the living and the dead, and ruptures the cosmological order that binds ancestors. Without access to reliable, traditional ritual sources, collective fear and vulnerability increase for survivors. In genocide and Holocaust studies, ritual restoration holds relevance for recovery by survivors and ancestors, and their collective sense of protection and cultural continuity. == References == "My Negro Problem—And Ours" is a controversial essay by Norman Podhoretz, published in Commentary magazine in February 1963. About The essay addresses Podhoretz's self-admitted racism, which he calls "the hatred I still feel for Negroes", based on his interactions with African-Americans while growing up as a white working-class Jewish boy in Brownsville, Brooklyn. In his integrated neighborhood, most people were either African-American or white. The white people were mostly Italians who spoke Italian and whose grandparents had immigrated from Sicily, or Yiddish-speaking Ashkenazi Jews from Eastern European immigrant backgrounds. In the essay, Podhoretz related incidents of bullying from African-American children in the neighborhood. He expresses that as a child he felt "puzzled" by the idea that "all Jews were rich" and that "all Negroes were persecuted", because his observation was that "the only Jews I knew were poor" and that Black people "were doing the only persecuting I knew about - and doing it, moreover, to me." Podhoretz relates an incident where a non-Jewish Black friend hit him and refused to play with him because "I had killed Jesus"; after asking his mother for an explanation, she "cursed the goyim and the Schwartzes, the Schwartzes and the goyim" in Yiddish and told him to ignore "such foolishness". Despite expressing disgust for interracial marriage, Podhoretz writes that widespread interrmariage and the subsequent erasing of racial differences could be a solution to racism: "I believe that the wholesale merging of the two races is the most desirable alternative for everyone concerned." Reception Receiving both praise for honesty and condemnation for racism, the essay has been called both notorious and brave. In We Real Cool: Black Men and Masculinity (2004), bell hooks writes that the essay shows a "fascination with black masculinity", noting that Podhoretz wrote that he "envied Negroes for what seemed to me their superior masculinity" and "what seems to be their superior physical grace and beauty." The Black Jewish writer Nylah Burton wrote in The Forward that the essay is "drivel" that "traffic[s] in hateful stereotypes" and refers to Podhoretz's views on Black masculinity as "creepy" and "fetishizing". Burton condemns Commentary for calling the essay "one of the Most Controversial and Powerful Essays Published in Commentary." : "How in 2018 can you brag about having published an article called 'My Negro Problem'? How can you call it the most powerful essay ever published? This fact alone should show us what this publication stands for: rampant anti-blackness, hidden under the thin veneer of 'intellectual rigor' ... Commentary in other words, still has a 'Negro Problem.'" See also African American–Jewish relations Racism in Jewish communities References External links My Negro Problem—And Ours Guerrilla theatre, generally rendered "guerrilla theater" in the US, is a form of guerrilla communication originated in 1965 by the San Francisco Mime Troupe, who, in spirit of the Che Guevara writings from which the term guerrilla is taken, engaged in performances in public places committed to "revolutionary sociopolitical change." The group performances, aimed against the Vietnam War and capitalism, sometimes contained nudity, profanity and taboo subjects that were shocking to some members of the audiences of the time. Guerrilla (Spanish for "little war"), as applied to theatrical events, describes the act of spontaneous, surprise performances in unlikely public spaces to an unsuspecting audience. Typically these performances intend to draw attention to a political/social issue through satire, protest, and carnivalesque techniques. Many of these performances were a direct result of the radical social movements of the late 1960s through mid-1970s. Guerrilla Theater, also referred to as guerrilla performance, has been sometimes related to the agitprop theater of the 1930s, but it is differentiated from agitprop by the inclusion of Dada performance tactics. Origins The term Guerrilla Theater was coined by Peter Berg, who in 1965 suggested it to R.G. Davis as the title of his essay on the actions of the San Francisco Mime Troupe, an essay that was first published in 1966. The term "guerrilla" was inspired by a passage in a 1961 Che Guevara essay, which read: The guerrilla fighter needs full help from the people.... From the very beginning he has the intention of destroying an unjust order and therefore an intention... to replace the old with something new. Davis had studied mime and modern dance in the 1950s and had discovered commedia dell'arte. In autumn 1966 around 20 members of the San Francisco Mime Troupe broke off and started their own collective called the Diggers, who took their name from a group of 17th century radicals in England. Guerrilla theater in practice Guerrilla theater shares its origins with many forms of political protest and street theatre including agitprop (agitation-propaganda), carnival, parades, pageants, political protest, performance art, happenings, and, most notably, the Dada movement and guerrilla art. Although this movement is widely studied in Theater History classrooms, the amount of research and documentation of guerrilla theater is surprisingly lacking. The term, "Guerrilla Theater" seems to have emerged during the mid-1960s primarily as an upshot of activist Radical Theater groups such as The Living Theatre, San Francisco Mime Troupe, Bread and Puppet Theater, El Teatro Campesino, and the Free Southern Theater. It also has important roots in Allan Kaprow's "happenings". The first widely documented guerrilla performances were carried out under the leadership of Abbie Hoffman and the Youth International Party (Yippies). One of their most publicized events occurred on August 24, 1967, at the New York Stock Exchange where Hoffman and other Yippies threw dollar bills onto the trading floor below. Creating a media frenzy, the event was publicized internationally. In his later publication, Soon to be a Major Motion Picture (1980), Hoffman refers to his television appearances with specially planned subversive tactics as "guerrilla theater." Guerrilla theater was used as a protest demonstration by the anti-war organization Vietnam Veterans Against the War. An article from the summer of 1971 published in the glossy magazine Ramparts detailed one such performance in Washington, D.C.: A squad of soldiers moved through the part adjoining the U.S. Capitol. They were grubby looking troopers, clad in jungle fatigues and "boonie hats" with wide brims turned up. Jumping a low fence, they began shouting at a group of tourists. 'All right. Hold it. Hold it. Nobody move. Nobody move.' Their voices were full of tension and anger. A man broke out of the crowd and started running. Several soldiers fired at once, and the man fell, clutching his stomach. Blood could be seen on the clean sidewalk. The tourists turned away in horror. 'Get a body count,' a soldier yelled. Another squad of soldiers emerged from under the Capitol steps. 'All right. ID. ID,' they screeched. 'You got no ID and you VC.' They quickly grabbed a young woman and led her away, binding her wrists behind her back and prodding her with their rifles.... They grabbed [a] young man and threw him on the ground, tying his hands behind his back. Several of the soldiers kicked him, seeming to aim for his groin. Then someone took out a long, thick hunting knife and lifted up the man's shirt, holding the knife to his bare stomach, and pushed against it slightly. 'You VC? You VC?' The man said nothing. He was pushed to his feet and shoved down again. Then he was told to get up. This time the knife was pushed to the side of his neck, and the same question was repeated. Still no answer. The man was dragged away.... Then the soldiers left, and a smaller, less angry group of men dressed in khaki fatigues passed out leaflets to the astonished tourists. "A US Infantry platoon just passed through here!" the pink colored piece of paper read in big bold letters. "If you had been Vietnamese... We might have burned your house. We might have shot your dog. We might have shot you... HELP US END THE WAR BEFORE THEY TURN YOUR SON INTO A BUTCHER OR A CORPSE." Post-1970s performance theater Another guerrilla performance group that continued the use of the term was the Guerrilla Girls. This group of feminist artist-activists was established in New York City in 1985 with the purpose of bringing attention to the lack of female artists in major art galleries and museums. The Guerrilla Girls began their work through guerrilla art tactics which broadened to include guerrilla theater. Some common practices in their guerrilla theater techniques that have been replicated by other groups include appearing in costume, using assumed names, and disguising their identity. Guerrilla theater groups The origins and legacy of guerrilla theater can be seen in the work of these political/performance groups: ACT UP Billionaires for Bush Billionaires for Wealthcare Bread and Puppet Theater The Church of Euthanasia Circus Amok Clandestine Insurgent Rebel Clown Army El Teatro Campesino Free Southern Theater Reclaim the Streets Reverend Billy and the Church of Stop Shopping San Francisco Mime Troupe Situationism Vietnam Veterans Against the War Youth International Party (Yippies) FEMEN Footnotes Further reading Durland, Steven. "Witness: The Guerrilla Theater of Greenpeace." Radical Street Performance. Jan Cohen-Cruz, ed. New York: Routledge, 1998, pp. 67–73. Hoffman, Abbie. "America Has More Television Sets Than Toilets." Radical Street Performance. Jan Cohen-Cruz, ed. New York: Routledge, 1998, pp. 190–195. External links Eden Silva Jequinto, "Guerrilla Theater Documentary," Eastside Arts Alliances, Feb. 24, 2013. —Video. A community garden is a piece of land gardened or cultivated by a group of people individually or collectively. Normally in community gardens, the land is divided into individual plots. Each individual gardener is responsible for their own plot and the yielding or the production of which belongs to the individual. In collective gardens the piece of land is not divided. A group of people cultivate it together and the harvest belongs to all participants. Around the world, community gardens exist in various forms, it can be located in the proximity of neighborhoods or on balconies and rooftops. Its size can vary greatly from one to another. Depending on the location can determine the price of community gardens Community gardens have experienced three waves of major development in North America. The earliest wave of community gardens development coincided with the Industrial Revolution and rapid urbanization process in Europe and North America; they were then called 'Jardin d'ouvrier' (or workers' garden). The second wave of community garden development happened during the WWI and WWII; they were part of "Liberty Gardens" and "Victory Gardens" respectively. The most recent wave of community garden development happened in the 1970s during the OPEC crisis, results of grassroots movement in quest for available land to combat against food insecurity. More recently, community gardens have seen a global resurgence. This may be related to several issues faced by the global population in the 21st century, such as ecological crisis, climate change and the new sanitary crisis. Community gardens contribute to the urban agriculture movement and the requests from citizens for more community gardens has been surging in recent years. Community gardens are also accessible in over 190 + countries/regions Background According to Marin Master Gardeners, "a community garden is any piece of land gardened by a group of people, utilizing either individual or shared plots on private or public land". Community gardens provide fresh products and plants as well as contribute to a sense of community and connection to the environment and an opportunity for satisfying labor and neighborhood improvement. They are publicly functioning in terms of ownership, access, and management, as well as typically owned in trust by local governments or not for profit associations. Community gardens vary widely throughout the world. In North America, community gardens range from "victory garden" areas where people grow small plots of vegetables, to large "greening" projects to preserve natural areas, to large parcels where the gardeners produce much more than they can use themselves. Non-profits in many major cities offer assistance to low-income families, children's groups, and community organizations by helping them develop and grow their own gardens. In the UK and the rest of Europe, the similar "allotment gardens" can have dozens of plots, each measuring hundreds of square meters and rented by the same family for generations. In the developing world, commonly held land for small gardens is a familiar part of the landscape, even in urban areas, where they may function as market gardens. Community gardens are often used in cities to provide fresh vegetables and fruits in "food deserts", which are urban neighborhoods where grocery stores are rare and residents may rely on processed food from convenience stores, gas stations, and fast-food restaurants. Some writers have considered re-framing the concept of “food deserts” as “food apartheid,” as the neighborhoods which do not have access to healthy food have been racially oppressed by segregation, redlining, and a lack of access to land. Some Black, Indigenous, and people of color have supported self-sustaining community gardens, with the understanding that their liberation requires access to land and to healthy food. Community gardens may help alleviate one effect of climate change, which is expected to cause a global decline in agricultural output, making fresh produce increasingly unaffordable. Community gardens are also an increasingly popular method of changing the built environment in order to promote health and wellness in the face of urbanization. The built environment has a wide range of positive and negative effects on the people who work, live, and play in a given area, including a person's chance of developing obesity. Community gardens encourage an urban community's food security, allowing citizens to grow their own food or for others to donate what they have grown. Advocates say locally grown food decreases a community's reliance on fossil fuels for transport of food from large agricultural areas and reduces a society's overall use of fossil fuels to drive in agricultural machinery. Community gardens improve users’ health through increased fresh vegetable consumption and providing a venue for exercise. The gardens also combat two forms of alienation that plague modern urban life by bringing urban gardeners closer in touch with the source of their food, and by breaking down isolation by creating a social community. Community gardens provide other social benefits, such as the sharing of food production knowledge with the wider community and safer living spaces. Ownership Land for a community garden can be publicly or privately held. In North America, often abandoned vacant lots are cleaned up and used as gardens. Because of their health and recreational benefits, community gardens may be included in public parks, similar to ball fields or playgrounds. Historically, community gardens have also served to provide food during wartime or periods of economic depression. Access to land and security of land tenure remains a major challenge for community gardeners worldwide, since in most cases the gardeners themselves do not own or control the land directly. Some gardens are grown collectively, with everyone working together; others are split into clearly divided plots, each managed by a different gardener (or group or family). Many community gardens have both "common areas" with shared upkeep and individual/family plots. Though communal areas are successful in some cases, in others there is a tragedy of the commons, which results in uneven workload on participants, and sometimes demoralization, neglect, and abandonment of the communal model. Some relate this to the largely unsuccessful history of collective farming. Unlike public parks, whether a community garden is open to the general public is dependent upon the lease agreements with the management body of the park and the community garden membership. Open- or closed-gate policies vary from garden to garden. Community gardens are managed and maintained by the gardeners themselves, rather than tended only by a professional staff. A second difference is food production: Unlike parks, where plantings are ornamental (or more recently ecological), community gardens are usually focused on food production. Types of gardens There are multiple types of community gardens. Neighborhood gardens are the most common type, where a group of people come together to grow fruits, vegetables and ornamental plants. They are identifiable as a parcel of private or public land where individual plots are rented by gardeners for a monthly or annual fee. Residential Gardens are typically shared among residents in apartment communities, assisted living, and affordable housing units. These gardens are organized and maintained by residents living on the premise. Institutional Gardens are attached to either public or private organizations and offer numerous beneficial services for residents. Benefits include mental or physical rehabilitation and therapy, as well as teaching a set of skills for job-related placement. Demonstration Gardens are used for educational and recreational purposes in mind. They often offer short seminars or presentations about gardening and provide the necessary tools to operate a community garden. Plot size In Britain, the 1922 Allotment act specifies "an allotment not exceeding 40 [square] poles in extent"; since a rod, pole or perch is 5.5 yards in length, 40 square rods is 1210 square yards or 10890 square feet (equivalent to a large plot of 90 ft x 121 ft). In practice, plot sizes vary; Lewisham offers plots with an "average size" of "125 meters square". In America there is no standardized plot size. For example, plots of 3 m × 6 m (10 ft × 20 ft = 200 square feet) and 3 m x 4.5 m (10 ft x 15 ft) are listed in Alaska. Montgomery Parks in Maryland lists plots of 200, 300, 400 and 625 square feet. In Canada, plots of 20 ft x 20 ft and 10 ft x 10 ft, as well as smaller "raised beds", are listed in Vancouver. Location Community gardens may be found in neighborhoods and on the grounds of schools, hospitals, and residential housing. The location of a community garden is a critical factor in how often the community garden is used and who visits it. Exposure to a community garden is much more likely for an individual if they are able to walk or drive to the location, as opposed to public transportation. The length of travel time is also a factor. Those who live within a 15-minute or less travel distance is more likely to visit a community garden as compared to those with a longer travel time. Such statistics should be taken into consideration when choosing a location for a community garden for a target population. The site location should also be considered for its soil conditions as well as sun conditions. An area with a fair amount of morning sunlight and shade in the afternoon is most ideal. While specifics vary from plant to plant, most do well with 6 to 8 full hours of sunlight. When considering a location, areas near industrial zones may require soil testing for contaminants. If soil is safe, the composition should be loose and well-draining. However, if the soil at the location cannot be used, synthetic soil may also be used in raised gardens beds or containers. Plant choice and physical layout Food production is central to most community and allotment gardens. However, restoration of natural areas and native plant gardens are also popular, as are "art" gardens. Many gardens have several different planting elements, and combine plots with such projects as small orchards, herbs and butterfly gardens. Individual plots can be used as "virtual" backyards, each highly diverse, creating a "quilt" of flowers, vegetables and folk art. Gardeners often grow in-ground—this type of garden contrasts most with an urban environment. Gardeners may also grow in raised beds, or in boxes, sometimes on top of a paved area. Gardens may include raised for use by people who cannot bend or work directly on the ground. Regardless of plant choice, planning out the garden layout beforehand will help avoid problems down the line. According to the Arizona Master Gardener Manual, taking measurements of the garden size, sunlight locations and planted crops vs. yield quantity, will ensure a detailed record that helps when making decisions for the coming years. Other considerations when laying out a plot are efficient use of space by using trellises for climbing crops, plant location so that taller plants (like sunflowers) do not block needed sunlight to shorter plants and grouping plants that have similar life cycles close together. Group and leadership selection Community gardeners in North America may be of any cultural background, young or old, new gardeners or seasoned growers, rich or poor. Because of this diversity, when gardeners share their harvest, they often learn about cultural foods created from the plants grown by other gardeners. Some community gardens “self-support” through membership dues, and others require a sponsor for tools, seeds, or money donations. Support may come from churches, schools, private businesses or parks and recreation departments. Local nonprofit beautification and community-building organizations may contribute as well. There are many different organizational models in use for community gardens. Most elect their leaders from within their membership. Others are run by individuals appointed by their management or sponsor. Some are managed by non-profit organizations, such as a community gardening association, a community association, a church, or other land-owner, others by a city's recreation or parks department, a school or a university. Gardens are often started when neighbors come together to commit to the organization, construction and management of a garden, and are assisted by experienced organizers such as the Green Guerillas of New York City. Alternatively, a garden may be organized "top down" by a municipal agency. In Santa Clara, California a non-profit by the name of Appleseeds offers free assistance in starting up new community gardens around the world. Rules and an 'operations manual' are invaluable tools; ideas for both are available at the American Community Gardening Association and in the United States, from local master gardeners and cooperative extensions. Membership fees In most cases, gardeners are expected to pay monthly or annual dues to pay for water, infrastructure, garden-provided tools, water hoses, ordinary maintenance, etc. Health effects of community gardens Community gardens have been shown to have positive health effects on those who participate in the programs, particularly in the areas of decreasing body mass index and lower rates of obesity. Studies have found that community gardens in schools have been found to improve average body mass index in children. A 2013 study found that 17% of obese or overweight children improved their body mass index over seven weeks. Specifically, 13% of the obese children achieved a lower body mass index in the overweight range, while 23% of overweight children achieved a normal body mass index. Many studies have been performed largely in low-income, Hispanic/Latino communities in the United States. In these programs, gardening lessons were accompanied by nutrition and cooking classes and optional parent engagement. Successful programs highlighted the necessity of culturally tailored programming. There is some evidence to suggest that community gardens have a similar effect in adults. A study found that community gardeners in Utah had a lower body mass index than their non-gardening siblings and unrelated neighbors. Administrative records were used to compare body mass indexes of community gardeners to that of unrelated neighbors, siblings, and spouses. Gardeners were less likely to be overweight or obese than their neighbors, and gardeners had lower body mass indexes than their siblings. However, there was no difference in body mass index between gardeners and their spouses which may suggest that community gardening creates healthy habits for the entire household. Participation in a community garden has been shown to increase both availability and consumption of fruits and vegetables in households. A study showed an average increase in availability of 2.55 fruits and 4.3 vegetables with participation in a community garden. It also showed that children in participating households consumed an average of two additional servings per week of fruits and 4.9 additional servings per week of vegetables. Community gardens also have notable positive effects on mental health and well-being. Participation in gardening activities has been associated with reduced stress, enhanced mood, and improved overall mental health. Studies show that engaging in community gardening fosters a sense of belonging and social connectedness, which can mitigate feelings of loneliness and isolation, particularly in urban environments. For instance, community gardens provide safe, communal spaces where individuals can form social bonds, build relationships, and support each other through shared activities. These interactions can help create resilient communities by improving both individual mental health and broader social networks. Policy implications There is strong support among American adults for local and state policies and policy changes that support community gardens. A study found that 47.2% of American adults supported such policies. However, community gardens compete with the interests of developers. Community gardens are largely impacted and governed by policies at the city level. In particular, zoning laws—which incentivize or deincentivize land development—strongly impact the possibility of community gardens. Rezoning is necessary in many cities for a parcel of land to be designated a community garden, but rezoning doesn't guarantee that a garden will not be developed in the future. Policies can be enacted to protect community gardens from future development. For example, New York State reached a settlement in 2002 which protected hundreds of community gardens which had been established by the Parks and Recreation Department GreenThumb Program from future development. At times, zoning policy lags behind the development of community gardens. In these cases, community gardens may exist illegally. Such was the case in Detroit when hundreds of community gardens were created in abandoned spaces around the city. The city of Detroit created agricultural zones in 2013 in the middle of urban areas to legitimize the over 355 “illegal” community gardens. Examples Australia The first Australian community garden was established in 1977 in Nunawading, Victoria followed soon after by Ringwood Community Garden in March 1980. Czech Republic The trend of community gardening in the Czech Republic is increasing. The first community garden was founded in 2002 and in 2020 there are more than 100. Japan In Japan, rooftops on some train stations have been transformed into community gardens. Plots are rented to local residents for $980 per year. These community gardens have become active open spaces now. Mali Often externally supported, community gardens become increasingly important in developing countries, such as West African (Mali) to bridge the gap between supply and requirements for micro-nutrients and at the same time strengthen an inclusive development. Singapore Spain Most older Spaniards grew up in the countryside and moved to the city to find work. Strong family ties often keep them from retiring to the countryside, and so urban community gardens are in great demand. Potlucks and paellas are common, as well as regular meetings to manage the affairs of the garden. Taiwan There is an extensive network of community gardens and collective urban farms in Taipei City often occupying areas of the city that are waiting for development. Flood-prone river banks and other areas unsuitable for urban construction often become legal or illegal community gardens. The network of the community gardens of Taipei are referred to as Taipei organic acupuncture of the industrial city. United Kingdom In the United Kingdom, community gardening is generally distinct from allotment gardening, though the distinction is sometimes blurred. Allotments are generally plots of land let to individuals for their cultivation by local authorities or other public bodies—the upkeep of the land is usually the responsibility of the individual plot owners. Allotments tend (but not invariably) to be situated around the outskirts of built-up areas. Use of allotment areas as open space or play areas is generally discouraged. However, there are an increasing number of community-managed allotments, which may include allotment plots and a community garden area. Many of the community gardens are members of Social Farms & Gardens (a registered charity). The community garden movement is of more recent provenance than allotment gardening, with many such gardens in built-up areas on patches of derelict land, waste ground or land owned by the local authority or a private landlord that is not being used for any purpose. They can also be on more rural land, often in partnership with a farmer or estate owner. A community garden in the United Kingdom is typically run by people from the local community as an independent, non-profit association or organization (though this may be wholly or partly funded by public money). For example, Norwich's Fifth Quarter Community Garden. It is also likely to perform a dual function as an open space or play area (in which role it may also be known as a 'city park') and—while it may offer plots to individual cultivators—the organization that administers the garden will normally have a great deal of the responsibility for its planting, landscaping and upkeep. An example inner-city garden of this sort is Islington's Culpeper Community Garden, which is a registered charity, or Camden's Phoenix Garden. United States See also Commons Community orchard Community-supported agriculture Notes == References == The Counterculture Hall of Fame, managed by High Times magazine, is a hall of fame primarily dedicated to celebrating the counterculture and the people who helped shape it. The hall was created in 1997 by High Times editor Steven Hager. Inductions were held annually on Thanksgiving as part of the Cannabis Cup event in Amsterdam. A short documentary was typically shown to introduce the inductee(s) to the audience, and a silver Cannabis Cup was awarded to each inductee. Inductees 1997 Bob Marley 1998 Louis Armstrong, Mezz Mezzrow 1999 Jack Kerouac, Neal Cassady, Allen Ginsberg, William S. Burroughs 2000 Ina May Gaskin 2001 Paul Krassner 2002 Bob Dylan, Joan Baez 2003 Jack Herer 2004 Stephen Gaskin 2005 John Trudell 2006 Rainbow Family veterans Barry "Plunker" Adams and Garrick Beck 2007 Tommy Chong, Cheech Marin 2008 Peter Tosh 2009 Thomas King Forcade 2010 Coke La Rock 2011 The Brotherhood of Eternal Love founder John Griggs 2012 Steven Hager 2014 Sasha Shulgin and Ann Shulgin 2015 The Grateful Dead References External links History of the Cannabis Cup Buddhist Doctrinal Classification refers to various systems used by Mahāyāna Buddhist traditions to classify and organize the numerous texts and teachings that have developed over the history of Buddhism. According to buddhologist Peter Gregory, these classification systems fulfill three interwoven roles for Buddhist traditions: hermeneutical, sectarian, and soteriological. From an hermeneutical standpoint, they function as a method of organizing Buddhist texts both chronologically and hierarchically, thereby producing a doctrinal structure that is internally coherent and logically consistent. In its sectarian application, different Buddhist schools evaluate and order scriptures based on their own doctrinal priorities, using this to legitimize their specific traditions. From a soteriological perspective, classification schemas map out a graded path of spiritual development, wherein the practitioner’s insight evolves from basic teachings toward the most advanced and profound realizations. One of the earliest such systems was the "Three Turnings of the Wheel of Dharma" (Sanskrit: tridharmacakra-pravartana, Tibetan: chos kyi 'khor lo gsum), an Indian Mahāyāna Buddhist framework for classifying and understanding the teachings of the Buddhist Sūtras and the teachings of Buddha Śākyamuni. This classification system first appears in the Saṃdhinirmocana Sūtra and in the works of the Yogācāra school. According to the three turnings schema, the Buddha's first sermons, as recorded in the Tripiṭaka of early Buddhist schools, constitute the "first turning" (which include all śrāvakayāna texts). The sūtras which focus on the doctrine of emptiness (śūnyatā) like the Prajñāpāramitā Sūtra corpus, are considered to comprise the "second turning" (which in this schema is considered provisional), and the sūtras which teach Yogācāra themes (especially the three natures doctrine), like the Saṃdhinirmocana Sūtra, comprise the final and ultimate "third turning". This and other similar classification systems later became prevalent in various modified forms in Tibetan Buddhism as well as in East Asian Buddhism. In East Asian Buddhism, doctrinal classification systems, called "panjiào" (判教), were developed in nearly all major Chinese Buddhist schools. Tibetan Buddhism generally uses the term "classification of tenets" (Sanskrit: siddhānta, Tibetan: grub mtha'), which is also a name for a whole genre of literature that focuses on this topic. Indian theories The idea of classifying various doctrines and teachings has its antecedents in Early Buddhist texts such as the Tevijja sutta and the Brahmajala sutta. These early Buddhist sources discuss the various worldviews of brahmins, sramanas and ascetics during the Buddha's time, explaining why they are inadequate and why the Buddha's teaching is superior to them. Earlier Mahayana Sutras mostly discuss the Buddha's teachings in two main categories: Hinayana ("Small" or "Lesser" vehicle) or Śrāvakayāna and the Mahayana or Vaipulya (Expansive) teachings. The schema of the three vehicles (yanas) is also another early classification scheme, which contains three main vehicles to awakening: Śrāvakayāna, Pratyekabuddhayāna and Mahayana. Some sutras complicate this classification however. Perhaps the most famous example is the Lotus Sutra, which teaches that the Buddha taught three vehicles only provisionally. In reality, they are ultimately a single teaching, the all inclusive One Vehicle (Skt.: ekayāna, Ch.:一乘; yīchéng). The Three Turnings The Saṃdhinirmocana Sūtra is the first work to introduce the "three turnings of the wheel of Dharma" schema, which became the normative classification system in the Yogācāra school. First Turning The first turning is traditionally said to have taken place at Deer Park in Sarnath near Varanasi in northern India. It consisted of the teaching of the four noble truths, dependent arising, the five aggregates, the sense fields, not-self, the thirty seven aids to awakening and all the basic Buddhist teachings common to all Buddhist traditions and found in the various Sutrapitaka and Vinaya collections. These teachings are known as the "Hinayana" teachings (lesser or small vehicle) in Mahayana. In East Asian Buddhism, it is called "the teaching of existence" (有相法輪) since it discusses reality from the point of view of phenomena (dharmas) which are explained as existing. The Abhidharma teachings of the various śrāvakayāna (i.e. non-Mahayana) traditions (such as Vaibhasika and Theravada) are generally also placed into this category. Second Turning The second turning is said to have taken place at Vulture Peak Mountain in Rajagriha, in Bihar, India. The second turning emphasizes the teachings of emptiness (Skt: śūnyatā) and the bodhisattva path. The main sutras of this second turning are considered to be the Prajñāpāramitā sutras. In East Asian Buddhism, the second turning is referred to as "the teaching that the original nature of all things is empty, that signs are not ultimately real" (無相法輪). The second turning is also associated with the bodhisattva Manjushri. The analytical texts of the Madhyamaka school of Nagarjuna are generally included under the second turning. Third Turning Yogācāra sources The first sutra source which mentions the "three turnings" is the Ārya-saṃdhi-nirmocana-sūtra (Noble sūtra of the Explanation of the Profound Secrets), the foundational sutra of the Yogācāra school. Major ideas in this text include the storehouse consciousness (ālayavijñāna), and the doctrine of cognition-only (vijñapti-mātra) and the "three natures" (trisvabhāva). The Saṃdhinirmocana affirms that the teachings of the earlier turnings authentic but are also incomplete and require further clarification and interpretation. According to the Saṃdhinirmocana, the previous two turnings all had an "underlying intent" which refers to the three natures (and their threefold lack of essence), the central doctrine of the third turning. The Saṃdhinirmocana also claims that its teachings are the ultimate and most profound truth which cannot lead to a nihilistic interpretation of the Dharma which clings to non-existence (unlike the second wheel, which can be misinterpreted in a negative way) and is also incontrovertible and irrefutable (whereas the second wheel can be refuted). As such, the third turning is also called "the wheel of good differentiation" (suvibhakta), and "the wheel for ascertaining the ultimate" (paramartha-viniscaya). In East Asian Buddhism, the third turning is referred to as “ultimate turn of the Dharma wheel” (無上法輪). Other Mahāyāna sutras are considered to be associated with the Yogācāra school, and thus, with the third turning (though these sutras themselves do not mention "three turnings"). These include the Laṅkāvatāra Sūtra and the Ghanavyūha Sūtra, both of which discuss Yogācāra topics like the ālayavijñāna, the three natures and mind-only idealism as well as tathāgatagarbha ideas. The teachings of the third turning are further elaborated in the numerous works of Yogācāra school masters like Asaṅga, Vasubandhu, Sthiramati, Dharmapāla, Śīlabhadra, Xuanzang, Jñānaśrīmitra and Ratnākaraśānti. In his Commentary on Distinguishing the Middle from the Extremes (Madhyāntavibhāga-bhāṣya), Vasubandhu comments on the three turnings and how they relate to the three natures. According to Vasubandhu, the first turning teaches the non-existence of the self (atman) through an analysis of the five aggregates. The second turning then establishes how the very (false) appearance of a (non-existent) self comes about from its aggregate parts through dependent arising. The third turning then, explains the fundamental nature of emptiness itself, which is how the non-existence of the self exists, i.e. the existence of the non-existent as explained by the three natures. In this sense, the ultimate truth in the third turning is said to be both existent and non-existent. In his Commentary on the Cheng weishi lun (成唯識 論述記; Taishō no. 1830), Kuiji (a student of Xuanzang), lists the following as the most important sutras for the Yogācāra school: Buddhāvataṃsaka Sūtra (華嚴) Saṃdhinirmocana Sūtra (深密) *Tathāgata-utpāda-guṇa-alaṃkāra-vyūha (如來出現功德莊嚴) Mahayana-abhidharma-sutra (阿毘達磨) Laṅkāvatāra Sūtra (楞迦) Ghanavyūha Sūtra (厚嚴) In Chinese Yogācāra, important treatises for the third turning included the Yogācārabhūmi-śastra, Xuanzang's Cheng Weishi Lun, and the Daśabhūmikasūtraśāstra (Shidi jing lun 十地經論, T.26.1522, also called Dilun), which is Vasubandhu's commentary on the Daśabhūmika-sūtra (Shidi jing 十地經). Buddha-nature teachings The Indian Yogācāra tradition eventually developed various works which synthesized Yogācāra with the tathāgatagarbha thought found in various Mahayana sutras. This synthesis merged the tathāgatagarbha teaching with the doctrine of the ālayavijñāna and the three natures doctrine. Some key sources of this Indian tendency are the Laṅkāvatāra Sūtra, Ghanavyūha Sūtra, and the Ratnagotravibhāga. This Yogācāra-Tathāgatagarbha tradition became influential in East Asian Buddhism and in Tibet. The translator Paramārtha (499-569 CE) was known for promoting this syncretic Yogācāra and for defending the theory of the "stainless consciousness" (amala-vijñāna), which is revealed once the ālaya-vijñāna is purified. As noted by Jan Westerhoff, the identification of buddha-nature teachings with the Yogācāra's third turning happened not only because several sutras (like the Laṅkāvatāra) explicitly synthesized the two doctrines, but also because:the notion of the tathāgatagarbha lines up more naturally with the characterization of ultimate reality we find in Yogācāra than with what we find in Madhyamaka. The latter's characterization of ultimate reality in terms of emptiness is primarily a negative one, it describes it in terms of what is not there (a substantially existent core, svabhava), while the former's is more positive, postulating a foundational consciousness that is the source of all appearance.Due to the influence of Yogācāra-Tathāgatagarbha thought, some Buddhist traditions also consider the tathāgatagarbha (also known as buddha-nature) teachings as part of the third turning. For example, the Jonang master Dölpopa Shérap Gyeltsen (1292-1361) held that the Tathāgatagarbha sutras contained the "final definitive statements on the nature of ultimate reality, the primordial ground or substratum beyond the chain of dependent origination." For Dölpopa, some of the key “sutras of definitive meaning” included: the Śrīmālādevī Siṃhanāda Sūtra, Tathāgatagarbha Sūtra, Mahāyāna Mahāparinirvāṇa Sūtra, Aṅgulimālīya Sūtra, Ghanavyūha Sūtra, Buddhāvataṃsakasūtra, Laṅkāvatāra Sūtra, and the Saṃdhinirmocana Sūtra. Dölpopa's classification of Tathāgatagarbha sutras was influential on numerous later Tibetan authors. The Rime master Jamgon Kongtrul (1813–1899) also held that these buddha-nature sutras belonged to the definitive third turning. The teachings found in several of the "treatises of Maitreya", such as the Madhyāntavibhāgakārikā, Ratnagotravibhāga and the Dharmadharmatāvibhāga are also considered to be part of the third turning by several schools of Tibetan Buddhism. Furthermore, in Tibetan Buddhism, Buddhist tantra and its associated scriptures are sometimes considered to also be part of the third turning. Definitive and provisional The schema of the three turnings found in Yogācāra texts identify Yogācāra teachings as the final and definitive interpretation of the Buddha's teaching. However, the schema was later adopted more widely, and different schools of Buddhism, as well as individual Buddhist thinkers, give different explanations as to whether the second or third turnings are "definitive" (Skt: nītārtha) or "provisional" or "implicit" (Skt: neyārtha, i.e. requiring interpretation). In the context of Buddhist hermeneutics, "definitive" refers to teachings which need no further explanation and are to be understood as is, while "implicit" or "provisional" refers to teachings which are expedient and useful but must be further interpreted and drawn out. In the Tibetan tradition, some schools like Nyingma hold that the second and third turnings are both definitive. Nyingma works tend to emphasize the complementarity of the second and third turning teachings. Meanwhile, the Gelug school considers only the second turning as definitive. The Gelug founder Tsongkhapa rejected the definitive nature of the Yogācāra texts and instead argued that the definitive sutras are only those which teach emptiness as the ultimate meaning. On this, he relies on the Teachings of Akshayamati Sutra. The Jonang school on the other hand, see only the third turning sutras as definitive, and hold the texts of the second turning as provisional. Other Indian sources Other Mahāyāna sutras also mention a similar idea of the Buddha teaching in different phases, some which are provisional and others which are considered final. The Dhāraṇīśvararāja sūtra (also known as the Tathāgatamahā­karuṇā­nirdeśa), mentions that it is part of the “irreversible turning” and uses the metaphor of the gradual process of refining beryl to describe the way the Buddha teaches in three phases of teaching: 1. "discourses on impermanence, suffering, no self, and unattractiveness, which provoke revulsion", 2. "discourses on emptiness, signlessness, and wishlessness" and finally 3. "discourses known as The Irreversible Wheel of the Dharma and The Purification of the Triple Sphere." Tibetan exegesis has generally seen this passage as referring to the three turnings (though the sutra itself does not use this terminology). The Dhāraṇīśvararāja is also important because it is a key source for the Ratnagotravibhāga, an influential buddha-nature focused treatise. The Mahāyāna Mahāparinirvāṇa Sūtra states that its teachings are the highest and ultimate Dharma. It also states that teachings on not-self and emptiness are provisional skillful means. The Mahāparinirvāṇa Sūtra considers the highest teachings to be those of the "vaitulya" ("well-balanced", or "extensive") Mahāyāna sūtras (such as the Mahāparinirvāṇa itself) which teach the eternal nature of the Tathagata, and how "all living beings possess buddha-nature." The sutra also contains a passage which outlines a rough system of teachings from coarse to subtle, comparing the teachings to the process of making ghee from milk. This passage was influential in East Asian Buddhist classification systems, entering mainstream Chinese Buddhist scholarship with work of Zhiyi. The passage states:From the cow there comes milk, from milk comes cream, from cream come butter curds, from butter curds comes butter, and from butter comes ghee. . . . Oh sons of good family, it is also thus with the Buddha [and his teaching]. From the Buddha come the twelve divisions of scripture, from the twelve divisions of scripture come the sūtras, from the sūtras come the vaipulya [Mahāyāna] sūtras, from the vaipulya sūtras comes Perfection of Wisdom, and from Perfection of Wisdom comes Mahāparinirvāṇa, which is to be compared to ghee. Ghee is analogous to the Buddha-nature. Buddhist scholastic literature also discusses and classifies numerous Buddhist and non-Buddhist views. Indian works which discuss and classify various competing doctrines include the Kathavatthu, the Mahavibhasa, Bhaviveka's Blaze of Reasoning and Shantaraksita's Tattvasamgraha. East Asian classification systems (panjiào) The classification of Buddhist teachings or "doctrinal taxonomies" (Chinese: 判教 panjiào) became a central feature of East Asian Buddhist scholasticism and doctrinal debate. By 600 AD there were 10 main classifications. The term is a shortened form of jiāoxiāng pànshì 教相判釋, referring to the systematic classification of Buddhist teachings based on factors such as thematic content and historical period. This form of doctrinal organization was typically carried out by exegetes who aimed to reconcile the wide variety of Buddhist scriptures by integrating them into a unified doctrinal framework. However, these classifications often reflected the exegete’s own institutional affiliations, with commentators generally promoting the teachings of their own tradition as central or supreme. The practice of doctrinal classification was a central feature of scriptural interpretation among the scholastic Buddhist traditions in China during the 5th to 8th centuries, particularly within schools such as Faxiang (法相), Tiantai (天台), and Huayan (華嚴). Notable figures associated with this method include Huiyuan (慧遠), Zhiyi (智顗), Fazang (法藏), and Zongmi (宗密). East Asian Madhyamaka school The Sanlun (Madhyamaka) school divided the teaching into three turnings of the wheel of Dharma, but with different definitions for each. This system was outlined by Jizang and consists of the following schema: The root wheel of the Avatamsaka sutra. The branch wheel of Hinayana and Mahayana texts (the Āgama, Vaipulya, and Prajñāpramita sūtras) The wheel that contracts all branches so as to bring them back to the root, the Lotus sutra. Tiantai The Chinese Tiantai school developed a doctrinal classification schema (panjiào) which organized the Buddhas teachings into five periods (五時) and Eight teachings: Five periods Flower Ornament period 華嚴時, The sudden teaching is delivered as the Avatamsaka sutra, containing the direct content of the Buddha’s enlightenment experience. Few can understand it. Deer Park period 鹿苑時 (represented by the Āgama sūtras 阿含經), represent a gradual and simpler teaching. the Vaipulya period 方等時 (represented by the Vimalakīrti Sūtra 淨名經 and so forth); this and the next period represent gradually deeper teachings. the Prajñā period 般若時 (represented by the Prajñāpāramitā sūtras 般若經). Lotus-Nirvāṇa period 法華涅槃時, Lotus sutra and Mahāparinirvāṇa-sūtra, a teaching that is neither sudden nor gradual. Eight teachings The Fourfold Teachings: Tripitaka Teaching (三藏教): the Sutra, Vinaya and Abhidharma Shared Teaching (通教): the teaching of emptiness, which is shared by Mahayana and Hinayana. Distinctive Teaching (別教): the teachings of the Bodhisattva path. Complete (Round) Teaching (圓教) - the complete and perfect teaching found in the Lotus Sutra , Nirvana Sutra and the Avatamsaka Sutra. The Fourfold Method classifies four different ways that the Buddha uses to guide sentient beings of different capacities: Gradual Teaching (漸教): Teaches the truth in stages Sudden Teaching (頓教): Reveals the ultimate truth directly and immediately Secret Teaching (祕密教): A teaching which communicates in a secret manner in which the Buddha's intent remains hidden to most and is understood only by certain members of the assembly. Variable Teaching (不定教): A method with no fixed teaching; the interpretation is not fixed but depends on the hearer's capacities. Huayen Likewise, the Huayen school had a five period panjiào of dharma teachings as taught by patriarch Fazang: The Hinayana teachings The Elementary Mahayana teachings, which includes the teachings of the Yogacara, and Madhyamaka schools The "Final Teaching" of Mahayana, based on the Buddha-nature teachings, especially those of the Awakening of Faith The Sudden Teaching, "which 'revealed' (hsien) rather than verbalised the teaching" The Complete, or Perfect Teachings of the Avatamsaka-sutra and the Huayan school. Zongmi The Chan and Huayan master Zongmi developed his own panjiao in his Inquiry into the Origin of Humanity. One influential and innovative change to Zongmi's panjiao is the fact that he included non-buddhist religions in it. This schema is as follows: Vehicle of humans and gods (人天教), which includes Confucianism and Daoism Hinayana (小乘教) The Mahayana teaching of phenomenal appearances (大乘法相教) The Mahayana refutation of phenomenal appearances (大乘破相教) The direct revelation of the Nature (顯性教) Japanese Buddhism Kukai in Japan wrote Himitsumandara jūjūshinron (祕密曼荼羅十住心論, Treatise on The Ten Stages of the Development of Mind) and Enchin also developed a Tendai classification system. Tibetan Buddhist tenet systems Tibetan Vajrayana schools sometimes refer to Buddhist tantra as a "fourth turning", adding it to the classic Indian "three turnings model". As explained by Lama Surya Das, some traditions consider Dzogchen as a fourth turning. The most common style of doctrinal classification system in Tibetan Buddhism however is found in a genre called "tenets" (Tibetan: grub mtha'), from the Sanskrit term Siddhānta (established doctrine, accepted conclusion). This genre of scholastic study and texts evolved from Indian doctrinal works, such as the Mahavibhasa, Bhaviveka's Blaze of Reasoning and Shantaraksita's Tattvasamgraha. These works categorized and discussed various Buddhist and non-Buddhist doctrines in a hierarchical fashion, refuting opposing doctrinal systems and culminating with the exposition of the proper correct "established doctrine" ("Siddhānta"). Tibetan Buddhists developed the genre further and numerous tenet works were written by figures such as Rongzompa, Chekawa Yeshe Dorje, Sakya Pandita, Longchenpa, Jamyang Shéba, and Changkya Rölpé Dorjé. The most common outline of basic tenets discussed in these works are four main schools of Indian Buddhist philosophy, which comprise two Hinayana schools: Vaibhāṣika, and Sautrāntika, and two Mahayana schools: Cittamātra (Mind-only), and Madhyamaka (which is sub-divided into the Prasaṅgika and Svatantrika camps). When discussing Vajrayana Buddhism, Tibetan Buddhism also contains doctrinal classification systems for the various classes of Tantra. Vajrayana is thus considered to be a distinct esoteric category, apart from "exoteric" Mahayana Buddhism, also labeled "sutric" Mahayana. The Nyingma school's Dzogchen tradition contains a unique classification system with nine types of teachings (or vehicles). See also Yana (Buddhism) Dhammacakkapavattana Sutta Dharmacakra Saṃdhinirmocana Sūtra References External links The Three Turnings of The Wheel of Dharma – Why They Are Each Essential to All of Us – Jay L. Garfield Three Turnings of the Wheel of the Dharma – James Blumenthal Bibliography Gregory, Peter N. (1981). "The p'an-chiao system of the Hua-yen school". T'oung Pao. LXVII (1–2): 10–41. Kanno, Hiroshi (2000). A Comparison of Zhiji`s and Jizang`s Views of the Lotus Sutra:, Annual Report of The International Research Institute for Advanced Buddhology at Soka University, vol III, 125–147 Liu, Ming-Wood (1993). The Chinese Madhyamaka Practice of "p'an-chiao": The Case of Chi-Tsang, Bulletin of the School of Oriental and African Studies, University of London 56 (1), 96–118 Mun, Chanju (2006). The History of Doctrinal Classification in Chinese Buddhism: A Study of the Panjiao Systems. Lanham, MD: University Press of America. ISBN 0761833528 Disownment occurs when a parent, sibling or a relative renounces or no longer accepts a child or a relative as a family member. Disownment might be due to actions perceived as reprehensible or lead to severe emotional consequences. Different from giving a child up for adoption, it is a social and interpersonal act and may take place later in the child's life, which means that the disowned child would have to make arrangements for future care. Among other things, it implies no responsibility for future care, making it similar to divorce or repudiation (of a spouse), meaning that the disowned child would have to find another residence to call home and be cared for. Disownment may entail disinheritance, familial exile, or shunning, or all three. A disowned child might no longer be welcome in their former family's home or be allowed to attend major family events. Conversely, a child might themselves seek to disown their parents or family through some form of emancipation. In some countries, disownment of a child is a form of child abandonment and is illegal when the child is a minor. Some countries condition a legal right of disownment within the family on evidence of specific familial conditions, such as an absence of normal familial ties (required in Austria), or abuse on the part of the person sought to be disowned (required in Spain). In Roman law, the rights called patria potestas included power of disownment. As to Italian law, see article 224 of the Civil Code. There was a process for disownment amongst the Tanala of Ikongo, and disownment was inflicted as a punishment by the antandroy. There was provision for disownment in the Code of Hammurabi. In Louisiana, the right to disown a child was called action en desaveu. In some cases, society and its institutions will accept an act of disownment. See also Family estrangement Honor killing == References == A self-clasping handshake is a gesture in which one hand is grasped by the other and held together in front of the body or over the head. In the United States, this gesture is a sign of victory, being made by the winning boxer at the end of a fight. Leaders of the Soviet Union, such as Nikita Khrushchev, used the gesture to symbolise friendship when visiting the United States, and so risked misunderstanding. == References == A son is a male offspring; a boy or a man in relation to his parents. The female counterpart is a daughter. From a biological perspective, a son constitutes a first degree relative. Social issues In pre-industrial societies and some current countries with agriculture-based economies, a higher value was, and still is, assigned to sons rather than daughters, giving males higher social status, because males were physically stronger, and could perform farming tasks more effectively. In China, a one-child policy was in effect until 2015 in order to address rapid population growth. Official birth records showed a rise in the level of male births since the policy was brought into law. This was attributed to a number of factors, including the illegal practice of sex-selective abortion and widespread under-reporting of female births. In patrilineal societies, sons will customarily inherit an estate before daughters. In some cultures, the eldest son has special privileges. For example, in Biblical times, the first-born male was bequeathed the most goods from his father. Some Japanese social norms involving the eldest son are: "that parents are more likely to live with their eldest child if their eldest child is a son" and "that parents are most likely to live with their eldest son even if he is not the eldest child". Christian symbolism Among Christians, "the Son" or Son of God refers to Jesus Christ. Trinitarian Christians view Jesus as the human incarnation of God the second person of the Trinity, known as God the Son. In the Gospels, Jesus sometimes refers to himself as the Son of Man. Indications in names In many cultures, the surname of the family means "son of", indicating a possible ancestry—i.e., that the whole family descends from a common ancestor. It may vary between the beginning or the termination of the surname. Arabic bin or ibn. Examples: "Ibn Sina" ("son of Sina"), "Ibn Khaldun" ("son of Khaldun"). Berber U (also spelled ou). Examples: "Usadden" ("son of Sadden"), "Uâli" ("son of Âli"). Ayt (also spelled ait or aït). Examples: "Ayt Buyafar" ("sons of Buyafar"), "Ayt Mellul" ("sons of Mellul"). n Ayt (also spelled nait or naït). Examples: "n Ayt Ndir" ("son of the Ndir tribe/family"), "Naït Zerrad" ("son of the Zerrad tribe or family"). Danish Sen. Examples: "Henriksen" ("son of Henrik"), "Jensen" ("son of Jens"), "Andersen" ("son of Anders"). Dutch Sen. Examples: "Jansen" ("son of Jan"), "Petersen" ("son of Peter"), "Pietersen" ("son of Pieter") Zoon. Examples: "Janszoon" ("son of Jan"), "Peterszoon" ("son of Peter"), "Pieterszoon" ("son of Pieter") English s. Examples: "Edwards" ("son of Edward"), "Williams" ("son of William"), "Jeffreys" ("son of Jeffrey") Son. Examples: "Jefferson" ("son of Jeffery"), "Wilson" ("son of William"), "Edson" ("son of Edward"), "Anderson" ("son of Ander"). French es. Example: "Fernandes" ("son of Fernand"). ot. Example: "Pierrot" ("son of Pierre"). de or d'. Example: "Danton" ("son of Anton"). Hebrew ben or bin before 1300 BC. Example: "Benjamin" ("son of a right-hand man"). Also, the Hebrew word for "person" is ben Adam, meaning "son of Adam". Hindi beta. Example: "Mera beta Tim" ("my son Tim"). बेटा. Example "मेरा बेटा टिम" ("my son Tim"). Hungarian -fi or -ffy. Examples: "Petőfi" ("son of Pető"), "Sándorfi" ("son of Sándor"), "Péterffy" ("son of Péter") (archaic spelling, indicates aristocratic origins). Irish Mac or Mc. Examples: "MacThomas" ("son of Thomas"), "McDonald" ("son of Donald"), "MacLean" ("son of Lean"). Italian di. Examples: "di Stefano" ("son of Steven"), "di Giovanni" ("son of John"), "di Giuseppe" ("son of Joseph"). de. Examples: "de Paolo" ("son of Paul"), "de Mauro" ("son of Maurus"), "de Giorgio" ("son of George"). d`. Examples: "d'Antonio" ("son of Anthony"), "d'Adriano" ("son of Adrian"), "d'Agostino" ("son of Augustine"). -i, which comes from Latin ending for Genitive. Examples: "Paoli" ("son of Paolo"), "Richetti" ("son of Richetto, a short name for Enrico"). Norwegian Son. Examples: "Magnusson" ("son of Magnus"); "Sigurdson" ("son of Sigurd"), "Odinson" ("son of Odin"). Persian pur/pour. Example: "Mahdipur" ("son of Mahdi"). zadeh. Example: "Muhammadzadeh" ("son/daughter of Muhammad"). Tagalog Anak Example: mga Anak ni Pedro (son and daughter of Pedro) Tamil Magan. Example: "En Magan Murugan" ("my son Murugan"). மகன். Example "என் மகன் முருகன்" ("my son Murugan"). Polish ski. Examples: "Janowski" ("son of John"), "Piotrowski" ("son of Peter"), "Michalski" ("son of Michael"). Portuguese Es. Examples: "Gonçalves" ("son of Gonçalo"), "Henriques" ("son of Henrique"), "Fernandes" ("son of Fernando"). Romanian a as prefix (except for female names that start in "a" and probably for others that start in vowels) and ei as suffix. Example: "Amariei" ("son of Mary"), "Adomniței" ("son of Domnița"), "Alenei" ("son of Elena/Leana"). escu or sometimes aşcu comes from the Latin -iscus which means "belonging to the people". Examples: "Petrescu" ("Petre's son"), "Popescu" ("Popa's son", Popa meaning Priest), "Constantinescu" ("son of Constantin"). Russian ov /ɒf/, ovich /əvɪtʃ/. Example: "Ivanov" ("son of Ivan"). ev /ɛf/, evich /ɨvɪtʃ/. Example: "Dmitriev" ("son of Dmitri"). Spanish Ez. Examples: "González" ("son of Gonzalo"), "Henríquez" ("son of Henrique"), "Fernández" ("son of Fernando"), Gómez ("son of Gome"), Sánchez ("son of Sancho"). Turkish oğlu. Examples: "Elbeyioğlu" ("son of foreigner Bey"), "Ağaoğlu" ("son of Ağa"), "Yusufoğlu" ("son of Yusuf"). zade. Examples: "Beyzade" (son of a Bey), "Aşıkpaşazade" ("son of Ashik Paşa), "Mehmedzade" (son of Mehmet). Ukrainian -enko or -ko, meaning simply "son of". Example: "Kovalenko" ("son of Koval") sky . Examples: "Stanislavsky" ("son of Stanislav"), "Chaykovsky" ("son of Chayko"), "Petrovsky" ("son of Petro"). shyn. Examples: "Petryshyn" ("son of Petro"), "Danylyshyn" ("son of Danylo"). chuk. Example: "Ivanchuk" ("son of Ivan"). Welsh ap or ab. Examples: "ap Rhys" ("son of Rhys", anglicized to "Price"), "ab Owain" ("son of Owen", anglicized to Bowen). Semitic The Arabic word for son is ibn. Because family and ancestry are important cultural values in the Arab world and Islam, Arabs and most Muslims (e.g. Bruneian) often use bin, which is a form of ibn, in their full names. The bin here means "son of." For example, the Arab name "Saleh bin Tarif bin Khaled Al-Fulani" translates as "Saleh, son of Tarif, son of Khaled; of the family Al-Fulani" (cf. Arab family naming conventions). Accordingly, the opposite of ibn/bin is abu, meaning "the father of." It is a retronym, given upon the birth of one's first-born son, and is used as a moniker to indicate the newly acquired fatherhood status, rather than a family name. For example, if Mahmoud's first-born son is named Abdullah, from that point on Mahmoud can be called "Abu Abdullah." This is cognate with the Hebrew language ben, as in "Judah ben Abram HaLevi," which means "Judah, son of Abram, the Levite." Ben is also a standalone name. References External links The dictionary definition of son at Wiktionary A zombie (Haitian French: zombi; Haitian Creole: zonbi; Kikongo: zumbi) is a mythological undead corporeal revenant created through the reanimation of a corpse. In modern popular culture, zombies appear in horror genre works. The term comes from Haitian folklore, in which a zombie is a dead body reanimated through various methods, most commonly magical practices in religions like Vodou. Modern media depictions of the reanimation of the dead often do not involve magic but rather science fictional methods such as fungi, radiation, gases, diseases, plants, bacteria, viruses, etc. The English word "zombie" was first recorded in 1819 in a history of Brazil by the poet Robert Southey, in the form of "zombi". Dictionaries trace the word's origin to African languages, relating to words connected to gods, ghosts and souls. One of the first books to expose Western culture to the concept of the voodoo zombie was W. B. Seabrook's The Magic Island (1929), the account of a narrator who encounters voodoo cults in Haiti and their resurrected thralls. A new version of the zombie, distinct from that described in Haitian folklore, emerged in popular culture during the latter half of the 20th century. This interpretation of the zombie, as an undead person that attacks and eats the flesh of living people, is drawn largely from George A. Romero's film Night of the Living Dead (1968), which was partly inspired by Richard Matheson's novel I Am Legend (1954). The word zombie is not used in Night of the Living Dead, but was applied later by fans. Following the release of such zombie films as Dawn of the Dead (1978) and The Return of the Living Dead (1985)—the latter of which introduced the concept of zombies that eat brains—as well as Michael Jackson's music video Thriller (1983), the genre waned for some years. The mid-1990s saw the introduction of Resident Evil and The House of the Dead, two break-out successes of video games featuring zombie enemies which would later go on to become highly influential and well-known. These games were initially followed by a wave of low-budget Asian zombie films such as the zombie comedy Bio Zombie (1998) and action film Versus (2000), and then a new wave of popular Western zombie films in the early 2000s, the Resident Evil and House of the Dead films, the 2004 Dawn of the Dead remake, and the British zombie comedy Shaun of the Dead (2004). The "zombie apocalypse" concept, in which the civilized world is brought low by a global zombie infestation, has since become a staple of modern zombie media, seen in such media as The Walking Dead franchise. The late 2000s and 2010s saw the humanization and romanticization of the zombie archetype, with the zombies increasingly portrayed as friends and love interests for humans. Notable examples of the latter include movies Warm Bodies and Zombies, novels American Gods by Neil Gaiman, Generation Dead by Daniel Waters, and Bone Song by John Meaney, animated movie Corpse Bride, TV series iZombie and Santa Clarita Diet, manga series Sankarea: Undying Love, and the light novel Is This a Zombie? In this context, zombies are often seen as stand-ins for discriminated groups struggling for equality, and the human–zombie romantic relationship is interpreted as a metaphor for sexual liberation and taboo breaking (given that zombies are subject to wild desires and free from social conventions). Etymology In Haitian folklore, a zombie (Haitian French: zombi, Haitian Creole: zonbi) is an animated corpse raised by magical means, such as witchcraft. The English word "zombie" is first recorded in 1819, in a history of Brazil by the poet Robert Southey, in the form of "zombi", actually referring to the Afro-Brazilian rebel leader named Zumbi and the etymology of his name in "nzambi". The Oxford English Dictionary gives the origin of the word as Central African and compares it to the Kongo words "nzambi" (god) and "zumbi" (fetish). A Kimbundu-to-Portuguese dictionary from 1903 defines the related word nzumbi as soul, while a later Kimbundu–Portuguese dictionary defines it as being a "spirit that is supposed to wander the earth to torment the living". How the creatures in contemporary zombie films came to be called "zombies" is not fully clear. The film Night of the Living Dead (1968) made no spoken reference to its undead antagonists as "zombies", describing them instead as "ghouls" (though ghouls, which derive from Arabic folklore, are demons, not undead). Although George A. Romero used the term "ghoul" in his original scripts, in later interviews he used the term "zombie". The word "zombie" is used exclusively by Romero in his script for his sequel Dawn of the Dead (1978), including once in dialog. According to Romero, film critics were influential in associating the term "zombie" to his creatures, and especially the French magazine Cahiers du Cinéma. He eventually accepted this linkage, even though he remained convinced at the time that "zombies" corresponded to the undead slaves of Haitian voodoo as depicted in White Zombie with Bela Lugosi. Folk beliefs Haiti Zombies are featured widely in Haitian rural folklore as dead persons physically revived by the act of necromancy of a bokor, a sorcerer or witch. The bokor is opposed by the houngan (priest) and the mambo (priestess) of the formal voodoo religion. A zombie remains under the control of the bokor as a personal slave, having no will of its own. The Haitian tradition also includes an incorporeal type of zombie, the "zombie astral", which is a part of the human soul. A bokor can capture a zombie astral to enhance his spiritual power. A zombie astral can also be sealed inside a specially decorated bottle by a bokor and sold to a client to bring luck, healing, or business success. It is believed that God eventually will reclaim the zombie's soul, so the zombie is a temporary spiritual entity. The two types of zombie reflect soul dualism, a belief of Bakongo religion and Haitian voodoo. Each type of legendary zombie is therefore missing one half of its soul (the flesh or the spirit). The zombie belief has its roots in traditions brought to Haiti by enslaved Africans and their subsequent experiences in the New World. It was thought that the voodoo deity Baron Samedi would gather them from their grave to bring them to a heavenly afterlife in Africa ("Guinea"), unless they had offended him in some way, in which case they would be forever a slave after death, as a zombie. A zombie could also be saved by feeding them salt. English professor Amy Wilentz has written that the modern concept of zombies was strongly influenced by Haitian slavery. Slave drivers on the plantations, who were usually slaves themselves and sometimes voodoo priests, used the fear of zombification to discourage slaves from committing suicide. While most scholars have associated the Haitian zombie with African cultures, a connection has also been suggested to the island's indigenous Taíno people, partly based on an early account of native shamanist practices written by Ramón Pané, a monk of the Hieronymite religious order and companion of Christopher Columbus. The Haitian zombie phenomenon first attracted widespread international attention during the United States occupation of Haiti (1915–1934), when a number of case histories of purported "zombies" began to emerge. The first popular book covering the topic was William Seabrook's The Magic Island (1929). Seabrooke cited Article 246 of the Haitian criminal code, which was passed in 1864, asserting that it was an official recognition of zombies. This passage was later used in promotional materials for the 1932 film White Zombie. Also shall be qualified as attempted murder the employment which may be made by any person of substances which, without causing actual death, produce a lethargic coma more or less prolonged. If, after the administering of such substances, the person has been buried, the act shall be considered murder no matter what result follows. In 1937, while researching folklore in Haiti, Zora Neale Hurston encountered the case of a woman who appeared in a village. A family claimed that she was Felicia Felix-Mentor, a relative, who had died and been buried in 1907 at the age of 29. The woman was examined by a doctor; X-rays indicated that she did not have a leg fracture that Felix-Mentor was known to have had. Hurston pursued rumors that affected persons were given a powerful psychoactive drug, but she was unable to locate individuals willing to offer much information. She wrote: "What is more, if science ever gets to the bottom of Vodou in Haiti and Africa, it will be found that some important medical secrets, still unknown to medical science, give it its power, rather than gestures of ceremony." Kongo A Central African origin for the Haitian zombie has been postulated based on two etymologies in the Kongo language, nzambi ("god") and zumbi ("fetish"). This root helps form the names of several deities, including the Kongo creator deity Nzambi Mpungu and the Louisiana serpent deity Li Grand Zombi (a local version of the Haitian Damballa), but it is in fact a generic word for a divine spirit. The common African conception of beings under these names is more similar to the incorporeal "zombie astral", as in the Kongo Nkisi spirits. A related, but also often incorporeal, undead being is the jumbee of the English-speaking Caribbean, considered to be of the same etymology; in the French West Indies also, local "zombies" are recognized, but these are of a more general spirit nature. South Africa The idea of physical zombie-like creatures is present in some South African cultures, where they are called xidachane in Sotho/Tsonga and maduxwane in Venda. In some communities, it is believed that a dead person can be zombified by a small child. It is said that the spell can be broken by a powerful enough sangoma. It is also believed in some areas of South Africa that witches can zombify a person by killing and possessing the victim's body to force it into slave labor. After rail lines were built to transport migrant workers, stories emerged about "witch trains". These trains appeared ordinary, but were staffed by zombified workers controlled by a witch. The trains would abduct a person boarding at night, and the person would then either be zombified or beaten and thrown from the train a distance away from the original location. Origin hypotheses Chemical Several decades after Hurston's work, Wade Davis, a Harvard ethnobotanist, presented a pharmacological case for zombies in a 1983 article in the Journal of Ethnopharmacology, and later in two popular books: The Serpent and the Rainbow (1985) and Passage of Darkness: The Ethnobiology of the Haitian Zombie (1988). Davis traveled to Haiti in 1982 and, as a result of his investigations, claimed that a living person can be turned into a zombie by two special powders being introduced into the bloodstream (usually through a wound). The first, French: coup de poudre ("powder strike"), includes tetrodotoxin (TTX), a powerful and frequently fatal neurotoxin found in the flesh of the pufferfish (family Tetraodontidae). The second powder consists of deliriant drugs such as datura. Together these powders were said to induce a deathlike state, in which the will of the victim would be entirely subjected to that of the bokor. Davis also popularized the story of Clairvius Narcisse, who was claimed to have succumbed to this practice. The most ethically questioned and least scientifically explored ingredient of the powders is part of a recently buried child's brain. The process described by Davis was an initial state of deathlike suspended animation, followed by re-awakening – typically after being buried – into a psychotic state. The psychosis induced by the drug and psychological trauma was hypothesised by Davis to reinforce culturally learned beliefs and to cause the individual to reconstruct their identity as that of a zombie, since they "knew" that they were dead and had no other role to play in the Haitian society. Societal reinforcement of the belief was hypothesized by Davis to confirm for the zombie individual the zombie state, and such individuals were known to hang around in graveyards, exhibiting attitudes of low affect. Davis's claim has been criticized, particularly the suggestion that Haitian witch doctors can keep "zombies" in a state of pharmacologically induced trance for many years. Symptoms of TTX poisoning range from numbness and nausea to paralysis – particularly of the muscles of the diaphragm – unconsciousness, and death, but do not include a stiffened gait or a deathlike trance. According to psychologist Terence Hines, the scientific community dismisses tetrodotoxin as the cause of this state, and Davis' assessment of the nature of the reports of Haitian zombies is viewed as overly credulous. Social Scottish psychiatrist R. D. Laing highlighted the link between social and cultural expectations and compulsion, in the context of schizophrenia and other mental illness, suggesting that schizogenesis may account for some of the psychological aspects of zombification. Particularly, this suggests cases where schizophrenia manifests a state of catatonia. Roland Littlewood, professor of anthropology and psychiatry, published a study supporting a social explanation of the zombie phenomenon in the medical journal The Lancet in 1997. The social explanation sees observed cases of people identified as zombies as a culture-bound syndrome, with a particular cultural form of adoption practiced in Haiti that unites the homeless and mentally ill with grieving families who see them as their "returned" lost loved ones, as Littlewood summarizes his findings in an article in Times Higher Education: I came to the conclusion that although it is unlikely that there is a single explanation for all cases where zombies are recognised by locals in Haiti, the mistaken identification of a wandering mentally ill stranger by bereaved relatives is the most likely explanation in many cases. People with a chronic schizophrenic illness, brain damage or learning disability are not uncommon in rural Haiti, and they would be particularly likely to be identified as zombies. Modern archetype evolution Pulliam and Fonseca (2014) and Walz (2006) trace the zombie lineage back to ancient Mesopotamia. In the Descent of Ishtar, the goddess Ishtar threatens: She repeats this same threat in a slightly modified form in the Epic of Gilgamesh. One of the first books to expose Western culture to the concept of the voodoo zombie was The Magic Island (1929) by W. B. Seabrook. This is the sensationalized account of a narrator who encounters voodoo cults in Haiti and their resurrected thralls. Time commented that the book "introduced 'zombi' into U.S. speech". Zombies have a complex literary heritage, with antecedents ranging from Richard Matheson and H. P. Lovecraft to Mary Shelley's Frankenstein drawing on European folklore of the undead. Victor Halperin directed White Zombie (1932), a horror film starring Bela Lugosi. Here zombies are depicted as mindless, unthinking henchmen under the spell of an evil magician. Zombies, often still using this voodoo-inspired rationale, were initially uncommon in cinema, but their appearances continued sporadically through the 1930s to the 1960s, with films including I Walked with a Zombie (1943) and Plan 9 from Outer Space (1959). Frankenstein by Mary Shelley, while not a zombie novel per se, foreshadows many 20th century ideas about zombies in that the resurrection of the dead is portrayed as a scientific process rather than a mystical one and that the resurrected dead are degraded and more violent than their living selves. Frankenstein, published in 1818, has its roots in European folklore, whose tales of the vengeful dead also informed the evolution of the modern conception of the vampire. Later notable 19th century stories about the avenging undead included Ambrose Bierce's "The Death of Halpin Frayser" and various Gothic Romanticism tales by Edgar Allan Poe. Though their works could not be properly considered zombie fiction, the supernatural tales of Bierce and Poe would prove influential on later writers such as H. P. Lovecraft, by Lovecraft's own admission. In the 1920s and early 1930s, Lovecraft wrote several novellae that explored the undead theme. "Cool Air", "In the Vault" and "The Outsider" all deal with the undead, but Lovecraft's "Herbert West–Reanimator" (1921) "helped define zombies in popular culture". This series of short stories featured Herbert West, a mad scientist, who attempts to revive human corpses, with mixed results. Notably, the resurrected dead are uncontrollable, mostly mute, primitive and extremely violent; though they are not referred to as zombies, their portrayal was prescient, anticipating the modern conception of zombies by several decades. Edgar Rice Burroughs similarly depicted animated corpses in the second book of his Venus series, again without using the terms "zombie" or "undead".Avenging zombies would feature prominently in the early 1950s EC Comics, which George A. Romero would later claim as an influence. The comics, including Tales from the Crypt, The Vault of Horror and Weird Science, featured avenging undead in the Gothic tradition quite regularly, including adaptations of Lovecraft's stories, which included "In the Vault", "Cool Air" and "Herbert West–Reanimator". Richard Matheson's 1954 novel I Am Legend, although classified as a vampire story, had a great impact on the zombie genre by way of George A. Romero. The novel and its 1964 film adaptation, The Last Man on Earth, which concern a lone human survivor waging war against a world of vampires, would by Romero's own admission greatly influence his 1968 low-budget film Night of the Living Dead, a work that was more influential on the concept of zombies than any literary or cinematic work before it. The monsters in the film and its sequels, such as Dawn of the Dead (1978) and Day of the Dead (1985), as well as the many zombie films it inspired, such as The Return of the Living Dead (1985) and Zombi 2 (1979), are usually hungry for human flesh, although Return of the Living Dead introduced the popular concept of zombies eating human brains. There has been an evolution in the zombie archetype from supernatural to scientific themes. I Am Legend and Night of the Living Dead began the shift away from Haitian dark magic, though did not give scientific explanations for zombie origins. A more decisive shift towards scientific themes came with the Resident Evil video game series in the late 1990s, which gave more realistic scientific explanations for zombie origins while drawing on modern science and technology, such as biological weaponry, genetic manipulation, and parasitic symbiosis. This became the standard approach for explaining zombie origins in popular fiction that followed Resident Evil. There has also been shift towards an action approach, which has led to another evolution of the zombie archietype, the "fast zombie" or running zombie. In contrast to Romero's classic slow zombies, "fast zombies" can run, are more aggressive and are often more intelligent. This type of zombie has origins in 1990s Japanese horror video games. In 1996, Capcom's survival horror video game Resident Evil featured zombie dogs that run towards the player. Later the same year, Sega's arcade shooter The House of the Dead introduced running human zombies, who run towards the player and can also jump and swim. The running human zombies introduced in The House of the Dead video games became the basis for the "fast zombies" that became popular in zombie films during the early 21st century, starting with 28 Days Later (2002), the Resident Evil and House of the Dead films and the 2004 Dawn of the Dead remake. These films also adopted an action approach to the zombie concept, which was also influenced by the Resident Evil and House of the Dead video games. Depictions in popular culture Film and television Films featuring zombies have been a part of cinema since the 1930s. White Zombie (directed by Victor Halperin in 1932) and I Walked with a Zombie (directed by Jacques Tourneur; 1943) were early serious examples, and Bob Hope's The Ghost Breakers (directed by George Marshall in 1940) being an early comedic take on the zombie genre. With George A. Romero's Night of the Living Dead (1968), the zombie trope began to be increasingly linked to consumerism and consumer culture. Today, zombie films are released with such regularity (at least 50 films were released in 2014 alone) that they constitute a separate subgenre of horror film. Voodoo-related zombie themes have also appeared in espionage or adventure-themed works outside the horror genre. For example, the original Jonny Quest series (1964) and the James Bond novel Live and Let Die as well as its film adaptation both feature Caribbean villains who falsely claim the voodoo power of zombification to keep others in fear of them. Romero's modern zombie archetype in Night of the Living Dead was influenced by several earlier zombie-themed films, including White Zombie, Revolt of the Zombies (1936) and The Plague of the Zombies (1966). Romero was also inspired by Richard Matheson's novel I Am Legend (1954), along with its film adaptation, The Last Man on Earth (1964). George A. Romero (1968–1985) The modern conception of the zombie owes itself almost entirely to George A. Romero's 1968 film Night of the Living Dead. In his films, Romero "bred the zombie with the vampire, and what he got was the hybrid vigour of a ghoulish plague monster". This entailed an apocalyptic vision of monsters that have come to be known as Romero zombies. Roger Ebert of the Chicago Sun-Times chided theater owners and parents who allowed children access to the film. "I don't think the younger kids really knew what hit them", complained Ebert, "They were used to going to movies, sure, and they'd seen some horror movies before, sure, but this was something else." According to Ebert, the film affected the audience immediately: The kids in the audience were stunned. There was almost complete silence. The movie had stopped being delightfully scary about halfway through, and had become unexpectedly terrifying. There was a little girl across the aisle from me, maybe nine years old, who was sitting very still in her seat and crying. Romero's reinvention of zombies is notable in terms of its thematics; he used zombies not just for their own sake, but as a vehicle "to criticize real-world social ills—such as government ineptitude, bioengineering, slavery, greed and exploitation—while indulging our post-apocalyptic fantasies". Night was the first of six films in Romero's Living Dead series. Its first sequel, Dawn of the Dead, was released in 1978. Lucio Fulci's Zombi 2 was released just months after Dawn of the Dead as an ersatz sequel (Dawn of the Dead was released in several other countries as Zombi or Zombie). Dawn of the Dead was the most commercially successful zombie film for decades, up until the zombie revival of the 2000s. The 1981 film Hell of the Living Dead referenced a mutagenic gas as a source of zombie contagion: an idea also used in Dan O'Bannon's 1985 film Return of the Living Dead. Return of the Living Dead featured zombies that hungered specifically for human brains. Relative Western decline (1985–1995) Zombie films in the 1980s and 1990s were not as commercially successful as Dawn of the Dead in the late 1970s. The mid-1980s produced few zombie films of note. Perhaps the most notable entry, the Evil Dead trilogy, while highly influential, are not technically zombie films, but films about demonic possession, despite the presence of the undead. 1985's Re-Animator, loosely based on the Lovecraft story, stood out in the genre, achieving nearly unanimous critical acclaim and becoming a modest success, nearly outstripping Romero's Day of the Dead for box office returns. After the mid-1980s, the subgenre was mostly relegated to the underground. Notable entries include director Peter Jackson's ultra-gory film Braindead (1992) (released as Dead Alive in the U.S.), Bob Balaban's comic 1993 film My Boyfriend's Back, where a self-aware high-school boy returns to profess his love for a girl and his love for human flesh, and Michele Soavi's Dellamorte Dellamore (1994) (released as Cemetery Man in the U.S.). Early Asian films (1985–1995) In 1980s Hong Kong cinema, the Chinese jiangshi, a zombie-like creature dating back to Qing dynasty era jiangshi fiction of the 18th and 19th centuries, were featured in a wave of jiangshi films, popularised by Mr. Vampire (1985). Hong Kong jiangshi films were popular in the Far East from the mid-1980s to the early 1990s. Prior to the 1990s, there were not many Japanese films related to what may be considered in the West as a zombie film. Early films such as The Discarnates (1988) feature little gore and no cannibalism, but it is about the dead returning to life looking for love rather than a story of apocalyptic destruction. One of the earliest Japanese zombie films with considerable gore and violence was Battle Girl: The Living Dead in Tokyo Bay (1991). Far East revival (1996–2001) According to Kim Newman in the book Nightmare Movies (2011), the "zombie revival began in the Far East" during the late 1990s, largely inspired by two Japanese zombie games released in 1996: Capcom's Resident Evil, which started the Resident Evil video game series that went on to sell 24 million copies worldwide by 2006, and Sega's arcade shooter House of the Dead. The success of these two 1996 zombie games inspired a wave of Asian zombie films. From the late 1990s, zombies experienced a renaissance in low-budget Asian cinema, with a sudden spate of dissimilar entries, including Bio Zombie (1998), Wild Zero (1999), Junk (1999), Versus (2000) and Stacy (2001). Most Japanese zombie films emerged in the wake of Resident Evil, such as Versus, Wild Zero, and Junk, all from 2000. The zombie films released after Resident Evil behaved similarly to the zombie films of the 1970s, except that they were influenced by zombie video games, which inspired them to dwell more on the action compared to the older Romero films. Global film revival (2001–2008) The zombie revival, which began in the Far East, eventually went global, following the worldwide success of the Japanese zombie games Resident Evil and The House of the Dead. Resident Evil in particular sparked a revival of the zombie genre in popular culture, leading to a renewed global interest in zombie films during the early 2000s. In addition to being adapted into the Resident Evil and House of the Dead films from 2002 onwards, the original video games themselves also inspired zombie films such as 28 Days Later (2002) and Shaun of the Dead (2004). This led to the revival of zombie films in global popular culture. The turn of the millennium coincided with a decade of box office successes in which the zombie subgenre experienced a resurgence: the Resident Evil movies (2002–2016), the British films 28 Days Later and 28 Weeks Later (2007), the Dawn of the Dead remake (2004), and the comedies Shaun of the Dead and Dance of the Dead (2008). The new interest allowed Romero to create the fourth entry in his zombie series: Land of the Dead, released in the summer of 2005. Romero returned to the series with the films Diary of the Dead (2008) and Survival of the Dead (2010). Generally, the zombies in these shows are the slow, lumbering and unintelligent kind, first made popular in Night of the Living Dead. The Resident Evil films, 28 Days Later and the Dawn of the Dead remake all set box office records for the zombie genre, reaching levels of commercial success not seen since the original Dawn of the Dead in 1978. Motion pictures created in the 2000s, like 28 Days Later, the House of the Dead and Resident Evil films, and the Dawn of the Dead remake, have featured zombies that are more agile, vicious, intelligent, and stronger than the traditional zombie. These new type of zombies, the fast zombie or running zombie, have origins in video games, with Resident Evil's running zombie dogs and especially The House of the Dead game's running human zombies. Spillover to television (2008–2015) The success of Shaun of the Dead led to more successful zombie comedies during the late 2000s to early 2010s, such as Zombieland (2009) and Cockneys vs Zombies (2012). By 2011, the Resident Evil film adaptations had also become the highest-grossing film series based on video games, after they grossed more than $1 billion worldwide. In 2013, the AMC series The Walking Dead had the highest audience ratings in the United States for any show on broadcast or cable with an average of 5.6 million viewers in the 18- to 49-year-old demographic. The film World War Z became the highest-grossing zombie film, and one of the highest-grossing films of 2013. At the same time, starting from the mid-2000s, a new type of zombie film has been growing in popularity: the one in which zombies are portrayed as humanlike in appearance and behavior, retaining the personality traits they had in life, and becoming friends or even romantic partners for humans rather than a threat to humanity. Notable examples of human–zombie romance include the stop-motion animated movie Corpse Bride, live-action movies Warm Bodies, Camille, Life After Beth, Burying the Ex, and Nina Forever, and TV series Pushing Daisies and Babylon Fields. According to zombie scholar Scott Rogers, "what we are seeing in Pushing Daisies, Warm Bodies, and iZombie is in many ways the same transformation [of the zombies] that we have witnessed with vampires since the 1931 Dracula represented Dracula as essentially human—a significant departure from the monstrous representation in the 1922 film Nosferatu". Rogers also notes the accompanying visual transformation of the living dead: while the "traditional" zombies are marked by noticeable disfigurement and decomposition, the "romantic" zombies show little or no such traits. Return to decline (2015–present) In the late 2010s, zombie films began declining in popularity, with elevated horror films gradually taking their place, such as The Witch (2015), Get Out (2017), A Quiet Place (2018) and Hereditary (2018). An exception is the low-budget Japanese zombie comedy One Cut of the Dead (2017), which became a sleeper hit in Japan, and it made box office history by earning over a thousand times its budget. One Cut of the Dead also received worldwide acclaim, with Rotten Tomatoes stating that it "reanimates the moribund zombie genre with a refreshing blend of formal daring and clever satire". The "romantic zombie" angle still remains popular, however: the late 2010s and early 2020s saw the release of the TV series American Gods, iZombie, and Santa Clarita Diet, as well as the 2018 Disney Channel Original Movie Zombies and sequels Zombies 2 (2020) and Zombies 3 (2022). Apocalypse Intimately tied to the concept of the modern zombie is that of the "zombie apocalypse": the breakdown of society as a result of an initial zombie outbreak that spreads quickly. This archetype has emerged as a prolific subgenre of apocalyptic fiction and has been portrayed in many zombie-related media after Night of the Living Dead. In a zombie apocalypse, a widespread (usually global) rise of zombies hostile to human life engages in a general assault on civilization. Victims of zombies may become zombies themselves. This causes the outbreak to become an exponentially growing crisis: the spreading phenomenon swamps normal military and law-enforcement organizations, leading to the panicked collapse of civilized society until only isolated pockets of survivors remain, scavenging for food and supplies in a world reduced to a pre-industrial hostile wilderness. Possible causes for zombie behavior in a modern population can be attributed to viruses, bacteria or other phenomena that reduce the mental capacity of humans, causing them to behave in a very primitive and destructive fashion. Subtext The usual subtext of the zombie apocalypse is that civilization is inherently vulnerable to the unexpected, and that most individuals, if desperate enough, cannot be relied on to comply with the author's ethos. The narrative of a zombie apocalypse carries strong connections to the turbulent social landscape of the United States in the 1960s, when Night of the Living Dead provided an indirect commentary on the dangers of conformity, a theme also explored in the novel The Body Snatchers (1954) and associated film Invasion of the Body Snatchers (1956). Many also feel that zombies allow people to deal with their own anxieties about the end of the world. One scholar concluded that "more than any other monster, zombies are fully and literally apocalyptic ... they signal the end of the world as we have known it". While zombie apocalypse scenarios are secular, they follow a religious pattern based on Christian ideas of an end-times war and messiah. Simon Pegg, who starred in and co-wrote the 2004 zombie comedy film Shaun of the Dead, wrote that zombies were the "most potent metaphorical monster". According to Pegg, whereas vampires represent sex, zombies represent death: "Slow and steady in their approach, weak, clumsy, often absurd, the zombie relentlessly closes in, unstoppable, intractable." He expressed his dislike for the trend for fast zombies, and argued that they should be slow and inept; just as a healthy diet and exercise can delay death, zombies are easy to avoid, but not forever. He also argued that this was essential for making them "oddly sympathetic... to create tragic anti-heroes... to be pitied, empathised with, even rooted for. The moment they appear angry or petulant, the second they emit furious velociraptor screeches (as opposed to the correct mournful moans of longing), they cease to possess any ambiguity. They are simply mean." Story elements Initial contacts with zombies are extremely dangerous and traumatic, causing shock, panic, disbelief and possibly denial, hampering survivors' ability to deal with hostile encounters. The response of authorities to the threat is slower than its rate of growth, giving the zombie plague time to expand beyond containment. This results in the collapse of the given society. Zombies take full control, while small groups of the living must fight for their survival. The stories usually follow a single group of survivors, caught up in the sudden rush of the crisis. The narrative generally progresses from the onset of the zombie plague, then initial attempts to seek the aid of authorities, the failure of those authorities, through to the sudden catastrophic collapse of all large-scale organization and the characters' subsequent attempts to survive on their own. Such stories are often squarely focused on the way their characters react to such an extreme catastrophe, and how their personalities are changed by the stress, often acting on more primal motivations (fear, self-preservation) than they would display in normal life. Literature In the 1990s, zombie fiction emerged as a distinct literary subgenre, with the publication of Book of the Dead (1990) and its follow-up Still Dead: Book of the Dead 2 (1992), both edited by horror authors John Skipp and Craig Spector. Featuring Romero-inspired stories from the likes of Stephen King, the Book of the Dead compilations are regarded as influential in the horror genre and perhaps the first true "zombie literature". Horror novelist Stephen King has written about zombies, including his short story "Home Delivery" (1990) and his novel Cell (2006), concerning a struggling young artist on a trek from Boston to Maine in hopes of saving his family from a possible worldwide outbreak of zombie-like maniacs. Max Brooks's novel World War Z (2006) became a New York Times bestseller. Brooks had previously authored The Zombie Survival Guide (2003), a zombie-themed parody of pop-fiction survival guides. Brooks has said that zombies are so popular because "Other monsters may threaten individual humans, but the living dead threaten the entire human race...Zombies are slate wipers." Seth Grahame-Smith's mashup novel Pride and Prejudice and Zombies (2009) combines the full text of Jane Austen's Pride and Prejudice (1813) with a story about a zombie epidemic within the novel's British Regency period setting. In 2009, Katy Hershbereger of St. Martin's Press stated: "In the world of traditional horror, nothing is more popular right now than zombies...The living dead are here to stay." 2000s and 2010s were marked by a decidedly new type of zombie novel, in which zombies retain their humanity and become friends or even romantic partners for humans; critics largely attribute this trend to the influence of Stephenie Meyer's vampire series Twilight. One of the most prominent examples is Generation Dead by Daniel Waters, featuring undead teenagers struggling for equality with the living and a human protagonist falling in love with their leader. Other novels of this period involving human–zombie romantic relationships include Bone Song by John Meaney, American Gods by Neil Gaiman, Midnight Tides by Steven Erikson, and Amy Plum's Die for Me series; much earlier examples, dating back to the 1980s, are Dragon on a Pedestal by Piers Anthony and Conan the Defiant by Steve Perry. Anime and manga There has been a growth in the number of zombie manga in the first decade of the 21st century, and in a list of "10 Great Zombie Manga", Anime News Network's Jason Thompson placed I Am a Hero at number 1, considering it "probably the greatest zombie manga ever". In second place was Living Corpse, and in third was Biomega, which he called "the greatest science-fiction virus zombie manga ever". During the late 2000s and early 2010s, there were several manga and anime series that humanized zombies by presenting them as protagonists or love interests, such as Sankarea: Undying Love and Is This a Zombie? (both debuted in 2009). Z ~Zed~ was adapted into a live action film in 2014. Video and performance art Artist Jillian McDonald has made several works of video art involving zombies and exhibited them in her 2006 show "Horror Make-Up", which debuted on 8 September 2006 at Art Moving Projects, a gallery in, Williamsburg, Brooklyn. Artist Karim Charredib has dedicated his work to the zombie figure. In 2007, he made a video installation at Villa Savoye called "Them !!!", wherein zombies walked in the villa like tourists. Games Zombies are a common undead creature type fantasy role playing games. In Dungeons & Dragons, zombies are one of the basic undead creature types, based on the zombie from folklore as well as more contemporary entertainment. Zombies are generally portrayed as supernatural creations, with variations such as the Ju-ju, Sea Zombie, and Zombie Lord. The Advanced Dungeons & Dragons 2nd edition game, however, also incorporated a creature called the yellow musk creeper, a creeping plant that drains the intelligence of its victims, possibly turning them into "zombies" under the plant's control. Ben Woodard found this to be an expression of the "seemingly endless morphology of fungal creep and toxicological capacity" within the game. In video games, the release of two 1996 horror games Capcom's Resident Evil and Sega's The House of the Dead sparked an international craze for zombie games. In 2013, George A. Romero said that it was the video games Resident Evil and House of the Dead "more than anything else" that popularised zombies in early 21st century popular culture. The modern fast-running zombies have origins in these games, with Resident Evil's running zombie dogs and especially House of the Dead's running human zombies, which later became a staple of modern zombie films. Zombies went on to become a popular theme for video games, particularly in the survival horror, stealth, first-person shooter and role-playing game genres. Important horror fiction media franchises in this area include Resident Evil, The House of the Dead, Silent Hill, Dead Rising, Dead Island, Left 4 Dead, Dying Light, State of Decay, The Last of Us and the Zombies game modes from the Call of Duty title series. A series of games has also been released based on the widely popular TV show The Walking Dead, first aired in 2010. World of Warcraft, first released in 2004, is an early example of a video game in which an individual zombie-like creature could be chosen as a player character (a previous game in the same series, Warcraft III, allowed a player control over an undead army). PopCap Games' Plants vs. Zombies, a humorous tower defense game, was an indie hit in 2009, featuring in several best-of lists at the end of that year. The massively multiplayer online role-playing game Urban Dead, a free grid-based browser game where zombies and survivors fight for control of a ruined city, is one of the most popular games of its type. DayZ, a zombie-based survival horror mod for ARMA 2, was responsible for over 300,000 unit sales of its parent game within two months of its release. Over a year later, the developers of the mod created a standalone version of the same game, which was in early access on Steam, and so far has sold 3 million copies since its release in December 2013. Romero would later opine that he believes that much of the 21st century obsessions with zombies can be traced more towards video games than films, noting that it was not until the 2009 film Zombieland that a zombie film was able to gross more than 100 million dollars. Outside of video games, zombies frequently appear in trading card games, such as Magic: The Gathering or Yu-Gi-Oh! Trading Card Game (which even has a Zombie-Type for its "monsters"), as well as in role-playing games, such as Dungeons & Dragons, tabletop games such as Zombies!!! and Dead of Winter: A Cross Roads Game, and tabletop wargames, such as Warhammer Fantasy and 40K. The game Humans vs. Zombies is a zombie-themed live-action game played on college campuses. Writing for Scientific American, Kyle Hill praised the 2013 game The Last of Us for its plausibility, basing its zombification process on a fictional strain of the parasitic Cordyceps fungus, a real-world genus whose members control the behavior of their arthropod hosts in "zombielike" ways to reproduce. Despite the plausibility of this mechanism (also explored in the novel The Girl with All the Gifts and the film of the same name), to date there have been no documented cases of humans infected by Cordyceps. Zombie video games have remained popular in the late 2010s, as seen with the commercial success of the Resident Evil 2 remake and Days Gone in 2019. This enduring popularity may be attributed, in part, to the fact that zombie enemies are not expected to exhibit significant levels of intelligence, making them relatively straightforward to program. However, less pragmatic advantages, such as those related to storytelling and representation, are increasingly important. American government On 18 May 2011, the United States' Centers for Disease Control and Prevention (CDC) published a graphic novel entitled Preparedness 101: Zombie Apocalypse, providing tips to survive a zombie invasion as a "fun new way of teaching the importance of emergency preparedness". The CDC used the metaphor of a zombie apocalypse to illustrate the value of laying in water, food, medical supplies, and other necessities in preparation for any and all potential disasters, be they hurricanes, earthquakes, tornadoes, floods, or hordes of zombies. In 2011, the U.S. Department of Defense drafted CONPLAN 8888, a training exercise detailing a strategy to defend against a zombie attack. Music Michael Jackson's music video Thriller (1983), in which he dances with a troupe of zombies, has been preserved as a cultural treasure by the Library of Congress' National Film Registry. Many instances of pop culture media have paid tribute to this video, including a gathering of 14,000 university students dressed as zombies in Mexico City, and 1,500 prisoners in orange jumpsuits recreating the zombie dance in a viral video. The Brooklyn hip hop trio Flatbush Zombies incorporate many tropes from zombie fiction and play on the theme of a zombie apocalypse in their music. They portray themselves as "living dead", describing their use of psychedelics such as LSD and psilocybin mushrooms as having caused them to experience ego death and rebirth. Social activism The zombie also appears as a metaphor in protest songs, symbolizing mindless adherence to authority, particularly that of law enforcement and the armed forces. Well-known examples include Fela Kuti's 1976 album Zombie and the Cranberries' 1994 single "Zombie". Organized zombie walks have been staged, either as performance art or as part of protests that parody political extremism or apathy. A variation of the zombie walk is the zombie run. Here participants do a 5 km run wearing a belt with several flag "lives". If the chasing zombies capture all of the flags, the runner becomes "infected". If he or she reaches the finish line, which may involve wide detours ahead of the zombies, then the participant is a "survivor". In either case, an appropriate participation medal is awarded. Theoretical academic studies Researchers have used theoretical zombie infections to test epidemiology modeling. One study found that all humans end up turned or dead. This is because the main epidemiological risk of zombies, besides the difficulties of neutralizing them, is that their population just keeps increasing; generations of humans merely "surviving" still have a tendency to feed zombie populations, resulting in gross outnumbering. The researchers explain that their methods of modelling may be applicable to the spread of political views or diseases with dormant infection. Adam Chodorow of the Sandra Day O'Connor College of Law at Arizona State University investigated the estate and income tax implications of a zombie apocalypse under United States federal and state tax codes. Neuroscientists Bradley Voytek and Timothy Verstynen have built a side career in extrapolating how ideas in neuroscience would theoretically apply to zombie brains. Their work has been featured in Forbes, New York Magazine, and other publications. See also List of zombie Nazi films List of zombie short films and undead-related projects Ophiocordyceps unilateralis, a fungus that creates so-called "zombie ants" or, more generally, behavior-altering parasites "Philosophical zombie", a person without a consciousness, used in philosophical thought experiments Skeleton (undead) Smombie (a combination of "smartphone" and "zombie") Clairvius Narcisse Zuvembie References Notes Bibliography Balmain, Colette (2006). Introduction to Japanese Horror Film. Edinburgh University Press. ISBN 978-1903254417. Further reading Ackermann, H.W.; Gauthier, J. (1991). "The Ways and Nature of the Zombi". The Journal of American Folklore. 104 (414): 466–494. doi:10.2307/541551. JSTOR 541551. Black, J. Anderson (2000) The Dead Walk Noir Publishing, Hereford, Herefordshire, ISBN 0-9536564-2-X Curran, Bob (2006) Encyclopedia of the Undead: A field guide to creatures that cannot rest in peace New Page Books, Franklin Lakes, New Jersey, ISBN 1-56414-841-6 Flint, David (2008) Zombie Holocaust: How the living dead devoured pop culture Plexus, London, ISBN 978-0-85965-397-8 Forget, Thomas (2007) Introducing Zombies Rosen Publishing, New York, ISBN 1-4042-0852-6; (juvenile) Graves, Zachary (2010) Zombies: The complete guide to the world of the living dead Sphere, London, ISBN 978-1-84744-415-8 Hurston, Zora Neale (2009) Tell My Horse: Voodoo and Life in Haiti and Jamaica, Harper Perennial. ISBN 978-0-06169-513-1 Mars, Louis P. (1945). "Media life zombies for the world". Man. 45 (22): 38–40. doi:10.2307/2792947. JSTOR 2792947. (Copy at Webster University) McIntosh, Shawn and Leverette, Marc (editors) (2008) Zombie Culture: Autopsies of the Living Dead Scarecrow Press, Lanham, Maryland, ISBN 0-8108-6043-0. Moreman, Christopher M., and Cory James Rushton (editors) (2011) Zombies Are Us: Essays on the Humanity of the Walking Dead. McFarland. ISBN 978-0-7864-5912-4. Shaka McGlotten, and Jones, Steve (editors) (2014) Zombies and Sexuality: Essays on Desire and the Living Dead. McFarland. ISBN 978-0-7864-7907-8. Bishop, Kyle William (2015) How Zombies Conquered Popular Culture: The Multifarious Walking Dead in the 21st Century. McFarland. ISBN 978-1-4766-2208-8. Szanter, Ashley, and Richards, Jessica K. (editors) (2017) Romancing the Zombie: Essays on the Undead as Significant "Other". McFarland. ISBN 978-1-4766-6742-3. Russell, Jamie (2005) Book of the dead: the complete history of zombie cinema FAB, Godalming, England, ISBN 1-903254-33-7 Waller, Gregory A. (2010) Living and the undead: slaying vampires, exterminating zombies University of Illinois Press, Urbana, Indiana, ISBN 978-0-252-07772-2 The Sonning Prize (Danish: Sonningprisen) is a Danish culture prize awarded biennially for outstanding contributions to European culture. It is named after the Danish editor and author Carl Johan Sonning (1879–1937), who established the prize by his will. A prize was first awarded in 1950 to Winston Churchill. However, a sequence of annual awards in this name was established in 1959 with the award to Albert Schweitzer followed by Bertrand Russell in 1960, the criterion being someone who "has accomplished meritorious work for the advancement of European civilization", and judged by a committee of the Senate of the University of Copenhagen. From 1971, it was awarded every second year. Prize winners are chosen by a committee chaired by the rector of the University of Copenhagen which decides on laureates from a selection of candidates proposed by European universities. The prize amounts to DKK 1 million (~€135,000) and the award ceremony is always held on or around 19 April (Sonning's birthday) in Copenhagen. Sonning Prize laureates Source: See also List of awards for contributions to culture List of European art awards Léonie Sonning Music Prize References External links Sonning Prize at the University of Copenhagen The Scheherazade Foundation CIC is a non-profit community interest company (CIC) established in 2020 to support cultural education and intercultural bridge-building. It is based in London, England. Formation The Scheherazade Foundation is a private, non-profit community interest company (CIC) established in 2020 by the writer and film-maker Tahir Shah and his daughter, Ariane Shah, to support cultural education and intercultural bridge-building. It is based in London, England. The company's registration number in the UK is 13038593. The CIC is named after Scheherazade, the storyteller and main female character in the frame story of the collection of Middle Eastern tales, One Thousand and One Nights. Aims The three main aims of The Scheherazade Foundation are: To seek to empower women – and in particular young women – who will step out into the world, becoming leaders and role models for future generations. To bridge cultures by striving towards shared values and know-how. To harness the teaching power of stories that has been a bedrock of the human experience since the dawn of civilization. Headquarters in Casablanca In 2022, work began on renovating a one-acre, walled property, Dar Khalifa (The Caliph's House), in the Ain Diab district of Casablanca, to turn it into the headquarters of The Scheherazade Foundation. The property consists of a mansion with thirty rooms, built around an interior garden or riad. According to Jason Webster in the Financial Times, Tahir Shah, who has previously written about his time in The Caliph's House, "hired artisans and craftsmen from across Morocco to work on fabulous zellij fountains, stucco screens with geometric Islamic designs, and intricately carved wooden Berber doorways." Activities Repatriation of Ethiopian artefacts In July 2021, The Scheherazade Foundation launched a crowdfunding appeal so that they could repatriate Ethiopian artefacts looted by British troops from Ethiopia in East Africa in 1868, following the Battle of Magdala. The crowdfunding enabled the foundation to purchase several items, including an imperial shield, handwritten Ethiopian religious texts, crosses, and a set of beakers, from a UK auction house and private dealers in Europe. On 8 September 2021, the items were presented to the Ethiopian Ambassador, Teferi Melesse Desta by the foundation's chief executive officer, Tahir Shah, at a ceremony at London's Athenaeum Club. The embassy would then return the artefacts to the Ethiopian ministry of culture in the following weeks, the religious pieces being offered to the Ethiopian Orthodox Church and the rest most likely destined for the National Museum in the capital, Addis Ababa. In a report in the Smithsonian Magazine, Dr. Alula Pankhurst, a member of Ethiopia's National Heritage Restitution Committee expressed hope that The Scheherazade Foundation's efforts would lead to further restitution initiatives, "especially at a time when retaining artefacts, notably human remains such as those of Prince Alemayehu in Windsor Chapel or sacred objects such as the holy Tabot Arks of the Covenant in the British Museum is becoming increasingly anachronistic, irrelevant and embarrassing." The Scheherazade Foundation aims to repatriate more Ethiopian artefacts, and a reporter for Returning Heritage expressed the opinion that there is also a strong case "for returning the eleven sacred tabots, concealed within the vaults of the British Museum." 2021 letter of appeal Several attempts have been made in the past to get the British Museum to repatriate artefacts in its possession, but the museum had argued that it "is forbidden by the British Museum Act of 1963 to restitute objects in its collection". However, a new legal opinion commission by the Scheherazade Foundation and drawn up by Samantha Knights QC points out that the 1963 Act "has a provision that allows disposal of objects 'unfit to be retained' and that can be disposed 'without detriment to the interests of students'", and that since the artefacts have been kept in the museum's vaults for the past 150 years, without allowing their study, copying or photography, the artefacts are of "no apparent use or relevance to the museum" and would therefore "fall within this category." On the basis of this new legal opinion, a letter was drawn up by the Scheherazade Foundation and sent to the trustees of the British Museum asking for the return of the eleven wood and stone tabots held there. Signatories to the appeal include seven members of the House of Lords including former deputy chief whip Don Foster, Baron Foster of Bath; actor and broadcaster Stephen Fry; actor Rupert Everett; author and broadcaster Lemn Sissay; former British Ambassador to Ethiopia Sir Harold Walker, and retired Archbishop of Canterbury George Carey. In a statement about the appeal, the museum said that "These documents need to be reviewed and addressed with full consideration, and more time is required before this can be looked at by trustees." Further developments In November 2021, Reuters confirmed that the artefacts acquired by The Scheherazade Foundation had been successfully returned to Ethiopia, and Ethiopia's tourism minister, Nasise Challa reported that, in addition, "we have started negotiations with the British Museum to bring back 12 tabots". Storytelling and teaching stories Tahir Shah comes from a family tradition of writers and storytellers, and he has studied traditional "teaching stories" for many years. These stories contain layers of deeper meaning, and Shah likens them to eating a fruit: "a pleasant experience that also contains a form of nutrition." One of the Scheherazade Foundation's main aims is to publish and disseminate these traditional tales, and also to host a storytelling festival at their headquarters, Dar Khalifa. Scheherazade Foundation Publishing The Scheherazade Foundation is in the process of publishing a number of books on stories and storytelling: * The Secrets of Scheherazade, Scheherazade Foundation Publishing, 2022, ISBN 978-1-915311-01-6 Tale of a Lantern and Other Stories, Scheherazade Foundation Publishing, 2022, ISBN 978-1-915311-03-0 The Elephant and The Tortoise and Other Stories, Scheherazade Foundation Publishing, 2022, ISBN 978-1-915311-04-7 The Monkey’s Fiddle and Other Stories, Scheherazade Foundation Publishing, 2022, ISBN 978-1-915311-05-4 Ghost of the Violet Well and Other Stories, Scheherazade Foundation Publishing, 2022, ISBN 978-1-915311-06-1 Many Wise Fools and Other Stories, Scheherazade Foundation Publishing, 2022, ISBN 978-1-915311-07-8 The Frog Prince and Other Stories, Scheherazade Foundation Publishing, 2022, ISBN 978-1-915311-08-5 The Three Lemons and Other Stories, Scheherazade Foundation Publishing, 2022, ISBN 978-1-915311-09-2 The Twelve-Headed Griffin and Other Stories, Scheherazade Foundation Publishing, 2022, ISBN 978-1-915311-10-8 The Antelope Boy and Other Stories, Scheherazade Foundation Publishing, 2022, ISBN 978-1-915311-11-5 The Treasure of A Thousand and One Nights, Scheherazade Foundation Publishing, 2022, ISBN 978-1-915311-13-9 On Stories and Storytelling, Scheherazade Foundation Publishing, 2022, ISBN 978-1-915311-14-6 On Storytelling, Scheherazade Foundation Publishing, 2022, ISBN 978-1-915311-15-3 On Teaching Stories, Scheherazade Foundation Publishing, 2022, ISBN 978-1-915311-16-0 On The Science of Storytelling, Scheherazade Foundation Publishing, 2022, ISBN 978-1-915311-17-7 On A Thousand and One Nights, Scheherazade Foundation Publishing, 2022, ISBN 978-1-915311-18-4 On Nasrudin, Scheherazade Foundation Publishing, 2022, ISBN 978-1-915311-19-1 On Mythology, Scheherazade Foundation Publishing, 2022, ISBN 978-1-915311-20-7 On Stories and Children, Scheherazade Foundation Publishing, 2022, ISBN 978-1-915311-21-4 On Folklore, Scheherazade Foundation Publishing, 2022, ISBN 978-1-915311-22-1 Why the Fish Laughed & Other Stories, The Scheherazade Foundation, 2023, ISBN 978-1-915311-23-8 Two Cats & Other Stories, The Scheherazade Foundation, 2023, ISBN 978-1-915311-24-5 Three Stories, The Scheherazade Foundation, 2023, ISBN 978-1-915311-25-2 The Twilight of the Gods & Other Stories, The Scheherazade Foundation, 2023, ISBN 978-1-915311-26-9 The Son of Seven Queens & Other Stories, The Scheherazade Foundation, 2023, ISBN 978-1-915311-27-6 The Moon Maiden & Other Stories, The Scheherazade Foundation, 2023, ISBN 978-1-915311-28-3 The Metamorphosis & Other Stories, The Scheherazade Foundation, 2023, ISBN 978-1-915311-29-0 The Celestial Sisters & Other Stories, The Scheherazade Foundation, 2023, ISBN 978-1-915311-30-6 Tales From The Arabian Nights I, The Scheherazade Foundation, 2023, ISBN 978-1-915311-31-3 East of the Sun, West of the Moon & Other Stories, The Scheherazade Foundation, 2023, ISBN 978-1-915311-32-0 The Well at the End of the World & Other Stories, The Scheherazade Foundation, 2023, ISBN 978-1-915311-33-7 See also Category:Shah family References Further reading Webster, Jason (24 February 2023). "In search of the white house of Casablanca". Financial Times. Archived from the original on 24 February 2023. Retrieved 24 February 2023. External links The Scheherazade Foundation web site Norms, in philosophy, are concepts (sentences) of practical import, oriented to affecting an action, rather than conceptual abstractions that describe, explain, and express. Normative sentences imply "ought-to" (or "may", "may not") types of statements and assertions, in distinction to sentences that provide "is" (or "was", "will") types of statements and assertions. Common normative sentences include commands, permissions, and prohibitions; common normative abstract concepts include sincerity, justification, and honesty. A popular account of norms describes them as reasons to take action, to believe, and to feel. Types of norms Orders and permissions express norms. Such norm sentences do not describe how the world is, they rather prescribe how the world should be. Imperative sentences are the most obvious way to express norms, but declarative sentences also may be norms, as is the case with laws or 'principles'. Generally, whether an expression is a norm depends on what the sentence intends to assert. For instance, a sentence of the form "All Ravens are Black" could on one account be taken as descriptive, in which case an instance of a white raven would contradict it, or alternatively "All Ravens are Black" could be interpreted as a norm, in which case it stands as a principle and definition, so 'a white raven' would then not be a raven. Those norms purporting to create obligations (or duties) and permissions are called deontic norms (see also deontic logic). The concept of deontic norm is already an extension of a previous concept of norm, which would only include imperatives, that is, norms purporting to create duties. The understanding that permissions are norms in the same way was an important step in ethics and philosophy of law. In addition to deontic norms, many other varieties have been identified. For instance, some constitutions establish the national anthem. These norms do not directly create any duty or permission. They create a "national symbol". Other norms create nations themselves or political and administrative regions within a nation. The action orientation of such norms is less obvious than in the case of a command or permission, but is essential for understanding the relevance of issuing such norms: When a folk song becomes a "national anthem" the meaning of singing one and the same song changes; likewise, when a piece of land becomes an administrative region, this has legal consequences for many activities taking place on that territory; and without these consequences concerning action, the norms would be irrelevant. A more obviously action-oriented variety of such constitutive norms (as opposed to deontic or regulatory norms) establishes social institutions which give rise to new, previously nonexistent types of actions or activities (a standard example is the institution of marriage without which "getting married" would not be a feasible action; another is the rules constituting a game: without the norms of soccer, there would not exist such an action as executing an indirect free kick). Any convention can create a norm, although the relation between both is not settled. There is a significant discussion about (legal) norms that give someone the power to create other norms. They are called power-conferring norms or norms of competence. Some authors argue that they are still deontic norms, while others argue for a close connection between them and institutional facts (see Raz 1975, Ruiter 1993). Linguistic conventions, for example, the convention in English that "cat" means cat or the convention in Portuguese that "gato" means cat, are among the most important norms. Games completely depend on norms. The fundamental norm of many games is the norm establishing who wins and loses. In other games, it is the norm establishing how to score points. Norms can be defined as rules that regulate one's social life within a particular group. Within such, there can be explicit and implicit laws that help enforce norms. For example, explicit laws bring reward and punishment, such as cheating. Implicit cultural conventions include blocking the top of the stairs on a subway, doing your makeup on the train, or even walking slowly in the city. Norms can be described as injunctive social norms or descriptive social norms. Injunctive social norms are norms agreed upon mental representation of what a group of people think. An example of such can include being kind to your parents, or giving up the seat for a pregnant lady on the bus. These all showcase what some people feel should be done. Descriptive social norms on the other hand are norms agreed upon mental representations of what a group of people actually think or feel. An example of such can include drinking in public. Although we know it should not take place, on the back of our mind we know it happens. Another example can even include jaywalking. This shows that there are actual laws of what shouldn’t occur, yet it still does. In society, there are many norms of reciprocity: door in the face, foot in the door, etc. One of the most common uses by people is door in the face. As human beings, we want to be liked by others and feel wanted. It is simply just human nature. This strategy uses reciprocating concessions to influence one's behavior. This norm of reciprocity includes asking someone for something big, which we know the likelihood of the answer will be no. You would then ask them for something smaller and they would be more likely to say yes. For example, if I ask a group of people for 100$, they are not likely to give it to me. However, if I turn around again and ask for 5$, they are more likely to give it to me. Many psychologists have done experiments to show the power of social learning and the influence it has on social norms to behavior. In 1961, Bandura studied to see if social behaviors can be gained from observation and imitation. 36 boys and 36 girls studied at the Stanford University Nursing School. Before the experiment was done, researchers wanted to see how aggressive they were on average on a scale of 1 to 5. Then, the overall 72 students were assigned to one of three groups. One group was assigned with the control group- no model, one group was assigned with an aggressive role model, and the other group was assigned with a non-aggressive role model. They then viewed a female model and a male model of each. The children were then placed independently into the room and were given aggressive/non-aggressive toys. The non-aggressive toys included a tea set, crayons, and three bears. The aggressive toys included a peg board, a dart gun, and a 3 foot bobo doll. The child was in the room for twenty minutes and was observed through a one way mirror. Observations were made every 5 seconds during the duration of 20 minutes. The researchers had found that children who had seen the aggressive model had aggressive responses compared to people that were in the non-aggression or control group. The boys were also more likely to imitate the behavior of the same sex models rather than the girls showing more violent behavior. The girls also acted more violently to the male models. These findings relate to norms as they show the influence of social norms on behavior. The young children were more likely to observe and copy the norms and be influenced by the behavior of others, especially those they may see as “older” or a “role model.” In recent years, research has opened up on the hypothesis that non-human animals are also capable of acting according to norms. This thesis, supported by various philosophers and ethologists, is the subject of a current debate that is primarily based on the distinction between different possible concepts of ‘norm’. Major characteristics One major characteristic of norms is that, unlike propositions, they are not descriptively true or false, since norms do not purport to describe anything, but to prescribe, create or change something. Deontologists would denote them to be "prescriptively true" or false. Whereas the truth of a descriptive statement is purportedly based on its correspondence to reality, some philosophers, beginning with Aristotle, assert that the (prescriptive) truth of a prescriptive statement is based on its correspondence to right desire. Other philosophers maintain that norms are ultimately neither true or false, but only successful or unsuccessful (valid or invalid), as their propositional content obtains or not (see also John Searle and speech act). There is an important difference between norms and normative propositions, although they are often expressed by identical sentences. "You may go out" usually expresses a norm if it is uttered by the teacher to one of the students, but it usually expresses a normative proposition if it is uttered to one of the students by one of his or her classmates. Some ethical theories reject that there can be normative propositions, but these are accepted by cognitivism. One can also think of propositional norms; assertions and questions arguably express propositional norms (they set a proposition as asserted or questioned). Another purported feature of norms, it is often argued, is that they never regard only natural properties or entities. Norms always bring something artificial, conventional, institutional or "unworldly". This might be related to Hume's assertion that it is not possible to derive ought from is and to G.E. Moore's claim that there is a naturalistic fallacy when one tries to analyse "good" and "bad" in terms of a natural concept. In aesthetics, it has also been argued that it is impossible to derive an aesthetical predicate from a non-aesthetical one. The acceptability of non-natural properties, however, is strongly debated in present-day philosophy. Some authors deny their existence, some others try to reduce them to natural ones, on which the former supervene. Other thinkers (Adler, 1986) assert that norms can be natural in a different sense than that of "corresponding to something proceeding from the object of the prescription as a strictly internal source of action". Rather, those who assert the existence of natural prescriptions say norms can suit a natural need on the part of the prescribed entity. More to the point, however, is the putting forward of the notion that just as descriptive statements being considered true are conditioned upon certain self-evident descriptive truths suiting the nature of reality (such as: it is impossible for the same thing to be and not be at the same time and in the same manner), a prescriptive truth can suit the nature of the will through the authority of it being based upon self-evident prescriptive truths (such as: one ought to desire what is really good for one and nothing else). Recent works maintain that normativity has an important role in several different philosophical subjects, not only in ethics and philosophy of law (see Dancy, 2000). See also Constitution Deontic logic Deontology Law (principle) Meta-ethics Norm (sociology) Normative Normative ethics Normative statement Philosophy of law Principle Law Rule of law Rule according to higher law Social norm Speech act Further reading Adler, Mortimer (1985), Ten Philosophical Mistakes, MacMillan, New York. Aglo, John (1998), Norme et symbole: les fondements philosophiques de l'obligation, L'Harmattan, Paris. Aglo, John (2001), Les fondements philosophiques de la morale dans une société à tradition orale, L'Harmattan, Paris. Alexy, Robert (1985), Theorie der Grundrechte, Suhrkamp, Frankfurt a. M.. Translation: A Theory of Constitutional Rights, Oxford University Press, Oxford: 2002. Bicchieri, Cristina (2006), The Grammar of Society: the Nature and Dynamics of Social Norms, Cambridge University Press, Cambridge. Dancy, Jonathan (ed) (2000), Normativity, Blackwell, Oxford. Garzón Valdés, Ernesto et al. (eds) (1997), Normative Systems in Legal and Moral Theory: Festschrift for Carlos E. Alchourrón and Eugenio Bulygin, Duncker & Humblot, Berlin. Korsgaard, Christine (2000), The Sources of Normativity, Cambridge University, Cambridge. Raz, Joseph (1975, 1990), Practical Reason and Norms, Oxford University Press, Oxford; 2nd edn 1990. Rosen, Bernard (1999), The Centrality of Normative Ethical Theory, Peter Lang, New York. Ruiter, Dick (1993), Institutional Legal Facts: Legal Powers and their Effects, Kluwer, Dordrecht. Turri, John (2016), Knowledge and the Norm of Assertion: An Essay in Philosophical Science, Open Book Publishers, Cambridge. von Wright, G. H. (1963), Norm and Action: a Logical Enquiry, Routledge & Kegan Paul, London. == References == Coptology is the scientific study of the Coptic people. Origin The European interest in Coptology may have started as early as the 15th century AD. The term was used in 1976 when the First International Congress of Coptology was held in Cairo under the title "Colloquium on the Future of Coptic Studies" (11-17 December). This was followed by the establishment of the "International Association for Coptic Studies". One of the founders of the Colloquium and the Association was Pahor Labib, director of the Coptic Museum in Cairo during 1951-65. The words 'Coptology' and 'Coptologist' were introduced into the English language by Aziz Suryal Atiya. Worldwide institutions There are now institutions that give more or less regular courses of Coptology in 47 countries around the world, including Australia, Great Britain, Canada, Germany, Israel, Spain, Switzerland, and the United States. A rotating chair of Coptic studies was opened at the American University in Cairo in 2002. Divisions Art and textiles Language History Architecture Literature Music Journals Coptologia publications - Journal of Coptic Thought and Spirituality Journal of Coptic Studies Göttinger Miszellen Prominent Coptologists See also Coptic history Coptic Museum Coptic Egypt: The Christians of the Nile Institute of Coptic Studies Egyptology References External links International Association for Coptic Studies موقع الدراسات القبطية والأرثوذكسية The NeuroGenderings Network is an international group of researchers in neuroscience and gender studies. Members of the network study how the complexities of social norms, varied life experiences, details of laboratory conditions and biology interact to affect the results of neuroscientific research. Working under the label of "neurofeminism", they aim to critically analyze how the field of neuroscience operates, and to build an understanding of brain and gender that goes beyond gender essentialism while still treating the brain as fundamentally material. Its founding was part of a period of increased interest and activity in interdisciplinary research connecting neuroscience and the social sciences. History The group, comprising scholars who specialized in feminism, queer theory and gender studies, formed to tackle "neurosexism" as defined by Cordelia Fine in her 2010 book Delusions of Gender: "uncritical biases in [neuroscientific] research and public perception, and their societal impacts on an individual, structural, and symbolic level." Research can suffer from neurosexism by failing to include the social factors and expectations that shape sex differences, which possibly leads to making inferences based on flawed data. By contrast, the network members advocate "neurofeminism", aiming to critically evaluate heteronormative assumptions of contemporary brain research and examine the impact and cultural significance of neuroscientific research on society's views about gender. This includes placing greater emphasis on neuroplasticity rather than biological determinism. Conferences In March 2010, the first conference – NeuroGenderings: Critical Studies of the Sexed Brain – was held in Uppsala, Sweden. Organisers Anelis Kaiser and Isabelle Dussauge described its long terms goals "to elaborate a new conceptual approach of the relation between gender and the brain, one that could help to head gender theorists and neuroscientists to an innovative interdisciplinary place, far away from social and biological determinisms but still engaging with the materiality of the brain." The NeuroGenderings Network was established at this event, with the group's first results published in a special issue of the journal Neuroethics. Further conferences have since been held on a biennial basis: NeuroCultures — NeuroGenderings II, September 2012 at the University of Vienna's physics department; NeuroGenderings III – The First International Dissensus Conference on Brain and Gender, May 2014 in Lausanne, Switzerland; and NeuroGenderings IV in March 2016, at Barnard College, New York City. Members The members of the NeuroGenderings Network are: Bibliography Books Fine, Cordelia (2010). Delusions of gender: how our minds, society, and neurosexism create difference. New York: W.W. Norton. ISBN 9780393068382. Bluhm, Robyn; Maibom, Heidi Lene; Jaap Jacobson, Anne (2012). Neurofeminism: issues at the intersection of feminist theory and cognitive science. New York: Palgrave Macmillan. ISBN 9780230296732. Also available to view by chapter online. Schmitz, Sigrid; Höppner, Grit, eds. (2014). Gendered neurocultures: feminist and queer perspectives on current brain discourses. challenge GENDER, 2. Wien: Zaglossus. ISBN 9783902902122. Rippon, Gina (2019). Gender and Our Brains: How New Neuroscience Explodes the Myths of the Male and Female Minds. New York: Knopf Doubleday. ISBN 9781524747039. Book chapters Kaiser, Anelis (2010). "Sex/Gender and neuroscience: focusing on current research". In Blomqvist, Martha; Ehnsmyr, Ester (eds.). Never mind the gap! Gendering science in transgressive encounters. Uppsala Sweden: Skrifter från Centrum för genusvetenskap. University Printers. pp. 189–210. ISBN 9789197818636. Schmitz, Sigrid (2014). "Sex, gender, and the brain – biological determinism versus socio-cultural constructivism". In Klinge, Ineke; Wiesemann, Claudia (eds.). Sex and gender in biomedicine: theories, methodologies, results. Akron, Ohio: University Of Akron Press. pp. 57–76. ISBN 9781935603689. Kaiser, Anelis; Dussauge, Isabelle (2014). "Re-queering the brain". In Bluhm, Robyn; Japp Jacobson, Anne; Maibom, Heidi Lene (eds.). Neurofeminism: issues at the intersection of feminist theory and cognitive science. Hampshire New York: Palgrave Macmillan. pp. 121–144. ISBN 9781349333929. Kraus, Cynthia (2016), "What is the feminist critique of neuroscience? A call for dissensus studies", in de Vos, Jan; Pluth, Ed, eds. (2016). Neuroscience and critique: exploring the limits of the neurological turn. London New York: Routledge. pp. 100–116. ISBN 9781138887350. Journal articles Hyde, Janet Shibley (September 2005). "The gender similarities hypothesis". American Psychologist. 60 (6): 581–592. CiteSeerX 10.1.1.374.1723. doi:10.1037/0003-066X.60.6.581. PMID 16173891. Nash, Alison; Grossi, Giordana (2007). "Picking Barbie™'s brain: inherent sex differences in scientific ability?". Journal of Interdisciplinary Feminist Thought. 2 (1): 5. Pdf. Schmitz, Sigrid; Höppner, Grit (25 July 2014). "Neurofeminism and feminist neurosciences: a critical review of contemporary brain research". Frontiers in Human Neuroscience. 8 (article 546): 546. doi:10.3389/fnhum.2014.00546. PMC 4111126. PMID 25120450. Roy, Deboleena (Spring 2016). "Neuroscience and feminist theory: a new directions essay". Signs. 41 (3): 531–552. doi:10.1086/684266. S2CID 146995854. Opposing publications Below is a list of works which cause the network concern due to their "neurodeterminist notions of a ‘sexed brain’ [which] are being transported into public discourse [..] without reflecting the biases in empirical work." Pease, Allan; Pease, Barbara (2001). Why men don't listen & women can't read maps. Sydney, NSW: Pease International. ISBN 9780957810815. Cahill, Larry (October 1, 2012). "His brain, her brain". Scientific American. 292 (5): 22–29. PMID 15882020. Cahill, Larry (January–February 2017). "An issue whose time has come (editorial)". Journal of Neuroscience Research. 95 (1–2): 12–13. doi:10.1002/jnr.23972. PMID 27870429. Brizendine, Louann (2009). The female brain. London: Bantham. ISBN 9781407039510. Brizendine, Louann (2011). The male brain. Edinburgh: Harmony. ISBN 9780767927543. Gray, John (2004). Men are from Mars, women are from Venus: the classic guide to understanding the opposite sex. New York: HarperCollins Publishers. ISBN 9780060574215. See also Feminist movements and ideologies Gender essentialism Heteronormativity Neuroscience of sex differences Neurogender Neuroqueer theory References External links Official website Sociocultural evolution, sociocultural evolutionism or social evolution are theories of sociobiology and cultural evolution that describe how societies and culture change over time. Whereas sociocultural development traces processes that tend to increase the complexity of a society or culture, sociocultural evolution also considers process that can lead to decreases in complexity (degeneration) or that can produce variation or proliferation without any seemingly significant changes in complexity (cladogenesis). Sociocultural evolution is "the process by which structural reorganization is affected through time, eventually producing a form or structure that is qualitatively different from the ancestral form". Most of the 19th-century and some 20th-century approaches to socioculture aimed to provide models for the evolution of humankind as a whole, arguing that different societies have reached different stages of social development. The most comprehensive attempt to develop a general theory of social evolution centering on the development of sociocultural systems, the work of Talcott Parsons (1902–1979), operated on a scale which included a theory of world history. Another attempt, on a less systematic scale, originated from the 1970s with the world-systems approach of Immanuel Wallerstein (1930–2019) and his followers. More recent approaches focus on changes specific to individual societies and reject the idea that cultures differ primarily according to how far each one has moved along some presumed linear scale of social progress. Most modern archaeologists and cultural anthropologists work within the frameworks of neoevolutionism, sociobiology, and modernization theory. Introduction Anthropologists and sociologists often assume that human beings have natural social tendencies but that particular human social behaviours have non-genetic causes and dynamics (i.e. people learn them in a social environment and through social interaction). Societies exist in complex social environments (for example: with differing natural resources and constraints) and adapt themselves to these environments. It is thus inevitable that all societies change. Specific theories of social or cultural evolution often attempt to explain differences between coeval societies by positing that different societies have reached different stages of development. Although such theories typically provide models for understanding the relationship between technologies, social structure or the values of a society, they vary as to the extent to which they describe specific mechanisms of variation and change. While the history of evolutionary thinking with regard to humans can be traced back at least to Aristotle and other Greek philosophers, early sociocultural-evolution theories – the ideas of Auguste Comte (1798–1857), Herbert Spencer (1820–1903) and Lewis Henry Morgan (1818–1881) – developed simultaneously with, but independently of, the work of Charles Darwin (1809–1882) and were popular from late in the 19th century to the end of World War I. The 19th-century unilineal evolution theories claimed that societies start out in a primitive state and gradually become more civilized over time; they equated the culture and technology of Western civilization with progress. Some forms of early sociocultural-evolution theories (mainly unilineal ones) have led to much-criticised theories like social Darwinism and scientific racism, sometimes used in the past by European imperial powers to justify existing policies of colonialism and slavery and to justify new policies such as eugenics. Most 19th-century and some 20th-century approaches aimed to provide models for the evolution of humankind as a single entity. However, most 20th-century approaches, such as multilineal evolution, focused on changes specific to individual societies. Moreover, they rejected directional change (i.e. orthogenetic, teleological or progressive change). Most archaeologists work within the framework of multilineal evolution. Other contemporary approaches to social change include neoevolutionism, sociobiology, dual inheritance theory, modernisation theory and postindustrial theory. In his seminal 1976 book The Selfish Gene, Richard Dawkins wrote that "there are some examples of cultural evolution in birds and monkeys, but ... it is our own species that really shows what cultural evolution can do". Stadial theory Enlightenment and later thinkers often speculated that societies progressed through stages: in other words, they saw history as stadial. While expecting humankind to show increasing development, theorists looked for what determined the course of human history. Georg Wilhelm Friedrich Hegel (1770–1831), for example, saw social development as an inevitable process. It was assumed that societies start out primitive, perhaps in a state of nature, and could progress toward something resembling industrial Europe. While earlier authors such as Michel de Montaigne (1533–1592) had discussed how societies change through time, the Scottish Enlightenment of the 18th century proved key in the development of the idea of sociocultural evolution. In relation to Scotland's union with England in 1707, several Scottish thinkers pondered the relationship between progress and the affluence brought about by increased trade with England. They understood the changes Scotland was undergoing as involving transition from an agricultural to a mercantile society. In "conjectural histories", authors such as Adam Ferguson (1723–1816), John Millar (1735–1801) and Adam Smith (1723–1790) argued that societies all pass through a series of four stages: hunting and gathering, pastoralism and nomadism, agriculture, and finally a stage of commerce. Philosophical concepts of progress, such as that of Hegel, developed as well during this period. In France, authors such as Claude Adrien Helvétius (1715–1771) and other philosophes were influenced by the Scottish tradition. Later thinkers such as Comte de Saint-Simon (1760–1825) developed these ideas. Auguste Comte (1798–1857) in particular presented a coherent view of social progress and a new discipline to study it: sociology. These developments took place in a context of wider processes. The first process was colonialism. Although imperial powers settled most differences of opinion with their colonial subjects through force, increased awareness of non-Western peoples raised new questions for European scholars about the nature of society and of culture. Similarly, effective colonial administration required some degree of understanding of other cultures. Emerging theories of sociocultural evolution allowed Europeans to organise their new knowledge in a way that reflected and justified their increasing political and economic domination of others: such systems saw colonised people as less evolved, and colonising people as more evolved. Modern civilization (understood as the Western civilization), appeared the result of steady progress from a state of barbarism, and such a notion was common to many thinkers of the Enlightenment, including Voltaire (1694–1778). The second process was the Industrial Revolution and the rise of capitalism, which together allowed and promoted continual revolutions in the means of production. Emerging theories of sociocultural evolution reflected a belief that the changes in Europe brought by the Industrial Revolution and capitalism were improvements. Industrialisation, combined with the intense political change brought about by the French Revolution of 1789 and the U.S. Constitution, which paved the way for the dominance of democracy, forced European thinkers to reconsider some of their assumptions about how society was organised. Eventually, in the 19th century three major classical theories of social and historical change emerged: sociocultural evolutionism the social cycle theory the Marxist theory of historical materialism. These theories had a common factor: they all agreed that the history of humanity is pursuing a certain fixed path, most likely that of social progress. Thus, each past event is not only chronologically, but causally tied to present and future events. The theories postulated that by recreating the sequence of those events, sociology could discover the "laws" of history. Sociocultural evolutionism and the idea of progress While sociocultural evolutionists agree that an evolution-like process leads to social progress, classical social evolutionists have developed many different theories, known as theories of unilineal evolution. Sociocultural evolutionism became the prevailing theory of early sociocultural anthropology and social commentary through the work of scholars like Auguste Comte (1798-1857), Edward Burnett Tylor (1832-1917), Lewis Henry Morgan (1818-1881), Benjamin Kidd (1858-1916), L. T. Hobhouse (1864-1929) and Herbert Spencer (1820-1903). Models incorporating distinct stages and ideas of linear models of progress not only had a great influence on future evolutionary approaches in the social sciences and humanities, but also shaped public, scholarly, and scientific discourse surrounding the rising individualism and "population thinking". Sociocultural evolutionism attempted to formalise social thinking along scientific lines, with the added influence from the biological theory of evolution. If organisms could develop over time according to discernible, deterministic laws, then it seemed reasonable that societies could as well. Human society was compared to a biological organism, and social-science equivalents of concepts like variation, natural selection, and inheritance were introduced as factors bearing on the progress of societies. The idea of "progress" led to that of a fixed "stages" through which human societies develop, usually numbering three – savagery, barbarism, and civilization – but sometimes many more. At that time, anthropology was rising as a new scientific discipline, separating itself from the traditional views of "primitive" cultures that were usually based on religious views. Already in the 18th century, some authors began to theorize on the evolution of humans. Montesquieu (1689–1755) writes about the relationship laws have with climate in particular and with the environment in general, specifically how different climatic conditions cause certain characteristics to become common among different people. He likens the development of laws, the presence or absence of civil liberty, differences in morality, and the whole development of different cultures to the climate of the respective people, concluding that the environment determines whether and how a people farms the land, which determines the way their society is built and the way their culture is constituted, or, in Montesquieu's words, the "general spirit of a nation". Jean-Jacques Rousseau (1712–1778) presents a conjectural stage-model of human sociocultural evolution: first, humans lived solitarily and only grouped when mating or raising children. Later, men and women lived together and shared childcare, thus building families, followed by tribes as the result of inter-family interactions, which lived in "the happiest and the most lasting epoch" of human history, before the corruption of civil society degenerated the species - again in a developmental stage-process. In the late 18th century, the Marquis de Condorcet (1743–1794) lists ten stages, or "epochs", each advancing the rights of man and perfecting the human race. Erasmus Darwin (1731-1802), Charles Darwin's grandfather, was an enormously influential natural philosopher, physiologist and poet whose remarkably insightful ideas included a statement of transformism and the interconnectedness of all forms of life. His works, which are enormously wide-ranging, also advance a theory of cultural transformation: his famous The Temple of Nature is subtitled 'the Origin of Society'. This work, rather than proposing in detail a strict transformation of humanity between different stages, instead dwells on Erasmus Darwin's evolutionary mechanism: Erasmus Darwin does not explain each stage one-by-one, trusting his theory of universal organic development, as articulated in the Zoonomia, to illustrate cultural development as well. Erasmus Darwin therefore flits with abandon through his chronology: Priestman notes that it jumps from the emergence of life onto land, the development of opposable thumbs, and the origin of sexual reproduction directly to modern historical events. Another more complex theorist was Richard Payne Knight (1751-1824), an influential amateur archeologist and universal theologian. Knight's The Progress of Civil Society: A Didactic Poem in Six Books (1796) fits precisely into the tradition of triumphant historical stages, beginning with Lucretius and reaching Adam Smith––but just for the first four books. In his final books, Knight then grapples with the French revolution and wealthy decadence. Confronted with these twin issues, Knight's theory ascribes progress to conflict: 'partial discord lends its aid, to tie the complex knots of general harmony'. Competition in Knight's mechanism spurs development from any one stage to the next: the dialectic of class, land and gender creates growth. Thus, Knight conceptualised a theory of history founded in inevitable racial conflict, with Greece representing 'freedom' and Egypt 'cold inactive stupor'. Buffon, Linnaeus, Camper and Monboddo variously forward diverse arguments about racial hierarchy, grounded in early theories of species change––though many thought that environmental changes could create dramatic changes in form without permanently altering the species or causing species transformation. However, their arguments still bear on race: Rousseau, Buffon and Monboddo cite orangutans as evidence of an earlier prelinguistic human type, and Monboddo even insisted Orangutans and certain African and South Asian races were identical. Other than Erasmus Darwin, the other pre-eminent scientific text with a theory of cultural transformation was advanced by Robert Chambers (1802-1871). Chambers was a Scottish evolutionary thinker and philosopher who, though he was then and now perceived as scientifically inadequate and criticized by prominent contemporaries, is important because he was so widely read. There are records of everyone from Queen Victoria to individual dockworkers enjoying his Robert Chambers' Vestiges of the Natural History of Creation (1844), including future generations of scientists. That The Vestiges did not establish itself as the scientific cutting edge is precisely the point, since the Vestiges's influence means it was both the concept of evolution the Victorian public was most likely to experience, and the scientific presupposition laid earliest in the minds of bright young scholars. Chambers propounded a 'principle of development' whereby everything evolved by the same mechanism and towards higher order structure or meaning. In his theory, life advanced through different 'classes', and within each class animals began at the lowest form and then advanced to more complex forms in the same class. In short, the progress of animals was like the development of a foetus. More than just an indistinct analogy, this parallel between embryology and species development had the status of a genuine causal mechanism in Chambers' theory: more advanced species developed longer as embryos into all their complexity. Motivated by this comparison, Chambers ascribed development to the 'laws of creation', though he also supposed that the whole development of species was in some way preordained: it was just that the preordination of the creator acted through establishing those laws. This, as discussed above, is similar to Spencer's later concept of development. Thus Chambers believed in a sophisticated theory of progress driven by a developmental analogy. In the mid-19th century, a "revolution in ideas about the antiquity of the human species" took place "which paralleled, but was to some extent independent of, the Darwinian revolution in biology." Especially in geology, archaeology, and anthropology, scholars began to compare "primitive" cultures to past societies and "saw their level of technology as parallel with that of Stone Age cultures, and thus used these peoples as models for the early stages of human evolution". A developmental model of the evolution of the mind, of culture, and of society was the result, paralleling the evolution of the human species: "Modern savages [sic] became, in effect, living fossils left behind by the march of progress, relics of the Paleolithic still lingering on into the present." Classical social evolutionism is most closely associated with the 19th-century writings of Auguste Comte and of Herbert Spencer (coiner of the phrase "survival of the fittest"). In many ways, Spencer's theory of "cosmic evolution" has much more in common with the works of Jean-Baptiste Lamarck (1744-1829) and of Auguste Comte than with contemporary works of Charles Darwin. Spencer also developed and published his theories several years earlier than Darwin. In regard to social institutions, however, there is a good case that Spencer's writings might be classified as discussing social evolutionism. Although he wrote that societies over time progressed – and that progress was accomplished through competition – he stressed that the individual rather than the collectivity is the unit of analysis that evolves; that, in other words, evolution takes place through natural selection and that it affects social as well as biological phenomenon. Nonetheless, the publication of Darwin's works proved a boon to the proponents of sociocultural evolution, who saw the ideas of biological evolution as an attractive explanation for many questions about the development of society. Both Spencer and Comte view society as a kind of organism subject to the process of growth—from simplicity to complexity, from chaos to order, from generalisation to specialisation, from flexibility to organisation. They agree that the process of societal growth can be divided into certain stages, have their beginning and eventual end, and that this growth is in fact social progress: each newer, more-evolved society is "better". Thus progressivism became one of the basic ideas underlying the theory of sociocultural evolutionism. However, Spencer's theories were more complex than just a romp up the great chain of being. Spencer based his arguments on an analogy between the evolution of societies and the ontogeny of an animal. Accordingly, he searched for "general principles of development and structure" or "fundamental principles of organization", rather than being content simply ascribing progress between social stages to the direct intervention of some beneficent deity. Moreover, he accepted that these conditions are "far less specific, far more modifiable, far more dependent on conditions that are variable": in short, that they are a messy biological process. Though Spencer's theories transcended the label of 'stagism' and appreciate biological complexity, they still accepted a strongly fixed direction and morality to natural development. For Spencer, interference with the natural process of evolution was dangerous and had to be avoided at all costs. Such views were naturally coupled to the pressing political and economic questions of the time. Spencer clearly thought society's evolution brought about a racial hierarchy with Caucasians at the top and Africans at the bottom. This notion is deeply linked to the colonial projects European powers were pursuing at the time, and the idea of European superiority used paternalistically to justify those projects. The influential German zoologist Ernst Haeckel even wrote that 'natural men are closer to the higher vertebrates than highly civilized Europeans', including not just a racial hierarchy but a civilizational one. Likewise, Spencer's evolutionary argument advanced a theory of statehood: "until spontaneously fulfilled a public want should not be fulfilled at all" sums up Spencer's notion about limited government and the free operation of market forces. This is not to suggest that stagism was useless or entirely motivated by colonialism and racism. Stagist theories were first proposed in contexts where competing epistemologies were largely static views of the world. Hence "progress" had in some sense to be invented, conceptually: the idea that human society would move through stages was a triumphant invention. Moreover, stages were not always static entities. In Buffon's theories, for example, it was possible to regress between stages, and physiological changes were species' reversibly adapting to their environment rather than irreversibly transforming. In addition to progressivism, economic analyses influenced classical social evolutionism. Adam Smith (1723–1790), who held a deeply evolutionary view of human society, identified the growth of freedom as the driving force in a process of stadial societal development. According to him, all societies pass successively through four stages: the earliest humans lived as hunter-gatherers, followed by pastoralists and nomads, after which society evolved to agriculturalists and ultimately reached the stage of commerce. With the strong emphasis on specialisation and the increased profits stemming from a division of labour, Smith's thinking also exerted some direct influence on Darwin himself. Both in Darwin's theory of the evolution of species and in Smith's accounts of political economy, competition between selfishly functioning units plays an important and even dominating rôle. Similarly occupied with economic concerns as Smith, Thomas R. Malthus (1766–1834) warned that given the strength of the sex drive inherent in all animals, Malthus argued, populations tend to grow geometrically, and population growth is only checked by the limitations of economic growth, which, if there would be growth at all, would quickly be outstripped by population growth, causing hunger, poverty, and misery. Far from being the consequences of economic structures or social orders, this "struggle for existence" is an inevitable natural law, so Malthus. Auguste Comte, known as "the father of sociology", formulated the law of three stages: human development progresses from the theological stage, in which nature was mythically conceived and man sought the explanation of natural phenomena from supernatural beings; through a metaphysical stage in which nature was conceived of as a result of obscure forces and man sought the explanation of natural phenomena from them; until the final positive stage in which all abstract and obscure forces are discarded, and natural phenomena are explained by their constant relationship. This progress is forced through the development of human mind, and through increasing application of thought, reasoning and logic to the understanding of the world. Comte saw the science-valuing society as the highest, most developed type of human organization. Herbert Spencer, who argued against government intervention as he believed that society should evolve toward more individual freedom, followed Lamarck in his evolutionary thinking, in that he believed that humans do over time adapt to their surroundings. He differentiated between two phases of development as regards societies' internal regulation: the "military" and "industrial" societies. The earlier (and more primitive) military society has the goal of conquest and defense, is centralised, economically self-sufficient, collectivistic, puts the good of a group over the good of an individual, uses compulsion, force and repression, and rewards loyalty, obedience and discipline. The industrial society, in contrast, has a goal of production and trade, is decentralised, interconnected with other societies via economic relations, works through voluntary cooperation and individual self-restraint, treats the good of individual as of the highest value, regulates the social life via voluntary relations; and values initiative, independence and innovation. The transition process from the military to industrial society is the outcome of steady evolutionary processes within the society. Spencer "imagined a kind of feedback loop between mental and social evolution: the higher the mental powers the greater the complexity of the society that the individuals could create; the more complex the society, the greater the stimulus it provided for further mental development. Everything cohered to make progress inevitable or to weed out those who did not keep up." Regardless of how scholars of Spencer interpret his relation to Darwin, Spencer became an incredibly popular figure in the 1870s, particularly in the United States. Authors such as Edward L. Youmans, William Graham Sumner, John Fiske, John W. Burgess, Lester Frank Ward, Lewis H. Morgan (1818–1881) and other thinkers of the gilded age all developed theories of social evolutionism as a result of their exposure to Spencer as well as to Darwin. In his 1877 classic Ancient Societies, Lewis H. Morgan, an anthropologist whose ideas have had much impact on sociology, differentiated between three eras: savagery, barbarism and civilization, which are divided by technological inventions, like fire, bow, pottery in the savage era, domestication of animals, agriculture, metalworking in the barbarian era and alphabet and writing in the civilization era. Thus Morgan drew a link between social progress and technological progress. Morgan viewed technological progress as a force behind social progress, and held that any social change—in social institutions, organizations or ideologies—has its beginnings in technological change. Morgan's theories were popularized by Friedrich Engels, who based his famous work The Origin of the Family, Private Property and the State on them. For Engels and other Marxists this theory was important, as it supported their conviction that materialistic factors—economic and technological—are decisive in shaping the fate of humanity. Edward Burnett Tylor (1832–1917), a pioneer of anthropology, focused on the evolution of culture worldwide, noting that culture is an important part of every society and that it is also subject to a process of evolution. He believed that societies were at different stages of cultural development and that the purpose of anthropology was to reconstruct the evolution of culture, from primitive beginnings to the modern state. Anthropologists Sir E.B. Tylor in England and Lewis Henry Morgan in the United States worked with data from indigenous people, who (they claimed) represented earlier stages of cultural evolution that gave insight into the process and progression of evolution of culture. Morgan had a significant influence on Karl Marx and on Friedrich Engels, who developed a theory of sociocultural evolution in which the internal contradictions in society generated a series of escalating stages that ended in a socialist society (see Marxism). Tylor and Morgan elaborated the theory of unilinear evolution, specifying criteria for categorising cultures according to their standing within a fixed system of growth of humanity as a whole and examining the modes and mechanisms of this growth. Theirs was often a concern with culture in general, not with individual cultures. Their analysis of cross-cultural data was based on three assumptions: contemporary societies may be classified and ranked as more "primitive" or more "civilized" there are a determinate number of stages between "primitive" and "civilized" (e.g. band, tribe, chiefdom, and state) all societies progress through these stages in the same sequence, but at different rates Theorists usually measured progression (that is, the difference between one stage and the next) in terms of increasing social complexity (including class differentiation and a complex division of labour), or an increase in intellectual, theological, and aesthetic sophistication. These 19th-century ethnologists used these principles primarily to explain differences in religious beliefs and kinship terminologies among various societies. Lester Frank Ward (1841–1913), sometimes referred to as the "father" of American sociology, rejected many of Spencer's theories regarding the evolution of societies. Ward, who was also a botanist and a paleontologist, believed that the law of evolution functioned much differently in human societies than it did in the plant and animal kingdoms, and theorized that the "law of nature" had been superseded by the "law of the mind". He stressed that humans, driven by emotions, create goals for themselves and strive to realize them (most effectively with the modern scientific method) whereas there is no such intelligence and awareness guiding the non-human world. Plants and animals adapt to nature; man shapes nature. While Spencer believed that competition and "survival of the fittest" benefited human society and sociocultural evolution, Ward regarded competition as a destructive force, pointing out that all human institutions, traditions and laws were tools invented by the mind of man and that that mind designed them, like all tools, to "meet and checkmate" the unrestrained competition of natural forces. Ward agreed with Spencer that authoritarian governments repress the talents of the individual, but he believed that modern democratic societies, which minimized the role of religion and maximized that of science, could effectively support the individual in his or her attempt to fully utilize their talents and achieve happiness. He believed that the evolutionary processes have four stages: First comes cosmogenesis, creation and evolution of the world. Then, when life arises, there is biogenesis. Development of humanity leads to anthropogenesis, which is influenced by the human mind. Finally there arrives sociogenesis, which is the science of shaping the evolutionary process itself to optimize progress, human happiness and individual self-actualization. Ward regarded modern societies as superior to "primitive" societies (one need only look to the impact of medical science on health and lifespan) and shared theories of white supremacy. Though he supported the Out-of-Africa theory of human evolution, he did not believe that all races and social classes were equal in talent. When a Negro rapes a white woman, Ward declared, he is impelled not only by lust but also by the instinctive drive to improve his own race. Ward did not think that evolutionary progress was inevitable and he feared the degeneration of societies and cultures, which he saw as very evident in the historical record. Ward also did not favor the radical reshaping of society as proposed by the supporters of the eugenics movement or by the followers of Karl Marx; like Comte, Ward believed that sociology was the most complex of the sciences and that true sociogenesis was impossible without considerable research and experimentation. Émile Durkheim, another of the "fathers" of sociology, developed a dichotomal view of social progress. His key concept was social solidarity, as he defined social evolution in terms of progressing from mechanical solidarity to organic solidarity. In mechanical solidarity, people are self-sufficient, there is little integration and thus there is the need for the use of force and repression to keep society together. In organic solidarity, people are much more integrated and interdependent and specialisation and cooperation are extensive. Progress from mechanical to organic solidarity is based firstly on population growth and increasing population density, secondly on increasing "morality density" (development of more complex social interactions) and thirdly on increasing specialisation in the workplace. To Durkheim, the most important factor in social progress is the division of labour. This was later used in the mid-1900s by the economist Ester Boserup (1910–1999) to attempt to discount some aspects of Malthusian theory. Ferdinand Tönnies (1855–1936) describes evolution as the development from informal society (where people have many liberties and there are few laws and obligations) to modern, formal rational society (dominated by traditions and laws, where people are restricted from acting as they wish). He also notes that there is a tendency to standardisation and unification when smaller societies are absorbed into a single, large, modern society. Thus Tönnies can be said to describe part of the process known today as globalization. Tönnies was also one of the first sociologists to claim that the evolution of society is not necessarily going in the right direction, that social progress is not perfect, and it can even be called a regression as the newer, more evolved societies are obtained only after paying a high cost, resulting in decreasing satisfaction of the individuals making up that society. Tönnies' work became the foundation of neoevolutionism. Although Max Weber is not usually counted as a sociocultural evolutionist, his theory of tripartite classification of authority can be viewed as an evolutionary theory as well. Weber distinguishes three ideal types of political leadership, domination and authority: charismatic domination traditional domination (patriarchs, patrimonialism, feudalism) legal (rational) domination (modern law and state, bureaucracy) Weber also notes that legal domination is the most advanced, and that societies evolve from having mostly traditional and charismatic authorities to mostly rational and legal ones. Critique and impact on modern theories The early 20th-century inaugurated a period of systematic critical examination, and rejection of the sweeping generalisations of the unilineal theories of sociocultural evolution. Cultural anthropologists such as Franz Boas (1858–1942), along with his students, including Ruth Benedict and Margaret Mead, are regarded as the leaders of anthropology's rejection of classical social evolutionism. However, the school of Boas ignore some of the complexity in evolutionary theories that emerged outside Herbert Spencer's influence. Charles Darwin's On the Origin of Species gave a mechanistic account of the origins and development of animals, quite apart from Spencer's theories that emphasized the inevitable human development through stages. Consequently, many scholars developed more sophisticated understandings of how cultures evolve, relying on deep cultural analogies, than the theories in Herbert Spencer's tradition. Walter Bagehot (1872) applied selection and inheritance to the development of human political institutions. Samuel Alexander (1892) discusses the natural selection of moral principles in society. William James (1880) considered the 'natural selection' of ideas in learning and scientific development. In fact, he identified a 'remarkable parallel […] between the facts of social evolution on the one hand, and of zoological evolution as expounded by Mr Darwin on the other'. Charles Sanders Peirce (1898) even proposed that the current laws of nature we have exist because they have evolved over time. Darwin himself, in Chapter 5 of the Descent of Man, proposed that human moral sentiments were subject to group selection: "A tribe including many members who, from possessing in a high degree the spirit of patriotism, fidelity, obedience, courage, and sympathy, were always ready to aid one another, and to sacrifice themselves for the common good, would be victorious over most other tribes; and this would be natural selection." Through the mechanism of imitation, cultures as well as individuals could be subject to natural selection. While these theories involved evolution applied to social questions, except for Darwin's group selection the theories reviewed above did not advance a precise understanding of how Darwin's mechanism extended and applied to cultures beyond a vague appeal to competition. Ritchie's Darwinism and Politics (1889) breaks this trend, holding that "language and social institutions make it possible to transmit experience quite independently of the continuity of race." Hence Ritchie saw cultural evolution as a process that could operate independently of and on different scales to the evolution of species, and gave it precise underpinnings: he was 'extending its range', in his own words, to ideas, cultures and institutions. Thorstein Veblen, around the same time, came to a similar insight: that humans evolve to their social environment, but their social environment in turn also evolves. Veblen's mechanism for human progress was the evolution of human intentionality: Veblen labelled men 'a creature of habit' and thought that habits were 'mentally digested' from those who influenced him. In short, as Hodgson and Knudsen point out, Veblen thinks: "the changing institutions in their turn make for a further selection of individuals endowed with the fittest temperament, and a further adaptation of individual temperament and habits to the changing environment through the formation of new institutions." Thus, Veblen represented an extension of Ritchie's theories, where evolution operates at multiple levels, to a sophisticated appreciation of how each level interacts with the other. This complexity notwithstanding, Boas and Benedict used sophisticated ethnography and more rigorous empirical methods to argue that Spencer, Tylor, and Morgan's theories were speculative and systematically misrepresented ethnographic data. Theories regarding "stages" of evolution were especially criticised as illusions. Additionally, they rejected the distinction between "primitive" and "civilized" (or "modern"), pointing out that so-called primitive contemporary societies have just as much history, and were just as evolved, as so-called civilized societies. They therefore argued that any attempt to use this theory to reconstruct the histories of non-literate (i.e. leaving no historical documents) peoples is entirely speculative and unscientific. They observed that the postulated progression, which typically ended with a stage of civilization identical to that of modern Europe, is ethnocentric. They also pointed out that the theory assumes that societies are clearly bounded and distinct, when in fact cultural traits and forms often cross social boundaries and diffuse among many different societies (and are thus an important mechanism of change). Boas in his culture-history approach focused on anthropological fieldwork in an attempt to identify factual processes instead of what he criticized as speculative stages of growth. His approach greatly influenced American anthropology in the first half of the 20th century, and marked a retreat from high-level generalization and from "systems building". Later critics observed that the assumption of firmly bounded societies was proposed precisely at the time when European powers were colonising non-Western societies, and was thus self-serving. Many anthropologists and social theorists now consider unilineal cultural and social evolution a Western myth seldom based on solid empirical grounds. Critical theorists argue that notions of social evolution are simply justifications for power by the élites of society. Finally, the devastating World Wars that occurred between 1914 and 1945 crippled Europe's self-confidence. After millions of deaths, genocide, and the destruction of Europe's industrial infrastructure, the idea of progress seemed dubious at best. Thus modern sociocultural evolutionism rejects most of classical social evolutionism due to various theoretical problems: The theory was deeply ethnocentric—it makes heavy value judgments about different societies, with Western civilization seen as the most valuable. It assumed all cultures follow the same path or progression and have the same goals. It equated civilization with material culture (technology, cities, etc.) Because social evolution was posited as a scientific theory, it was often used to support unjust and often racist social practices – particularly colonialism, slavery, and the unequal economic conditions present within industrialized Europe. Social Darwinism is especially criticised, as it purportedly led to some philosophies used by the Nazis. Max Weber, disenchantment, and critical theory Weber's major works in economic sociology and the sociology of religion dealt with the rationalization, secularisation, and so called "disenchantment" which he associated with the rise of capitalism and modernity. In sociology, rationalization is the process whereby an increasing number of social actions become based on considerations of teleological efficiency or calculation rather than on motivations derived from morality, emotion, custom, or tradition. Rather than referring to what is genuinely "rational" or "logical", rationalization refers to a relentless quest for goals that might actually function to the detriment of a society. Rationalization is an ambivalent aspect of modernity, manifested especially in Western society – as a behaviour of the capitalist market, of rational administration in the state and bureaucracy, of the extension of modern science, and of the expansion of modern technology. Weber's thought regarding the rationalizing and secularizing tendencies of modern Western society (sometimes described as the "Weber Thesis") would blend with Marxism to facilitate critical theory, particularly in the work of thinkers such as Jürgen Habermas (born 1929). Critical theorists, as antipositivists, are critical of the idea of a hierarchy of sciences or societies, particularly with respect to the sociological positivism originally set forth by Comte. Jürgen Habermas has critiqued the concept of pure instrumental rationality as meaning that scientific-thinking becomes something akin to ideology itself. For theorists such as Zygmunt Bauman (1925–2017), rationalization as a manifestation of modernity may be most closely and regrettably associated with the events of the Holocaust. Modern theories When the critique of classical social evolutionism became widely accepted, modern anthropological and sociological approaches changed respectively. Modern theories are careful to avoid unsourced, ethnocentric speculation, comparisons, or value judgments; more or less regarding individual societies as existing within their own historical contexts. These conditions provided the context for new theories such as cultural relativism and multilineal evolution. In the 1920s and 1930s, Gordon Childe revolutionized the study of cultural evolutionism. He conducted a comprehensive pre-history account that provided scholars with evidence for African and Asian cultural transmission into Europe. He combated scientific racism by finding the tools and artifacts of the indigenous people from Africa and Asia and showed how they influenced the technology of European culture. Evidence from his excavations countered the idea of Aryan supremacy and superiority. Adopting "Kosinna's basic concept of the archaeological culture and his identification of such cultures as the remains of prehistoric peoples" and combining it with the detailed chronologies of European prehistory developed by Gustaf Oscar Montelius, Childe argued that each society needed to be delineated individually on the basis of constituent artefacts which were indicative of their practical and social function. Childe explained cultural evolution by his theory of divergence with modifications of convergence. He postulated that different cultures form separate methods that meet different needs, but when two cultures were in contact they developed similar adaptations, solving similar problems. Rejecting Spencer's theory of parallel cultural evolution, Childe found that interactions between cultures contributed to the convergence of similar aspects most often attributed to one culture. Childe placed emphasis on human culture as a social construct rather than products of environmental or technological contexts. Childe coined the terms "Neolithic Revolution", and "Urban Revolution" which are still used today in the branch of pre-historic anthropology. In 1941 anthropologist Robert Redfield wrote about a shift from 'folk society' to 'urban society'. By the 1940s cultural anthropologists such as Leslie White and Julian Steward sought to revive an evolutionary model on a more scientific basis, and succeeded in establishing an approach known as neoevolutionism. White rejected the opposition between "primitive" and "modern" societies but did argue that societies could be distinguished based on the amount of energy they harnessed, and that increased energy allowed for greater social differentiation (White's law). Steward on the other hand rejected the 19th-century notion of progress, and instead called attention to the Darwinian notion of "adaptation", arguing that all societies had to adapt to their environment in some way. The anthropologists Marshall Sahlins and Elman Service prepared an edited volume, Evolution and Culture, in which they attempted to synthesise White's and Steward's approaches. Other anthropologists, building on or responding to work by White and Steward, developed theories of cultural ecology and ecological anthropology. The most prominent examples are Peter Vayda and Roy Rappaport. By the late 1950s, students of Steward such as Eric Wolf and Sidney Mintz turned away from cultural ecology to Marxism, world systems theory, dependency theory and Marvin Harris's cultural materialism. Today most anthropologists reject 19th-century notions of progress and the three assumptions of unilineal evolution. Following Steward, they take seriously the relationship between a culture and its environment to explain different aspects of a culture. But most modern cultural anthropologists have adopted a general systems approach, examining cultures as emergent systems and arguing that one must consider the whole social environment, which includes political and economic relations among cultures. As a result of simplistic notions of "progressive evolution", more modern, complex cultural evolution theories (such as dual inheritance theory, discussed below) receive little attention in the social sciences, having given way in some cases to a series of more humanist approaches. Some reject the entirety of evolutionary thinking and look instead at historical contingencies, contacts with other cultures, and the operation of cultural symbol systems. In the area of development studies, authors such as Amartya Sen have developed an understanding of 'development' and 'human flourishing' that also question more simplistic notions of progress, while retaining much of their original inspiration. Neoevolutionism Neoevolutionism was the first in a series of modern multilineal evolution theories. It emerged in the 1930s and extensively developed in the period following the Second World War and was incorporated into both anthropology and sociology in the 1960s. It bases its theories on empirical evidence from areas of archaeology, palaeontology, and historiography and tries to eliminate any references to systems of values, be it moral or cultural, instead trying to remain objective and simply descriptive. While 19th-century evolutionism explained how culture develops by giving general principles of its evolutionary process, it was dismissed by the Historical Particularists as unscientific in the early 20th century. It was the neo-evolutionary thinkers who brought back evolutionary thought and developed it to be acceptable to contemporary anthropology. Neo-evolutionism discards many ideas of classical social evolutionism, namely that of social progress, so dominant in previous sociology evolution-related theories. Then neo-evolutionism discards the determinism argument and introduces probability, arguing that accidents and free will greatly affect the process of social evolution. It also supports counterfactual history—asking "what if" and considering different possible paths that social evolution may take or might have taken, and thus allows for the fact that various cultures may develop in different ways, some skipping entire stages others have passed through. Neo-evolutionism stresses the importance of empirical evidence. While 19th-century evolutionism used value judgments and assumptions for interpreting data, neo-evolutionism relies on measurable information for analysing the process of sociocultural evolution. Leslie White, author of The Evolution of Culture: The Development of Civilization to the Fall of Rome (1959), attempted to create a theory explaining the entire history of humanity. The most important factor in his theory is technology. Social systems are determined by technological systems, wrote White in his book, echoing the earlier theory of Lewis Henry Morgan. He proposes a society's energy consumption as a measure of its advancement. He differentiates between five stages of human development. In the first, people use the energy of their own muscles. In the second, they use the energy of domesticated animals. In the third, they use the energy of plants (so White refers to agricultural revolution here). In the fourth, they learn to use the energy of natural resources: coal, oil, gas. In the fifth, they harness nuclear energy. White introduced a formula, P=E·T, where E is a measure of energy consumed, and T is the measure of efficiency of technical factors utilising the energy. This theory is similar to Russian astronomer Nikolai Kardashev's later theory of the Kardashev scale. Julian Steward, author of Theory of Culture Change: The Methodology of Multilinear Evolution (1955, reprinted 1979), created the theory of "multilinear" evolution which examined the way in which societies adapted to their environment. This approach was more nuanced than White's theory of "unilinear evolution." Steward rejected the 19th-century notion of progress, and instead called attention to the Darwinian notion of "adaptation", arguing that all societies had to adapt to their environment in some way. He argued that different adaptations could be studied through the examination of the specific resources a society exploited, the technology the society relied on to exploit these resources, and the organization of human labour. He further argued that different environments and technologies would require different kinds of adaptations, and that as the resource base or technology changed, so too would a culture. In other words, cultures do not change according to some inner logic, but rather in terms of a changing relationship with a changing environment. Cultures therefore would not pass through the same stages in the same order as they changed—rather, they would change in varying ways and directions. He called his theory "multilineal evolution". He questioned the possibility of creating a social theory encompassing the entire evolution of humanity; however, he argued that anthropologists are not limited to describing specific existing cultures. He believed that it is possible to create theories analysing typical common culture, representative of specific eras or regions. As the decisive factors determining the development of given culture he pointed to technology and economics, but noted that there are secondary factors, like political system, ideologies and religion. All those factors push the evolution of a given society in several directions at the same time; hence the application of the term "multilinear" to his theory of evolution. Marshall Sahlins, co-editor with Elman Service of Evolution and Culture (1960), divided the evolution of societies into 'general' and 'specific'. General evolution is the tendency of cultural and social systems to increase in complexity, organization and adaptiveness to environment. However, as the various cultures are not isolated, there is interaction and a diffusion of their qualities (like technological inventions). This leads cultures to develop in different ways (specific evolution), as various elements are introduced to them in different combinations and at different stages of evolution. In his Power and Prestige (1966) and Human Societies: An Introduction to Macrosociology (1974), Gerhard Lenski expands on the works of Leslie White and Lewis Henry Morgan, developing the ecological-evolutionary theory. He views technological progress as the most basic factor in the evolution of societies and cultures. Unlike White, who defined technology as the ability to create and utilise energy, Lenski focuses on information—its amount and uses. The more information and knowledge (especially allowing the shaping of natural environment) a given society has, the more advanced it is. He distinguishes four stages of human development, based on advances in the history of communication. In the first stage, information is passed by genes. In the second, when humans gain sentience, they can learn and pass information through by experience. In the third, humans start using signs and develop logic. In the fourth, they can create symbols and develop language and writing. Advancements in the technology of communication translate into advancements in the economic system and political system, distribution of goods, social inequality and other spheres of social life. He also differentiates societies based on their level of technology, communication and economy: (1) hunters and gatherers, (2) agricultural, (3) industrial, and (4) special (like fishing societies). Talcott Parsons, author of Societies: Evolutionary and Comparative Perspectives (1966) and The System of Modern Societies (1971) divided evolution into four subprocesses: (1) division, which creates functional subsystems from the main system; (2) adaptation, where those systems evolve into more efficient versions; (3) inclusion of elements previously excluded from the given systems; and (4) generalization of values, increasing the legitimization of the ever more complex system. He shows those processes on 4 stages of evolution: (I) primitive or foraging, (II) archaic agricultural, (III) classical or "historic" in his terminology, using formalized and universalizing theories about reality and (IV) modern empirical cultures. However, these divisions in Parsons' theory are the more formal ways in which the evolutionary process is conceptualized, and should not be mistaken for Parsons' actual theory. Parsons develops a theory where he tries to reveal the complexity of the processes which take form between two points of necessity, the first being the cultural "necessity," which is given through the values-system of each evolving community; the other is the environmental necessities, which most directly is reflected in the material realities of the basic production system and in the relative capacity of each industrial-economical level at each window of time. Generally, Parsons highlights that the dynamics and directions of these processes is shaped by the cultural imperative embodied in the cultural heritage, and more secondarily, an outcome of sheer "economic" conditions. Michel Foucault's recent, and very much misunderstood, concepts such as biopower, biopolitics and power-knowledge has been cited as breaking free from the traditional conception of man as cultural animal. Foucault regards both the terms "cultural animal" and "human nature"as misleading abstractions, leading to a non-critical exemption of man and anything can be justified when regarding social processes or natural phenomena (social phenomena). Foucault argues these complex processes are interrelated, and difficult to study for a reason so those 'truths' cannot be topled or disrupted. For Foucault, the many modern concepts and practices that attempt to uncover "the truth" about human beings (either psychologically, sexually, religion or spiritually) actually create the very types of people they purport to discover. Requiring trained "specialists" and knowledge codes and know how, rigorous pursuit is "put off" or delayed which makes any kind of study not only a 'taboo' subject but deliberately ignored. He cites the concept of 'truth' within many human cultures and the ever flowing dynamics between truth, power, and knowledge as a resultant complex dynamics (Foucault uses the term regimes of truth) and how they flow with ease like water which make the concept of 'truth' impervious to any further rational investigation. Some of the West's most powerful social institutions are powerful for a reason, not because they exhibit powerful structures which inhibit investigation or it is illegal to investigate there historical foundation. It is the very notion of "legitimacy" Foucault cites as examples of "truth" which function as a "Foundationalism" claims to historical accuracy. Foucault argues, systems such as medicine, prisons, and religion, as well as groundbreaking works on more abstract theoretical issues of power are suspended or buried into oblivion. He cites as further examples the 'Scientific study' of population biology and population genetics as both examples of this kind of "Biopower" over the vast majority of the human population giving the new founded political population their 'politics' or polity. With the advent of biology and genetics teamed together as new scientific innovations notions of study of knowledge regarding truth belong to the realm of experts who will never divulge their secrets openly, while the bulk of the population do not know their own biology or genetics this is done for them by the experts. This functions as a truth ignorance mechanism: "where the "subjugated knowledge's", as those that have been both written out of history and submerged in it in a masked form produces what we now know as truth. He calls them "Knowledge's from below" and a "historical knowledge of struggles". Genealogy, Foucault suggests, is a way of getting at these knowledge's and struggles; "they are about the insurrection of knowledge's." Foucault tries to show with the added dimension of "Milieu" (derived from Newtonian mechanics) how this Milieu from the 17th century with the development of the biological and physical sciences managed to be interwoven into the political, social and biological relationship of men with the arrival of the concept Work placed upon the industrial population. Foucault uses the term Umwelt, borrowed from Jakob von Uexküll, meaning environment within. Technology, production, cartography the production of nation states and government making the efficiency of the body politic, law, heredity and consanguine not only sound genuine and beyond historical origin and foundation it can be turned into 'exact truth' where the individual and the societal body are not only subjugated and nullified but dependent upon it. Foucault is not denying that genetic or biological study is inaccurate or is simply not telling the truth what he means is that notions of this newly discovered sciences were extended to include the vast majority (or whole populations) of populations as an exercise in "regimes change". Foucault argues that the conceptual meaning from the Middle Ages and Canon law period, the Geocentric model, later superseded by the Heliocentrism model placing the position of the law of right in the Middle ages (exclusive right or its correct legal term Sui generis) was the divine right of kings and absolute monarchy where the previous incarnation of truth and rule of political sovereignty was considered absolute and unquestioned by political philosophy (monarchs, popes and emperors). However, Foucault noticed that this Pharaonic version of political power was transversed and it was with 18th-century emergence of capitalism and liberal democracy that these terms began to be "democratized". The modern Pharaonic version represented by the president, the monarch, the pope and the prime minister all became propagandized versions or examples of symbol agents all aimed at towards a newly discovered phenomenon, the population. As symbolic symbol agents of power making the mass population having to sacrifice itself all in the name of the newly formed voting franchise we now call Democracy. However, this was all turned on its head (when the Medieval rulers were thrown out and replaced by a more exact apparatus now called the state) when the human sciences suddenly discovered: "The set of mechanisms through which the basic biological features of the human species became an object of a political strategy and took on board the fundamental facts that humans were now a biological species." Sociobiology Sociobiology departs perhaps the furthest from classical social evolutionism. It was introduced by Edward Wilson in his 1975 book Sociobiology: The New Synthesis and followed his adaptation of evolutionary theory to the field of social sciences. Wilson pioneered the attempt to explain the evolutionary mechanics behind social behaviours such as altruism, aggression, and nurturance. In doing so, Wilson sparked one of the greatest scientific controversies of the 20th century by introducing and rejuvenating neo-Darwinian modes of thinking in many social sciences and the humanities, leading to reactions ranging from fundamental opposition, not only from social scientists and humanists but also from Darwinists who see it as "excessively simplistic in its approach", to calls for a radical restructuring of the respective disciplines on an evolutionary basis. The current theory of evolution, the modern evolutionary synthesis (or neo-darwinism), explains that evolution of species occurs through a combination of Darwin's mechanism of natural selection and Gregor Mendel's theory of genetics as the basis for biological inheritance and mathematical population genetics. Essentially, the modern synthesis introduced the connection between two important discoveries; the units of evolution (genes) with the main mechanism of evolution (selection). Due to its close reliance on biology, sociobiology is often considered a branch of the biology, although it uses techniques from a plethora of sciences, including ethology, evolution, zoology, archaeology, population genetics, and many others. Within the study of human societies, sociobiology is closely related to the fields of human behavioral ecology and evolutionary psychology. Sociobiology has remained highly controversial as it contends genes explain specific human behaviours, although sociobiologists describe this role as a very complex and often unpredictable interaction between nature and nurture. The most notable critics of the view that genes play a direct role in human behaviour have been biologists Richard Lewontin, Steven Rose, and Stephen Jay Gould. Given the convergence of much of sociobiology's claims with right-wing politics, this approach has seen severe opposition both with regard to its research results as well as its basic tenets; this has led even Wilson himself to revisit his claims and state his opposition to some elements of modern sociobiology. Since the rise of evolutionary psychology, another school of thought, Dual Inheritance Theory, has emerged in the past 25 years that applies the mathematical standards of Population genetics to modeling the adaptive and selective principles of culture. This school of thought was pioneered by Robert Boyd at UCLA and Peter Richerson at UC Davis and expanded by William Wimsatt, among others. Boyd and Richerson's book, Culture and the Evolutionary Process (1985), was a highly mathematical description of cultural change, later published in a more accessible form in Not by Genes Alone (2004). In Boyd and Richerson's view, cultural evolution, operating on socially learned information, exists on a separate but co-evolutionary track from genetic evolution, and while the two are related, cultural evolution is more dynamic, rapid, and influential on human society than genetic evolution. Dual Inheritance Theory has the benefit of providing unifying territory for a "nature and nurture" paradigm and accounts for more accurate phenomenon in evolutionary theory applied to culture, such as randomness effects (drift), concentration dependency, "fidelity" of evolving information systems, and lateral transmission through communication. Nicholas Christakis also advances similar ideas about "evolutionary sociology" in his 2019 book, Blueprint: The Evolutionary Origins of a Good Society, emphasizing the relevance of underlying evolutionary forces that have helped to shape all societies, whatever their cultural differences. Theory of modernization Theories of modernization are closely related to the dependency theory and development theory. While they have been developed and popularized in the 1950s and 1960s, their ideological and epistemic ancestors can be traced back until at least the early 20th century when progressivist historians and social scientists, building upon Darwinian ideas that the roots of economic success in the US had to be found in its population structure, which, as an immigrant society, was composed of the strongest and fittest individuals of their respective countries of origin, had started to supply the national myth of US-American manifest destiny with evolutionary reasoning. Explicitly and implicitly, the US became the yardstick of modernisation, and other societies could be measured in the extent of their modernity by how closely they adhered to the US-American example. Modernization Theories combine the previous theories of sociocultural evolution with practical experiences and empirical research, especially those from the era of decolonization. The theory states that: Western countries are the most developed, and the rest of the world (mostly former colonies) is in the earlier stages of development, and will eventually reach the same level as the Western world. Development stages go from the traditional societies to developed ones. Third World countries have fallen behind with their social progress and need to be directed on their way to becoming more advanced. Developing from classical social evolutionism theories, the theory of modernization stresses the modernization factor: many societies are simply trying (or need) to emulate the most successful societies and cultures. It also states that it is possible to do so, thus supporting the concepts of social engineering and that the developed countries can and should help those less developed, directly or indirectly. Among the scientists who contributed much to this theory are Walt Rostow, who in his The Stages of Economic Growth: A Non-Communist Manifesto (1960) concentrates on the economic system side of the modernization, trying to show factors needed for a country to reach the path to modernization in his Rostovian take-off model. David Apter concentrated on the political system and history of democracy, researching the connection between democracy, good governance and efficiency and modernization. David McClelland (The Achieving Society, 1967) approached this subject from the psychological perspective, with his motivations theory, arguing that modernization cannot happen until given society values innovation, success and free enterprise. Alex Inkeles (Becoming Modern, 1974) similarly creates a model of modern personality, which needs to be independent, active, interested in public policies and cultural matters, open to new experiences, rational and able to create long-term plans for the future. Some works of Jürgen Habermas are also connected with this subfield. The theory of modernization has been subject to some criticism similar to that levied against classical social evolutionism, especially for being too ethnocentric, one-sided and focused on the Western world and its culture. Contemporary perspectives Political perspectives The Cold War period was marked by rivalry between two superpowers, both of which considered themselves to be the most highly evolved cultures on the planet. The USSR painted itself as a socialist society which emerged from class struggle, destined to reach the state of communism, while sociologists in the United States (such as Talcott Parsons) argued that the freedom and prosperity of the United States were a proof of a higher level of sociocultural evolution of its culture and society. At the same time, decolonization created newly independent countries who sought to become more developed—a model of progress and industrialization which was itself a form of sociocultural evolution. Technological perspectives Many argue that the next stage of sociocultural evolution consists of a merger with technology, especially information processing technology. Several cumulative major transitions of evolution have transformed life through key innovations in information storage and replication, including RNA, DNA, multicellularity, and also language and culture as inter-human information processing systems. in this sense it can be argued that the carbon-based biosphere has generated a system (human society) capable of creating technology that will result in a comparable evolutionary transition. "Digital information has reached a similar magnitude to information in the biosphere. It increases exponentially, exhibits high-fidelity replication, evolves through differential fitness, is expressed through artificial intelligence (AI), and has facility for virtually limitless recombination. Like previous evolutionary transitions, the potential symbiosis between biological and digital information will reach a critical point where these codes could compete via natural selection. Alternatively, this fusion could create a higher-level superorganism employing a low-conflict division of labor in performing informational tasks...humans already embrace fusions of biology and technology. We spend most of our waking time communicating through digitally mediated channels, ...most transactions on the stock market are executed by automated trading algorithms, and our electric grids are in the hands of artificial intelligence. With one in three marriages in America beginning online, digital algorithms are also taking a role in human pair bonding and reproduction". Anthropological perspectives Current political theories of the new tribalists consciously mimic ecology and the life-ways of indigenous peoples, augmenting them with modern sciences. Ecoregional Democracy attempts to confine the "shifting groups", or tribes, within "more or less clear boundaries" that a society inherits from the surrounding ecology, to the borders of a naturally occurring ecoregion. Progress can proceed by competition between but not within tribes, and it is limited by ecological borders or by Natural Capitalism incentives which attempt to mimic the pressure of natural selection on a human society by forcing it to adapt consciously to scarce energy or materials. Gaians argue that societies evolve deterministically to play a role in the ecology of their biosphere, or else die off as failures due to competition from more efficient societies exploiting nature's leverage. Thus, some have appealed to theories of sociocultural evolution to assert that optimizing the ecology and the social harmony of closely knit groups is more desirable or necessary than the progression to "civilization." A 2002 poll of experts on Neoarctic and Neotropic indigenous peoples (reported in Harper's magazine) revealed that all of them would have preferred to be a typical New World person in the year 1491, prior to any European contact, rather than a typical European of that time. This approach has been criticised by pointing out that there are a number of historical examples of indigenous peoples doing severe environmental damage (such as the deforestation of Easter Island and the extinction of mammoths in North America) and that proponents of the goal have been trapped by the European stereotype of the noble savage. The role of war in the development of states and societies Particularly since the end of the Cold War, there has been a growing number of scholars in the social sciences and humanities who came to complement the more presentist neo-evolutionary research with studies into the more distant past and its human inhabitants. A key element in many of these analyses and theories is warfare, which Robert L. Carneiro called the "prime mover in the origin of the state". He theorizes that given the limited availability of natural resources, societies will compete against each other, with the losing group either moving out of the area now dominated by the victorious one, or, if the area is circumscribed by an ocean or a mountain range and re-settlement is thus impossible, will be either subjugated or killed. Thus, societies become larger and larger, but, facing the constant threat of extinction or assimilation, they were also forced to become more complex in their internal organisation both in order to remain competitive as well as to administer a growing territory and a larger population. Carneiro's ideas have inspired great number of subsequent research into the role of war in the process of political, social, or cultural evolution. An example of this is Ian Morris who argues that given the right geographic conditions, war not only drove much of human culture by integrating societies and increasing material well-being, but paradoxically also made the world much less violent. Large-scale states, says Morris, evolved because only they provided enough stability both internally and externally to survive the constant conflicts which characterise the early history of smaller states, and the possibility of war will continue to force humans to invent and evolve. War drove human societies to adapt in a step-wise process, and each development in military technology either requires or leads to comparable developments in politics and society. Many of the underlying assumptions of Morris's thinking can be traced back in some form or another not only to Carneiro but also to Jared Diamond, and particularly his 1997 book Guns, Germs, and Steel. Diamond, who explicitly opposes racist evolutionary tales, argues that the ultimate explanation of why different human development on different continents is the presence or absence of domesticable plants and animals as well as the fact that the east-west orientation of Eurasia made migration within similar climates much easier than the south-north orientation of Africa and the Americas. Nevertheless, he also stresses the importance of conflict and warfare as a proximate explanation for how Europeans managed to conquer much of the world, given how societies who fail to innovate will "tend to be eliminated by competing societies". Similarly, Charles Tilly argues that what drove the political, social, and technological change which, after centuries of great variation with regard to states, lead to the European states ultimately all converging on the national state was coercion and warfare: "War wove the European network of national states, and preparation for war created the internal structures of states within it." He describes how war became more expensive and complex due to the introduction of gunpowder and large armies and thus required significantly large states in order to provide the capital and manpower to sustain these, which at the same time were forced to develop new means of extraction and administration. However, Norman Yoffee has criticised such theorists who, based on general evolutionary frameworks, came to formulate theories of the origins of states and their evolution. He claimed that in no small part due to the prominence of neoevolutionary explanations which group different societies into groups in order to compare them and their progress both to themselves and to modern ethnographic examples, while focusing mostly on political systems and a despotic élite who held together a territorial state by force, "much of what has been said of the earliest states, both in the professional literature as well as in popular writings, is not only factually wrong but also is implausible in the logic of social evolutionary theory". See also References Cited sources Sztompka, Piotr (2002). Socjologia. Znak. ISBN 83-240-0218-9. Bibliography Readings from an evolutionary anthropological perspective External links Sociocultural evolution on Principia Cybernetica Web Classical Sociological Theory: Comte and Spencer Secular Cycles and Millennial Trends Larousse Gastronomique (French pronunciation: [laʁus ɡastʁɔnɔmik]) is an encyclopedia of gastronomy first published by Éditions Larousse in Paris in 1938. The majority of the book is about French cuisine, and contains recipes for French dishes and cooking techniques. The first edition included few non-French dishes and ingredients; later editions include many more. Background The first edition (1938) was edited by Prosper Montagné, with the collaboration of Dr Alfred Gottschalk, with prefaces by each of the author-chefs Georges Auguste Escoffier and Philéas Gilbert (1857–1942). Gilbert was a collaborator in the creation of this book as well as Le Guide Culinaire (1903) with Escoffier, leading to some cross-over with the two books. It caused Escoffier to note when he was asked to write the preface that he could "see with my own eyes", and "Montagné cannot hide from me the fact that he has used Le Guide as a basis for his new book, and certainly used numerous recipes." The third English edition (2001), which runs to approximately 1,350 pages, has been modernized and includes additional material on other cuisines. It is also available in a concise edition (2003). A new, updated and revised edition was released in October 2009, published by Hamlyn in the UK. Bibliography Larousse Gastronomique, Prosper Montagné, maître cuisinier, avec la collaboration du docteur Gottschalk, Paris, Editions Larousse, 1938. 2001 2nd edition, ISBN 2-03-560227-0, with assistance from a gastronomic committee chaired by Joël Robuchon James, Kenneth. Escoffier: The King of Chefs. Hambledon and London: Cambridge University Press, 2002. English translations Montagné, Prosper. Larousse gastronomique: the encyclopedia of food, wine & cookery, Ed. Charlotte Turgeon and Nina Froud. New York, Crown Publishers, 1961. The English translation of the 1938 edition. ISBN 0-517-50333-6 Montagné, Prosper. Larousse Gastronomique: The New American Edition of the World's Greatest Culinary Encyclopedia. Edited by Jenifer Harvey Lang. New York: Crown, 1988. Second English edition.ISBN 0-609-60971-8 Courtine, Robert. Larousse Gastronomique: the world's greatest cookery encyclopedia. London: Hamlyn, 1988. ISBN 0-600-32390-0 Montagné, Prosper. Larousse Gastronomique: The New American Edition of the World's Greatest Culinary Encyclopedia, Ed. Jenifer Harvey Lang. New York, Crown Publishers, 1998. ISBN 0-517-57032-7 Montagné, Prosper. Larousse Gastronomique: The World's Greatest Culinary Encyclopedia. Edited by Jenifer Harvey Lang. New York: Clarkson Potter, 2001. Third English edition. ISBN 978-0-600-62042-6 Montagné, Prosper. Larousse Gastronomique: The World's Greatest Culinary Encyclopedia. Edited by Joël Robuchon. New York: Clarkson Potter, 2009. Fourth English edition. ISBN 978-0-307-46491-0 == References == A gardener is someone who practices gardening, either professionally or as a hobby. Description A gardener is any person involved in gardening, arguably the oldest occupation, from the hobbyist in a residential garden, the home-owner supplementing the family food with a small vegetable garden or orchard, to an employee in a plant nursery or the head gardener in a large estate. Garden design and maintenance The garden designer is someone who will design the garden, and the gardener is the person who will undertake the work to produce the desired outcome. Design The term gardener is also used to describe garden designers and landscape architects, who are involved chiefly in the design of gardens, rather than the practical aspects of horticulture. Garden design is considered to be an art in most cultures, distinguished from gardening, which generally means garden maintenance. Vita Sackville-West, Gertrude Jekyll and William Robinson were garden designers as well as gardeners. Garden design is the creation of a plan for the construction of a garden, in a design process. The product is the garden, and the garden designers attempt to optimize the given general conditions of the soil, location and climate, ecological, and geological conditions and processes to choose the right plants in corresponding conditions. The design can include different themes such as perennial, butterfly, wildlife, Japanese, water, tropical, or shade gardens. In 18th-century Europe, country estates were refashioned by landscape gardeners into formal gardens or landscaped park lands, such as at Versailles, France, or Stowe Gardens, England. Today, landscape architects and garden designers continue to design both private garden spaces, residential estates and parkland, public parks and parkways to site planning for campuses and corporate office parks. Professional landscape designers are certified by the Association of Professional Landscape Designers. Maintenance The designer also provides directions and supervision during construction, and the management of establishment and maintenance once the garden has been created. The gardener is the person who has the skill to maintain the garden's design. The gardener's labor during the year include planting flowers and other plants, weeding, pruning, grafting, deadheading, mixing and preparation of insecticides and other products for pest control, and tending garden compost. Weeds tend to thrive at the expense of the more refined edible or ornamental plants. Gardeners need to control weeds using physical or chemical methods to stop weeds from reaching a mature stage of growth when they could be harmful to domesticated plants. Early activities such as starting young plants from seeds for later transplantation are usually performed in early spring. See also == References == A welcommittee is usually a group of volunteers of an organization with the goal of reaching out to new members in a sort of Welcome Wagon-approach. The word "welcommittee" is a portmanteau combining the words, "welcome" and "committee." Among some United Kingdom residents, the word is sometimes shortened to "welco" or "WelCom." The term originated in the 1950s in science fiction fandom with the National Fantasy Fan Federation which still has an active welcommittee. Other notable fandom-based welcommitees include the Star Trek Welcommittee started by Jacqueline Lichtenberg and the British Science Fiction Association. The concept also has been adopted by many host committees of various science fiction conventions as an outreach to those new to fandom. Although the welcommittee concept has a long history in fandom, the advent of the Internet culture has led the term to evolve beyond the fandom subculture. A recent trend is for the term to be adopted by various wiki communities. In fact, Wikipedia's sister project, Wikinews has an active welcommittee. External links StarTrekFans.net transcript of a chat with Jacqueline Lichtenberg about the origins of welcommittees Wikinews Welcommittee == References == In German history, the Hitler Youth generation refers to the generation of Germans born approximately between 1922 and 1930 and who experienced childhood, adolescence, and early adulthood in Nazi Germany (1933–1945). It is one of several terms used in social historians and sociologists similar to the Flakhelfer generation or Forty-fivers which may differ slightly in scope. It is conventionally argued that the immersion of this group within Nazi social and ideological structures, such as the Hitler Youth or League of German Girls, made this group the most "fanatical" adherents of Nazi ideology. According to the historian Gabriele Rosenthal: The members of the Hitler-youth generation (born approximately between 1922 and 1930), experienced their childhood and youth in the 'Third Reich'. In school and youth movements they were socialized in the ideology of National Socialism. As children and youths these were, according to Nazi propaganda, the 'guarantors of the future', and they were raised to establish a new society. Their self-confidence was developed and strengthened by the establishment of youth movements which had not been available to previous generations. [...] National Socialist pedagogues were also successful in arousing enthusiasm in these young people for the Nazi Weltanschauung and the war. Many of these youngsters were glad to be able to join the auxiliary forces towards the end of the war. The older members of the generation were conscripted into the Flak-auxiliary, and then at the very end into the regular army. The size of this generation is estimated at approximately nine million and the following cohort is sometimes described as the War generation. In contrast with older age groups it is also argued that the Hitler Youth generation emerged from the Second World War with little experience of combat and mortality than older age groups and were accordingly a preponderant demography during the early post-war years in West Germany and East Germany as late as the 1960s. References Further reading Von Plato, Alexander (1995). "The Hitler Youth generation and its role in the two post-war German states". In Roseman, Mark (ed.). Generations in Conflict: Youth Revolt and Generation Formation in Germany, 1770-1968. Cambridge: Cambridge University Press. pp. 210–226. ISBN 9780521441834. McDougall, A. (2008). "A Duty to Forget? The 'Hitler Youth Generation' and the Transition from Nazism to Communism in Postwar East Germany, c. 1945-49". German History. 26 (1): 24–46. doi:10.1093/gerhis/ghm002. Moses, A.D. (1999). "The Forty-Fivers: A Generation Between Fascism and Democracy". German Politics & Society. 17 (1 (50)): 94–126. doi:10.3167/104503099782486941. ISSN 1045-0300. JSTOR 23737346. Sahrakorpi, Tiia (2020). "Memory, Family, and the Self in Hitler Youth Generation Narratives". Journal of Family History. 45 (1): 88–108. doi:10.1177/0363199019880254. Kohut, Thomas August (2012). A German Generation: An Experiential History of the Twentieth Century. New Haven: Yale University Press. ISBN 9780300192452. Generation Z (or Gen Z), colloquially also referred to as 'zoomers', is the demographic cohort succeeding Millennials and preceding Generation Alpha. Researchers and popular media use the mid-to-late 1990s as starting birth years, while they use the early 2010s as the ending birth years, with the generation generally being defined as those born between 1997 and 2012. Gen Z's political identity is difficult to pin down due to their tendency for self-reporting based on the people and situations they are in. They often adjust or hide their beliefs to avoid conflict or judgment from friends and family. In the late 2010s, Generation Z was often portrayed as a progressive cohort, showing strong support for social issues such as fourth-wave feminism, LGBTQ+ rights, gun control, and climate change. Gen Z largely voted Democratic before 2024, reflecting their progressive values. The generation was once described as "pro-government" and sometimes referred to as "the most progressive generation ever," with some expressing a more favorable view of socialism. This perception was later challenged, particularly in the context of the 2024 United States presidential election, with a large segment of American Gen Z men aligning themselves with Republicans more than women. The political divide within Gen Z became increasingly pronounced, particularly along gender lines. However, Vox has found that ideologically, young women have become more liberal but not more Democratic, while young men have become more Republican but not more conservative. According to Politico, age plays a large role in political ideology of Gen Z depending on how young they were during world events such as the COVID-19 pandemic; older members of Gen Z leaned progressive while younger members were more conservative. Movements associated with Gen Z so far include fourth-wave feminism, School Strike for Climate, March for Our Lives and Students Against Discrimination. Contrary to older generations, who mainly receive news from television news, Generation Z receives their information predominantly from social media. Voting patterns A consistent trend in much of the Western world since after World War II is that older people are more likely to vote than their younger counterparts, and they tend to vote for more right-leaning or conservative candidates. According to Sean Simpsons of Ipsos, people are more likely to vote when they have more at stake, such as children to raise, homes to maintain, and income taxes to pay. The generational gap of political positions could be due to the fact that today's youths grew up in different sociopolitical environments to their parents and grandparents - environments which make them more likely to go to university, correlating with socially liberal views on certain issues. Millennials are also the first generation to own less than their predecessors at the same age, and, with Gen Z exhibiting similar trends, economically redistributive policies tend to be popular with the generation. In recent years, a political gender gap between Gen Z men and women emerged, with Gen Z men leaning towards conservatism and right-wing populism, and women leaning towards support for progressive and left-wing policies and left-wing populism, a gap that has been detected by various research and numerous countries, but more predominantly in Argentina, Brazil, the United States and South Korea. Europe In the 2019 European Parliament election in France, the support of the far-right National Rally (formerly the National Front) among those 35 and under nearly halved as compared to the 2014 election, where the Front won the plurality of votes in that age group. It is important to note that (depending on how the generation is defined) the 2014 election either had one year or no years of Generation Z voters, while the 2019 election had four to six years. In the first round of the 2017 presidential election, National Front candidate Marine Le Pen won more votes from people between the ages of 18 and 35 than any other candidate. Moreover, the share of women aged 18 to 26 who backed the Le Pen political family rose from 9% in 1988 to 32% in 2017, closing the gender gap in the process. In Austria, more than one in two men between the ages of 18 and 29 voted for the nationalist Freedom Party in the 2016 presidential election, and 30% of those ages 18 to 29 voted for the Freedom Party in the 2017 parliamentary elections. However, in the 2019 European Parliament election, the Freedom Party came in third with the same age group (17%), behind both the Greens (28%) and the Social Democratic Party (22%). In the 2019 European Parliament election in Germany, 30% of voters aged 18 to 30 voted for the Green Party, beating every other political party by wide margins in that particular age group. In Italy, the populist party Five Star Movement won the youth vote overwhelmingly in the 2018 national election. But in the 2019 European Parliament election, the right-wing party Lega Nord won the largest vote-share among both millennials and Gen Z. In the 2019 European Parliament election in Poland, the national-conservative Law and Justice party came first among those aged 18 to 29. In the 2022 school elections in Sweden, in which 80% percent of all Swedish teenagers participated, 9% voted for left-wing parties, 21% voted for centre-left parties, 8% voted for a centrist party, 41% voted for centre-right parties, and 21% voted for a right-wing party. Political scientist George Tilley notes that, in the United Kingdom, while older people tend to vote for the Conservative Party, young people tend to choose the Labour Party, the Liberal Democratic Party, or the Green Party. In the 2019 UK general election, the Labour Party had a 43-point lead over the Conservative Party among voters aged 18 to 24. Winning the support of young people does not necessarily translate to increasing young voters' turnouts in the UK, and positive reactions on social media may not lead to success at the ballot box. Despite reports of a surge in turnouts among young voters in the 2015 and 2017 United Kingdom general elections, statistical scrutiny by the British Elections Study revealed that the margin of error was too large to determine whether or not there was a significant increase or decrease in the number of young participants. In both cases, turnouts among those aged 18 to 24 was between 40% and 50%. Latin America In 2022, exit polls revealed that voters aged 18 to 24 intended to vote overwhelmingly (by a margin of over 35%) for Gustavo Petro in the runoff for the Colombian presidential election. Petro's victory marked the first time that a leftist politician was elected to the office. The same phenomenon was seen in the 2022 presidential election in Brazil, where polls estimate that 51% of young voters between 16 and 24 years voted for the Workers' Party candidate, Luiz Inácio Lula da Silva, elected with 50.9% of the votes. The Socialism and Freedom Party (PSOL), that targets young voters from the middle class, was the fastest growing party in Brazil since 2017. Argentina, on the other hand, has seen an uprising of right wing and pro-market movements in younger generations, as polls estimate that the majority of young voters between 16 and 24 years voted Javier Milei for president in the 2023 Argentine general election. United States In a 2016 mock election of upper elementary, middle, and high school students conducted by Houghton Mifflin Harcourt, Hillary Clinton beat Donald Trump among the students, with Clinton receiving 46% of the vote, Donald Trump receiving 41%, and other candidates receiving 12%. A 2017 survey produced by MTV and the Public Religion Research Institute found that 72% of Americans aged 15 to 24 held unfavorable views of President Donald Trump. A 2018 Pew Research Center poll found that among those aged 13 to 21, only 30% approved of Trump's job performance, and in a 2020 Politico poll, only about 28% of those aged 18 to 23 approved of Donald Trump's job performance. In the 2018 midterm elections, voter turnout among 18 to 29-year-olds went up to 36% from 20% in 2014, the largest percentage point increase for any age group. This increase in youth turnout has been attributed to a rebuttal of Trump among young millennials and older members of Generation Z, as well as to the get out the vote initiatives that were part of the student-led gun violence protest March for Our Lives, which was described as "Generation Z's first social movement." However, according to a field survey by The Washington Post interviewing every fifth person at the protest, only ten percent of the participants were 18 years of age or younger. Meanwhile, the adult participants of the protest had an average age of just under 49. According to a Politico survey, a 2020 presidential election with only Generation Z voters would see Joe Biden win 51% of the popular vote, compared to Donald Trump's 25%. However, almost half of Biden voters said they were voting against Trump rather than for Biden, reflected in lower approval ratings for both Joe Biden and congressional Democrats than the general public. In the 2020 presidential election, 50% of those aged 18 to 29 voted, compared to 39% in the 2016 election. In addition, voter turnout among newly eligible voters (ages 18 and 19) was 46%, stronger than previous years. The increase has also been attributed to the expansion of postal voting due to the COVID-19 pandemic. Maxwell Frost (born in 1997) has become the first and only member of Generation Z so far to be elected into the United States Congress. He ran for Florida's 10th congressional district representing the Democratic Party. In the 2024 U.S. election, Gen Z voters displayed notable gender-based differences in their voting patterns. Gen Z women, more than men, tended to support progressive candidates, influenced by concerns about issues like reproductive rights, climate change, and Fourth-wave feminism. This pattern was consistent with broader trends, where younger women were more likely to back Democratic candidates. Meanwhile, Gen Z men shifted further to Republican candidates like Trump. Explanations for Trump's gains with young men include the economy, appealing to them through podcasters like Joe Rogan, "speaking to" their mental health struggles, less outreach from liberals, and men feeling "left behind" amid progress for women. Social values Family values and abortion A 2018 Pew Research Center survey studied various family values among generations. Among Generation Z, 67% were indifferent towards premarital cohabitation. 49% considered single motherhood to be neither a positive or a negative for society. 62% saw increased ethnic or racial diversity as good for society and 53% for interracial marriage. In the case of financial responsibility in a two-parent household, majorities from across the generations (79% of Generation Z) answered that it should be shared by both parents. Across all the generations surveyed, at least 84% thought that both parents ought to be responsible for rearing children. About 13% of Generation Z thought that mothers should be the primary caretaker of children, while very few thought that fathers should be the ones mainly responsible. In 2016, the Varkey Foundation and Populus conducted an international study examining the attitudes of 20,000 people aged 15 to 21 in twenty countries. The survey showed that 66% of people aged 15 to 21 favored legal abortion, but there was significant variation among the countries surveyed: support for this procedure was strongest in France, the United Kingdom, and Canada, but lowest in Argentina, Brazil, and Nigeria. Gallup polls conducted in 2020 in the U.S. revealed that 61% of people aged 18 to 29—older members of Generation Z and younger Millennials—considered themselves pro-choice, while only 34% identified as pro-life. In general, the older someone was, the less likely that they supported access to abortion. Nevertheless, anti-abortion sentiments among American youths in the early 2020s were sufficiently strong to motivate them to participate in the annual March for Life in Washington D.C. Following the overturning of Roe v. Wade in 2022, 65% of men and 71% of women aged 18 to 29 in America supported legal abortion in "most or all cases". However, in the 2024 presidential election, only 43% of Gen Z men said that abortion was a "critical issue". Gender equality and LGBT rights The 2016 Varkey Foundation and Populus survey also asked about people's viewpoints on moral questions regarding sex and gender. Overall, 89% supported equality between men and women, with support being the highest in Canada and China (both 94%), and the lowest in Japan (74%) and Nigeria (68%). 74% favored recognizing transgender rights, but with large national differences, from an overwhelming majority of 83% in Canada to a bare majority of 57% in Nigeria. 63% approved of same-sex marriage, but there were again huge variations among countries. 81% of young Germans and 80% of young Canadians agreed that same-sex couples should be allowed to marry, compared to only 33% of young Turks and 16% of young Nigerians who did. In a 2018 Pew Research Center survey, 48% of Generation Z participants said that same-sex marriage was good for society. Civil liberties A 2020 Politico survey showed that American voters aged 18 to 23 support the Black Lives Matter movement to a greater extent (68%) than all registered voters (54%), and support police much less (39%) than all registered voters (66%). While Democrats of all ages generally agree that black people are treated unfairly in the United States at equal rates (roughly 80%), Gen Z Republicans agree at much greater rates (43%) than millennial Republicans (30%) or older generations (20-23%). Gen Z also supported protesters and protests more than registered voters by a 12% margin, and were more likely to view protesting as an effective way to make political change. In a worldwide Deloitte survey, 60% of Generation Z say that systemic racism is widespread in society. Over two in ten also reported that they felt discriminated against "all the time", with this number rising to three in ten among sexual minorities, and four in ten among ethnic minorities. A 2016 Varkey Foundation and Populus survey found that, worldwide, young people's support for free speech dwindled if it was deemed offensive to a religion (56% support) or a minority group (49%). Economic values Generation Z is much more likely to turn to their parents for financial advice than Millennials. Unlike their predecessors, members of Generation Z are more cautious of developing financial debt and many are already saving for retirement. Their money-saving habits are reminiscent of those who came of age during the Great Depression. However, because a portion spend so much time online playing video games, they make many in-game purchases that add up over time without realizing how much money they are actually spending. In any case, in the second quarter of 2019, the number of people from Generation Z carrying a credit card balance increased by 41% compared to that of 2018 (from 5,483,000 to 7,746,000), as the first wave of this demographic cohort became old enough to take out a mortgage, a loan, or to have credit-card debt, according to TransUnion. The financial industry expects continued growth in credit activity by Generation Z, whose rate of credit delinquency is comparable to those of the Millennials and Generation X. According to a 2019 report from the financial firm Northwestern Mutual, student loans were the top source of debt for Generation Z, at 25%. For comparison, mortgages were the top source of debt for the Baby Boomers (28%) and Generation X (30%); for the Millennials, it was credit card bills (25%). Generation Z views socialism more positively than previous generations, especially in the United States. In a 2018 Gallup poll, 51% of Americans aged 18 to 29—young Millennials and older Gen Z—have a positive view of socialism, compared to 45% having a positive view of capitalism. In a 2019 poll by YouGov and Victims of Communism Memorial Foundation found that more than half of Gen Z Americans have an unfavorable view of capitalism, and almost two-thirds said they were likely to vote for a socialist candidate. Much of this shift has been attributed to Bernie Sanders's 2016 and 2020 presidential campaigns. Additionally, according to the 2020 Edelman Trust Barometer, 57% of those ages 18 to 34 worldwide say that modern-day capitalism "does more harm than good", though this figure roughly corresponds with all other age groups. Though still generally popular among all age groups, Europeans ages 18 to 29 are less likely than older groups to support fiscal redistribution. This age group in the EU is also more likely to attribute poverty to “laziness or lack of willpower," though this sentiment is not the majority view of the cohort in most countries in Europe. According to a 2021 Forefront Market Research poll commissioned by the Institute of Economic Affairs, a conservative think tank, 67% of young people in the UK (those aged 16 to 34) say they would like to live in a socialist economic system. The poll also found that overwhelming majorities of Gen-Z Britons (aged 16 to 22) blame capitalism for the UK housing crisis to some extent, label climate change as a capitalist problem, and would like to nationalize utilities and railways. According to a 2018 International Federation of Accountants survey of G20 countries, the top three public policy priorities for members of Generation Z are the stability of the national economy, the quality of education, and the availability of jobs. The bottom issues, on the other hand, were addressing income and wealth inequality, making regulations smarter and more effective, and improving the effectiveness of international taxation. Moreover, healthcare is a top priority for Generation Z in Canada, France, Germany, and the United States. Tackling wealth and income inequality is deemed of vital importance in Indonesia, Saudi Arabia, and Turkey. International affairs Polling has shown that Generation Z is, on the whole, less nationalist than previous generations, a shift stemming in part from increased contact with other cultures. In 2019, Harvard University's Institute of Politics Youth Poll asked voters aged 18 to 29 – younger Millennials and the first wave of Generation Z – what they would like to be priorities for US foreign policy. They found that the top issues for these voters were countering terrorism and protecting human rights (both 39%), and protecting the environment (34%). Preventing nuclear proliferation and defending U.S. allies were not as important to young American voters. According to the Eurobarometer in 2016 to 2018, over 60% of Europeans between 15 and 24 viewed globalization positively, in stark contrast with older cohorts. A 2018 YouGov poll asked British voters whether leaving the European Union was a good idea in hindsight. They found that 42% said yes while 45% said no. Among them, 19% of those between the ages of 18 and 24 said yes, as did 61% of pensioners. Overall the British public has not changed their minds on the issue as this aligned with the results of the 2016 Brexit referendum when younger voters were more likely to vote to stay in the European Union. Immigration In a 2016 Varkey Foundation and Populus international survey, the question of whether or not those 15 to 21 favored legal migration received mixed responses. Overall, 31% believed their governments should make it easier for immigrants to work and live legally in their countries while 23% said it should be more difficult, a margin of 8%. While 72% of Brazilian youths thought their government was doing too little to address the international refugee crisis, only 16% of young Turks did; in the U.K. that number was 48%. A YouGov poll conducted in the spring of 2018 revealed that 41% of Britons between the ages of 18 and 24 thought that immigration to their country was "too high", compared to 58% of those 25 to 49. Gaza war Members of Generation Z are more likely to support Palestine and oppose Israel than any other generation. According to an October 2023 NPR/PBS NewsHour/Marist National poll which surveyed 1,313 U.S. adults, 48% of Millennials and Generation Z said that the U.S. government should publicly voice support for Israel, compared to 63% of Generation X, 83% of the baby boomers, and 86% from the Silent and Greatest Generation. In December 2023, More in Common conducted a survey among British citizens from various age groups. Generation Z respondents were the most likely to consider Hamas to be freedom fighters and the least likely to consider Hamas a terrorist organisation. Environmentalism Generation Z is more likely than other generations to believe that climate change is real and to support climate change mitigation. Polling from 2018 of Americans over the age of 13 by Pew suggested that 54% of Generation Z believed that climate change is real (making them the second most likely cohort to do so) and is due to human activities, while only 10% reject the scientific consensus on climate change. A worldwide Deloitte survey saw that climate change and protecting the environment top Generation Z's primary concerns, with curbing unemployment and expanding health care access not far behind. A 2019 poll of over 10,000 internet users aged 18 to 25 years old in 22 countries around the world found that 41% of responders believed that climate change was one of the most important issues facing the world, the most popular response. The second most common problem cited was environment-related pollution at 36%. Top priorities on a national level differed but environmental concerns also polled relatively strongly in this category. A 2020 survey conducted for Newsround of Britons aged 8 to 16 suggested that 80% of young Generation Z viewed climate change as a problem, with more than a third thinking it was "very important". 58% of respondents worried about the impact that climate change would have on their future. 19% said they faced nightmares about the topic whilst 17% said it had affected their eating and sleeping habits. 41% of respondents did not trust adults to properly address the crisis. Addressing climate change is very important for Generation Z in India and South Korea. In contrast, less than 20% of Europeans aged 15 to 24 viewed climate change as a top 2 challenge facing the EU according to the Eurobarometer. One of the earliest political movements primarily driven by Generation Z was School Strike for Climate in the late 2010s. The movement saw millions of young people around the world, inspired by the activities of Swedish teenage activist Greta Thunberg, protest for greater action on climate change. Other views Trust in institutions In 2019, the Pew Research Center interviewed over 2,000 Americans aged 18 and over on their views of various components of the federal government. They found that 54% of the people between the ages of 18 and 29 wanted larger government compared to 43% who preferred smaller government and fewer services. Older people were more likely to pick the second option. According to a 2018 survey by Pew Research Center, 70% of Generation Z want the government to play a more active role in solving their problems. However Gen Z's trust in institutions may have seen a large decrease: In the Fall 2024 Harvard Youth Poll, only 11% of those ages 18-29 felt that the United States is “generally headed in the right direction,” and a 2023 poll by the American Public Media Research Lab found that only 27% of Americans ages 18-25 “agree strongly” that democracy is the best system of government, compared with 48% for all ages. Gun ownership The March for Our Lives, a protest taking place in the aftermath of the Stoneman Douglas High School shooting in 2018 was described by various media outlets as being led by students and young people. Some even describe it as the political "awakening" of Generation Z or that these protesters were "the voice of a generation on gun control." In a 2019 Rasmussen poll, US Gen Z and Millennial respondents supported stricter gun laws at the highest rates—68%. Majorities of older generations also agreed. In a 2017 poll, Pew found that among the age group 18 to 29, 27% personally owned a gun and 16% lived with a gun owner, for a total of 43% living in a household with at least one gun. Nationwide, a similar percentage of American adults lived in a household with a gun. COVID-19 pandemic According to a worldwide Deloitte survey, 69% of those in Gen Z reported that they took their government's public health guidelines seriously during 2020. Members of Generation Z surveyed also feel that they are doing more than others in their countries to limit the spread of COVID-19. See also Generation Youth activism Youth politics Gen-Z for Change Voters of Tomorrow Millennial politics Notes == References == Doors Open Days (also known as Open House or Open Days in some communities) provide free access to buildings not normally open to the public. The first Doors Open Day took place in France in 1984, and the concept has spread to other places in Europe (see European Heritage Days), North America, Australia and elsewhere. Doors Open Days promotes architecture and heritage sites to a wider audience within and beyond the country's borders. It is an opportunity to discover hidden architectural gems and to see behind doors that are rarely open to the public for free. Open Doors Days trace their origin to the 1990 Door Open Day held as part of Glasgow's year as European City of Culture. Heritage Open Days in England Heritage Open Days established in 1994 celebrate English architecture and culture allowing visitors free access to historical landmarks that are either not usually open to the public, or would normally charge an entrance fee. List of Doors Open events in England Open House London Scotland Doors Open Days is organised by the Scottish Civic Trust. Alongside Scottish Archaeology Month, the open days form Scotland's contribution to European Heritage Days. This joint initiative between the Council of Europe and the European Union aims to give people a greater understanding of each other through sharing and exploring cultural heritage. 49 countries across Europe take part annually, in September. During Glasgow's year as European City of Culture in 1990, organisers ran an Open Doors event, an event credited with popularizing the Doors Open concept and spreading it to other countries. Its popularity encouraged other areas to take part the following year and were coordinated by the Scottish Civic Trust. Doors Open Days now take place throughout Scotland thanks to a team of area coordinators. These coordinators work for a mixture of organisations: local councils, civic trusts, heritage organisations and archaeological trusts. Scotland is one of the few participating countries where events take place every weekend in September, with different areas choosing their own dates. More than 900 buildings now take part. In 2008, over 225,000 visits were made generating £2 million for the Scottish economy. It is estimated that 5,000 or more volunteers give their time to run activities and open doors for members of the public. Doors Open Days was supported in 2009 by Homecoming Scotland 2009, a year-long initiative that marked the 250th anniversary of the birth of Scotland's national poet, Robert Burns. It was funded by the Scottish Government and part financed by the European Union through the European Regional Development Fund. Its aim was to engage Scots at home, as well as motivate people of Scottish descent and those who simply love Scotland, to take part in an inspirational celebration of Scottish culture and heritage. Open Doors events in Wales Funded and organised by the Wales conservation organisation, Cadw, an Open Doors festival takes place every September, giving free access to many Cadw and non-Cadw sites. It was claimed to be "the largest annual free celebration of architecture and heritage" in the UK. In 2021 more than 150 historic sites took part in the event. Open house in Australia Open House events are organised in Australia in partnership with Open House Worldwide. The first Open House event took place in Melbourne in 2008 (the Melbourne Open House event has since changed into an event primarily focused on modern architecture). This was followed by Brisbane in 2010, and Adelaide and Perth in 2012. Canada Doors Open Canada began in 2000. List of Doors Open events in Canada Doors Open Newfoundland and Labrador Doors Open Ottawa Doors Open Toronto Doors Open Saskatoon Doors Open London Doors Open Peterborough United States List of Doors Open events in the U.S. Doors Open Baltimore, first weekend in October Doors Open Buffalo Open House Chicago Doors Open Lowell Open House New York Doors Open Milwaukee Doors Open Minneapolis Doors Open Pittsburgh, first weekend in October Doors Open Rhode Island Passport DC, embassy open houses in Washington, D.C., in May. See also Brisbane Open House Open House Brno Tourism in Scotland External links Media related to Open doors days at Wikimedia Commons == Notes == Gastronationalism or culinary nationalism is the use of food and its history, production, control, preparation and consumption as a way of promoting nationalism and national identity. It may involve arguments between two or more regions or countries about whether a particular dish or preparation is claimed by one of those regions or countries and has been appropriated or co-opted by the others. Gastronationalism has been criticized as an example of banal nationalism. Origins and development Atsuko Ichijo and Ronald Ranta have called food "fundamentally political" and "one of the essential commodities with which political powers at various levels are concerned".: 1–2 Food historian Michelle T. King suggests that cuisine is a natural focus for studies of nationalism, pointing out dozens of such treatments over the first decades of the 21st century.: 1 She also argues Asia's culinary nationalism has been particularly intense.: 1 Examples of gastronationalism include efforts by state bodies, nongovernmental bodies, businesses and business groups, and individuals.: 121–124 New York University professor Fabio Parasecoli has defined food as an expression of identity. Conflict between two or more regions or countries about whether a particular dish or preparation is claimed by one of those regions or countries and has been appropriated or co-opted by the others is not uncommon, especially in areas where there has been violent conflict. Dishes affected by these culinary wars tend to be those with "a clearly symbolic ethnic significance". They also tend to be dishes that "represent territorial aspirations" and can be developed and prepared only by settled – and therefore indigenous – peoples. Lavash and harissa are wheat-based, therefore cannot have been developed by nomads but only by an agricultural society. Many of the debates center around the idea that a "settled" society – that is, an agricultural rather than nomadic one – is somehow superior, and that claiming a dish only achievable in an agricultural society helps prove the area was agricultural at a certain point. This idea was official policy in the Soviet Union. According to OpenDemocracy, "evidence of ancient agricultural development is cherished by nationalists on both sides." Mary Douglas said "national food cultures become a blinding fetish which, if disregarded, may be as dangerous as an explosion". In 2006 researcher Liora Gvion argued that cuisines of poverty – typically, traditional foods – "reveal the inter-connection between the culinary discourse and the political one" and that the issue was tied up with those of access to land and national identity. Sociologist Michaela DeSoucey in 2010 described the concept of gastronationalism as the use of food and its history, production, control, and consumption as a way of promoting nationalism. According to DeSoucey, gastronationalism uses food to promote a sense of national identity and affects how members of the national community develop "national sentiments and taste preferences for certain foods." She argues that the issues go beyond simple nationalism and involve livelihoods and a "struggle for markets" as the identification of a certain food with a certain area means the ability to sell a food product is affected for those inside or outside the area. She also points out that such arguments are often not intended to reach agreement but instead to raise awareness of the food product and generate interest in obtaining it. Kingston University's Ranta in 2018 said a group's claims to a particular food become important when a "cause or agenda is behind the claim". In 2013 Al Jazeera noted that gastronationalism had been an ongoing issue in Armenia, Azerbaijan, and Georgia as each country "vie[d] for the recognition of certain dishes as their own" and was causing tension among neighboring countries with already-troubled relationships. In 2020 an article published by the Cambridge University Press found that while the concept of gastronationalism had not been fully developed in academia, the scholarship was developing quickly. In 2024 journalist Francesca Barca, writing in Voxeurop, called gastronationalism ' "neither neutral nor harmless, but is an aspect of what is called "banal nationalism",' quoting Gastronationalizmo author Michael Antonio Fino as saying "Gastronationalism is one of the most insidious forms of this 'banal nationalism' because it is met with a certain indulgence, and mistaken for patriotic pride." National cuisine Food historian King differentiates between gastronationalism, or culinary nationalism, and national cuisine, saying that culinary nationalism "suggests a dynamic process of creation and contestation" while national cuisine "calls to mind a specific and static product".: 3 Political usage According to Barca, gastronationalism is used as a political symbol for a party or movement's values, and in particular traditional or populist values. She argues "The passage from fork to defence of traditional values is short", quoting Matteo Salvini saying the "defence of our products is a battle of civilisation: in politics everything can be negotiated, but here Made in Italy either is or is not" and called tortellini made without pork an erasure of "our history". According to Fino, "national gastronomic identity becomes an occasion for belonging, opposition to others, a claim to superiority". When the Czech Republic entered the European Union, controversies developed over traditional Czech foods, such as the Czech style of goulash, which traditionally is allowed to rest unchilled overnight before serving, a step forbidden in commercial production by European Union food-handling rules; pomazánkové máslo, which in Czech is "spread butter" but which by EU standards has a fat content too low to be called butter; and the Czech style of rum, which because it is potato-based cannot be called rum under EU rules; conservative politicians objected that entering the EU was removing these foods from traditional cuisine. According to Boróka Parászka, writing in Hvg, Hungarian politicians regularly use food to "invoke identity", citing as an example Hungarian Prime Minister Viktor Orbán's frequent discussion and images of cooking and food in his communications. Governmental and non-governmental bodies Codex Alimentarius Commission The Codex Alimentarius Commission is a project of the Food and Agriculture Organization and the World Health Organization which creates advice regarding food handling, labeling, and ethical standards, including those around marketing a food as originating in a certain place.: 122–124 Intangible Cultural Heritage designation In some cases United Nations Educational, Scientific and Cultural Organization (UNESCO) has made statements favoring one side or the other of such an argument, sometimes after being asked to name a food to a UNESCO Intangible Cultural Heritage list for a country, which has increased passions on either side.: 147–157 In 1972 UNESCO adopted the Convention Concerning the Protection of the World Cultural and Natural Heritage or World Heritage Convention.: 147 Protected Geographical Status In Europe, mandatory origin labeling is "one of the most prickly topics" in European Union (EU) policy discussions. In December 2019 France, Greece, Italy, Portugal and Spain asked the EU to strengthen food origin labeling; Politico called the request a "bombshell", as it weakens the idea of a single market. The Protected Geographical Status as of 2016 had been applied to over a thousand food items.: 14 Fino calls such protections "a powerful tool in the hands of member countries to feed nationalism". Examples Azerbaijan's National Culinary Centre, a non-governmental organization (NGO) publishes information discussing Azerbaijan's national cuisine and accusing Armenian cuisine of imitating Azerbaijan. The NGO's CEO said, ""Since 1989, the issue of Armenian pretentions towards Azerbaijan's culinary traditions has been discussed at the highest level, by specialists and academics, many times. Every pan-Turkish, Islamic dish, including those from Azerbaijan, is claimed as Armenian – they are trying to prove that an Armenian culinary tradition exists." Armenia's Society for the Preservation and Development of Armenian Culinary Traditions, an academic body, has discussed the Armenian culinary tradition. During the hummus wars, multiple corporations and business groups became involved as part of their marketing campaigns.: 121–123 Notable examples Arepas Colombia and Venezuela have a "heated and longstanding rivalry" over the origins of the arepa. The dish is a staple of both cuisines. Venezuelan president Nicolás Maduro "has tried to use arepas as a nationalist rallying point, if not a political tool, claiming the food is from his country alone", according to the New York Times. According to food anthropologist Ocarina Castillo of the Central University of Venezuela, the dish is likely thousands of years old and originated in the region now occupied by the two countries before colonizers of the area drew borders. Borscht Borscht is believed to have originated in Kievan Rus' and specifically in the area of modern-day Ukraine, but according to historian Alison K. Smith, the dish's "Ukrainian origins have been largely obscured" as it became ubiquitous in Russian cuisine.: 30, 33, 45–47 The dish was described in Sergei Drukovtsov's Cooking Notes (1779),: 45 but is believed to have entered popular Russian cuisine from the popular 1939 Soviet cookbook Book of Tasty and Healthy Food that included dishes from various cuisines of the USSR's member states. The state-sponsored cookbook was created by Commissar of Food Anastas Mikoyan in a conscious attempt at creating a Soviet national cuisine for nation-building purposes. In 2019, the official Twitter account of Ministry of Foreign Affairs of the Russian Federation referred to borscht as "one of Russia's most famous & beloved #dishes & a symbol of traditional cuisine" in one of their tweets, sparking outrage in Ukraine, where it was widely seen as an attempt at cultural appropriation. In response, Ukraine applied for the inclusion of borscht in the UNESCO Intangible Cultural Heritage List and launched a five-year culinary diplomacy strategy dubbed 'borsch diplomacy' where borscht plays a central role. UNESCO added the soup to the organization's list for Ukraine in 2020. Shortly after the 2022 Russian invasion of Ukraine, Russian Foreign Ministry spokesperson Maria Zakharova said the fact Ukrainians "didn't want to share borscht" was an example of alleged "xenophobia, Nazism, extremism in all forms" that led to the invasion. Shortly after, UNESCO added "Culture of Ukrainian borscht cooking" to the List of Intangible Cultural Heritage in Need of Urgent Safeguarding, citing the invasion. According to The Smithsonian, "The designation by the international cultural authority was widely seen as a landmark decision in the ongoing cultural dispute between the two countries on borshch’s true country of origin." Chinese cuisine Taiwan has presented Taiwanese cuisine as the only remnant of traditional Chinese culture and cuisine, which the Nationalist Party argued had "been destroyed on the Chinese Mainland after the Communist takeover".: 56–72 On the other hand, some Taiwanese object to the politically fraught inclusion of Taiwanese cuisine under the banner of regional Chinese cuisine and argue that it is "inaccurate". In 2011, the Michelin Green Guide to Taiwan attributed the origins of minced pork rice to Shandong. This led to a fierce debate in Taiwan with many people insisting that minced pork rice originated in Taiwan, while others viewed it as a Shandong dish that simply caught on in Taiwan. Shanghainese people have criticized the Taiwanese restaurant chain Din Tai Fung of misrepresenting the xiao long bao as a Taiwanese dish. In October 2020, a Japanese bakery c'est très fou launched the product "Taiwanese pineapple bun", which received criticism from Hong Kongers for suggesting the product originated in Taiwan. Dolma Dolma or tolma is claimed by both Armenia and Azerbaijan. Armenia holds an annual tolma festival, always at a site that has historical significance in its conflicts with Azerbaijan. Gallo pinto Both Nicaragua and Costa Rica claim gallo pinto as their own, and the dish's origin is a point of contention between the two countries. The competition between the two countries over ownership of the dish is sometimes referred to as the "Gallo Pinto War". Falafel Falafel is argued over by Israel and various Arab states; according to Jennie Ebeling, writing in the Review of Middle East Studies, the dish "is loaded with issues of national identity". According to Alexander Lee, writing for History Today in 2019, "More often than not, arguments about the origins of falafel are refracted through the lens of political rivalries. Particularly for the Israelis and the Palestinians, ownership of this most distinctively Levantine dish is inexorably bound up with issues of legitimacy and national identity. By claiming falafel for themselves, they are each, in a sense, claiming the land itself – and dismissing the other as an interloper or occupier." The dish features prominently in Israeli cuisine and has been called a national dish. Some Palestinians and other Arabs have objected to the identification of falafel with Israeli cuisine as amounting to cultural appropriation. Palestinian author Reem Kassis wrote that the food has become a proxy for political conflict. Joseph Massad, a Jordanian-American professor at Columbia University, has called the characterization as Israeli of falafel and other dishes of Arab origin in American and European restaurants to be part of a broader issue of appropriation by colonizers. The dish and its politico-cultural significance were the subject of a 2013 documentary by Ari Cohen, Falafelism: The Politics of Food in the Middle East. According to the Toronto Star, Cohen intended the film to be about "the unifying power of falafel". The earliest documented references to falafel date back to late 19th-century Egypt, following the British occupation in 1882. In 2002, Concordia University's chapter of Hillel served falafel at an event, prompting accusations of appropriation from a pro-Palestinian student group. Feta Until 1999, the term feta was used only by Greek producers. During the 1990s, Denmark and Germany challenged the labelling, arguing that the word 'feta' was Italian and that other EU countries shared climate and geography with parts of Greece and should be permitted to label their feta-style cheeses as Feta. In 2002 the European Union granted the sole rights to use the name to Greece.: 122 Foie gras Foie gras has been protected as a name and signifier of traditional identity by France; conflict is common with animal rights activists. Hainanese chicken rice Hainanese chicken rice is claimed by both Malaysia and Singapore. The conflict dates to 1965, when the two countries split. Both countries claim its origin and accuse the other of having appropriated the dish into their own national cuisine. Harissa Harissa is claimed by both Armenia and Turkey, where it is called keshkek. Keshkek was recognized by UNESCO on its intangible cultural heritage list, which has caused passionate debate, with Armenians arguing that the dish's main ingredient, wheat, indicates it could not have been developed in Turkey, where the tradition was nomadic. Hummus Hummus is argued over by Israel, Palestine, Syria, and Lebanon. The disagreement is sometimes referred to as the "hummus wars".: 3 : 121–123 The hummus wars also refers to the creation by Sabra, a US food company, of "the world's largest hummus plate" as a marketing event.: 121–123 Israeli company Osem responded with a larger hummus plate, and soon was followed by a group of Lebanese chefs working with the Association of Lebanese Industrialists's campaign "Hands Off Our Dishes", which claimed hummus as Lebanese and objected to the marketing of the dish as Israeli.: 121–123 Fadi Abboud, then president of ALI and later tourism minister for the country, threatened legal action against Israel for marketing hummus and other commercial food products as Israeli.: 121–123 A series of record-breaking hummus plates followed from various middle eastern countries.: 121–123 Abboud characterized the hummus wars as being not about just hummus but about "the organized theft carried out by Israel" in connection to the culture of the entire Arab region.: 121–123 Various academic theories argue the dish has its origins in Turkey, Syria, Lebanon, or Egypt. However, the earliest mention of Hummus comes from a 13th-century cookbook written by the Syrian historian Ibn al-Adim. The strongest evidence currently points to Syria as the origin of Hummus. Jollof rice West African countries typically have at least one variant form of jollof rice, with Ghana, Nigeria, Sierra Leone, Liberia and Cameroon particularly competitive as to which country makes the best jollof. In the mid-2010s this expanded into the "Jollof Wars". The rivalry is especially prominent between Nigeria and Ghana. In 2016 Sister Deborah released "Ghana Jollof", which denigrated the Nigerian version and Nigerians for being proud of their version. Soon after, a physical fight over insufficient jollof supplies at a Ghanaian political rally sparked delighted mockings of Ghanaians by Nigerians. Of particular sensitivity in jollof-making communities is the inclusion of non-traditional ingredients, which are defined country to country and are seen as making the jollof inauthentic. In 2014 a recipe released by Jamie Oliver that included cherry tomatoes, coriander, lemon, and parsley, none of which are used in any traditional recipe, caused outraged reactions to the point Oliver's team had to issue a statement. Kimchi Both South Korea and North Korea claim kimchi.: xii North Korea argues that South Korea's decreasing consumption (and increasing commercialization of production) is proof that the dish is more strongly associated with North Korea.: xii Traditional kimchi-making in South Korea in 2013 was given Intangible Cultural Heritage status by UNESCO: xii : 123 and in 2015 in North Korea. Japan also has interested itself in kimchi, arguing with South Korea over the Codex Alimentarius Commission's (CAC) international standardization of the dish, a disagreement often called the kimchi war.: 81–82 : 123 Japan produced and exported an instant version of kimchi, which South Korea argued should not be called kimchi due to the lack of fermentation.: 81–82 During the 1996 Atlanta Olympics, Japan proposed making kimuchi, the Japanese name of the dish, an official food of the Olympics.: 81–82 In 2001 the CAC adopted an international standard which requires fermentation in order for a product to be exported as kimchi.: 81–82 China has also claimed kimchi, which in China is called pao cai, which is also the name of a similar traditional Sechuan pickle. In 2020 the International Organization for Standardization (ISO) announced regulations for the Sechuan pao cai. Although the ISO stated in the listing that the regulations did not apply to kimchi, China's state-run Global Times called it "an international standard for the kimchi industry led by China". South Korea has called out as appropriation both the Japanese and Chinese marketing of the dish.: 123 Lavash Lavash is claimed by Armenia, Azerbaijan and Turkey; the Armenians argue that lavash is traditionally prepared in a tonir, which indicates development in a non-nomadic society such as Armenia. Accusations in Armenian media centered around Turkey and Azerbaijan claiming the dish because they wanted to conceal their early nomadic lifestyle. Nasi Lemak Nasi Lemak is a traditional dish in Southeast Asian cuisine. It is claimed by both Malaysia and Singapore. Shopska salad Shopska salad, which is considered a national dish of Bulgaria, is claimed by Bulgaria, Croatia, Czechia, North Macedonia, and Serbia. Bulgaria requested protected geographical indication from the European Union; Serbia objected. Tortillas During Mexico's tortilla riots, protesters chanted, "tortillas si, pan no!", expressing their nationalistic objection to replacing tortillas, with which they identified on a nationalistic level, with bread, which they saw as a colonialist introduction.: 2 Turkish coffee UNESCO has included Turkish coffee in its list of items of Intangible Cultural Heritage.: 14 The style of coffee is also claimed by Greece. Washoku Washoku, a traditional food culture of the Japanese, was in 2013 added to the UNESCO Intangible Cultural Heritage List and in 2017 described by Leiden University's Katarzyna J. Cwiertka as "a myth fabricated for the purpose of Japanese nation-branding".: 151 According to Ichijo and Ranta, Japan's efforts to promote Japanese cuisine in other countries is "regarded as a way of increasing export of Japanese agricultural produce and attracting more tourists".: 151 See also Gastrodiplomacy Gastrotourism Politics of food == References == Family cookbooks are books which contain a variety of recipes collected by specific families. Whilst these cookbooks are sometimes later published, the concept is of a commonplace book where useful recipes are retained and passed on to later generations. The recipes can be developed by the family or collated from other sources – and may be so familiar to the family that the origin is forgotten or not acknowledged. Family cookbooks as memory Whilst the primary function of a family cookbook is as a scrapbook to collect recipes for various types of meals and cooking techniques, an important function is also to provide context and promote familial memory. Scholars use these kinds of collected manuscripts to give insights into family history in the era when they were written, such as about gender roles, household environments, and women's emancipation. Commercial family cookbooks Celebrities, chefs, and notable families write and market family cookbooks which share their family recipes. See also Family traditions == References == Four Dharma Seals are the four characteristics which reflect some Buddhist teaching . It is said that if a teaching contains the Four Dharma Seals then it can be considered Buddha Dharma. This is despite the fact that some believe that the Dharma Seals were all introduced after Gautama Buddha died. The Four Seals The Four Seals can be variously translated as follows: All compounded things are impermanent Emotions are prone to suffering All phenomena are without inherent existence Nirvana is beyond extremes All compounded things are impermanent. All contaminated things are suffering. All phenomena are empty and devoid of self. Nirvana is true peace. Everything conditioned is impermanent. Everything influenced by delusion is suffering. All things are empty and selfless. Nirvana is peace. As suffering is not an inherent aspect of existence sometimes the second seal is omitted to make Three Dharma Seals. However, when the second seal is taken to refer to existence contaminated by or influenced by the mental afflictions of ignorance, attachment, and anger and their conditioning actions (karma), this omission is not necessary. See also Three marks of existence Ātman (Buddhism) == References == The Freedom From Religion Foundation (FFRF) is an American nonprofit organization that advocates for atheists, agnostics, and nontheists. Formed in 1976, FFRF promotes the separation of church and state, and challenges the legitimacy of federal and state government support for faith-based programs, such as chaplaincy services. It supports groups such as nonreligious students and clergy who want to leave their faith. History The FFRF was co-founded by Anne Nicol Gaylor and her daughter, Annie Laurie Gaylor, in 1976 and was incorporated nationally on April 15, 1978, who split with Madalyn Murray O'Hair’s American Atheists, in response to O’Hair’s antisemitism. The organization was supported by over 19,000 members in 2012 and operated from an 1855-era building in Madison, Wisconsin, that once served as a church rectory. In March 2011, FFRF, along with the Richard Dawkins Foundation for Reason and Science, began The Clergy Project, a confidential on-line community that supports clergy as they leave their faith. In 2012, it gave its first Freedom From Religion Foundation and Clergy Project "Hardship Grant" to Jerry DeWitt, a former pastor who left the ministry to join the atheist movement. FFRF provides financial support to the Secular Student Alliance, an organization that has affiliate groups for nonreligious students on college campuses. In 2015, FFRF announced Nonbelief Relief, a related organization that obtained and later gave up its federal tax-exempt status. Nonbelief Relief was unsuccessful in a lawsuit against the IRS because it lacked standing to challenge the Form 990 exemption that applies to churches. Nonbelief Relief is a humanitarian agency for atheists, agnostics, freethinkers, and their supporters. Nonbelief Relief was created by the executive board of FFRF to remediate conditions of human suffering and injustice on a global scale, whether the result of natural disasters, human actions or adherence to religious dogma. On 7 November 2024, Kat Grant published an article titled "What is a woman", on Freethought Now!, a website operated by the FFRF. The article argued that “any attempt to define womanhood on biological terms is inadequate” and that “a woman is whoever she says she is”. In response, Jerry Coyne wrote a rebuttal titled "Biology Is Not Bigotry", defending the "biological definition of ‘woman’ based on gamete type". Coyne's rebuttal was initially published on Freethought Now!. However, the FFRF later retracted Coyne's page. On 27 December, the FFRF published a statement saying that "Publishing this post was an error of judgment, and we have decided to remove it as it does not reflect our values or principles". In response, Steven Pinker, Richard Dawkins and Jerry Coyne resigned. First Pinker and Dawkins resigned, and on December 29, 2024, Jerry Coyne also resigned from the honorary board in objection to what they considered the problematic gender-ideological capture of the institute. Coyne stated that LGBTQ people have rights, but some of the desired rights are in conflict with rights of other groups in society. As a result of the division over the issues, FFRF dissolved the honorary board. Media and publications The FFRF publishes a newspaper, Freethought Today, ten times a year. Since 2006, as the Freethought Radio Network, FFRF has produced the Freethought Radio show, an hour-long show broadcast live on WXXM-FM Saturdays at 11 a.m. CDT. It had also been broadcast on Air America before that service ceased operation in March 2010. The show is hosted by the co-presidents of FFRF, Dan Barker and Annie Laurie Gaylor. Regular features include "Theocracy Alert" and "Freethinkers Almanac". The latter highlights historic freethinkers, many of whom are also songwriters. The show's intro and outro make use of John Lennon's Imagine song. Annie Laurie Gaylor, co-president of the FFRF, is the author of the nonfiction book on clergy child sexual abuse scandals Betrayal of Trust: Clergy Abuse of Children (out of print) and the editor of Women Without Superstition: No Gods – No Masters and the anthology Woe to the Women. She edited the FFRF newspaper Freethought Today until July 2008. Her husband, Dan Barker, author of Losing Faith in Faith: From Preacher to Atheist, Godless: How an Evangelical Preacher Became One of America's Leading Atheists, The Good Atheist: Living a Purpose-Filled Life Without God, Life Driven Purpose, God: The Most Unpleasant Character in all Fiction, and Just Pretend: A Freethought Book for Children, is a musician and songwriter, a former Pentecostal Christian minister, and co-president of FFRF. Litigation and issues Social programs Social services In June 2004, the FFRF challenged the constitutionality of the White House Office of Faith-Based and Community Initiatives. The Foundation's complaint alleged that "the use of money appropriated by Congress under Article I, section 8, to fund conferences that various executive branch agencies hold to promote President Bush's 'Faith-Based and Community Initiatives'" conflicted with the First Amendment. The suit "contended that the defendant officials violated the Establishment Clause by organizing national and regional conferences at which faith-based organizations allegedly 'are singled out as being particularly worthy of federal funding because of their religious orientation, and the belief in God is extolled as distinguishing the claimed effectiveness of faith-based social services.'" The FFRF also alleged that "the defendant officials 'engage in myriad activities, such as making public appearances and giving speeches, throughout the United States, intended to promote and advocate for funding for faith-based organizations." The FFRF further asserted, "Congressional appropriations [are] used to support the activities of the defendants." In 2007 the Supreme Court ruled 5–4 that taxpayers do not have the right to challenge the constitutionality of expenditures made by the executive branch. In May 2007, the FFRF, on behalf of Indiana taxpayers, challenged the creation of a chaplaincy pilot program for the Indiana Family and Social Services Administration (FSSA). The FSSA hired Pastor Michael L. Latham, a Baptist minister, in 2006, at a salary of $60,000 a year. In September 2007, in response to the FFRF's suit, Indiana ended the program. Health care In April 2003, the FFRF, on behalf of Montana residents, sued the Montana Office of Rural Health and its executive director David M. Young along with the Montana State University-Bozeman and the Montana Faith-Health Cooperative. It was alleged that Young favored faith-based nursing parish programs for state funding. In October 2004, the Federal District Court for the District of Montana held that the state's "direct and preferential funding of inherently and pervasively religious parish nursing programs was undertaken for the impermissible purpose, and has the impermissible effect, of favoring and advancing the integration of religion into the provision of secular health care services." According to the court, the state funding of faith-based healthcare violated the First Amendment. In April 2006, the FFRF sued to challenge the pervasive integration of "spirituality" into health care by the Department of Veteran Affairs. Specifically stating that the practice of asking patients about their religion in spiritual assessments, the use of chaplains to treat patients, and drug and alcohol treatment programs that incorporate religion violated the separation of state and church. The case was later dismissed after the Hein decision because of lack of standing. Education In 2001, the FFRF, on behalf of anonymous plaintiffs, sued the Rhea County School District. The plaintiffs alleged that weekly bible classes were being held for all students in the elementary schools. In June 2004, the Sixth Circuit Court of Appeals affirmed a district judgment holding that it was unconstitutional for the school district to "teach the Bible as literal truth" to students. In March 2005, the FFRF filed suit against the University of Minnesota because of its involvement with the Minnesota Faith Health Consortium, a partnership with Luther Seminary, which is affiliated with the Evangelical Lutheran Church of America, and Fairview Health Services, stating that state taxpayer funds are helping to fund a faith-based organization. In September 2005, the University agreed to end the partnership and to cease teaching "courses on the intersection of faith and health", with the FFRF agreeing to drop its lawsuit. In April 2005, the FFRF filed a lawsuit against the U.S. Department of Education because of its distribution of funds to the Alaska Christian College, a Bible college run by the Evangelical Covenant Church of Alaska. The foundation stated that in the students' first year at the college, they take only religious-based courses, and finish that year with a Certificate of Biblical Studies. The college, the foundation says, "does not offer traditional college courses, such as math or English". In October 2005 the FFRF and the U.S. Department of Education settled the lawsuit, with the Department of Education agreeing to suspend the $435,000 federal grant from 2005 and unspent portions of grants from the previous year. A December 2020 article by Hemant Mehta outlined recent FFRF efforts. FFRF argues to limit official role of Pastor Mark Thornton at Boise State. A letter sent by the FFRF Staff Attorney Chris Line included: "Boise State football players have no government-imposed burden on their religion, so there is no need – or legitimate legal reason – for Boise State to provide a chaplain for them." Legal Counsel for the University responded with the following: "We have been in communication with the Athletic Department to provide some education about this issue and to ensure measures are taken now and in the future to resolve the issue and establish appropriate constitutional boundaries. Mr. Thornton did not travel with the football team to our recent game in Wyoming and the university will no longer include a chaplain in its travel party. Written references to Mr. Thornton as the chaplain of the football team have been or are in the process of being removed and no future references will be made in writing or otherwise." Mehta continues: "None of that means students can't seek Thornton out on their own. They’ve always been free to do that. But Thornton can't – and shouldn't – have any sort of official role there." Criminal justice programs In October 2000, the FFRF brought suit, as taxpayers in the state of Wisconsin, against Faith Works located in Milwaukee. Their case stated that a faith-based addiction-treatment program should not be used as a court-ordered treatment program using taxpayer funds. In January 2002, the ruling was decided in the FFRF's favor; that receiving hundreds of thousands of dollars in public money is in violation of the Establishment Clause. The judge wrote "Because I find that the Department of Workforce Development's grant to Faith Works constitutes unrestricted, direct funding of an organization that engages in religious indoctrination, I conclude that this funding stream violates the establishment clause." On Appeal, in April 2003, the Seventh Circuit later ruled against the FFRF on the narrower issue of whether prisoners joining specific faith-based programs on their own free will are coerced by government endorsement of religion. The FFRF brought a suit against the awarding of a federal grant to MentorKids USA, a group providing mentors to children of prisoners, alleging that only Christian mentors were hired and that they were to give monthly reports on the children's religious activities. In January 2005, the court vacated HHS's funding of this group citing "federal funds have been used by the MentorKids program to advance religion in violation of the Establishment Clause". In May 2006, the FFRF filed suit against the Federal Bureau of Prisons alleging that its decision to fund not only multi-faith-based but also single-faith-based programs violated constitutional standards for separation of state and church. The parties later agreed to a dismissal of that claim, but additional counts within the lawsuit, alleging separate violations, continued. Religion in the public sphere Employment issues In 1995, the FFRF sued the state of Wisconsin for designating Good Friday as a state legal holiday. In 1996, the federal district court ruled that Wisconsin's Good Friday holiday was indeed a First Amendment violation because, in reference to Wisconsin's Good Friday holiday law, the "promotion of Christianity is the primary purpose of the law." Public funding FFRF opposed the city of Versailles, Kentucky helping a church get federal funding to create a local disaster relief center. The FFRF is filling a lawsuit on behalf of four residents against the state of South Carolina to oppose the funding to Christian Learning Centers of Greenville County to build a private religious school, and the FFRF is challenging that it is unconstitutional. Religious displays on public property In December 2007, the FFRF, on behalf of a group of concerned Green Bay residents and invoking the First Amendment rights of all of the city's residents, sued the city because of the placement of a nativity scene at Green Bay's city hall. Before the case was heard, the city removed the nativity scene. The judge then dismissed the suit, citing lack of jurisdiction. Since the nativity scene already was removed and a moratorium imposed on future such displays, there remained no basis for continued dispute. He went on to say, "the plaintiffs have already won. ... the Plaintiffs have won a concrete victory that changes the circumstances on the ground." In 2011, in response to the refusal of the city of Warren, Michigan, to remove a nativity display in the civic center, the FFRF sought to place a winter solstice display. The mayor refused the request and the FFRF brought suit. The suit was dismissed by Judge Zatkoff of the U.S. District Court; the dismissal was upheld by the U.S. 6th Circuit Court in 2013. In September 2011, the FFRF, along with the American Civil Liberties Union (ACLU), sued the Giles County, Virginia, school district on behalf of anonymous plaintiffs. A display of the Ten Commandments had been placed beside a copy of the U.S. Constitution at Giles County public schools. Prior to the suit, in January and June 2011, the FFRF and the ACLU had sent letters to the school board requesting removal of the display. The school superintendent ordered that the displays of the Ten Commandments be removed. The Giles County school board met in June 2011 and voted to overturn the superintendent's decision to remove the display. After the suit was filed, the school board in 2012 agreed to remove the display and to pay attorneys' fees. In November 2011, Wisconsin Governor Scott Walker referred to the Capitol's Christmas tree as a "Christmas tree" instead of a "holiday tree". The FFRF, which opposed prior efforts to restore the name to "Christmas tree" objected to the title. In May 2012, the FFRF, acting on a complaint from a resident, asked the city of Woonsocket, Rhode Island, to remove a Latin cross from a World War I and II memorial on public land. The city refused to do so. The FFRF states that it is currently looking for a plaintiff in the area to represent for a suit, which the FFRF have yet to do, citing the difficulty with another case that occurred with another plaintiff in the state, Jessica Ahlquist, in the case Ahlquist v. Cranston. On July 24, 2012, after receiving a letter from the FFRF, the Steubenville, Ohio, city council decided to remove the image of the Christ the King Chapel at the Franciscan University of Steubenville from its town logo. In August 2012, the FFRF, on behalf of a resident, threatened a lawsuit challenging a Latin cross that had been displayed on top of the water tower of Whiteville, Tennessee. After the FFRF wrote three initial letters, but before the lawsuit was filed, the town removed one arm of the cross. The removal cost the town $4,000, and as part of the settlement the town paid $20,000 in the FFRF's attorneys fees. The town also agreed never to replace the missing arm and not to place other crosses on public property. In August 2012, the FFRF, on behalf of a Montana resident, sued the United States Forest Service. A special use permit for the placement of a statue of Jesus on federal land was granted in 1954 at the request of the Knights of Columbus. The Forest Service continued to grant renewals of the permit until 2010. When the Service declined to renew, the Knights declined to remove the statue citing "tradition" and the "historical" value of the statue. After on-line protests the statue was allowed to stay and the permit granted. The FFRF filed suit in February 2012. In June 2013, a federal judge found in favor of the defendants, allowing the statue to remain. In August 2013, the FFRF filed an appeal of the decision. The Ninth Circuit Court of Appeals rejected FFRF's arguments and upheld the memorial. In 2012, the FFRF wrote several letters to Prudhommes Restaurant, in Columbia, Pennsylvania, explaining that offering a 10% discount to Sunday patrons who present a church bulletin is a violation of state and federal law, specifically the Civil Rights Act of 1964. The individual who brought the matter to the FFRF's attention has filed a discrimination complaint with the Pennsylvania Human Relations Commission. The FFRF was only involved in an advisory capacity. The Pennsylvania Human Relations Commission entered a final order allowing the restaurant to continue the church bulletin discount. A lighted cross in a public park in Honesdale, Pennsylvania, was removed by the borough in 2018 after complaints from FFRF. Not far from the park a solar-powered 28-foot cross was erected by a local resident on his own property. Prayer in government/schools In October 2008, the FFRF filed suit against the U.S. government over the statute establishing the National Day of Prayer (NDoP). In 2010, Federal judge Barbara Brandriff Crabb ruled it unconstitutional as it is "an inherently religious exercise that serves no secular function". This ruling was appealed by the U.S. government. In April 2011, the U.S. Seventh Circuit Court of Appeals dismissed the FFRF's challenge to the NDoP, holding that the FFRF did not have standing to challenge the NDoP statute or proclamations and that only the President was injured enough to challenge the NDoP statute. The FFRF, in January 2013, after receiving a complaint from a resident, asked the city council of Rapid City, South Dakota, to eliminate its practice of beginning each city council meeting with a Christian prayer. After the FFRF sent a second letter in February 2013, the mayor stated at that time that prayers would continue. Joseph Richardson, of Lake County, Florida, delivered a secular invocation on behalf of "non-religious citizens" at a county commission meeting on Tuesday, December 6, 2022. Following the invocation the director of Lake County Public Works Fred Schneider took the microphone and delivered a prayer at the request of the Lake County Commissioner Sean Parks. Christopher Line, an FFRF attorney, wrote a letter to the Lake County Commission Chairman Kirby Smith stating, "This Christian prayer, delivered because the invocation Mr. Richardson gave was not sufficiently Christian, was discriminatory, unconstitutional, and a slap in the face to all of Lake County's non-Christian citizens. [...] as long as the board continues to allow citizens to deliver invocations to begin its meetings, it must treat all invocations the same, with no ‘corrective’ Christian prayer offered after a non-Christian prayer has finished." Sean Parks stated that he was, "saddened to hear that Mr. Richardson felt he was mistreated during the invocation" and "We would welcome them [the Central FL Freethought Community] back if they wish to lead the invocation in the future." In 2022, while Samuel Felinton was a high school junior, he was forced to attend a Christian revival while attending Huntington High School. He was obliged to stay and watch the assembly despite attempting to leave. This assembly would later make international news, by causing a multi-day walkout alongside Max Nibert and himself, including hundreds of their peers. In 2023, Felinton, alongside other parents, students, and the Freedom From Religion Foundation, settled a lawsuit against the Cabell County Board of Education to implement a ban on teacher-run religious events being held within school hours on campus. Internal Revenue Service Parish exemption The FFRF filed suit against the IRS over the parish exemption that allows "ministers of the gospel" to claim part of their salary as an income-tax-free housing allowance. This was originally filed in 2009, in California, then subsequently dropped and re-filed in 2011, in Wisconsin, because of standing. In August 2012, a federal judge stated that the suit could go forward. In August 2013, the Justice Department argued that leaders of an atheist group may qualify for the parish exemption. Gaylor states "this is not what we are after", going on to say that the government should not give religious groups any special treatment. On November 21, 2013, a federal judge ruled in the FFRF's favor. In January 2014, the Department of Justice filed an appeal in federal court. In November 2014, the U.S. Court of Appeals for the Seventh Circuit issued its decision, concluding that the federal tax code provision that treats church-provided housing allowances to ministers as income tax-free must stand. Electioneering In November 2012, the FFRF filed a lawsuit against the IRS for not enforcing its own electioneering laws. The FFRF cited in its suit the placement of full-page ads by the Billy Graham Evangelistic Association; the diocese requiring priests to read a statement urging Catholics to vote; and the institution of "Pulpit Freedom Sunday". The group claimed that not enforcing the federal tax codes that prohibit tax-exempt religious organizations from electioneering is a violation of the First Amendment of the Constitution. The group stated that the increasing involvement of religious institutions in politics was "blatantly and deliberately flaunting the electioneering restrictions". The IRS had filed a motion to dismiss in federal court, but in August 2013 it was decided that the lawsuit could proceed stating that the FFRF "has standing to seek an order requiring the IRS to treat religious organizations no more favorably than it treats the Foundation". In 2014, the federal judge dismissed the lawsuit after the parties reached an agreement. State capitol signs Florida In December 2013, the FFRF was permitted to hang a banner at the capitol after a nativity scene was placed by a private group. Illinois On December 23, 2009, William J. Kelly, conservative activist and candidate for Illinois Comptroller, attempted to remove a FFRF sign at a Christmas display. The case was dismissed on several grounds, including that the lawsuit ran afoul of the First Amendment prohibition against content-based discrimination and that the plaintiff's rights had not been violated. Washington A plaque with the same text as the Wisconsin State Capitol sign was displayed for the 2008 Christmas season at the state capitol in Olympia, Washington, next to a nativity scene. The sign was stolen and then later found and returned to the state capitol. The addition of the sign incited a large number of individuals and groups to request other additions, such as a Festivus pole, a request by the Westboro Baptist Church for a sign stating "Santa Claus will take you to hell" (among other things), a sign paying homage to the Flying Spaghetti Monster, and many others. Wisconsin The FFRF maintains a sign in the Wisconsin State Capitol during the Christmas season, which reads: At this season of THE WINTER SOLSTICE may reason prevail. There are no gods, no devils, no angels, no heaven or hell. There is only our natural world. Religion is but myth and superstition that hardens hearts and enslaves minds. In 2013, a natural nativity featuring Charles Darwin, Albert Einstein and Mark Twain as the three wise men, the Statue of Liberty and an astronaut as angels and an African American girl baby doll to represent that "humankind was birthed in Africa" was added. Texas State Capitol In 2015, the FFRF applied to put a "secular Nativity" scene in the Texas State Capitol. The scene featured the Bill of Rights, three Founding Fathers, and the Statue of Liberty and a sign that wished everyone a "Happy Winter Solstice". The then governor of Texas, Greg Abbott, demanded it be removed. Following a series of legal challenges, in 2018, a three-judge panel of the Fifth Circuit Court of Appeals ruled that FFRF's rights were violated. The Court also vacated the ruling of the trial court and sent the case back for consideration of FFRF's request for an injunction. Rhode Island In 2013, the FFRF was allowed to place a sign in the rotunda, after complaints from its members, as a response to the crèches and other religious symbols that are already in place at the statehouse. Dayton, Tennessee On July 14, 2017, a statue of Clarence Darrow was unveiled in Dayton, Tennessee, on the Rhea County Courthouse lawn, funded by a $150,000 donation from the FFRF. The courthouse was the site of the historic 1925 Scopes Monkey Trial wherein Darrow unsuccessfully defended a teacher, John T. Scopes, who was found guilty of teaching evolution in a public school in violation of what was then a Tennessee state law. The statue was placed just a few feet away from a statue of William Jennings Bryan, Darrow's creationist opponent in the trial, which had been erected in 2005 by nearby Bryan College. Athens, Texas In 2011, the FFRF filed a letter of complaint regarding the placement of a nativity scene on Henderson County courthouse property. After it was decided that the nativity scene would remain, the FFRF petitioned to have its own banner placed on the site, but county officials declined to discuss its placement. The FFRF banner was placed without permission on the courthouse property, but was soon removed. The banner stated: "At this season of the winter solstice, let reason prevail. There are no Gods, no devils, no angels, no Heaven or Hell. There is only our natural world. Religion is but a myth and superstition that hardens hearts and enslaves minds". In April 2012, the county judge denied FFRF's request to place the same banner on the courthouse property. Events and activities FFRF has held conventions since 1977, one year after the group formed and one year prior to its official incorporation. Conventions have included speakers such as Christopher Hitchens, awards presented to recognize contributions to the advancement of the freethought community, FFRF held NonPrayer Breakfasts, with what it described as moments of bedlam instead of moments of silence, and piano music by FFRF co-president Dan Barker. The Emperor Has No Clothes Award has been awarded by FFRF since 1999 in recognition of what it called "plain speaking" on the shortcomings of religion by public figures. Past recipients include: Finances In 2013, Charity Navigator gave FFRF a four-star rating and reported that FFRF had revenues of US$3,878,938, with a net surplus (after expenses) of $1,715,563 and net assets of $11,519,770. Officer compensation for the "co-presidents", husband and wife Dan Barker and Annie Laurie Gaylor, was $88,700 and $86,500 ($175,200 combined) or approximately 10% of the net surplus. See also References External links Official website "Freedom From Religion Foundation". Internal Revenue Service filings. ProPublica Nonprofit Explorer. Dream sharing is the process of documenting or discussing both night and daydreams with others. Dreams are novel but realistic simulations of waking social life. One of the primary purposes of sharing dreams is entertainment. Dream sharing is a strategy that tests and strengthens the bond between people. A dream can be described as a calculated social interaction and a way to bring individuals closer together. Individuals choose to share dreams with those that they know well or want to know well. Dreams are a common denominator amongst humans of all nations and cultures. Increasing the rate of discussion regarding dreams leads to more understanding about the personality of someone otherwise difficult to connect with due to language or cultural barriers. Demographics Currently, dream sharing is more prevalent in certain demographics. Women are found to share and discuss dreams and nightmares more frequently than men. During this discovery, dream and nightmare recall were controlled to be proportional frequencies across the two sexes, signifying that the differences in dream sharing were not due to biological dream factors such as memory, but from the stigma around men sharing personal thoughts with each other. Men remain independent and reject the need for social support. Often these traits are developed from male social stigma. Personality traits such as openness and extraversion were also positively correlated with dream-sharing frequency. Relationships When couples talk about their dreams with each other, it seems to be linked to feeling closer in their relationship. In other words, the more they share their dreams, the stronger their sense of intimacy. This suggests that open communication about dreams may contribute to a deeper connection between romantic partners. Engaging in conversations about dreams enhances the levels of empathy the listener feels toward the dreamer and also fosters a deeper connection by investigating the intricate landscapes of the subconscious mind. As individuals share their dreams, a unique window into their thoughts, emotions, and aspirations opens up, creating a rich tapestry of understanding. This exchange of dreams can cultivate empathy by providing insight into the dreamer's inner world, fostering a more profound appreciation for their experiences and perspectives. Stress relief Dream sharing is also associated with stress relief. The relationship between dreams and stress relief is complex and can vary from person to person. A few ways in which dreaming and sharing dreams might contribute to stress relief are emotional processing, catharsis, symbolic exploration, social connection, and mindfulness and relaxation. History The sharing of dreams dates back at least as far as 4000-3000 BC in permanent form on clay tablets. In ancient Egypt, dreams were among the items recorded in the form of hieroglyphics. In ancient Egyptian culture dream sharing had a religious context as priests doubled as dream interpreters. Those whose dreams were especially vivid or significant were thought to be blessed and were given special status in these ancient societies. Likewise, people who were able to interpret dreams were thought to receive these gifts directly from the gods, and they enjoyed a special status in society as well. The respect for dreams changed radically early in the 19th century, and dreams in that era were often dismissed as reactions to anxiety, outside noises or even bad food and indigestion. During this period of time, dreams were thought to have no meaning at all, and interest in dream interpretation all but evaporated. This all changed, however, with the arrival of Sigmund Freud later in the 19th century. Freud stunned the world of psychiatry by stressing the importance of dreams, and he revived the once dead art of dream interpretation. Freud's interpretation Freud represented the view that in order to understand one's unconscious, dreams are to be dissected and discussed. See also Dream diary Dream interpretation References == Further reading == Birthright Armenia, also known as Depi Hayk (Armenian: Դեպի Հայք), is a volunteer internship enhancement program for diaspora Armenians that also offers travel reimbursements to eligible participants to assist in the development of Armenia. To volunteer with Birthright Armenia, the applicant has to be of Armenian heritage (at least one grandparent must be fully Armenian), between the ages of 21 and 32 years, and must have graduated from secondary school. The program operates year-round. Organizational background Birthright Armenia was established in 2003 by Edele Hovnanian who realized that every Armenian should have the opportunity to immerse themselves in Armenia to understand its people, culture, and struggles. The organization's goals include strengthening ties between Armenia and the Armenian Diaspora youth representatives by providing them an opportunity to take part in the daily life of fellow Armenians. In 2007, the organization expanded its presence by opening an office in Gyumri. By 2012, in order to support the transition for alumni who move to Armenia, the "Pathway to Armenia program" was launched, which provided support with lodging and employment search. In 2013, Birthright Armenia further extended its reach by opening an office in Vanadzor. In 2015, the program celebrated a significant milestone by reaching 1,000 participants. Program Participants commit to a minimum of 30 hours of weekly volunteering or a university internship while given the choice of residing with local host families. The organization offers a variety of volunteer opportunities among 1,300+ partner organizations across sectors such as architecture, marketing, tourism, healthcare, agriculture, IT, and business. The program also includes weekly excursions, in-office Armenian language classes, and weekly forums with guest speakers who talk about different topics regarding Armenia. Eligibility To participate, applicants must have at least one grandparent of Armenian ethnicity. They must also be aged between 21 and 32, having graduated from high school. Applicants born in Armenia must have left Armenia before the age of 15 and lived abroad for at least 10 years. Applicants who have primarily resided in Armenia for longer than the last three years are not eligible. Participants must agree to a minimum 9 week stay. Maximum program duration is one year. For the shorter term, there is a subsection of the program called Birthright Lite. Birthright Lite The organization has another option for full-time working professionals, regardless of age, who have limited time away from their jobs. They can apply to volunteer for four to nine weeks. Birthright Lite participants pay for travel, lodging, or host family costs. Alumni Birthright Armenia has an alumni network of more than 3000 individuals from 57 countries, 300 of which have repatriated to Armenia. The organization supports alumni initiatives through grants, which enable alumni to make positive impacts both in Armenia and around the world. Also, Birthright Armenia supports those who want to stay in Armenia after completing the program. Alumni can receive free temporary housing, as well as employment support, regular consultations with staff members, and access to a network of experienced repatriates. Similar organizations Birthright Armenia was inspired by Birthright Israel, which offers educational and cultural exchange opportunities related to Jewish history and culture. Another similar project is Birthright Greece, aimed at the Greek Diaspora. See also Armenian Volunteer Corps Diaspora tourism Genealogy tourism Repat Armenia Repatriation of Armenians References External links Official website Birthright Armenia - Subject of USC case study "Moving Forward" Volunteerism in The Homeland, Part I: Redefining Service: Philanthropist Edele Hovnanian A father is the male parent of a child. Besides the paternal bonds of a father to his children, the father may have a parental, legal, and social relationship with the child that carries with it certain rights and obligations. A biological father is the male genetic contributor to the creation of the infant, through sexual intercourse or sperm donation. A biological father may have legal obligations to a child not raised by him, such as an obligation of monetary support. An adoptive father is a man who has become the child's parent through the legal process of adoption. A putative father is a man whose biological relationship to a child is alleged but has not been established. A stepfather is a non-biological male parent married to a child's preexisting parent and may form a family unit but generally does not have the legal rights and responsibilities of a parent in relation to the child. The adjective "paternal" refers to a father and comparatively to "maternal" for a mother. The verb "to father" means to procreate or to sire a child from which also derives the noun "fathering". Biological fathers determine the sex of their child through a sperm cell which either contains an X chromosome (female), or Y chromosome (male). Related terms of endearment are dad (dada, daddy), baba, papa, pappa, papasita, (pa, pap) and pop. A male role model that children can look up to is sometimes referred to as a father-figure. Responsible and positive parenting In today's world, the terms responsible parenting and positive parenting are often used. UNICEF distinguishes the term positive parenting. Positive parenting is parenting that creates an environment conducive to child development that prioritizes healthy parent-child relationships. Responsible parenting is parenting that implies the fulfillment of the functions assigned to them by parents and is manifested in individual and social aspects, includes raising children, as well as taking into account the stage before the birth of a child, maintaining family relations with already adult children. Paternal rights The paternity rights of a father with regard to his children differ widely from country to country, often reflecting the level of involvement and roles expected by that society. Unlike motherhood, fatherhood is not mentioned in Universal Declaration of Human Rights. Paternity leave Parental leave is when a father takes time off to support his newly born or adopted baby. Paid paternity leave first began in Sweden in 1976, and is paid in more than half of European Union countries. In the case of male same-sex couples the law often makes no provision for either one or both fathers to take paternity leave. Child custody Fathers' rights movements, such as Fathers 4 Justice, argue that family courts are biased against fathers. Child support Child support is an ongoing periodic payment made by one parent to the other; it is normally paid by the parent who does not have custody. Paternity fraud An estimated 2% of British fathers experiences paternity fraud during a non-paternity event, bringing up a child they wrongly believe to be their biological offspring. Role of the father In almost all cultures, fathers are regarded as secondary caregivers. This perception is slowly changing with more and more fathers becoming primary caregivers while mothers go to work, or in single parenting situations and male same-sex parenting couples. Fatherhood in the Western World In the West, the image of the married father as the primary wage-earner is changing. The social context of fatherhood plays an important part in the well-being of men and their children. In the United States 16% of single parents were men as of 2013. Importance of father or father-figure Involved fathers offer developmentally specific provisions to their children and are impacted themselves by doing so. Active father figures may play a role in reducing behavior and psychological problems in young adults. An increased amount of father–child involvement may help increase a child's social stability, educational achievement,: 5 and their potential to have a solid marriage as an adult. Their children may also be more curious about the world around them and develop greater problem-solving skills. Children who were raised with fathers perceive themselves to be more cognitively and physically competent than their peers without a father. Mothers raising children together with a father reported less severe disputes with their child. The father-figure is not always a child's biological father, and some children will have a biological father as well as a step- or nurturing father. When a child is conceived through sperm donation, the donor will be the "biological father" of the child. Fatherhood as legitimate identity can be dependent on domestic factors and behaviors. For example, a study of the relationship between fathers, their sons, and home computers found that the construction of fatherhood and masculinity required that fathers display computer expertise. Determination of parenthood Roman law defined fatherhood as "Mater semper certa; pater est quem nuptiae demonstrant" ("The [identity of the] mother is always certain; the father is whom the marriage vows indicate"). The recent emergence of accurate scientific testing, particularly DNA testing, has resulted in the family law relating to fatherhood experiencing rapid changes. History of fatherhood Many male animals do not participate in the rearing of their young. The development of human men as creatures which are involved in their offspring's upbringing took place during the stone age. In medieval and most of modern European history, caring for children was predominantly the domain of mothers, whereas fathers in many societies provide for the family as a whole. Since the 1950s, social scientists and feminists have increasingly challenged gender roles in Western countries, including that of the male breadwinner. Policies are increasingly targeting fatherhood as a tool of changing gender relations. Research from various societies suggest that since the middle of the 20th century fathers have become increasingly involved in the care of their children. Patricide In early human history there have been notable instances of patricide. For example: Tukulti-Ninurta I (r. 1243–1207 B.C.E.), Assyrian king, was killed by his own son after sacking Babylon. Sennacherib (r. 704–681 B.C.E.), Assyrian king, was killed by two of his sons for his desecration of Babylon. King Kassapa I (473 to 495 CE) creator of the Sigiriya citadel of ancient Sri Lanka killed his father king Dhatusena for the throne. Emperor Yang of Sui in Chinese history allegedly killed his father, Emperor Wen of Sui. Beatrice Cenci, Italian noblewoman who, according to legend, killed her father after he imprisoned and raped her. She was condemned and beheaded for the crime along with her brother and her stepmother in 1599. Lizzie Borden (1860–1927) allegedly killed her father and her stepmother with an axe in Fall River, Massachusetts, in 1892. She was acquitted, but her innocence is still disputed. Iyasus I of Ethiopia (1654–1706), one of the great warrior emperors of Ethiopia, was deposed by his son Tekle Haymanot in 1706 and subsequently assassinated. In more contemporary history there have also been instances of father–offspring conflicts, such as: Chiyo Aizawa (born 1939) murdered her own father who had been raping her for fifteen years, on October 5, 1968, in Japan. The incident changed the Criminal Code of Japan regarding patricide. Kip Kinkel (born 1982), an Oregon boy who was convicted of killing his parents at home and two fellow students at school on May 20, 1998. Sarah Marie Johnson (born 1987), an Idaho girl who was convicted of killing both parents on the morning of September 2, 2003. Dipendra of Nepal (1971–2001) reportedly massacred much of his family at a royal dinner on June 1, 2001, including his father King Birendra, mother, brother, and sister. Christopher Porco (born 1983), was convicted on August 10, 2006, of the murder of his father and attempted murder of his mother with an axe. Terminology Biological fathers Baby Daddy – a biological father who bears financial responsibility for a child, but with whom the mother has little or no contact. Birth father – the biological father of a child who, due to adoption or parental separation, does not raise the child or cannot take care of one. Biological father – or sometimes simply referred to as "Father" is the genetic father of a child. Posthumous father – father died before children were born (or even conceived in the case of artificial insemination). Putative father – unwed man whose legal relationship to a child has not been established but who is alleged to be or claims that he may be the biological father of a child. Sperm donor – an anonymous or known biological father who provides his sperm to be used in artificial insemination or in vitro fertilisation in order to father a child for a third-party female. Also used as a slang term meaning "baby daddy". Surprise father – where the men did not know that there was a child until possibly years afterward Teenage father/youthful father – father who is still a teenager. Non-biological (social and legal relationship) Adoptive father – the father who has adopted a child Cuckolded father – where the child is the product of the mother's adulterous relationship DI Dad – social/legal father of children produced via Donor Insemination (where a donor's sperm were used to impregnate the DI Dad's partner) Father-in-law – the father of one's spouse Foster father – child is raised by a man who is not the biological or adoptive father Mother's partner – assumption that current partner fills father role Mother's husband – under some jurisdictions (e.g., in Quebec civil law), if the mother is married to another man, the latter will be defined as the father Presumed father – where a presumption of paternity has determined that a man is a child's father regardless of if he actually is or is not the biological father Social father – where a man takes de facto responsibility for a child, such as caring for one who has been abandoned or orphaned (the child is known as a "child of the family" in English law) Stepfather – a married non-biological father where the child is from a previous relationship Fatherhood defined by contact level Absent father – father who cannot or will not spend time with his child(ren) Second father – a non-parent whose contact and support is robust enough that near parental bond occurs (often used for older male siblings who significantly aid in raising a child, sometimes for older men who took care of younger friends (only males) who have no families) Stay-at-home dad – the male equivalent of a housewife with child, where his spouse is breadwinner Weekend/holiday father – where child(ren) only stay(s) with father on weekends, holidays, etc. Non-human fatherhood For some animals, it is the fathers who take care of the young. Darwin's frog (Rhinoderma darwini) fathers carry eggs in the vocal pouch. Most male waterfowl are very protective in raising their offspring, sharing scout duties with the female. Examples are the geese, swans, gulls, loons, and a few species of ducks. When the families of most of these waterfowl travel, they usually travel in a line and the fathers are usually the ones guarding the offspring at the end of the line while the mothers lead the way. The female seahorse (Hippocampus) deposits eggs into the pouch on the male's abdomen. The male releases sperm into the pouch, fertilizing the eggs. The embryos develop within the male's pouch, nourished by their individual yolk sacs. Male catfish keep their eggs in their mouth, foregoing eating until they hatch. Male emperor penguins alone incubate their eggs; females do no incubation. Rather than building a nest, each male protects his egg by balancing it on the tops of his feet, enclosed in a special brood pouch. Once the eggs are hatched, the females will rejoin the family. Male beavers secure their offspring along with the females during their first few hours of their lives. As the young beavers mature, their fathers will teach them how to search for materials to build and repair their own dams, before they disperse to find their own mates. Wolf fathers help feed, protect, and play with their pups. In some cases, several generations of wolves live in the pack, giving pups the care of grandparents, aunts/uncles, and siblings, in addition to parents. The father wolf is also the one who does most of the hunting when the females are securing their newborn pups. Coyotes are monogamous and male coyotes hunt and bring food to their young. Dolphin fathers help in the care of the young. Newborns are held on the surface of the water by both parents until they are ready to swim on their own. A number of bird species have active, caring fathers who assist the mothers, such as the waterfowls mentioned above. Apart from humans, fathers in few primate species care for their young. Those that do are tamarins and marmosets. Particularly strong care is also shown by siamangs where fathers carry infants after their second year. In titi and owl monkeys fathers carry their infants 90% of the time with "titi monkey infants developing a preference for their fathers over their mothers". Silverback gorillas have less role in the families but most of them serve as an extra protecting the families from harm and sometimes approaching enemies to distract them so that his family can escape unnoticed. Many species, though, display little or no paternal role in caring for offspring. The male leaves the female soon after mating and long before any offspring are born. It is the females who must do all the work of caring for the young. A male bear leaves the female shortly after mating and will kill and sometimes eat any bear cub he comes across, even if the cub is his. Bear mothers spend much of their cubs' early life protecting them from males. (Many artistic works, such as advertisements and cartoons, depict kindly "papa bears" when this is the exact opposite of reality.) Domesticated dog fathers show little interest in their offspring, and unlike wolves, are not monogamous with their mates and are thus likely to leave them after mating. Male lions will tolerate cubs, but only allow them to eat meat from dead prey after they have had their fill. A few are quite cruel towards their young and may hurt or kill them with little provocation. A male who kills another male to take control of his pride will also usually kill any cubs belonging to that competing male. However, it is also the males who are responsible for guarding the pride while the females hunt. However, the male lions are the only felines that actually have a role in fatherhood. Male rabbits generally tolerate kits but unlike the females, they often show little interest in the kits and are known to play rough with their offspring when they are mature, especially towards their sons. This behaviour may also be part of an instinct to drive the young males away to prevent incest matings between the siblings. The females will eventually disperse from the warren as soon as they mature but the father does not drive them off like he normally does to the males. Horse stallions and pig boars have little to no role in parenting, nor are they monogamous with their mates. They will tolerate young to a certain extent, but due to their aggressive male nature, they are generally annoyed by the energetic exuberance of the young and may hurt or even kill the young. Thus, stud stallions and boars are not kept in the same pen as their young or other females. Finally, in some species neither the father nor the mother provides any care. This is true for most insects, reptiles, and fish. See also Father complex Fathers' rights movement Father's Day Mother Nuclear family Paternal age effect Paternal bond Putative father Putative father registry Patriarch Patricide Parenting Responsible fatherhood Shared Earning/Shared Parenting Marriage Sociology of fatherhood Sky father Single parent "Father" can also refer metaphorically to a person who is considered the founder of a body of knowledge or of an institution. In such context the meaning of "father" is similar to that of "founder". See List of persons considered father or mother of a field. Further reading Elizabeth Preston (27 Jun 2021). "The riddle of how humans evolved to have fathers". Knowable Magazine / BBC.com. References Bibliography Inhorn, Marcia C.; Chavkin, Wendy; Navarro, José-Alberto, eds. (2015). Globalized fatherhood. New York: Berghahn. ISBN 9781782384373. Studies by anthropologists, sociologists, and cultural geographers - Kraemer, Sebastian (1991). "The Origins of Fatherhood: An Ancient Family Process". Family Process. 30 (4): 377–392. doi:10.1111/j.1545-5300.1991.00377.x. PMID 1790784. Diamond, Michael J. (2007). My father before me : how fathers and sons influence each other throughout their lives. New York: W.W. Norton. ISBN 9780393060607. Collier, Richard (2013). "Rethinking men and masculinities in the contemporary legal profession: the example of fatherhood, transnational business masculinities, and work-life balance in large law firms". Nevada Law Journal. 13 (2): 7. In society, the term low culture identifies the forms of popular culture that have mass appeal, often broadly appealing to the middle or lower cultures of any given society. This is in contrast to the forms of high culture that appeal to a smaller, often upper-class proportion of the populace. Culture theory proposes that both high culture and low culture are subcultures within a society, because the culture industry mass-produces each type of popular culture for every socioeconomic class. Despite being viewed as characteristic of less-educated social classes, low culture is still often enjoyed by upper classes as well. This makes the content that falls under this categorization the most broadly consumed kind of media in a culture overall. Various forms of low culture can be found across a variety of cultures, with the physical objects composing these mediums often being constructed from less expensive, perishable materials. The phrase low culture has come to be viewed by some as a derogatory idea in and of itself, existing to put down elements of pop or tribal culture that others may deem to be "inferior." Standards and definitions In Popular Culture and High Culture: An Analysis and Evaluation of Taste (1958), Herbert J. Gans said: Aesthetic standards of low culture stress substance, form being totally subservient, and there is no explicit concern with abstract ideas or even with fictional forms of contemporary social problems and issues. . . . Low culture emphasizes morality, but limits itself to familial and individual problems and [specific] values, which apply to such problems. Low culture is content to depict traditional working class values winning out over the temptation to give in to conflicting impulses and behavior patterns.In other words, low culture is often associated with media that presents smaller-scale or individual experiences that are easier for the general public to identify with. History Physical artifacts from low culture are normally cheaply and often crudely made, as well as often small, in contrast to the comparatively grand public art or luxury objects of high culture. While this is a necessity for this low culture media to be broadly disseminated, it has also contributed to its reputation as low-brow or of lesser merit. The cheapness of the materials, many of which are perishable, generally means that their survival and preservation in modern times is rare. There are exceptions, especially in pottery and graffiti on stone. An ostracon is a small piece of pottery (or sometimes stone) which has been written on, for any of a number of purposes, among which curse tablets or more positive magical spells such as love magic are common. Wood must have been a common material, but survives for long periods only in certain climatic conditions, such as Egypt and other very arid areas, and permanently wet and slightly acidic peat bogs. Once printing (and paper) became relatively cheap, popular prints became increasingly widespread by the late Renaissance. This technology also allowed for the production of cheap texts in street literature such as broadsides and broadside ballads, typically new topical words to a familiar tune. These examples became extremely common, but were treated as ephemera, so survival of this material is relatively uncommon. Folk music is another notable historical manifestation of low culture. Much traditional folk music was only written down, and later mechanically recorded, in the 19th century, as growing nationalist sentiments in many countries generated interest from middle class enthusiasts. In comparison to other forms of music, such as music written for orchestras or by well-known classical composers, folk music was considered a product of low culture given its association with the more popular, cruder tastes of those who created it. This social separation between folk and classical music was also influenced by the traditions and expectations followed by the latter, which was often written for use in religious settings that demanded certain consistencies in the musical structure. Instead, folk music (along with its successor, contemporary folk music) is thought of as a reflection of common themes present in its community of origin. These combined traits help define folk music as an early, widespread form of low culture, for which the lower/working classes were both the largest producers and consumers. Modern day The phrase trash culture began to enter the public lexicon in the 1980s as a classification for these kinds of recent low-cultural expressions. This kind of content is often considered to be either vulgar, in poor taste, or lacking in-depth artistic merit. With the explosion of tabloid journalism and sensationalistic reality television throughout the late 20th century, many modern artists such as Brett Easton Ellis would use these works as inspirations to bridge the gap between the confines of high and low culture. The result of this on his work in particular has been media that could belong to either categorization based on the grotesque nature of his works content mixed with the depth more characteristic of other high-brow works. Culture as social class Each social class possesses its own versions of high and low culture, the definition and content of which are determined by the socioeconomic and educational particulars of the people who compose said social class. This falls in line with the sociological theory known as habitus, which states that the way that people perceive and respond to the social world they inhabit is through their personal habits, skills, and disposition of character. Therefore, what exactly constitutes high culture and low culture has specific meanings and usages that are collectively determined by the members of any respective social class. However, people of higher social classes often view the cultural objects they consume as having a higher societal standing than that taken in by lower classes. This makes the distinction between high and low culture one drawn along social standings, a trend that has resulted in the art and content that makes up low culture being regularly discredited throughout history. Variation by country The demographics who make up lower social classes have often been given specific phrases to refer to their classification as being of a lower social standing. These varying groups, who are usually made up of younger and poorer individuals, are often viewed as being a part of regionally specific delinquent subcultures. The following variations of these types of groups are stereotypical of the audiences who consume low culture works. Bogan – Unrefined or unsophisticated person in Australia and New Zealand. Chav – Stereotype of anti-social youth dressed in sportswear in the United Kingdom. Dres – Member of a Polish chav-like subculture that originated in the 1990s. Flaite – Chilean urban lower-class youth. Gopnik – Russian and Eastern European term for delinquent. Characterized as wearing Adidas tracksuits. Redneck – Derogatory term applied to a white person from the rural South of the United States. Skeet (Newfoundland) – A derogatory, stereotypical phrase used within the Canadian province of Newfoundland and Labrador used to refer to someone who is ignorant, aggressive and unruly. Popular examples Popular prints Common throughout the 1400s to 1700s in Europe, popular prints highlight some of the key identifying features of low culture media. Shortly after innovations in printing technology such as moveable type, popular prints became a useful tool for spreading political, religious, and social ideas to the working class—emphasis was placed on adding artwork or visually appealing designs in order to maximize readership in societies which, at the time, were not fully literate. These prints contributed to pioneering satirical content, varying portrayals of common subcultures at the time, and other subject matter that is still found in modern-day low culture media. Popular prints are also observed in Chinese society from the late 1800s to mid-1900s, in which they were intended to be readily accessible to the majority of consumers (i.e., the working or middle classes). Similarly to those found in Europe, the Chinese prints would emphasize design elements such as colorful designs and relatively inexpensive production, which led to their frequent consumption for both spreading various sociopolitical ideas or decorative purposes. This usage of brief, eye-catching marketing strategy allowed the prints, much like their European counterparts, to appeal to a wider audience that would be receptive to both the entertainment and political content they contained. Although the reach of popular prints was both far spanning and effective, its classification as a lower art made it less desirable to higher classes, especially during the art form's earlier years when the drawings themselves were more crude and simply produced due to necessity. As printing techniques advanced, and the quality of the art itself improved during the 17th and 18th centuries, higher social classes began to take more interest in it. This has allowed for more of this genre's later works to survive into the modern age, with earlier, 15th century era works being lost to time due to the perishable nature of printing materials. Toilet humor Also referred to as potty or scatological humor, toilet humor is a brand of off-color humor that deals with defecation, urination, and other bodily functions that would often be deemed as societally taboo. Although most forms of off-color comedy could be viewed as a kind of low culture, toilet humor in particular has received this connotation due to the comedy style's frequent interest amongst toddlers and young children, for whom cultural taboos related to the acknowledgement of waste excretion still have a degree of novelty. For this reason, toilet humor has come to be regularly viewed as juvenile, although it has continued to find success in a number of modern settings such as in the Captain Underpants and South Park media franchises. This relation between low culture and what is enjoyed by children demonstrates a regular pattern in who is seen to find low culture appealing. Lowbrow art movement Arising in the Los Angeles, California, area during the 1960s, Lowbrow was an underground visual art movement that took inspiration from other popular forms of low culture art of the time such as underground comix, punk music, tiki culture, and graffiti. The phrase for this style was coined by artist Robert Williams, who decided to name a 1979 book containing his paintings as The Lowbrow Art of Robt. Williams, in opposition to the idea of high-brow art following the initial rejection of recognition of his work from several pre-established art institutions. The movement has also been referred to as "pop surrealism" in some circles (and likewise Williams himself has referred to it as "cartoon-tainted abstract surrealism"), where the feeling appears to be that the label "lowbrow" may be inappropriate, given the artistic merit found in the movement's artists. Despite initial pushback from contemporary critics, the movement has begun to be taken more seriously as the years have gone on, with the first formal gallery exhibition displaying the works of the movement being orchestrated in 1992 by Greg Escalante at the Julie Rico Gallery in Santa Monica. Internet memes Many well-known examples of modern low culture are represented by online memes that can quickly spread through various social media or messaging platforms. In the context of modern internet culture, memes are cultural ideas (often in the form of images, videos, or vernacular phrases) that have gradually developed certain contextual meanings for their audiences. Memes are often shared by internet users in an informal setting, often with the intent of humor, satire, or social commentary; these behaviors frequently lead to certain images reaching a status of near-universal recognition and fame across the internet. The rapid popularization of Pepe the Frog as a meme in the mid-2010s highlights the variety of symbolic purposes that any one meme can be applied to represent. The image, which can be traced back to a comic book character created by Matt Furie in 2005, was heavily reused and shared across various online forums in later years. Eventually, Pepe's presence spread to a number of far-right communities on the website 4chan, in which it later became associated with hate speech. This sudden shift in usage prompted more serious analysis in other circles, including sociology and other academic communities, as the meme's spread and highly varied usage by different online groups represented a unique kind of media that could influence future political discourse, particularly among middle- or lower-class internet users. Reality television Since its inception in the 1990s, the reality television genre has been a commonly cited example of low culture in contemporary times. Reality television has been labelled as low culture due to its usage of the word "reality" when most drama and conflicts are manufactured, widespread appeal, glorification of wealth and fame, and promotion of materialism. English actor and filmmaker, Gary Oldman described reality television as "the museum of social decay." The genre's overreliance on schadenfreude, the pleasure of seeing another's humiliation or misfortune is also a contributor to why reality television is often cited as low culture. American digital magazine, Entertainment Weekly wrote "Do we watch reality television for precious insight into the human condition? Please. We watch for those awkward scenes that make us feel a smidge better about our own little unfilmed lives" regarding the genre succeeding from the public humiliation of others. Mass media Audience All cultural products (especially high culture) have a certain demographic to which they appeal most. In regard to low culture, it often appeals to very simple and basic human emotional needs, while also offering a perceived return to innocence. This escape from real world problems comes from the experience of being able to live vicariously through the lives of others by viewing them through various forms of media. While the audiences that consume low culture tend to originate from lower socioeconomic classes, those considered 'elite' can also interact with this media. An example of this interaction between classes can be found in outsider art, which is often created by individuals without a background in the fine arts—interestingly, it has been heavily associated with consumption by higher classes throughout the 20th century in a notable example of higher classes consuming media that was neither generated by nor specifically intended to catch their attention. Stereotypes Low culture can often be formulaic, employing trope conventions, stock characters, and character archetypes in a manner that can be perceived as more simplistic, crude, emotive, unbalanced, or blunt compared to the ways in which a piece of high culture would implement them. This leads to the perception of high culture as being more subtle, balanced, or refined and open for interpretation in comparison with its lower counterpart. Modern media that would often be constituted as low culture often continues to implement stereotypes, often to comment or critique them in a satirical manner. Cross-cultural artifacts The use and display of different cultural artifacts, especially in the West, has been studied as an example of low culture consumed by upper classes. Certain examples of these artifacts, such as artwork from African cultures, may be found in higher-income establishments with no ties to these cultures, a phenomenon that has been described as "cultural omnivorousness [sic]," with the aim of creating a more distinguished air in the interior design of the owners' business or living spaces. These cases exemplify another means by which media deemed low culture can still be consumed by socioeconomic classes besides those with which it is chiefly associated, or notably for which it has primarily been created. Example of low culture AI slop – Low effort content generated entirely by generative artificial intelligence Brain rot – Low quality online content Bread and circuses – Figure of speech referring to a superficial means of appeasement Content farm – Online platform that generates low-effort online content at a rapid rate to satisfy algorithms Kitsch – Art that is considered naïve or overly sentimental in how banal or obvious it is Outsider art – Art created outside the boundaries of official culture by those untrained in the arts Philistinism – Hostility to intellect, art and beauty Tribal art, also known as primitive art – Art made by the indigenous tribes that has been categorized by some as "low culture" Tabloid journalism – A popular style of sensationalized journalism known for being lurid and vulgar in nature (similar to yellow journalism) Toilet humour – Type of off-color humor dealing with defecation, urination and flatulence See also Culture industry – Expression suggesting that popular culture is used to manipulate mass society into passivity Culturology – A branch of social sciences with the scientific understanding and analysis of cultures as a whole High culture – Cultural objects which a society deems to be intellectually and artistically exemplary (the opposite of low culture) Mass society – Society based on relations between huge numbers of people, whose prototypical denizen is the "mass man"* Off-color humor – Figure of speech used to describe jokes of a vulgar nature Middlebrow – Art qualification meaning middle of the road, often from a lack of or deviation from convention Trash culture – Artistic or entertainment expressions considered of low cultural profile Working-class culture – Cultures created by or popular among working-class people == References == Cultural activism is the denomination of several creative practices and activities which challenge dominant interpretations and constructions of the world while presenting alternative socio-political and spatial imaginaries in ways which challenge relationships between art, politics, participation, and spectatorship. It implies the use and creation of cultural products to promote social change and may include art, literature, music, cinema, among others. Notable examples include culture jamming, subvertising, muralling, rebel clowning, urban knitting, guerrilla urbanism, political theatre and many other whimsical, non-violent approaches to protest and activism. References == External links == Janusz is a pejorative term used in Poland by Poles to describe individuals embodying stereotypical negative traits attributed to their own society. Typically depicted as a middle-aged man with a mustache, he is often portrayed wearing a white tank top, white socks paired with sandals, and having a large belly, frequently carrying a discount store shopping bag. Janusz's favorite pastimes include drinking beer and watching television. The archetypal Janusz is sometimes illustrated with the head of a proboscis monkey. Description The stereotypical partner of Janusz is Grażyna, characterized as unintelligent, with backcombed or permed hair, a penchant for shopping, and an interest in gossip. Both Janusz and Grażyna are often portrayed as discussing topics beyond their understanding, behaving cunningly, and disregarding social etiquette. Their children, exemplifying stereotypes associated with younger Poles, such as aversion to work, reliance on social benefits, and a sense of entitlement, are referred to as Karyna and Seba. Disdain for Janusz represents broader contempt for what is perceived as provincial Poland and is predominantly expressed by the middle class and hipster subcultures. The term can also refer to an ignoramus in a broader sense. This meaning is evident in the phrase "Janusz biznesu" ("Janusz of business"), which denotes a person fixated on maximizing short-term gains while embodying negative stereotypes of Polish entrepreneurs. In internet memes portraying the "Janusz biznesu" archetype, the image of Stanisław Derehajło, Deputy Marshal of Podlaskie Voivodeship from 2018 to 2021, was frequently utilized. See also Janusz == References == Cultural diversity is the quality of diverse or different cultures, as opposed to monoculture. It has a variety of meanings in different contexts, sometimes applying to cultural products like art works in museums or entertainment available online, and sometimes applying to the variety of human cultures or traditions in a specific region, or in the world as a whole. It can also refer to the inclusion of different cultural perspectives in an organization or society. Cultural diversity can be affected by political factors such as censorship or the protection of the rights of artists, and by economic factors such as free trade or protectionism in the market for cultural goods. Since the middle of the 20th century, there has been a concerted international effort to protect cultural diversity, involving the United Nations Educational, Scientific and Cultural Organization (UNESCO) and its member states. This involves action at international, national, and local levels. Cultural diversity can also be promoted by individual citizens in the ways they choose to express or experience culture. Characteristics In the context of national and international efforts to promote or preserve cultural diversity, the term applies to five overlapping domains: economic: the availability of diverse cultural goods or services, artistic: the variety of artistic genres and styles that coexist, participatory: the participation of diverse ethnic groups in a nation's culture, heritage: the diversity of cultural traditions that are represented in heritage institutions such as museums, and multicultural: the variety of ethnic groups and their traditions that are visible in a country. Of these five, the economic meaning has come to dominate in international negotiations. Nations have principally looked to protect cultural diversity by strengthening the ability of their domestic cultural industries to sell goods or services. Since the 1990s, UNESCO has mainly used "cultural diversity" for the international aspects of diversity, preferring the term "cultural pluralism" for diversity within a country. Governments and international bodies use "cultural diversity" in both a broad and a narrow sense. The broad meaning takes its inspiration from anthropology. It includes lifestyles, value systems, traditions, and beliefs in addition to creative works. It emphasises an ongoing process of interaction and dialogue between cultures. This meaning has been promoted to the international community by UNESCO, since the 2001 Universal Declaration on Cultural Diversity. In practice, governments use a narrower, more traditional, meaning that focuses on the economic domain mentioned above. In the international legal context, cultural diversity has been described as analogous to biodiversity. The General Conference of UNESCO took this position in 2001, asserting in Article 1 of the Universal Declaration on Cultural Diversity that "cultural diversity is as necessary for humankind as biodiversity is for nature." The authors John Cavanagh and Jerry Mander took this analogy further, describing cultural diversity as "a sort of cultural gene pool to spur innovation toward ever higher levels of social, intellectual and spiritual accomplishment." Quantification Cultural diversity is difficult to quantify. One measure of diversity is the number of identifiable cultures. The United Nations Department of Economic and Social Affairs reports that, although their numbers are relatively small, indigenous peoples account for 5,000 distinct cultures and thus the majority of the world's cultural diversity. Another aspect of cultural diversity is measured by counting the number of languages spoken in a region or in the world as a whole. By this measure, the world's cultural diversity is rapidly declining. Research carried out in the 1990s by David Crystal suggested that at that time, on average, one language was falling into disuse every two weeks. He calculated that if that rate of the language death were to continue, then by the year 2100, more than 90% of the languages currently spoken in the world will have gone extinct. In 2003, James Fearon of Stanford University published, in the Journal of Economic Growth, a list of countries based on the diversity of ethnicities, languages, and religions. International legal context At the international level, the notion of cultural diversity has been defended by UNESCO since its founding in 1945, through a succession of declarations and legal instruments. Many of the international legal agreements addressing cultural diversity were focused on intellectual property rights, and thus on tangible cultural expressions that can be bought or sold. The World Heritage List, established in 1972 by UNESCO, mainly listed architectural features and monuments. In the late 20th century, the diplomatic community recognised a need to protect intangible cultural heritage: the traditions, social structures, and skills that support creative expression. International efforts to define and protect this aspect of culture began with the 1989 UNESCO Recommendation on the Safeguarding of Traditional Culture and Folklore. UNESCO's Proclamation of Masterpieces of the Oral and Intangible Heritage of Humanity began in 2001, highlighting specific masterpieces to promote the responsibility of nations to protect intangible cultural heritage. Further proclamations were added in 2003 and 2005, bringing the total number of masterpieces to ninety. In 2001, UNESCO also hosted expert meetings to create a definition of intangible cultural heritage and a more legally binding treaty to protect it, resulting in the Convention for the Safeguarding of the Intangible Cultural Heritage. This was passed in 2003 and came into force in 2006. One result of this convention was the 2008 creation of UNESCO Representative List of Intangible Heritage, which incorporated the masterpieces from the 2001, 2003, and 2005 proclamations. The first international instrument enshrining the value of cultural diversity and intercultural dialogue was the UNESCO Universal Declaration on Cultural Diversity, adopted unanimously in 2001. It calls on nations and institutions to work together for the preservation of culture in all its forms, and for policies that help to share ideas across cultures and inspire new forms of creativity. UNESCO no longer interpreted "culture" in terms of artistic masterpieces. With the Universal Declaration, it adopted a more expansive understanding based on anthropology. This defined cultural diversity as "the set of distinctive spiritual, material, intellectual, and emotional features of society or a social group", including lifestyles, value systems, traditions, and beliefs. The twelve articles of the Universal Declaration were published with an action plan for ways to promote cultural diversity. This action plan connected cultural diversity explicitly to human rights including freedom of expression, freedom of movement, and protection of indigenous knowledge. The declaration identifies cultural diversity as a source of innovation and creativity, as well as a driver of both economic development and personal development. UNESCO made a submission to a 2002 UN report on Human Rights and Cultural Diversity, quoting part of the declaration to emphasise that cultural diversity must not be used to infringe the rights of minorities and that cultural diversity requires the protection of individual freedoms. Citing the Universal Declaration, the United Nations General Assembly established the World Day for Cultural Diversity for Dialogue and Development in December 2002. This continues to be celebrated on May 21 each year. The Convention for the Safeguarding of the Intangible Cultural Heritage drew attention to increasing cultural homogenization by economic globalization and motivated UNESCO to negotiate a treaty protecting cultural diversity. The resulting Convention on the Protection and Promotion of the Diversity of Cultural Expressions (the "2005 Convention") was adopted in October 2005. This was the first international treaty to establish rights and obligations specifically relating to culture. The convention builds on the 2001 declaration by naming linguistic diversity as a fundamental part of cultural diversity and stating that cultural diversity depends on the free flow of ideas. To date, 151 signatory states, as well as the European Union, have registered their ratification of the convention, or a legally equivalent process. The 2005 Convention created an International Fund for Cultural Diversity (IFCD), funded by voluntary contributions. This makes funding available to developing countries that are parties to the convention for specific activities that develop their cultural policies and cultural industries. As of April 2023, UNESCO reports that 140 projects in 69 developing countries have been carried out with funding from the IFCD. Factors Cultural policy scholar Johnathan Vickery has observed that cultural diversity, like biological diversity, is continually under threat from various factors. Cultural diversity, linguistic diversity and species diversity show a partially comparable pattern. These threats often come from other cultural expressions, as when imported entertainment undermines interest in a nation's own culture. Other examples he mentions include religious revivals and modern Western education systems. Factors that promote a country's cultural diversity include migration and a nation's openness to discussing and celebrating cultural differences (which is itself an aspect of culture). The actions of governments, international bodies, and civil society (meaning non-governmental and cultural sector organisations) can promote or restrict cultural diversity. As part of the international effort to promote and preserve cultural diversity, the 2005 Convention established processes to monitor progress towards a favourable environment, including global reports every four years and national reports from individual states. Imperialism and colonialism Colonialism has frequently involved an intentional destruction of cultural diversity, when the colonising powers use education, media, and violence to replace the languages, religions, and cultural values of the colonised people with their own. This process of forced assimilation has been used many times in history, particularly by the European colonial powers from the 18th to 20th centuries, taking the form of forced conversion to the coloniser's religion, privatisation of community property, and replacement of systems of work. The protection of indigenous peoples' rights to maintain their own languages, religions, and culture has been enshrined in treaties including the 1965 International Covenant on Civil and Political Rights and the 1989 UN Convention on the Rights of the Child. Artistic freedom Artistic freedom, as defined by the 2005 Convention, includes the freedom of artists to work without government interference, and also the freedom of citizens to access diverse cultural content. Governments can repress these freedoms through censorship or surveillance of artists, or can choose to actively protect artists and their free expression. According to the 2017 and 2022 global reports, attacks against artists — including prosecution, imprisonment, or even killing — have increased in recent years. In 2020, 978 cases were reported around the world, compared to 771 in 2019 and 673 in 2018. Musicians are the most threatened group, especially rap musicians, whose lyrics tend to be provocative and politically challenging. While online services have provided new ways for artists to distribute images, music, and video to large audiences, they have brought their own threats to freedom in the form of censorship, surveillance, and trolling. The 2022 global report found that some countries had repealed laws restricting free expression, including blasphemy and defamation laws, but that in practice artistic freedom was not being better monitored or protected. Mobility of artists and cultural professionals Mobility restrictions present challenges to professionals in the cultural and creative industries, specifically to those from the Global South. Artists and cultural professionals need to travel to perform to new audiences or to attend a residency or to engage in networking. Their ability to do so depends on their country of origin; the holder of a German passport can travel to 176 countries without a visa while for an Afghan passport the number of countries is 24. Travel restrictions, including difficulties in obtaining visas, often impede artists from the Global South to participate in art biennales or film festivals, even when invited to receive an award or to promote their works. The 2022 global report found that, despite governments and civil society organisations taking this inequality more seriously, concrete improvements are lacking. Thus, the ability of artists from the Global South to reach audiences in the Global North "remains extremely weak". Governance of culture As well as protecting free expression and free movement, governments can promote cultural diversity by recognising and enforcing the rights of artists. The working conditions of artists are affected by their rights to organise labor unions, to workplace safety, and to social security protections for times when their work does not produce income. These economic and social rights are formally recognised by the International Covenant on Economic, Social and Cultural Rights passed by the UN in 1966 and by the 1980 Recommendation concerning the Status of the Artist adopted by UNESCO in 1980. Social security in particular allows a more diverse range of citizens to take part in artistic activities, because without it, financially insecure people are discouraged from working in a field with unstable income. Gender equality in cultural and creative industries A gender gap persists worldwide concerning equal pay, access to funding and prices charged for creative works. Consequently, women remain under-represented in key creative roles and are outnumbered in decision-making positions. As of 2018, women made up only 34% of Ministers for Culture (compared to 24% in 2005) and only 31% of national arts program directors. Generally, women are better represented in specific cultural fields such as arts education and training (60%), book publishing and press (54%), audiovisual and interactive media (26%), as well as design and creative services (33%). The 2022 global report found that cultural industries were increasingly making gender equality a priority, but that actual progress was slow. Though 48.1% of the work in cultural and entertainment sectors is done by women, the report concluded that they are still under-represented in leadership positions, get less public funding, and get less recognition for their work. Trade and investment in cultural goods and services Between 2015 and 2017, at least eight bilateral and regional free trade agreements have introduced cultural clauses or list of commitments that promote the objectives and principles of the 2005 Convention. Despite the lack of the promotion of the objectives and principles of the 2005 Convention with regard to the negotiation of mega-regional partnership agreements, some parties to the Trans Pacific Partnership (TTP) have succeeded in introducing cultural reservations to protect and promote the diversity of cultural expressions. The growth of online digital content has increased the diversity of culture that a person can get immediate access to, but also increased the threat to cultural diversity by making it easier for a small number of large companies to flood markets with their cultural products. Digital delivery of culture has also given a great deal of power to companies in the technology sector. Cultural platforms Organisations that promote access to culture can reflect diversity in what they choose to host or to exclude. Google Arts and Culture and Europeana are among the platforms who state a commitment to promoting cultural diversity. For Google Arts and Culture, diversity implies "working with communities that have historically been left out of the mainstream cultural narrative" while Europeana acknowledges that "stories told with/by cultural heritage items have not historically been representative of the population, and so we strive to share lesser-told stories from underrepresented communities." Individual choices Individual citizens can experience and promote cultural diversity through their own choices, including the choice to share their own culture. The "Do One Thing for Diversity and Inclusion" campaign has been run annually since 2011 by the United Nations Alliance of Civilizations (UNAOC) as a way to commemorate the World Day for Cultural Diversity. It encourages people to explore the music, literature, art, and traditions of unfamiliar cultures and to share their own culture with strangers. The American lawyer Juliette Passer describes the UNESCO Universal Declaration on Cultural Diversity as prompting each individual to consider their own and others' diverse identities: "We need social and educational experiences plus reflection on the experience to go beyond reliance on stereotypes. The more we interact with diverse others and mindfully reflect on the experience, the more we can improve our competency with differences." National and local initiatives In September 2002, the city of Porto Alegre in Brazil organized a world meeting for culture, bringing together mayors and technical directors of culture from different cities of the world, with observers from civil society. The cities of Porto Alegre and Barcelona have proposed the drafting of a reference document for the development of local cultural policies, inspired by Agenda 21, created in 1992 for the environment. The Culture 21 was thus designed with the aim of including cultural diversity at the local level. The document was approved on May 8, 2004 during the first edition of the Universal Forum of Cultures in Barcelona. See also References Further reading Tierney, Stephen, ed. (2007). Accommodating cultural diversity. Applied legal philosophy. Aldershot: Ashgate. ISBN 978-0-7546-2603-9. External links UNESCO: Diversity of Cultural Expressions In most Wikipedia articles, verification is simple. You find the fact you need in a reliable source, add a citation to it, and move on. This is not true of date ranges for recent generations (Generation Z, Generation Alpha, etc.) because different sources all give different dates. Reasons for this include: Generations have no scientific definitions. Generations have no authoritative or official definitions. Definitions of more recent generations are still in flux. There are two incompatible systems for setting generation date ranges. In the face of this, editors are left to specify multiple possibilities or a range for start and end dates instead of a single year, which many don't find satisfying. But there really is no other way. Generations are not a scientific concept. They're used to some extent by social scientists, but their primarily use is in marketing. There is no experiment you can run to determine the start year of a generation. There is no organization with the authority to set the date range for a generation. Wikipedia editors are left to survey the ranges given at different websites. Date ranges for older generations, up to the Generation X or Millennials, are static, but younger generations continue to see new date ranges from different sources. There are two ways of thinking about named generations. The Pew Research Center, an American think tank which looks at social issues and whose definitions are used by many other organizations, exemplifies the first one. They look at historical events and survey data to determine shared experiences which justify calling people in a certain age range a generation. For example, from the 2019 press release announcing their official end year for Millenials, "But for analytical purposes, we believe 1996 is a meaningful cutoff between Millennials and Gen Z for a number of reasons, including key political, economic and social factors that define the Millennial generation’s formative years." Another approach is taken by McCrindle Research, the Australian marketing and consulting firm that coined the name of Generation Alpha (and Beta). From a white paper on their website, "Generational definitions are most useful when they span a set age range and so allow meaningful comparisons across generations. That is why the generations today each span 15 years with Generation Y (Millennials) born from 1980 to 1994; Generation Z from 1995 to 2009 and Generation Alpha from 2010 to 2024." This does make sense, but it doesn't have widespread acceptance yet. The above considerations are also true for the names of generations, and the generation after Alpha could (but probably won't) still settle on a name other than Generation Beta. A deletion discussion in early 2024 determined that there wasn't yet a quorum of reliable sources, and the article was deleted then. It was recreated in January 2025. As of January 2025 sources all say that the date range is 2025-2039, but this isn't a done deal. The sources are all news outlets (no researchers, consulting companies, census bureaus, etc.), and the sources all cite McCrindle. This may well change (see above). Postmodernism encompasses a variety of artistic, cultural, and philosophical movements that claim to mark a break from modernism. They have in common the conviction that it is no longer possible to rely upon previous ways of depicting the world. Still, there is disagreement among experts about its more precise meaning even within narrow contexts. The term began to acquire its current range of meanings in literary criticism and architectural theory during the 1950s–1960s. In opposition to modernism's alleged self-seriousness, postmodernism is characterized by its playful use of eclectic styles and performative irony, among other features. Critics claim it supplants moral, political, and aesthetic ideals with mere style and spectacle. In the 1990s, "postmodernism" came to denote a general – and, in general, celebratory – response to cultural pluralism. Proponents align themselves with feminism, multiculturalism, and postcolonialism. Building upon poststructural theory, postmodern thought defined itself by the rejection of any single, foundational historical narrative. This called into question the legitimacy of the Enlightenment account of progress and rationality. Critics allege that its premises lead to a nihilistic form of relativism. In this sense, it has become a term of abuse in popular culture. Definitions "Postmodernism" is "a highly contested term", referring to "a particularly unstable concept", that "names many different kinds of cultural objects and phenomena in many different ways". It may be described simply as a general mood or Zeitgeist. Although postmodernists are generally united in their effort to transcend the perceived limits of modernism, "modernism" also means different things to different critics in various arts. Further, there are outliers on even this basic stance; for instance, literary critic William Spanos conceives postmodernism not in period terms but in terms of a certain kind of literary imagination so that pre-modern texts such as Euripides' Orestes or Cervantes' Don Quixote count as postmodern. According to scholar Louis Menand, "Postmodernism is the Swiss Army knife of critical concepts. It's definitionally overloaded, and it can do almost any job you need done." From an opposing perspective, media theorist Dick Hebdige criticized the vagueness of the term, enumerating a long list of otherwise unrelated concepts that people have designated as postmodernism, from "the décor of a room" or "a 'scratch' video", to fear of nuclear armageddon and the "implosion of meaning", and stated that anything that could signify all of those things was "a buzzword". All this notwithstanding, scholar Hans Bertens offers the following: If there is a common denominator to all these postmodernisms, it is that of a crisis in representation: a deeply felt loss of faith in our ability to represent the real, in the widest sense. No matter whether they are aesthestic [sic], epistemological, moral, or political in nature, the representations that we used to rely on can no longer be taken for granted. In practical terms, postmodernisms share an attitude of skepticism towards grand explanations and established ways of doing things. In art, literature, and architecture, this attitude blurs boundaries between styles and genres, and encourages freely mixing elements, challenging traditional distinctions like high art versus popular art. In science, it emphasizes multiple ways of seeing things, and how our cultural and personal backgrounds shape how we see the world, making it impossible to be completely objective. In philosophy, education, history, politics, and many other fields, it encourages critical re-examination of established institutions and social norms, embracing diversity, and breaking down disciplinary boundaries. Though these ideas weren't strictly new, postmodernism amplified them, using an often playful, at times deeply critical, attitude of pervasive skepticism to turn them into defining features. Historical overview Two broad cultural movements, modernism and postmodernism, emerged in response to profound changes in the Western world. The Industrial Revolution, urbanization, secularization, technological advances, two world wars, and globalization deeply disrupted the social order. Modernism emerged in the late 1800s, seeking to redefine fundamental truths and values through a radical rethinking of traditional ideas and forms across many fields. Postmodernism emerged in the mid-20th century with a skeptical perspective that questioned the notion of universal truths and reshaped modernist approaches by embracing the complexity and contradictions of modern life. The term "postmodernism" first appeared in print in 1870, but it only began to enter circulation with its current range of meanings in the 1950s—60s. Early appearances The term "postmodern" was first used in 1870 by the artist John Watkins Chapman, who described "a Postmodern style of painting" as a departure from French Impressionism. Similarly, the first citation given by the Oxford English Dictionary is dated to 1916, describing Gus Mager as "one of the few 'post' modern painters whose style is convincing". Episcopal priest and cultural commentator J. M. Thompson, in a 1914 article, uses the term to describe changes in attitudes and beliefs in the critique of religion, writing, "the raison d'être of Post-Modernism is to escape from the double-mindedness of modernism by being thorough in its criticism by extending it to religion as well as theology, to Catholic feeling as well as to Catholic tradition". In 1926, Bernard Iddings Bell, president of St. Stephen's College and also an Episcopal priest, published Postmodernism and Other Essays, which marks the first use of the term to describe an historical period following modernity. The essay criticizes lingering socio-cultural norms, attitudes, and practices of the Enlightenment. It is also critical of a purported cultural shift away from traditional Christian beliefs. The term "postmodernity" was first used in an academic historical context as a general concept for a movement by Arnold J. Toynbee in a 1939 essay, which states that "Our own Post-Modern Age has been inaugurated by the general war of 1914–1918". In 1942, the literary critic and author H. R. Hays describes postmodernism as a new literary form. Also in the arts, the term was first used in 1949 to describe a dissatisfaction with the modernist architectural movement known as the International Style. Although these early uses anticipate some of the concerns of the debate in the second part of the 20th century, there is little direct continuity in the discussion. Just when the new discussion begins, however, is also a matter of dispute. Various authors place its beginnings in the 1950s, 1960s, 1970s, and 1980s. Theoretical development In the mid-1970s, the American sociologist Daniel Bell provided a general account of the postmodern as an effectively nihilistic response to modernism's alleged assault on the Protestant work ethic and its rejection of what he upheld as traditional values. The ideals of modernity, per his diagnosis, were degraded to the level of consumer choice. This research project, however, was not taken up in a significant way by others until the mid-1980s when the work of Jean Baudrillard and Fredric Jameson, building upon art and literary criticism, reintroduced the term to sociology. Discussion about the postmodern in the second part of the 20th century was most articulate in areas with a large body of critical discourse around the modernist movement. Even here, however, there continued to be disagreement about such basic issues as whether postmodernism is a break with modernism, a renewal and intensification of modernism, or even, both at once, a rejection and a radicalization of its historical predecessor. While discussions in the 1970s were dominated by literary criticism, these were supplanted by architectural theory in the 1980s. Some of these conversations made use of French poststructuralist thought, but only after these innovations and critical discourse in the arts did postmodernism emerge as a philosophical term in its own right. In literary and architectural theory According to Hans Bertens and Perry Anderson, the Black Mountain poets Charles Olson and Robert Creeley first introduced the term "postmodern" in its current sense during the 1950s. Their stance against modernist poetry – and Olson's Heideggerian orientation – were influential in the identification of postmodernism as a polemical position opposed to the rationalist values championed by the Enlightenment project. During the 1960s, this affirmative use gave way to a pejorative use by the New Left, who used it to describe a waning commitment among youth to the political ideals socialism and communism. The literary critic Irving Howe, for instance, denounced postmodern literature for being content to merely reflect, rather than actively attempt to refashion, what he saw as the "increasingly shapeless" character of contemporary society. In the 1970s, this changed again, largely under the influence of the literary critic Ihab Hassan's large-scale survey of works that he said could no longer be called modern. Taking the Black Mountain poets an exemplary instance of the new postmodern type, Hassan celebrates its Nietzschean playfulness and cheerfully anarchic spirit, which he sets off against the high seriousness of modernism. (Yet, from another perspective, Friedrich Nietzsche's attack on Western philosophy and Martin Heidegger's critique of metaphysics posed deep theoretical problems not necessarily a cause for aesthetic celebration. Their further influence on the conversation about postmodernism, however, would be largely mediated by French poststructuralism.) If literature were at the center of the discussion in the 1970s, architecture was at the center in the 1980s. The architectural theorist Charles Jencks, in particular, connected the artistic avant-garde to social change in a way that captured attention outside of academia. Jencks, much influenced by the American architect Robert Venturi, celebrated a plurality of forms and encourages participation and active engagement with the local context of the built environment. He presented this as in opposition to the "authoritarian style" of International Modernism. The influence of poststructuralism In the 1970s, postmodern criticism increasingly came to incorporate poststructuralist theory, particularly the deconstructive approach to texts most strongly associated with Jacques Derrida, who attempted to demonstrate that the whole foundationalist approach to language and knowledge was untenable and misguided. It is during this period that postmodernism came to be particularly equated with a kind of anti-representational self-reflexivity. In the 1980s, some critics began to take an interest in the work of Michel Foucault. This introduced a political concern about social power-relations into discussions about postmodernism. This was also the beginning of the affiliation of postmodernism with feminism and multiculturalism. The art critic Craig Owens, in particular, not only made the connection to feminism explicit, but went so far as to claim feminism for postmodernism wholesale, a broad claim resisted by even many sympathetic feminists such as Nancy Fraser and Linda Nicholson. Generalization Although postmodern criticism and thought drew on philosophical ideas from early on, "postmodernism" was only introduced to the expressly philosophical lexicon by Jean-François Lyotard in his 1979 The Postmodern Condition: A Report on Knowledge. This work served as a catalyst for many of the subsequent intellectual debates around the term. By the 1990s, postmodernism had become increasingly identified with critical and philosophical discourse directly about postmodernity or the postmodern idiom itself. No longer centered on any particular art or even the arts in general, it instead turned to address the more general problems posed to society in general by a new proliferation of cultures and forms. It is during this period that it also came to be associated with postcolonialism and identity politics. Around this time, postmodernism also began to be conceived in popular culture as a general "philosophical disposition" associated with a loose sort of relativism. In this sense, the term also started to appear as a "casual term of abuse" in non-academic contexts. Others identified it as an aesthetic "lifestyle" of eclecticism and playful self-irony. The "Science Wars" The basis for what became known later as the Science Wars was the 1962 publication of The Structure of Scientific Revolutions by the physicist and historian of science Thomas Kuhn. Kuhn presented the direction of scientific inquiry — the kind of questions that can be asked, and what counts as a correct answer — as governed by a "paradigm" defining what counts as "normal science" during any given period. While not based on postmodern ideas or Continental philosophy, Kuhn's intervention set the agenda for much of The Postmodern Condition and has subsequently been presented as the beginning of "postmodern epistemology" in the philosophy of science. In Kuhn's 1962 framework, the assumptions introduced by new paradigms make them "mutually incommensurable" with previous ones, although they may provide improved explanations of the material world. A more radical version of incommensurablity, introduced by the philosopher of science Paul Feyerabend, made stronger claims that connected the largely Anglo-American debate about science to the development of poststructuralism in France. To some, the stakes were more than epistemological. The philosopher Israel Scheffler, for instance, argued that the ever-expanding body of scientific knowledge embodies a sort of "moral principle" protecting society from its authoritarian and tribal tendencies. In this way, with the addition of the poststructuralist influence, the debate about science expanded into a debate about Western culture in general. The French political philosophers Alain Renaut and Luc Ferry began a series of responses to this interpretation of postmodernism, and these inspired the physicist Alan Sokal to submit a deliberately nonsensical paper to a postmodernist journal, where it was accepted and published in 1996. Although the so-called Sokal hoax proved nothing about postmodernism or science, it added to the public perception of a high-stakes intellectual "war" that had already been introduced to the general public by popular books published in the late '80s and '90s. By the late '90s, however, the debate had largely subsided, in part due to the recognition that it had been staged between strawman versions of postmodernism and science alike. In the arts Postmodernism encompasses a wide range of artistic movements and styles. In visual arts, pop art, conceptual art, feminist art, video art, minimalism, and neo-expressionism are among the approaches recognized as postmodern. The label extends to diverse musical genres and artists: John Cage, Madonna, and punk rock all meet postmodern definitions. Literature, film, architecture, theater, fashion, dance, and many other creative disciplines saw postmodern expression. As an example, Andy Warhol's pop art across multiple mediums challenged traditional distinctions between high and low culture, and blurred the lines between fine art and commercial design. His work, exemplified by the iconic Campbell's Soup Cans series during the 1960s, brought the postmodernist sensibility to mainstream attention. Criticism of postmodernist movements in the arts include objections to departure from beauty, the reliance on language for the art to have meaning, a lack of coherence or comprehensibility, deviation from clear structure, and consistent use of dark and negative themes. Architecture Scholarship regarding postmodernism and architecture is closely linked with the writings of critic-turned-architect Charles Jencks, beginning with lectures in the early 1970s and his essay "The Rise of Post-Modern Architecture" from 1975. His magnum opus, however, is the book The Language of Post-Modern Architecture, first published in 1977, and since running to seven editions (in which he famously wrote: "Modern architecture died in St. Louis, Missouri, on 15 July 1972 at 3:32 p.m. (or thereabouts) when the infamous Pruitt–Igoe scheme, or rather several of its slab blocks, were given the final coup de grâce by dynamite."). Jencks makes the point that postmodernism (like modernism) varies for each field of art, and that for architecture it is not just a reaction to modernism but what he terms double coding: "Double Coding: the combination of Modern techniques with something else (usually traditional building) in order for architecture to communicate with the public and a concerned minority, usually other architects." In their book, "Revisiting Postmodernism", Terry Farrell and Adam Furman argue that postmodernism brought a more joyous and sensual experience to the culture, particularly in architecture. For instance, in response to the modernist slogan of Ludwig Mies van der Rohe that "less is more", the postmodernist Robert Venturi rejoined that "less is a bore". Dance The term "postmodern dance" is most strongly associated with the Judson Dance Theater, located in New York's Greenwich Village during the 1960s and 1970s. Perhaps its most important principle is taken from the composer John Cage's efforts to break down the distinction between art and life, developed in particular by the American dancer and choreographer Merce Cunningham, Cage's partner. The Judson dancers "[stripped] dance of its theatrical conventions such as virtuoso technique, fanciful costumes, complex storylines, and the traditional stage [and] drew on everyday movements (sitting, walking, kneeling, and other gestures) to create their pieces, often performing them in ordinary spaces." Anna Halprin's San Francisco Dancers' Workshop, established in the 1950s to explore beyond the technical constraints of modern dance, pioneered ideas later developed at Judson; Halprin, Simone Forti, and Yvonne Rainer are considered "giants of the field". The Judson collective included trained dancers, visual artists, filmmakers, writers, and composers, exchanging approaches, and critiquing traditional dance, with a focus "more on the intellectual process of creating dance than the end result". The end of the 1970s saw a distancing from this analytical postmodern dance, and a return to the expression of meaning. In the 1980s and 1990s, dance began to incorporate other typically postmodern features such as the mixing of genres, challenging high–low cultural distinctions, and incorporating a political dimension. Film Postmodern film aims to subvert the mainstream conventions of narrative structure and characterization, and to test the audience's suspension of disbelief. Typically, such films also break down the cultural divide between high and low art and often upend typical portrayals of gender, race, class, genre, and time with the goal of creating something that does not abide by traditional narrative expression. Certain key characteristics are used to separate the postmodern from modernist cinema and traditional narrative film. One is an extensive use of homage or pastiche, imitating the style or character of other artistic works. A second is meta-reference or self-reference, highlighting the relation of the image to other images in media and not to any kind of external reality. Viewers are reminded that the film itself is only a film, perhaps through the use of intertextuality, in which the film's characters reference other works of fiction. A third characteristic is stories that unfold out of chronological order, deconstructing or fragmenting time to emphasize the constructed nature of film. Another common element is a bridging of the gap between highbrow and lowbrow,. Contradictions of all sorts are crucial to postmodernism. Ridley Scott's Blade Runner (1982) has been widely studied as a prime example of postmodernism. The setting is a future dystopia where "replicants", enhanced android workers nearly indistinguishable from humans, are hunted down when they escape from their jobs. The film blurs boundaries between genres and cultures, and fuses disparate styles and periods: futuristic visuals "mingle with drab 1940s clothes and offices, punk rock hairstyles, pop Egyptian styles and oriental culture." The blending of film noir and science-fiction into tech noir illustrates the deconstruction of both cinema and genre. The film can also be seen as an example of major studios using the "mystique and cachet of the term 'postmodern' as a sales pitch", resulting in Hollywood movies that "demonstrate all the postmodern characteristics". From another perspective, "critical responses to Blade Runner fall on either side of a modern/postmodern line" – critical analysis from "modernist" and "postmodernist" approaches produce entirely different interpretations. Literature In 1971, the American literary theorist Ihab Hassan made "postmodernism" popular in literary studies with his influential book, The Dismemberment of Orpheus: Toward a Postmodern Literature. According to scholar David Herwitz, American writers such as John Barth (who had controversially declared that the novel was "exhausted" as a genre), Donald Barthelme, and Thomas Pynchon responded in various ways to the stylistic innovations of Finnegans Wake and the late work of Samuel Beckett. Postmodern literature often calls attention to issues regarding its own complicated connection to reality. The postmodern novel plays with language, twisted plots, multiple narrators, and unresolved endings, unsettling the conventional idea of the novel as faithfully reflecting the world. In Postmodernist Fiction (1987), Brian McHale details the shift from modernism to postmodernism, arguing that postmodernist works developed out of modernism, moving from concern with questions about the nature and limits of knowledge about one's "world" ("epistemological dominant") to concern with questions of modes of being and existence in relation to "different kinds of worlds" ("ontological dominant"). McHale's "What Was Postmodernism?" (2007) follows Raymond Federman's lead in now using the past tense when discussing postmodernism. Others argue that postmodernism in literature utilizes compositional and semantic practices such as inclusivity, intentional indiscrimination, nonselection, and "logical impossibility." Music Postmodern influence extends across all areas of music; its accessibility to a general audience requires an understanding of references, irony, and pastiche that varies widely between artists and their works. In popular music, Madonna, David Bowie, and Talking Heads have been singled out by critics and scholars as postmodern icons. The belief that art music – serious, classical music – holds higher cultural and technical value than folk and popular traditions, lost influence under postmodern analysis, as musical hybrids and crossovers attracted scholarly attention. Across musical traditions, postmodernism can be identified through several core characteristics: genre mixing; irony, humor, and self-parody; "surface" exploration with less concern for formal structure than in modernist approaches; and a return to tonality. This represents a loss of authority of the Eurocentric perspective on music and the rise of world music as influenced by postmodern values. Composers took different routes: some returned to traditional modes over experimentation, others challenged the authority of dominant musical structures, others intermingled disparate sources. The composer Jonathan Kramer has written that avant-garde musical compositions (which some would consider