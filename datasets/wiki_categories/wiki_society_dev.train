Muslim Caliphates. The cause to liberate the "Holy Land" remained a major focus throughout medieval history, fueling many consecutive crusades, only the first of which was successful (although it resulted in many atrocities, in Europe as well as elsewhere). Charlemagne ("Charles the Great" in English) became king of the Franks. He conquered Gaul (modern day France), northern Spain, Saxony, and northern and central Italy. In 800, Pope Leo III crowned Charlemagne Holy Roman Emperor. Under his rule, his subjects in non-Christian lands like Germany converted to Christianity. After his reign, the empire he created broke apart into the kingdom of France (from Francia meaning "land of the Franks"), Holy Roman Empire and the kingdom in between (containing modern day Switzerland, northern-Italy, Eastern France and the low-countries). Starting in the late 8th century, the Vikings began seaborne attacks on the towns and villages of Europe. Eventually, they turned from raiding to conquest, and conquered Ireland, most of England, and northern France (Normandy). These conquests were not long-lasting, however. In 954 Alfred the Great drove the Vikings out of England, which he united under his rule, and Viking rule in Ireland ended as well. In Normandy the Vikings adopted French culture and language, became Christians and were absorbed into the native population. By the beginning of the 11th century Scandinavia was divided into three kingdoms, Norway, Sweden, and Denmark, all of which were Christian and part of Western civilization. Norse explorers reached Iceland, Greenland, and even North America, however only Iceland was permanently settled by the Norse. A period of warm temperatures from around 1000–1200 enabled the establishment of a Norse outpost in Greenland in 985, which survived for some 400 years as the most westerly outpost of Christendom. From here, Norseman attempted their short-lived European colony in North America, five centuries before Columbus. In the 10th century another marauding group of warriors swept through Europe, the Magyars. They eventually settled in what is today Hungary, converted to Christianity and became the ancestors of the Hungarian people. A West Slavic people, the Poles, formed a unified state by the 10th century and having adopted Christianity also in the 10th century but with pagan rising in the 11th century. By the start of the second millennium AD, the West had become divided linguistically into three major groups. The Romance languages, based on Latin, the language of the Romans, the Germanic languages, and the Celtic languages. The most widely spoken Romance languages were French, Italian, Portuguese and Spanish. Four widely spoken Germanic languages were English, German, Dutch, and Danish. Irish and Scots Gaelic were two widely spoken Celtic languages in the British Isles. High Middle Ages: 1000–1300 Art historian Kenneth Clark wrote that Western Europe's first "great age of civilisation" was ready to begin around the year 1000. From 1100, he wrote: "every branch of life – action, philosophy, organisation, technology [experienced an] extraordinary outpouring of energy, an intensification of existence". Upon this period rests the foundations of many of Europe's subsequent achievements. By Clark's account, the Catholic Church was very powerful, essentially internationalist and democratic in its structures and run by monastic organisations generally following the Rule of Saint Benedict. Men of intelligence usually joined religious orders and those of intellectual, administrative or diplomatic skill could advance beyond the usual restraints of society – leading churchmen from faraway lands were accepted in local bishoprics, linking European thought across wide distances. Complexes like the Abbey of Cluny became vibrant centres with dependencies spread throughout Europe. Ordinary people also trekked vast distances on pilgrimages to express their piety and pray at the site of holy relics. Monumental abbeys and cathedrals were constructed and decorated with sculptures, hangings, mosaics and works belonging to one of the greatest epochs of art and providing stark contrast to the monotonous and cramped conditions of ordinary living. Abbot Suger of the Abbey of St. Denis is considered an influential early patron of Gothic architecture and believed that love of beauty brought people closer to God: "The dull mind rises to truth through that which is material". Clark calls this "the intellectual background of all the sublime works of art of the next century and in fact has remained the basis of our belief of the value of art until today". By the year 1000 feudalism had become the dominant social, economic and political system. At the top of society was the monarch, who gave land to nobles in exchange for loyalty. The nobles gave land to vassals, who served as knights to defend their monarch or noble. Under the vassals were the peasants or serfs. The feudal system thrived as long as peasants needed protection by the nobility from invasions originating inside and outside of Europe. So as the 11th century progressed, the feudal system declined along with the threat of invasion. In 1054, after centuries of strained relations, the Great Schism occurred over differences in doctrine, splitting the Christian world between the Catholic Church, centered in Rome and dominant in the West, and the Orthodox Church, centered in Constantinople, capital of the Byzantine Empire. The last pagan land in Europe was converted to Christianity with the conversion of the Baltic peoples in the High Middle Ages, bringing them into Western civilization as well. As the medieval period progressed, the aristocratic military ideal of Chivalry and institution of knighthood based around courtesy and service to others became culturally important. Large Gothic cathedrals of extraordinary artistic and architectural intricacy were constructed throughout Europe, including Canterbury Cathedral in England, Cologne Cathedral in Germany and Chartres Cathedral in France (called the "epitome of the first great awakening in European civilisation" by Kenneth Clark). The period produced ever more extravagant art and architecture, but also the virtuous simplicity of such as St Francis of Assisi (expressed in the Prayer of St Francis) and the epic poetry of Dante Alighieri's Divine Comedy. As the Church grew more powerful and wealthy, many sought reform. The Dominican and Franciscan Orders were founded, which emphasized poverty and spirituality. Women were in many respects excluded from political and mercantile life, however, leading churchwomen were an exception. Medieval abbesses and female superiors of monastic houses were powerful figures whose influence could rival that of male bishops and abbots: "They treated with kings, bishops, and the greatest lords on terms of perfect equality;. . . they were present at all great religious and national solemnities, at the dedication of churches, and even, like the queens, took part in the deliberation of the national assemblies...". The increasing popularity of devotion to the Virgin Mary (the mother of Jesus) secured maternal virtue as a central cultural theme of Catholic Europe. Kenneth Clark wrote that the 'Cult of the Virgin' in the early 12th century "had taught a race of tough and ruthless barbarians the virtues of tenderness and compassion". In 1095, Pope Urban II called for a Crusade to re-conquer the Holy Land from Muslim rule, when the Seljuk Turks prevented Christians from visiting the holy sites there. For centuries prior to the emergence of Islam, Asia Minor and much of the Mid East had been a part of the Roman and later Byzantine Empires. The Crusades were originally launched in response to a call from the Byzantine Emperor for help to fight the expansion of the Turks into Anatolia. The First Crusade succeeded in its task, but at a serious cost on the home front, and the crusaders established rule over the Holy Land. However, Muslim forces reconquered the land by the 13th century, and subsequent crusades were not very successful. The specific crusades to restore Christian control of the Holy Land were fought over a period of nearly 200 years, between 1095 and 1291. Other campaigns in Spain and Portugal (the Reconquista), and Northern Crusades continued into the 15th century. The Crusades had major far-reaching political, economic, and social impacts on Europe. They further served to alienate Eastern and Western Christendom from each other and ultimately failed to prevent the march of the Turks into Europe through the Balkans and the Caucasus. After the fall of the Roman Empire, many of the classical Greek texts were translated into Arabic and preserved in the medieval Islamic world, from where the Greek classics along with Arabic science and philosophy were transmitted to Western Europe and translated into Latin during the Renaissance of the 12th century and 13th century. Cathedral schools began in the Early Middle Ages as centers of advanced education, some of them ultimately evolving into medieval universities. During the High Middle Ages, Chartres Cathedral operated the famous and influential Chartres Cathedral School. The medieval universities of Western Christendom were well-integrated across all of Western Europe, encouraged freedom of enquiry and produced a great variety of fine scholars and natural philosophers, including Robert Grosseteste of the University of Oxford, an early expositor of a systematic method of scientific experimentation; and Saint Albert the Great, a pioneer of biological field research. The Italian University of Bologna is considered the oldest continually operating university. Philosophy in the High Middle Ages focused on religious topics. Christian Platonism, which modified Plato's idea of the separation between the ideal world of the forms and the imperfect world of their physical manifestations to the Christian division between the imperfect body and the higher soul was at first the dominant school of thought. However, in the 12th century the works of Aristotle were reintroduced to the West, which resulted in a new school of inquiry known as scholasticism, which emphasized scientific observation. Two important philosophers of this period were Saint Anselm and Saint Thomas Aquinas, both of whom were concerned with proving God's existence through philosophical means. The Summa Theologica by Aquinas was one of the most influential documents in medieval philosophy and Thomism continues to be studied today in philosophy classes. Theologian Peter Abelard wrote in 1122 "I must understand in order that I may believe... by doubting we come to questioning, and by questioning we perceive the truth". In Normandy, the Vikings adopted French culture and language, mixed with the native population of mostly Frankish and Gallo-Roman stock and became known as the Normans. They played a major political, military, and cultural role in medieval Europe and even the Near East. They were famed for their martial spirit and Christian piety. They quickly adopted the Romance language of the land they settled in, their dialect becoming known as Norman, an important literary language. The Duchy of Normandy, which they formed by treaty with the French crown, was one of the great large fiefs of medieval France. The Normans are famed both for their culture, such as their unique Romanesque architecture, and their musical traditions, as well as for their military accomplishments and innovations. Norman adventurers established a kingdom in Sicily and southern Italy by conquest, and a Norman expedition on behalf of their duke led to the Norman Conquest of England. Norman influence spread from these new centres to the Crusader States in the Near East, to Scotland and Wales in Great Britain, and to Ireland. Relations between the major powers in Western society: the nobility, monarchy and clergy, sometimes produced conflict. If a monarch attempted to challenge church power, condemnation from the church could mean a total loss of support among the nobles, peasants, and other monarchs. The Holy Roman Emperor Henry IV, one of the most powerful men of the 11th century, stood three days bare-headed in the snow at Canossa in 1077, in order to reverse his excommunication by Pope Gregory VII. As monarchies centralized their power as the Middle Ages progressed, nobles tried to maintain their own authority. The sophisticated Court of Holy Roman Emperor Frederick II was based in Sicily, where Norman, Byzantine, and Islamic civilization had intermingled. His realm stretched through Southern Italy, through Germany and in 1229, he crowned himself King of Jerusalem. His reign saw tension and rivalry with the Papacy over control of Northern Italy. A patron of education, Frederick founded the University of Naples. Plantagenet kings first ruled the Kingdom of England in the 12th century. Henry V left his mark with a famous victory against larger numbers at the Battle of Agincourt, while Richard the Lionheart, who had earlier distinguished himself in the Third Crusade, was later romanticised as an iconic figure in English folklore. A distinctive English culture emerged under the Plantagenets, encouraged by some of the monarchs who were patrons of the "father of English poetry", Geoffrey Chaucer. The Gothic architecture style was popular during the time, with buildings such as Westminster Abbey remodelled in that style. King John's sealing of the Magna Carta was influential in the development of common law and constitutional law. The 1215 Charter required the King to proclaim certain liberties, and accept that his will was not arbitrary – for example by explicitly accepting that no "freeman" (non-serf) could be punished except through the law of the land, a right which is still in existence today. Political institutions such as the Parliament of England and the Model Parliament originate from the Plantagenet period, as do educational institutions including the universities of Cambridge and Oxford. From the 12th century onward inventiveness had re-asserted itself outside of the Viking north and the Islamic south of Europe. Universities flourished, mining of coal commenced, and crucial technological advances such as the lock, which enabled sail ships to reach the thriving Belgian city of Bruges via canals, and the deep sea ship guided by magnetic compass and rudder were invented. Late Middle Ages: 1300–1500 A cooling in temperatures after about 1150 saw leaner harvests across Europe and consequent shortages of food and flax material for clothing. Famines increased and in 1316 serious famine gripped Ypres. In 1410, the last of the Greenland Norseman abandoned their colony to the ice. From Central Asia, Mongol invasions progressed towards Europe throughout the 13th century, resulting in the vast Mongol Empire which became the largest empire of history and ruled over almost half of the human population and expanded through the world by 1300. The Papacy had its court at Avignon from 1305 to 1378 This arose from the conflict between the Papacy and the French crown. A total of seven popes reigned at Avignon; all were French, and all were increasingly under the influence of the French crown. Finally in 1377 Gregory XI, in part because of the entreaties of the mystic Saint Catherine of Sienna, restored the Holy See to Rome, officially ending the Avignon papacy. However, in 1378 the breakdown in relations between the cardinals and Gregory's successor, Urban VI, gave rise to the Western Schism – which saw another line of Avignon Popes set up as rivals to Rome (subsequent Catholic history does not grant them legitimacy). The period helped weaken the prestige of the Papacy in the buildup to the Protestant Reformation. In the Later Middle Ages, the Black Plague struck Europe, arriving in 1347. Europe was overwhelmed by the outbreak of bubonic plague, probably brought to Europe by the Mongols. The fleas hosted by rats carried the disease and it devastated Europe. Major cities like Paris, Hamburg, Venice and Florence lost half their population. Around 20 million people – up to a third of Europe's population – died from the plague before it receded. The plague periodically returned over the coming centuries. The last centuries of the Middle Ages saw the waging of the Hundred Years' War between England and France. The war began in 1337 when the king of France laid claim to English-ruled Gascony in southern France, and the king of England claimed to be the rightful king of France. At first, the English conquered half of France and seemed likely to win the war, until the French were rallied by a peasant girl, who would later become a saint, Joan of Arc. Although she was captured and executed by the English, the French fought on and won the war in 1453. After the war, France gained all of Normandy, excluding the city of Calais, which it gained in 1558. Following the Mongols from Central Asia came the Ottoman Turks. By 1400 they had captured most of modern-day Turkey and extended their rule into Europe through the Balkans and as far as the Danube, surrounding even the fabled city of Constantinople. Finally, in 1453, one of Europe's greatest cities fell to the Turks. The Ottomans under the command of Sultan Mehmed II, fought a vastly outnumbered defending army commanded by Emperor Constantine XI – the last "Emperor of the Eastern Roman Empire" – and blasted down the ancient walls with the terrifying new weaponry of the cannon. The Ottoman conquests sent refugee Greek scholars westward, contributing to the revival of the West's knowledge of the learning of Classical Antiquity. Probably the first clock in Europe was installed in a Milan church in 1335, hinting at the dawning mechanical age. By the 14th century, the middle class in Europe had grown in influence and number as the feudal system declined. This spurred the growth of towns and cities in the West and improved the economy of Europe. This, in turn helped begin a cultural movement in the West known as the Renaissance, which began in Italy. Italy was dominated by city-states, many of which were nominally part of the Holy Roman Empire, and were ruled by wealthy aristocrats like the Medicis, or in some cases, by the pope. Renaissance and Reformation The Renaissance: 14th to 17th century The Renaissance, originating from Italy, ushered in a new age of scientific and intellectual inquiry and appreciation of ancient Greek and Roman civilizations. The merchant cities of Florence, Genoa, Ghent, Nuremberg, Geneva, Zurich, Lisbon and Seville provided patrons of the arts and sciences and unleashed a flurry of activity. The Medici became the leading family of Florence and fostered and inspired the birth of the Italian Renaissance along with other families of Italy, such as the Visconti and Sforza of Milan, the Este of Ferrara, and the Gonzaga of Mantua. Greatest artists like Brunelleschi, Botticelli, Da Vinci, Michelangelo, Giotto, Donatello, Titian and Raphael produced inspired works – their paintwork was more realistic-looking than had been created by medieval artists and their marble statues rivalled and sometimes surpassed those of Classical Antiquity. Michelangelo carved his masterpiece David from marble between 1501 and 1504. Humanist historian Leonardo Bruni, split the history in the antiquity, Middle Ages and modern period. Churches began being built in the Romanesque style for the first time in centuries. While art and architecture flourished in Italy and then the Netherlands, religious reformers flowered in Germany and Switzerland; printing was establishing itself in the Rhineland and navigators were embarking on extraordinary voyages of discovery from Portugal and Spain. Around 1450, Johannes Gutenberg developed a printing press, which allowed works of literature to spread more quickly. Secular thinkers like Machiavelli re-examined the history of Rome to draw lessons for civic governance. Theologians revisited the works of St Augustine. Important thinkers of the Renaissance in Northern Europe included the Catholic humanists Desiderius Erasmus, a Dutch theologian, and the English statesman and philosopher Thomas More, who wrote the seminal work Utopia in 1516. Humanism was an important development to emerge from the Renaissance. It placed importance on the study of human nature and worldly topics rather than religious ones. Important humanists of the time included the writers Petrarch and Boccaccio, who wrote in both Latin as had been done in the Middle Ages, as well as the vernacular, in their case Tuscan Italian. As the calendar reached the year 1500, Europe was blossoming – with Leonardo da Vinci painting his Mona Lisa portrait not long after Christopher Columbus reached the Americas (1492), Amerigo Vespucci proved that America is not a part of India, the Portuguese navigator Vasco Da Gama sailed around Africa into the Indian Ocean and Michelangelo completed his paintings of Old Testament themes on the ceiling of the Sistine Chapel in Rome (the expense of such artistic exuberance did much to spur the likes of Martin Luther in Northern Europe in their protests against the Church of Rome). For the first time in European history, events North of the Alps and on the Atlantic Coast were taking centre stage. Important artists of this period included Bosch, Dürer, and Breugel. In Spain Miguel de Cervantes wrote the novel Don Quixote, other important works of literature in this period were the Canterbury Tales by Geoffrey Chaucer and Le Morte d'Arthur by Sir Thomas Malory. The most famous playwright of the era was the Englishman William Shakespeare whose sonnets and plays (including Hamlet, Romeo and Juliet and Macbeth) are considered some of the finest works ever written in the English language. Meanwhile, the Christian kingdoms of northern Iberia continued their centuries-long fight to reconquer the peninsula from its Muslim rulers. In 1492, the last Islamic stronghold, Granada, fell, and Iberia was divided between the Christian kingdoms of Spain and Portugal. Iberia's Jewish and Muslim minorities were forced to convert to Catholicism or be exiled. The Portuguese immediately looked to expand outward sending expeditions to explore the coasts of Africa and engage in trade with the mostly Muslim powers on the Indian Ocean, making Portugal wealthy. In 1492, a Spanish expedition of Christopher Columbus found the Americas during an attempt to find a western route to East Asia. From the East, however, the Ottoman Turks under Suleiman the Magnificent continued their advance into the heart of Christian Europe – besieging Vienna in 1529. The 16th century saw the flowering of the Renaissance in the rest of the West. In the Kingdom of Poland, astronomer Nicolaus Copernicus deduced that the geocentric model of the universe was incorrect, and that in fact the planets revolve around the Sun. In the Netherlands, the invention of the telescope and the microscope resulted in the investigation of the universe and the microscopic world. The father of modern science Galileo and Christiaan Huygens developed more advance telescopes and used these in their scientific research. The father of microbiology, Antonie van Leeuwenhoek pioneered the use of the microscope in the study of microbes and established microbiology as a scientific discipline. Advances in medicine and understanding of the human anatomy also increased in this time. Gerolamo Cardano partially invented several machines and introduced essential mathematics theories. In England, Sir Isaac Newton pioneered the science of physics. These events led to the so-called Scientific Revolution, which emphasized experimentation. The Reformation: 1500–1650 The other major movement in the West in the 16th century was the Reformation, which would profoundly change the West and end its religious unity. The Reformation began in 1517 when the Catholic monk Martin Luther wrote his 95 Theses, which denounced the wealth and corruption of the church, as well as many Catholic beliefs, including the institution of the papacy and the belief that, in addition to faith in Christ, "good works" were also necessary for salvation. Luther drew on the beliefs of earlier church critics, like the Bohemian Jan Hus and the Englishman John Wycliffe. Luther's beliefs eventually ended in his excommunication from the Catholic Church and the founding of a church based on his teachings: the Lutheran Church, which became the majority religion in northern Germany. Soon other reformers emerged, and their followers became known as Protestants. In 1525, Ducal Prussia became the first Lutheran state. In the 1540s the Frenchman John Calvin founded a church in Geneva which forbade alcohol and dancing, and which taught God had selected those destined to be saved from the beginning of time. His Calvinist Church gained about half of Switzerland and churches based on his teachings became dominant in the Netherlands (the Dutch Reformed Church) and Scotland (the Presbyterian Church). In England, when the Pope failed to grant King Henry VIII a divorce, he declared himself head of the Church in England (founding what would evolve into today's Church of England and Anglican Communion). Some Englishmen felt the church was still too similar to the Catholic Church and formed the more radical Puritanism. Many other small Protestant sects were formed, including Zwinglianism, Anabaptism and Mennonism. Although they were different in many ways, Protestants generally called their religious leaders ministers instead of priests, and believed only the Bible, and not Tradition offered divine revelation. Britain and the Dutch Republic allowed Protestant dissenters to migrate to their North American colonies – thus the future United States found its early Protestant ethos – while Protestants were forbidden to migrate to the Spanish colonies (thus South America retained its Catholic hue). A more democratic organisational structure within some of the new Protestant movements – as in the Calvinists of New England – did much also to foster a democratic spirit in Britain's American colonies. The Catholic Church responded to the Reformation with the Counter Reformation. Some of Luther and Calvin's criticisms were heeded: the selling of indulgences was reined in by the Council of Trent in 1562. But exuberant baroque architecture and art was embraced as an affirmation of the faith and new seminaries and orders were established to lead missions to far off lands. An important leader in this movement was Saint Ignatius of Loyola, founder of the Society of Jesus (Jesuit Order) which gained many converts and sent such famous missionaries as Saints Matteo Ricci to China, Francis Xavier to India and Peter Claver to the Americas. As princes, kings and emperors chose sides in religious debates and sought national unity, religious wars erupted throughout Europe, especially in the Holy Roman Empire. Emperor Charles V was able to arrange the Peace of Augsburg between the warring Catholic and Protestant nobility. However, in 1618, the Thirty Years' War began between Protestants and Catholics in the empire, which eventually involved neighboring countries like France. The devastating war finally ended in 1648. In the Peace of Westphalia ending the war, Lutheranism, Catholicism and Calvinism were all granted toleration in the empire. The two major centers of power in the empire after the war were Protestant Prussia in the north and Catholic Austria in the south. The Dutch, who were ruled by the Spanish at the time, revolted and gained independence, founding a Protestant country. The Elizabethan era is famous above all for the flourishing of English drama, led by playwrights such as William Shakespeare and for the seafaring prowess of English adventurers such as Sir Francis Drake. Her 44 years on the throne provided welcome stability and helped forge a sense of national identity. One of her first moves as queen was to support the establishment of an English Protestant church, of which she became the Supreme Governor of what was to become the Church of England. By 1650, the religious map of Europe had been redrawn: Scandinavia, Iceland, north Germany, part of Switzerland, Netherlands and Britain were Protestant, while the rest of the West remained Catholic. A byproduct of the Reformation was increasing literacy as Protestant powers pursued an aim of educating more people to be able to read the Bible. Rise of Western empires: 1500–1800 From its dawn until modern times, the West had suffered invasions from Africa, Asia, and non-Western parts of Europe. By 1500 Westerners took advantage of their new technologies, sallied forth into unknown waters, expanded their power and the Age of Discovery began, with Western explorers from seafaring nations like Portugal and Castile (later Spain) and later Holland, France and England setting forth from the "Old World" to chart faraway shipping routes and discover "new worlds". In 1492, the Genovese born mariner, Christopher Columbus set out under the auspices of the Crown of Castile (Spain) to seek an oversea route to the East Indies via the Atlantic Ocean. Rather than Asia, Columbus landed in the Bahamas, in the Caribbean. Spanish colonization followed and Europe established Western Civilization in the Americas. The Portuguese explorer Vasco da Gama led the first sailing expedition directly from Europe to India in 1497–1499, by the Atlantic and Indian oceans, opening up the possibility of trade with the East other than via perilous overland routes like the Silk Road. Ferdinand Magellan, a Portuguese explorer working for the Spanish Crown (under the Crown of Castile), led an expedition in 1519–1522 which became the first to sail from the Atlantic Ocean into the Pacific Ocean and the first to cross the Pacific. The Spanish explorer Juan Sebastián Elcano completed the first circumnavigation of the Earth (Magellan was killed in the Philippines). The Americas were deeply affected by European expansion, due to conquest, sickness, and introduction of new technologies and ways of life. The Spanish Conquistadors conquered most of the Caribbean islands and overran the two great New World empires: the Aztec Empire of Mexico and the Inca Empire of Peru. From there, Spain conquered about half of South America, all of Central America and much of North America. Portugal also expanded in the Americas, attempting to establish some fishing colonies in northern North America first (with a relatively limited duration) and conquering half of South America and calling their colony Brazil. These Western powers were aided not only by superior technology like gunpowder, but also by Old World diseases which they inadvertently brought with them, and which wiped out large segments of the Amerindian population. The native populations, called Indians by Columbus, since he originally thought he had landed in Asia (but often called Amerindians by scholars today), were converted to Catholicism and adopted the language of their rulers, either Spanish or Portuguese. They also adopted much of Western culture. Many Iberian settlers arrived, and many of them intermarried with the Amerindians resulting in a so-called Mestizo population, which became the majority of the population of Spain's American empires. Other European colonial powers followed in their wake, most prominently the English, Dutch, and French. All three nations established colonies through North and South America and the West Indies. English colonies were established on Caribbean islands such as Barbados, Saint Kitts and Antigua and on North America (largely through a proprietary system) in regions such as Maryland, Massachusetts and Rhode Island. Dutch and French colonization efforts followed a similar pattern, focusing on the Caribbean and North America. The islands of Aruba, Curaçao and Sint Maarten gradually came under Dutch control, while the Dutch established the colony of New Netherland in North America. France gradually colonized Louisiana and Quebec during the 17th and 18th centuries, and transformed its West Indian colony of Saint-Domingue into the wealthiest European overseas possession in the 18th century through a slave-based plantation economy. In the Americas, it seems that only the most remote peoples managed to stave off complete assimilation by Western and Western-fashioned governments. These include some of the northern peoples (i.e., Inuit), some peoples in the Yucatán, Amazonian forest dwellers, and various Andean groups. Of these, the Quechua people, Aymara people, and Maya people are the most numerous: at around 10–11 million, 2 million, and 7 million, respectively. Bolivia is the only country in the Americas with an indigenous majority. Contact between the Old and New Worlds produced the Columbian Exchange, named after Columbus. It involved the transfer of goods unique to one hemisphere to another. Westerners brought cattle, horses, and sheep to the New World, and from the New World Europeans received tobacco, potatoes, and bananas. Other items becoming important in global trade were the sugarcane and cotton crops of the Americas, and the gold and silver brought from the Americas not only to Europe but elsewhere in the Old World. As European settlers began to colonize the Americas, numerous cash crop plantations sprung up to accommodate increasing demand in Europe. Initially, the labour source of these plantations came from European indentured servants; however, soon this system was supplemented by enslaved Africans imported by European slavers from Africa to the Americas via the transatlantic slave trade. Roughly 12 million enslaved Africans were forcibly transported to the Americas, primarily to the West Indies and South America. Once there, they were primarily forced to work on these plantations in brutal conditions, cultivating crops such as sugar, cotton and tobacco. Together with European trade to Africa and American trade to Europe, this trade was known as the "triangular trade". Slavery continued to underpin the economies of European colonies throughout the Americas until the abolitionist movement and slave resistance led to its abolition in the 19th century. After trading with African rulers for some time, Westerners began establishing colonies in Africa. The Portuguese conquered ports in North, West and East Africa and inland territory in what is today Angola and Mozambique. They also established relations with the Kingdom of Kongo in central Africa before, and eventually the Kongolese converted to Catholicism. The Dutch established colonies in modern-day South Africa, which attracted many Dutch settlers. Western powers also established colonies in West Africa. However, most of the continent remained unknown to Westerners and their colonies were restricted to Africa's coasts. Westerners also expanded in Asia. The Portuguese controlled port cities in the East Indies, India, Persian Gulf, Sri Lanka, Southeast Asia and China. During this time, the Dutch began their colonisation of the Indonesian archipelago, which became the Dutch East Indies in the early 19th century, and gained port cities in Sri Lanka and Malaysia and India. Spain conquered the Philippines and converted the inhabitants to Catholicism. Missionaries from Iberia (including some from Italy and France) gained many converts in Japan until Christianity was outlawed by Japan's emperor. Some Chinese also became Christian, although most did not. Most of India was divided up between England and France. As Western powers expanded they competed for land and resources. In the Caribbean, pirates attacked each other and the navies and colonial cities of countries, in hopes of stealing gold and other valuables from a ship or city. This was sometimes supported by governments. For example, England supported the pirate Sir Francis Drake in raids against the Spanish. Between 1652 and 1678, the three Anglo-Dutch wars were fought, of which the last two were won by the Dutch. At the end of the Napoleonic Wars, England gained New Netherland (which was traded with Suriname and Dutch South Africa). In 1756, the Seven Years' War, or French and Indian War began. It involved several powers fighting on several continents. In North America, English soldiers and colonial troops defeated the French, and in India the French were also defeated by England. In Europe Prussia defeated Austria. When the war ended in 1763, New France and eastern Louisiana were ceded to England, while western Louisiana was given to Spain. France's lands in India were ceded to England. Prussia was given rule over more territory in what is today Germany. The Dutch navigator Willem Janszoon had been the first documented Westerner to land in Australia in 1606: section III.B Another Dutchman, Abel Tasman later touched mainland Australia, and mapped Tasmania and New Zealand for the first time, in the 1640s. The English navigator James Cook became first to map the east coast of Australia in 1770. Cook's extraordinary seamanship greatly expanded European awareness of far shores and oceans: his first voyage reported favourably on the prospects of colonisation of Australia; his second voyage ventured almost to Antarctica (disproving long held European hopes of an undiscovered Great Southern Continent); and his third voyage explored the Pacific coasts of North America and Siberia and brought him to Hawaii, where an ill-advised return after a lengthy stay saw him clubbed to death by natives. Europe's period of expansion in early modern times greatly changed the world. New crops from the Americas improved European diets. This, combined with an improved economy thanks to Europe's new network of colonies, led to a demographic revolution in the West, with infant mortality dropping, and Europeans getting married younger and having more children. The West became more sophisticated economically, adopting Mercantilism, in which companies were state-owned and colonies existed for the good of the mother country. Enlightenment Absolutism and the Enlightenment: 1500–1800 The West in the early modern era went through great changes as the traditional balance between monarchy, nobility and clergy shifted. With the feudal system all but gone, nobles lost their traditional source of power. Meanwhile, in Protestant countries, the church was now often headed by a monarch, while in Catholic countries, conflicts between monarchs and the Church rarely occurred and monarchs were able to wield greater power than they ever had in Western history. Under the doctrine of the Divine right of kings, monarchs believed they were only answerable to God: thus giving rise to absolutism. At the opening of the 15th century, tensions were still going on between Islam and Christianity. Europe, dominated by Christians, remained under threat from the Muslim Ottoman Turks. The Turks had migrated from central to western Asia and converted to Islam years earlier. Their capture of Constantinople in 1453, thus extinguishing the Eastern Roman Empire, was a crowning achievement for the new Ottoman Empire. They continued to expand across the Middle East, North Africa and the Balkans. Under the leadership of the Spanish, a Christian coalition destroyed the Ottoman navy at the battle of Lepanto in 1571 ending their naval control of the Mediterranean. However, the Ottoman threat to Europe was not ended until a Polish led coalition defeated the Ottoman at the Battle of Vienna in 1683. The Turks were driven out of Buda (the eastern part of Budapest they had occupied for a century), Belgrade, and Athens – though Athens was to be recaptured and held until 1829. The 16th century is often called Spain's Siglo de Oro (golden century). From its colonies in the Americas it gained large quantities of gold and silver, which helped make Spain the richest and most powerful country in the world. One of the greatest Spanish monarchs of the era was Charles I (1516–1556, who also held the title of Holy Roman Emperor Charles V). His attempt to unite these lands was thwarted by the divisions caused by the Reformation and ambitions of local rulers and rival rulers from other countries. Another great monarch was Philip II (1556–1598), whose reign was marked by several Reformation conflicts, like the loss of the Netherlands and the Spanish Armada. These events and an excess of spending would lead to a great decline in Spanish power and influence by the 17th century. After Spain began to decline in the 17th century, the Dutch, by virtue of its sailing ships, became the greatest world power, leading the 17th century to be called the Dutch Golden Age. The Dutch followed Portugal and Spain in establishing an overseas colonial empire – often under the corporate colonialism model of the East India and West India Companies. After the Anglo-Dutch Wars, France and England emerged as the two greatest powers in the 18th century. Louis XIV became king of France in 1643. His reign was one of the most opulent in European history. He built a large palace in the town of Versailles. The Holy Roman Emperor exerted no great influence on the lands of the Holy Roman Empire by the end of the Thirty Years' War. In the north of the empire, Prussia emerged as a powerful Protestant nation. Under many gifted rulers, like King Frederick the Great, Prussia expanded its power and defeated its rival Austria many times in war. Ruled by the Habsburg dynasty, Austria became a great empire, expanding at the expense of the Ottoman Empire and Hungary. One land where absolutism did not take hold was England, which had trouble with revolutionaries. Elizabeth I, daughter of Henry VIII, had left no direct heir to the throne. The rightful heir was actually James VI of Scotland, who was crowned James I of England. James's son, Charles I resisted the power of Parliament. When Charles attempted to shut down Parliament, the Parliamentarians rose up and soon all of England was involved in a civil war. The English Civil War ended in 1649 with the defeat and execution of Charles I. Parliament declared a kingless Commonwealth but soon appointed the anti-absolutist leader and staunch Puritan Oliver Cromwell as Lord Protector. Cromwell enacted many unpopular Puritan religious laws in England, like outlawing alcohol and theaters, although religious diversity may have grown. (It was Cromwell, after all, that invited the Jews back into England after the Edict of Expulsion.) After his death, the monarchy was restored under Charles's son, who was crowned Charles II. His brother, James II succeeded him. James and his infant son James Francis Edward Stuart were Catholics. Not wanting to be ruled by a Catholic dynasty, Parliament invited James's daughter Mary and her husband William of Orange, to rule as co-monarchs. They agreed on the condition James would not be harmed. Realizing he could not count on the Protestant English army to defend him, he abdicated following the Glorious Revolution of 1688. Before William III and Mary II were crowned however, Parliament forced them to sign the English Bill of Rights, which guaranteed some basic rights to all Englishmen, granted religious freedom to non-Anglican Protestants, and firmly established the rights of Parliament. In 1707, the Act of Union of 1707 were passed by the parliaments of Scotland and England, merging Scotland and England into a single Kingdom of Great Britain, with a single parliament. This new kingdom also controlled Ireland which had previously been conquered by England. Following the Irish Rebellion of 1798, in 1801 Ireland was formally merged with Great Britain to form the United Kingdom of Great Britain and Ireland. Ruled by the Protestant Ascendancy, Ireland eventually became an English-speaking land, though the majority population preserved distinct cultural and religious outlooks, remaining predomininantly Catholic except in parts of Ulster and Dublin. By then, the British experience had already contributed to the American Revolution. The Polish–Lithuanian Commonwealth was an important European center for the development of modern social and political ideas. It was famous for its rare quasi-democratic political system, praised by philosophers such as Erasmus; and, during the Counter-Reformation, was known for near-unparalleled religious tolerance, with peacefully coexisting Catholic, Jewish, Eastern Orthodox, Protestant and Muslim communities. With its political system the Commonwealth gave birth to political philosophers such as Andrzej Frycz Modrzewski (1503–1572), Wawrzyniec Grzymała Goślicki (1530–1607) and Piotr Skarga (1536–1612). Later, works by Stanisław Staszic (1755–1826) and Hugo Kołłątaj (1750–1812) helped pave the way for the Constitution of 3 May 1791, which historian Norman Davies calls "the first constitution of its kind in Europe". Polish–Lithuanian Commonwealth's constitution enacted revolutionary political principles for the first time on the European continent. The Komisja Edukacji Narodowej, Polish for Commission of National Education, formed in 1773, was the world's first national Ministry of Education and an important achievement of the Polish Enlightenment. The intellectual movement called the Age of Enlightenment began in this period as well. Its proponents opposed the absolute rule of the monarchs, and instead emphasized the equality of all individuals and the idea that governments should derive their existence from the consent of the governed. Enlightenment thinkers called philosophes (French for philosophers) idealized Europe's classical heritage. They looked at Athenian democracy and the Roman Republic as ideal governments. They believed reason held the key to creating an ideal society. The Englishman Francis Bacon espoused the idea that senses should be the primary means of knowing, while the Frenchman René Descartes advocated using reason over the senses. In his works, Descartes was concerned with using reason to prove his own existence and the existence of the external world, including God. Another belief system became popular among philosophes, Deism, which taught that a single god had created but did not interfere with the world. This belief system never gained popular support and largely died out by the early 19th century. Thomas Hobbes was an English philosopher, best known today for his work on political philosophy. His 1651 book Leviathan established the foundation for most of Western political philosophy from the perspective of social contract theory. The theory was examined also by John Locke (Second Treatise of Government (1689)) and Rousseau (Du contrat social (1762)). Social contract arguments examine the appropriate relationship between government and the governed and posit that individuals unite into political societies by a process of mutual consent, agreeing to abide by common rules and accept corresponding duties to protect themselves and one another from violence and other kinds of harm. In 1690 John Locke wrote that people have certain natural rights like life, liberty and property and that governments were created in order to protect these rights. If they did not, according to Locke, the people had a right to overthrow their government. The French philosopher Voltaire criticized the monarchy and the Church for what he saw as hypocrisy and for their persecution of people of other faiths. Another Frenchman, Montesquieu, advocated division of government into executive, legislative and judicial branches. The French author Rousseau stated in his works that society corrupted individuals. Many monarchs were affected by these ideas, and they became known to history as the enlightened despots. However, most only supported Enlightenment ideas that strengthened their own power. The Scottish Enlightenment was a period in 18th century Scotland characterised by an outpouring of intellectual and scientific accomplishments. Scotland reaped the benefits of establishing Europe's first public education system and a growth in trade which followed the Act of Union with England of 1707 and expansion of the British Empire. Important modern attitudes towards the relationship between science and religion were developed by the philosopher/historian David Hume. Adam Smith developed and published The Wealth of Nations, the first work in modern economics. He believed competition and private enterprise could increase the common good. The celebrated bard Robert Burns is still widely regarded as the national poet of Scotland. European cities like Paris, London, and Vienna grew into large metropolises in early modern times. France became the cultural center of the West. The middle class grew even more influential and wealthy. Great artists of this period included El Greco, Rembrandt, and Caravaggio. By this time, many around the world wondered how the West had become so advanced, for example, the Orthodox Christian Russians, who came to power after conquering the Mongols that had conquered Kiev in the Middle Ages. They began westernizing under Czar Peter the Great, although Russia remained uniquely part of its own civilization. The Russians became involved in European politics, dividing up the Polish–Lithuanian Commonwealth with Prussia and Austria. Revolution: 1770–1815 During the late 18th century and early 19th century, much of the West experienced a series of revolutions that would change the course of history, resulting in new ideologies and changes in society. The first of these revolutions began in North America. The Thirteen Colonies of British North America had by this period. The majority of the population was of English, Scottish, Welsh and Irish descent, while significant minorities included people of French, Dutch and German and Africa descent, as well as some Native Americans. Most of the population was Anglican, others were Congregationalist or Puritan, while minorities included other Protestant churches like the Society of Friends and the Lutherans, as well as some Roman Catholics and Jews. The colonies had their own great cities and universities and continually welcomed new immigrants, mostly from Britain. After the expensive Seven Years' War, Britain needed to raise revenue, and felt the colonists should bare the brunt of the new taxation it felt was necessary. The colonists greatly resented these taxes and protested the fact they could be taxed by Britain but had no representation in the government. After Britain's King George III refused to seriously consider colonial grievances raised at the first Continental Congress, some colonists took up arms. Leaders of a new pro-independence movement were influenced by Enlightenment ideals and hoped to bring an ideal nation into existence. On 4 July 1776, the colonies declared independence with the signing of the United States Declaration of Independence. Drafted primarily by Thomas Jefferson, the document's preamble eloquently outlines the principles of governance that would come to increasingly dominate Western thinking over the ensuing century and a half: We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are life, liberty and the pursuit of happiness. That to secure these rights, Governments are instituted among Men, deriving their just powers from the consent of the governed, That whenever any Form of Government becomes destructive of these ends, it is the Right of the People to alter or to abolish it, and to institute new Government. George Washington led the new Continental Army against the British forces, who had many successes early in this American Revolution. After years of fighting, the colonists formed an alliance with France and defeated the British at Yorktown, Virginia in 1781. The treaty ending the war granted independence to the colonies, which became The United States of America. The other major Western revolution at the turn of the 19th century was the French Revolution. In 1789 France faced an economical crisis. The King called, for the first time in more than two centuries, the Estates General, an assembly of representatives of each estate of the kingdom: the First Estate (the clergy), the Second Estate (the nobility), and the Third Estate (middle class and peasants); in order to deal with the crisis. As the French society was gained by the same Enlightenment ideals that led to the American revolution, in which many Frenchmen, such as Lafayette, took part; representatives of the Third Estate, joined by some representatives of the lower clergy, created the National Assembly, which, unlike the Estates General, provided the common people of France with a voice proportionate to their numbers. The people of Paris feared the King would try to stop the work of the National Assembly and Paris was soon consumed with riots, anarchy, and widespread looting. The mobs soon had the support of the French Guard, including arms and trained soldiers, because the royal leadership essentially abandoned the city. On the fourteenth of July 1789 a mob stormed the Bastille, a prison fortress, which led the King to accept the changes. On 4 August 1789 the National Constituent Assembly abolished feudalism sweeping away both the seigneurial rights of the Second Estate and the tithes gathered by the First Estate. It was the first time in Europe, where feudalism was the norm for centuries, that such a thing happened. In the course of a few hours, nobles, clergy, towns, provinces, companies, and cities lost their special privileges. At first, the revolution seemed to be turning France into a constitutional monarchy, but the other continental Europe powers feared a spread of the revolutionary ideals and eventually went to war with France. In 1792 King Louis XVI was imprisoned after he had been captured fleeing Paris and the Republic was declared. The Imperial and Prussian armies threatened retaliation on the French population should it resist their advance or the reinstatement of the monarchy. As a consequence, King Louis was seen as conspiring with the enemies of France. His execution on 21 January 1793 led to more wars with other European countries. During this period France effectively became a dictatorship after the parliamentary coup of the radical leaders, the Jacobin. Their leader, Robespierre oversaw the Reign of Terror, in which thousands of people deemed disloyal to the republic were executed. Finally, in 1794, Robespierre himself was arrested and executed, and more moderate deputies took power. This led to a new government, the French Directory. In 1799, a coup overthrew the Directory and General Napoleon Bonaparte seized power as dictator and even an emperor in 1804. Liberté, égalité, fraternité (French for "Liberty, equality, fraternity"), now the national motto of France, had its origins during the French Revolution, though it was only later institutionalised. It remains another iconic motto of the aspirations of Western governance in the modern world. Some influential intellectuals came to reject the excesses of the revolutionary movement. Political theorist Edmund Burke had supported the American Revolution, but turned against the French Revolution and developed a political theory which opposed governing based on abstract ideas, and preferred 'organic' reform. He is remembered as a father of modern Anglo-conservatism. In response to such critiques, the American revolutionary Thomas Paine published his book The Rights of Man in 1791 as a defence of the ideals of the French Revolution. The spirit of the age also produced early works of feminist philosophy – notably Mary Wollstonecraft's 1792 book: A Vindication of the Rights of Woman. Napoleonic Wars The Napoleonic Wars were a series of conflicts involving Napoleon's French Empire and changing sets of European allies by opposing coalitions that ran from 1803 to 1815. As a continuation of the wars sparked by the French Revolution of 1789, they revolutionized European armies and played out on an unprecedented scale, mainly due to the application of modern mass conscription. French power rose quickly, conquering most of Europe, but collapsed rapidly after France's disastrous invasion of Russia in 1812. Napoleon's empire ultimately suffered complete military defeat resulting in the restoration of the Bourbon monarchy in France. The wars resulted in the dissolution of the Holy Roman Empire and sowed the seeds of nascent nationalism in Germany and Italy that would lead to the two nations' consolidation later in the century. Meanwhile, the Spanish Empire began to unravel as French occupation of Spain weakened Spain's hold over its colonies, providing an opening for nationalist revolutions in Spanish America. As a direct result of the Napoleonic Wars, the British Empire became the foremost world power for the next century, thus beginning Pax Britannica. France had to fight on multiple battlefronts against the other European powers. A nationwide conscription was voted to reinforce the old royal army made of noble officers and professional soldiers. With this new kind of army, Napoleon was able to beat the European allies and dominate Europe. The revolutionary ideals, based no more on feudalism but on the concept of a sovereign nation, spread all over Europe. When Napoleon eventually lost and the monarchy reinstated in France, these ideals survived and led to the revolutionary waves of the 19th century that brought democracy to many European countries. With the success of the American Revolution, the Spanish Empire also began to crumble as their American colonies sought independence as well. In 1808, when Joseph Bonaparte was installed as the Spanish King by the Napoleonic French, the Spanish resistance resorted to governing Juntas. When the Supreme Central Junta of Seville fell to the French in 1810, the Spanish American colonies developed themselves governing Juntas in the name of the deposed King Ferdinand VII (upon the concept known as "Retroversion of the sovereignty to the people"). As this process led to open conflicts between independentists and loyalists, the Spanish American Independence Wars immediately ensued; resulting, by the 1820s, in the definitive loss for the Spanish Empire of all its American territories, with the exception of Cuba and Puerto Rico. Rise of the English-speaking world: 1815–1870 The years following Britain's victory in the Napoleonic Wars were a period of expansion for Britain as it rebuilt the British Empire. The new United States grew even more rapidly. This period of expansion would help establish Anglicanism as the dominant religion, English as the dominant language, and English and Anglo-American culture as the dominant culture of two continents and many other lands outside the British Isles. Industrial Revolution in the English-speaking world Rapid economic growth following the Napoleonic Wars was the continuing product of the ever-expanding Industrial Revolution. The revolution began in Britain, where Thomas Newcomen developed a steam engine in 1712 to pump seeping water out of mines. This engine at first was powered by water, but soon coal and wood were heavily used. The British first learned to use steam power effectively. In 1804, the first steam powered railroad locomotive was developed in Britain, which allowed goods and people to be transported at faster speeds than ever before in history. Soon, large numbers of goods were being produced in factories. This resulted in great societal changes, and many people settled in the cities where the factories were located. Factory work could often be brutal. With no safety regulations, people became sick from contaminants in the air in textile mills for, example. Many workers were also horribly maimed by dangerous factory machinery. Since workers relied only on their small wages for sustenance, entire families were forced to work, including children. These and other problems caused by industrialism resulted in some reforms by the mid-19th century. The economic model of the West also began to change, with mercantilism being replaced by capitalism, in which companies, and later, large corporations, were run by individual investors. New ideological movements began as a result of the Industrial Revolution, including the Luddite movement, which opposed machinery, feeling it did not benefit the common good, and the socialists, whose beliefs usually included the elimination of private property and the sharing of industrial wealth. Unions were founded among industrial workers to help secure better wages and rights. Another result of the revolution was a change in societal hierarchy, especially in Europe, where nobility still occupied a high level on the social ladder. Capitalists emerged as a new powerful group, with educated professionals like doctors and lawyers under them, and the various industrial workers at the bottom. These changes were often slow however, with Western society as a whole remaining primarily agricultural for decades. Great Britain: 1815–1870 From 1837 until 1901, Queen Victoria reigned over Great Britain and the ever-expanding British Empire. The Industrial Revolution accelerated, making Britain the most powerful nation. It enjoyed relative peace and stability from 1815 until 1914, this period is often called the Pax Britannica, from the Latin "British Peace". The monarch became more a figurehead and symbol of national identity; actual power was in the hands of the prime minister and the cabinet, and was based on a majority in the House of Commons. Two rival parties were the Conservative Party and the Liberal Party. The Liberal constituency was made up mostly of businessmen, as many Liberals supported the idea of a free market. Conservatives were supported by the aristocracy and gentry landowners. Control of Parliament switched back and forth between the parties. Overall, it was a period of reform. In 1832 more representation was granted to new industrial cities, and laws barring Catholics from serving in Parliament were repealed, although discrimination against Catholics, especially Irish Catholics, continued. Other reforms granted near universal manhood suffrage, and state-supported elementary education for all Britons. More rights were granted to workers as well. Ireland had been ruled from London since the Middle Ages. After the Protestant Reformation the British Establishment began a campaign of discrimination against Roman Catholic and Presbyterian Irish, who lacked many rights under the Penal Laws, and the majority of the agricultural land was owned by the Protestant Ascendancy. Great Britain and Ireland had become a single nation ruled from London without the autonomous Parliament of Ireland after the Act of Union of 1800 was passed, creating the United Kingdom of Great Britain and Ireland. In the mid-19th century, Ireland suffered a devastating famine, which killed 10% of the population and led to massive emigration: see Irish diaspora. British Empire: 1815–1870 Throughout the 19th century, Britain's power grew enormously and the sun quite literally "never set" on the British Empire, for it had outposts on every occupied continent. It consolidated control over such far flung territories as Canada and Jamaica in the Americas, Australia and New Zealand in Oceania; Malaya, Hong Kong and Singapore in the Far East and a line of colonial possessions from Egypt to the Cape of Good Hope through Africa. All of India was under British rule by 1858. In 1804, the Shah of the declining Mughal Empire had formally accepted the protection of the British East India Company. Many Britons settled in India, establishing a rich ruling class. They then expanded into neighbouring Burma. Among the British born in India were the immensely influential writers Rudyard Kipling (1865) and George Orwell (1903). In the Far East, Britain went to war with the declining Qing dynasty of China when it tried to stop British merchants in China from selling the opium to the Chinese public. The First Opium War (1840–1842), ended in a British victory, and China was forced to remove barriers to British trade and cede several ports and the island of Hong Kong to Britain. Soon, other powers sought these same privileges with China and China was forced to agree, ending Chinese isolation from the rest of the world. In 1853 an American expedition opened up Japan to trade with first the U.S., and then the rest of the world. In 1833, Britain ended slavery by buying out all the owners throughout its empire after a successful campaign by abolitionists. Furthermore, Britain had a great deal of success attempting to get other powers to outlaw the practice as well. As British settlement of southern Africa continued, the descendants of the Dutch in southern Africa, called the Boers or Afrikaners, whom Britain had ruled since the Anglo-Dutch Wars, migrated northward, disliking British rule. Explorers and missionaries like David Livingstone became national heroes. Cecil Rhodes founded Rhodesia and a British army under Lord Kitchener secured control of Sudan in the 1898 Battle of Omdurman. Canada: 1815–1870 Following the American Revolution, many Loyalists to Britain fled north to what is today Canada (where they were called United Empire Loyalists). Joined by mostly British colonists, they helped establish early colonies like Ontario and New Brunswick. British settlement in North America increased, and soon there were several colonies both north and west of the early ones in the northeast of the continent, these new ones included British Columbia and Prince Edward Island. Rebellions broke out against British rule in 1837, but Britain appeased the rebels' supporters in 1867 by confederating the colonies into Canada, with its own prime minister. Although Canada was still firmly within the British Empire, its people now enjoyed a great degree of self-rule. Canada was unique in the British Empire in that it had a French-speaking province, Quebec, which Britain had gained rule over in the Seven Years' War. Australia and New Zealand: 1815–1870 The First Fleet of British convicts arrived at New South Wales, Australia in 1788 and established a British outpost and penal colony at Sydney Cove. These convicts were often petty 'criminals', and represented the population spill-over of Britain's Industrial Revolution, as a result of the rapid urbanisation and dire crowding of British cities. Other convicts were political dissidents, particularly from Ireland. The establishment of a wool industry and the enlightened governorship of Lachlan Macquarie were instrumental in transforming New South Wales from a notorious prison outpost into a budding civil society. Further colonies were established around the perimeter of the continent and European explorers ventured deep inland. A free colony was established at South Australia in 1836 with a vision for a province of the British Empire with political and religious freedoms. The colony became a cradle of democratic reform. The Australian gold rushes increased prosperity and cultural diversity and autonomous democratic parliaments began to be established from the 1850s onward. The native inhabitants of Australia, called the Aborigines, lived as hunter-gatherers before European arrival. The population, never large, was largely dispossessed without treaty agreements nor compensations through the 19th century by the expansion of European agriculture, and, as had occurred when Europeans arrived in North and South America, faced superior European weaponry and suffered greatly from exposure to Old World diseases such as smallpox, to which they had no biological immunity. From the early 19th century, New Zealand was being visited by European explorers, sailors, missionaries, traders and adventurers (known as Pākehā) and was administered by Britain from the nearby colony of New South Wales. In 1840 Britain signed the Treaty of Waitangi with the natives of New Zealand, the Māori, in which Britain gained sovereignty over the archipelago. As more European settlers arrived, clashes resulted and the New Zealand colonial government fought several wars before defeating the Māori. By 1870, New Zealand had a population made up mostly of European descent. United States: 1815–1870 Following independence from Britain, the United States began expanding westward, and soon a number of new states had joined the union. In 1803, the United States purchased the Louisiana Territory from France, whose emperor, Napoleon I, had regained it from Spain. Soon, America's growing population was settling the Louisiana Territory, which geographically doubled the size of the country. At the same time, a series of revolutions and independence movements in Spain and Portugal's American empires resulted in the liberation of nearly all of Latin America, as the region composed of South America, most of the Caribbean, and North America from Mexico south became known. At first Spain and its allies seemed ready to try to reconquer the colonies, but the U.S. and Britain opposed this, and the reconquest never took place. From 1821 on, the U.S. bordered the newly independent nation of Mexico. An early problem faced by the Mexican republic was what to do with its sparsely populated northern territories, which today make up a large part of the American West. The government decided to try to attract Americans looking for land. Americans arrived in such large numbers that both the provinces of Texas and California had majority white, English-speaking populations. This led to a culture clash between these provinces and the rest of Mexico. When Mexico became a dictatorship under General Antonio López de Santa Anna, the Texans declared independence. After several battles, Texas gained independence from Mexico, although Mexico later claimed it still had a right to Texas. After existing as a republic modeled after the U.S. for several years, Texas joined the United States in 1845. This led to border disputes between the U.S. and Mexico, resulting in the Mexican–American War. The war ended with an American victory, and Mexico had to cede all its northern territories to the United States, and recognize the independence of California, which had revolted against Mexico during the war. In 1850, California joined the United States. In 1848, the U.S. and Britain resolved a border dispute over territory on the Pacific coast, called the Oregon Country by giving Britain the northern part and the U.S. the southern part. In 1867, the U.S. expanded again, purchasing the Russian colony of Alaska, in northwestern North America. Politically, the U.S. became more democratic with the abolishment of property requirements in voting, although voting remained restricted to white males. By the mid-19th century, the most important issue was slavery. The Northern states generally had outlawed the practice, while the Southern states not only had kept it legal but came to feel it was essential to their way of life. As new states joined the union, lawmakers clashed over whether they should be slave states or free states. In 1860, the anti-slavery candidate Abraham Lincoln was elected president. Fearing he would try to outlaw slavery in the whole country, several southern states seceded, forming the Confederate States of America, electing their own president and raising their own army. Lincoln countered that secession was illegal and raised an army to crush the rebel government, thus the advent of the American Civil War (1861–65). The Confederates had a skilled military that even succeeded in invading the northern state of Pennsylvania. However, the war began to turn around, with the defeat of Confederates at Gettysburg, Pennsylvania, and at Vicksburg, which gave the Union control of the important Mississippi River. Union forces invaded deep into the South, and the Confederacy's greatest general, Robert E. Lee, surrendered to Ulysses S. Grant of the Union in 1865. After that, the south came under Union occupation, ending the American Civil War. Lincoln was tragically assassinated in 1865, but his dream of ending slavery, exhibited in the wartime Emancipation Proclamation, was carried out by his Republican Party, which outlawed slavery, granted blacks equality and black males voting rights via constitutional amendments. However, although the abolishment of slavery would not be challenged, equal treatment for blacks would be. The Gettysburg Address, Lincoln's most famous speech and one of the most quoted political speeches in United States history, was delivered at the dedication of the Soldiers' National Cemetery in Gettysburg, Pennsylvania on 19 November 1863, during the Civil War, four and a half months after the Battle of Gettysburg. Describing America as a "nation conceived in Liberty and dedicated to the proposition that all men are created equal", Lincoln famously called on those gathered: [We here] highly resolve that these dead shall not have died in vain; that this nation, under God, shall have a new birth of freedom; and that government of the people, by the people, for the people, shall not perish from the earth. Continental Europe: 1815–1870 The years following the Napoleonic Wars were a time of change in Europe. The Industrial Revolution, nationalism, and several political revolutions transformed the continent. Industrial technology was imported from Britain. The first lands affected by this were France, the Low Countries, and western Germany. Eventually the Industrial Revolution spread to other parts of Europe. Many people in the countryside migrated to major cities like Paris, Berlin, and Amsterdam, which were connected like never before by railroads. Europe soon had its own class of wealthy industrialists, and large numbers of industrial workers. New ideologies emerged as a reaction against perceived abuses of industrial society. Among these ideologies were socialism and the more radical communism, created by the German Karl Marx. According to communism, history was a series of class struggles, and at the time industrial workers were pitted against their employers. Inevitably the workers would rise up in a worldwide revolution and abolish private property, according to Marx. Communism was also atheistic, since, according to Marx, religion was simply a tool used by the dominant class to keep the oppressed class docile. Several revolutions occurred in Europe following the Napoleonic Wars. The goal of most of these revolutions was to establish some form of democracy in a particular nation. Many were successful for a time, but their effects were often eventually reversed. Examples of this occurred in Spain, Italy, and Austria. Several European nations stood steadfastly against revolution and democracy, including Austria and Russia. Two successful revolts of the era were the Greek and Serbian wars of independence, which freed those nations from Ottoman rule. Another successful revolution occurred in the Low Countries. After the Napoleonic Wars, the Netherlands was given control of modern-day Belgium, which had been part of the Holy Roman Empire. The Dutch found it hard to rule the Belgians, due to their Catholic religion and French language. In the 1830s, the Belgians successfully overthrew Dutch rule, establishing the Kingdom of Belgium. In 1848 a series of revolutions occurred in Prussia, Austria, and France. In France, the king, Louis-Philippe, was overthrown and a republic was declared. Louis Napoleon, nephew of Napoleon I was elected the republic's first president. Extremely popular, Napoleon was made Napoleon III (since Napoleon I's son had been crowned Napoleon II during his reign), Emperor of the French, by a vote of the French people, ending France's Second Republic. Revolutionaries in Prussia and Italy focused more on nationalism, and most advocated the establishment of unified German and Italian states, respectively. In the city-states of Italy, many argued for a unification of all the Italian kingdoms into a single nation. Obstacles to this included the many Italian dialects spoken by the people of Italy, and the Austrian presence in the north of the peninsula. Unification of the peninsula began in 1859. The powerful Kingdom of Sardinia (also called Savoy or Piedmont) formed an alliance with France and went to war with Austria in that year. The war ended with a Sardinian victory, and Austrian forces left Italy. Plebiscites were held in several cities, and the majority of people voted for union with Sardinia, creating the Kingdom of Italy under Victor Emmanuel II. In 1860, the Italian nationalist Garibaldi led revolutionaries in an overthrow of the government of the Kingdom of the Two Sicilies. A plebiscite held there resulted in a unification of that kingdom with Italy. Italian forces seized the eastern Papal States in 1861. In 1866 Venetia became part of Italy after Italy's ally, Prussia, defeated that kingdom's rulers, the Austrians, in the Austro-Prussian War. In 1870, Italian troops conquered the Papal States, completing unification. Pope Pius IX refused to recognize the Italian government or negotiate settlement for the loss of Church land. Prussia in the middle and late parts of the 19th century was ruled by its king, Wilhelm I, and its skilled chancellor, Otto von Bismarck. In 1864, Prussia went to war with Denmark and gained several German-speaking lands as a result. In 1866, Prussia went to war with the Austrian Empire and won, and created a confederation of it and several German states, called the North German Confederation, setting the stage for the 1871 formation of the German Empire. After years of dealing with Hungarian revolutionaries, whose kingdom Austria had conquered centuries earlier, the Austrian emperor, Franz Joseph agreed to divide the empire into two parts: Austria and Hungary, and rule as both Emperor of Austria and king of Hungary. The new Austro-Hungarian Empire was created in 1867. The two peoples were united in loyalty to the monarch and Catholicism. There were changes throughout the West in science, religion and culture between 1815 and 1870. Europe in 1870 differed greatly from its state in 1815. Most Western European nations had some degree of democracy, and two new national states had been created, Italy and Germany. Political parties were formed throughout the continent and with the spread of industrialism, Europe's economy was transformed, although it remained very agricultural. Culture, arts and sciences: 1815–1914 The 19th and early 20th centuries saw important contributions to the process of modernisation of Western art and Literature and the continuing evolution in the role of religion in Western societies. Napoleon re-established the Catholic Church in France through the Concordat of 1801. The end of the Napoleonic wars, signaled by the Congress of Vienna, brought Catholic revival and the return of the Papal States. In 1801, a new political entity was formed, the United Kingdom of Great Britain and Ireland, which merged the kingdoms of Great Britain and Ireland, thus increasing the number of Catholics in the new state. Pressure for abolition of anti-Catholic laws grew and in 1829 Parliament passed the Roman Catholic Relief Act 1829, giving Catholics almost equal civil rights, including the right to vote and to hold most public offices. While remaining a minority religion in the British Empire, a steady stream of new Catholics would continue to convert from the Church of England and Ireland, notably John Henry Newman and the poets Gerard Manley Hopkins and Oscar Wilde. The Anglo-Catholic movement began, emphasizing the Catholic traditions of the Anglican Church. New churches like the Methodist, Unitarian, and LDS Churches were founded. Many Westerners became less religious in this period, although a majority of people still held traditional Christian beliefs. The 1859 publication of On the Origin of Species, by the English naturalist Charles Darwin, provided an alternative hypothesis for the development, diversification, and design of human life to the traditional poetic scriptural explanation known as Creationism. According to Darwin, only the organisms most able to adapt to their environment survived while others became extinct. Adaptations resulted in changes in certain populations of organisms which could eventually cause the creation of new species. Modern genetics started with Gregor Johann Mendel, a German-Czech Augustinian monk who studied the nature of inheritance in plants. In his 1865 paper "Versuche über Pflanzenhybriden" ("Experiments on Plant Hybridization"), Mendel traced the inheritance patterns of certain traits in pea plants and described them mathematically. Louis Pasteur and Joseph Lister made discoveries about bacteria and its effects on humans. Geologists at the time made discoveries indicating the world was far older than most believed it to be. Early batteries were invented and a telegraph system was also invented, allowing global communication. In 1869 Russian chemist Dmitri Mendeleev published his Periodic table. The success of Mendeleev's table came from two decisions he made: The first was to leave gaps in the table when it seemed that the corresponding element had not yet been discovered. The second decision was to occasionally ignore the order suggested by the atomic weights and switch adjacent elements, such as cobalt and nickel, to better classify them into chemical families. At the end of the 19th century, a number of discoveries were made in physics which paved the way for the development of modern physics – including Maria Skłodowska-Curie's work on radioactivity. In Europe by the 19th century, fashion had shifted away from such artistic styles as Mannerism, Baroque and Rococo and sought to revert to the earlier, simpler art of the Renaissance by creating Neoclassicism. Neoclassicism complemented the intellectual movement known as the Enlightenment, which was similarly idealistic. Ingres, Canova, and Jacques-Louis David are among the best-known neoclassicists. Just as Mannerism rejected Classicism, so did Romanticism reject the ideas of the Enlightenment and the aesthetic of the Neoclassicists. Romanticism emphasized emotion and nature, and idealized the Middle Ages. Important musicians were Franz Schubert, Pyotr Tchaikovsky, Richard Wagner, Fryderyk Chopin, and John Constable. Romantic art focused on the use of color and motion in order to portray emotion, but like classicism used Greek and Roman mythology and tradition as an important source of symbolism. Another important aspect of Romanticism was its emphasis on nature and portraying the power and beauty of the natural world. Romanticism was also a large literary movement, especially in poetry. Among the greatest Romantic artists were Eugène Delacroix, Francisco Goya, Karl Bryullov, J. M. W. Turner, John Constable, Caspar David Friedrich, Ivan Aivazovsky, Thomas Cole, and William Blake. Romantic poetry emerged as a significant genre, particularly during the Victorian Era with leading exponents including William Wordsworth, Samuel Taylor Coleridge, Robert Burns, Edgar Allan Poe and John Keats. Other Romantic writers included Sir Walter Scott, Lord Byron, Alexander Pushkin, Victor Hugo, and Goethe. Some of the best regarded poets of the era were women. Mary Wollstonecraft had written one of the first works of feminist philosophy, A Vindication of the Rights of Woman which called for equal education for women in 1792 and her daughter, Mary Shelley became an accomplished author best known for her 1818 novel Frankenstein, which examined some of the frightening potential of the rapid advances of science. In early 19th-century Europe, in response to industrialization, the movement of Realism emerged. Realism sought to accurately portray the conditions and hardships of the poor in the hopes of changing society. In contrast with Romanticism, which was essentially optimistic about mankind, Realism offered a stark vision of poverty and despair. Similarly, while Romanticism glorified nature, Realism portrayed life in the depths of an urban wasteland. Like Romanticism, Realism was a literary as well as an artistic movement. The great Realist painters include Jean-Baptiste-Siméon Chardin, Gustave Courbet, Jean-François Millet, Camille Corot, Honoré Daumier, Édouard Manet, Edgar Degas (both considered as Impressionists), Ilya Repin, and Thomas Eakins, among others. Writers also sought to come to terms with the new industrial age. The works of the Englishman Charles Dickens (including his novels Oliver Twist and A Christmas Carol) and the Frenchman Victor Hugo (including Les Misérables) remain among the best known and widely influential. The first great Russian novelist was Nikolai Gogol (Dead Souls). Then came Ivan Goncharov, Nikolai Leskov and Ivan Turgenev. Leo Tolstoy (War and Peace, Anna Karenina) and Fyodor Dostoevsky (Crime and Punishment, The Idiot, The Brothers Karamazov) soon became internationally renowned to the point that many scholars such as F. R. Leavis have described one or the other as the greatest novelist ever. In the second half of the century Anton Chekhov excelled in writing short stories and became perhaps the leading dramatist internationally of his period. American literature also progressed with the development of a distinct voice: Mark Twain produced his masterpieces Tom Sawyer and Adventures of Huckleberry Finn. In Irish literature, the Anglo-Irish tradition produced Bram Stoker and Oscar Wilde writing in English and a Gaelic Revival had emerged by the end of the 19th century. The poetry of William Butler Yeats prefigured the emergence of the 20th-century Irish literary giants James Joyce, Samuel Beckett and Patrick Kavanagh. In Britain's Australian colonies, bush balladeers such as Henry Lawson and Banjo Paterson brought the character of a new continent to the pages of world literature. The response of architecture to industrialisation, in stark contrast to the other arts, was to veer towards historicism. The railway stations built during this period are often called "the cathedrals of the age". Architecture during the Industrial Age witnessed revivals of styles from the distant past, such as the Gothic Revival—in which style the iconic Palace of Westminster in London was re-built to house the mother parliament of the British Empire. Notre Dame de Paris Cathedral in Paris was also restored in the Gothic style, following its desecration during the French Revolution. Out of the naturalist ethic of Realism grew a major artistic movement, Impressionism. The Impressionists pioneered the use of light in painting as they attempted to capture light as seen from the human eye. Edgar Degas, Édouard Manet, Claude Monet, Camille Pissarro, and Pierre-Auguste Renoir, were all involved in the Impressionist movement. As a direct outgrowth of Impressionism came the development of Post-Impressionism. Paul Cézanne, Vincent van Gogh, Paul Gauguin, Georges Seurat are the best known Post-Impressionists. In Australia the Heidelberg School was expressing the light and colour of Australian landscape with a new insight and vigour. The Industrial Revolution which began in Britain in the 18th century brought increased leisure time, leading to more time for citizens to attend and follow spectator sports, greater participation in athletic activities, and increased accessibility. The bat and ball sport of cricket was first played in England during the 16th century and was exported around the globe via the British Empire. A number of popular modern sports were devised or codified in Britain during the 19th century and obtained global prominence – these include Ping Pong, modern tennis, Association football, netball and rugby. The United States also developed popular international sports during this period. English migrants took antecedents of baseball to America during the colonial period. American football resulted from several major divergences from rugby, most notably the rule changes instituted by Walter Camp. Basketball was invented in 1891 by James Naismith, a Canadian physical education instructor working in Springfield, Massachusetts in the United States. Baron Pierre de Coubertin, a Frenchman, instigated the modern revival of the Olympic Games, with the first modern Olympics being held in Athens in 1896. New Imperialism: 1870–1914 The years between 1870 and 1914 saw the expansion of Western power. By 1914, the Western and some Asian and Eurasian empires like the Empire of Japan, Russian Empire, Ottoman Empire, and Qing China dominated the entire planet. The major Western players in this New Imperialism were the United Kingdom, Russia, France, Germany, Italy, the Netherlands, Spain, Portugal, Belgium, Denmark, Sweden-Norway, and the United States. Japan was the only non-Western power involved in this new era of imperialism. Although the West had had a presence in Africa for centuries, its colonies were limited mostly to Africa's coast. Europeans, including the Britons Mungo Park and David Livingstone, the German Johannes Rebmann, and the Frenchman René Caillié, explored the interior of the continent, allowing greater European expansion in the later 19th century. The period between 1870 and 1914 is often called the Scramble for Africa, due to the competition between European nations for control of Africa. In 1830, France occupied Algeria in North Africa. Many Frenchman settled on Algeria's Mediterranean coast. In 1882 Britain annexed Egypt. France eventually conquered most of Morocco and Tunisia as well. Libya was conquered by the Italians. Spain gained a small part of Morocco and modern-day Western Sahara. West Africa was dominated by France, although Britain ruled several smaller West African colonies. Germany also established two colonies in West Africa, and Portugal had one as well. Central Africa was dominated by the Belgian Congo. At first the colony was ruled by Belgium's king, Leopold II, however his regime was so brutal the Belgian government took over the colony. The Germans and French also established colonies in Central Africa. The British and Italians were the two dominant powers in East Africa, although France also had a colony there. Southern Africa was dominated by Britain. Tensions between the British Empire and the Boer Republics led to the Boer Wars, fought on and off between the 1880s and 1902, ending in a British victory. In 1910 Britain united its South African colonies with the former Boer republics and established the Union of South Africa, a dominion of the British Empire. The British established several other colonies in Southern Africa. The Portuguese and Germans also established a presence in Southern Africa. The French conquered the island of Madagascar. By 1914, Africa had only two independent nations, Liberia, a nation founded in West Africa by free black Americans earlier in the 19th century, and the ancient kingdom of Ethiopia in East Africa. Many Africans, like the Zulus and Ashanti, resisted European rule, but in the end Europe succeeded in conquering and transforming the continent. Missionaries arrived and established schools, while industrialists helped establish rubber, diamond and gold industries on the continent. Perhaps the most ambitious change by Europeans was the construction of the Suez Canal in Egypt, allowing ships to travel from the Atlantic to the Indian Ocean without having to go all the way around Africa. In Asia, China was defeated by Britain in the First Opium War and later Britain and France in the Arrow War, forcing it to open up to trade with the West. Soon every major Western power as well as Russia and Japan had spheres of influence in China, although the country remained independent. Southeast Asia was divided between French Indochina and British Burma. One of the few independent nations in this region at the time was Siam. The Dutch continued to rule their colony of the Dutch East Indies, while Britain and Germany also established colonies in Oceania. India remained an integral part of the British Empire, with Queen Victoria being crowned Empress of India. The British even built a new capital in India, New Delhi. The Middle East remained largely under the rule of the Ottoman Empire and Persia. Britain, however, established a sphere of influence in Persia and a few small colonies in Arabia and coastal Mesopotamia. The Pacific islands were conquered by Germany, the U.S., Britain, France, and Belgium. In 1893, the ruling class of colonists in Hawaii overthrew the Hawaiian monarchy of Queen Liliuokalani and established a republic. Since most of the leaders of the overthrow were Americans or descendants of Americans, they asked to be annexed by the United States, which agreed to the annexation in 1898. Latin America was largely free from foreign rule throughout this period, although the United States and Britain had a great deal of influence over the region. Britain had two colonies on the Latin American mainland, while the United States, following 1898, had several in the Caribbean. The U.S. supported the independence of Cuba and Panama, but gained a small territory in central Panama and intervened in Cuba several times. Other countries also faced American interventions from time to time, mostly in the Caribbean and southern North America. Competition over control of overseas colonies sometimes led to war between Western powers, and between Western powers and non-Westerners. At the turn of the 20th century, Britain fought several wars with Afghanistan to prevent it from falling under the influence of Russia, which ruled all of Central Asia excluding Afghanistan. Britain and France nearly went to war over control of Africa. In 1898, the United States and Spain went to war after an American naval ship was sunk in the Caribbean. Although today it is generally held that the sinking was an accident, at the time the U.S. held Spain responsible and soon American and Spanish forces clashed everywhere from Cuba to the Philippines. The U.S. won the war and gained several Caribbean colonies including Puerto Rico and several Pacific islands, including Guam and the Philippines. Important resistance movements to Western Imperialism included the Boxer Rebellion, fought against the colonial powers in China, and the Philippine–American War, fought against the United States, both of which failed. The Russo-Turkish War (1877–78) left the Ottoman Empire little more than an empty shell, but the failing empire was able to hang on into the 20th century, until its final partition, which left the British and French colonial empires in control of much of the former Ottoman ruled Arab countries of the Middle East (British Mandate of Palestine, British Mandate of Mesopotamia, French Mandate of Syria, French Mandate of Lebanon, in addition to the British occupation of Egypt from 1882). Even though this happened centuries after the West had given up its futile attempts to conquer the "Holy Land" under religious pretexts, this fueled resentment against the "Crusaders" in the Islamic world, which along with the nationalisms hatched under Ottoman rule, contributed to the development of Islamism. The expanding Western powers greatly changed the societies they conquered. Many connected their empires via railroad and telegraph and constructed churches, schools, and factories. Great powers and the First World War: 1870–1918 By the late 19th century, the world was dominated by a few great powers, including Great Britain, the United States, and Germany. France, Russia, Austria-Hungary, and Italy were also great powers. Western inventors and industrialists transformed the West in the late 19th century and early 20th century. The American Thomas Edison pioneered electricity and motion picture technology. Other American inventors, the Wright brothers, completed the first successful airplane flight in 1903. The first automobiles were also invented in this period. Petroleum became an important commodity after the discovery it could be used to power machines. Steel was developed in Britain by Henry Bessemer. This very strong metal, combined with the invention of elevators, allowed people to construct very tall buildings, called skyscrapers. In the late 19th century, the Italian Guglielmo Marconi was able to communicate across distances using radio. In 1876, the first telephone was invented by Alexander Graham Bell, a British expatriate living in America. Many became very wealthy from this Second Industrial Revolution, including the American entrepreneurs Andrew Carnegie and John D. Rockefeller. Unions continued to fight for the rights of workers, and by 1914 laws limiting working hours and outlawing child labor had been passed in many Western countries. Culturally, the English-speaking nations were in the midst of the Victorian era, named for Britain's queen. In France, this period is called the Belle Époque, a period of many artistic and cultural achievements. The suffragette movement began in this period, which sought to gain voting rights for women, with New Zealand and Australian parliaments granting women's suffrage in the 1890s. However, by 1914, only a dozen U.S. states had given women this right, although women were treated more and more as equals of men before the law in many countries. Cities grew as never before between 1870 and 1914. This led at first to unsanitary and crowded living conditions, especially for the poor. However, by 1914, municipal governments were providing police and fire departments and garbage removal services to their citizens, leading to a drop in death rates. Unfortunately, pollution from burning coal and wastes left by thousands of horses that crowded the streets worsened the quality of life in many urban areas. Paris, lit up by gas and electric light, and containing the tallest structure in the world at the time, the Eiffel Tower, was often looked to as an ideal modern city, and served as a model for city planners around the world. United States: 1870–1914 Following the American Civil War, great changes occurred in the United States. After the war, the former Confederate States were put under federal occupation and federal lawmakers attempted to gain equality for blacks by outlawing slavery and giving them citizenship. After several years, however, Southern states began rejoining the Union as their populations pledged loyalty to the United States government, and in 1877 Reconstruction as this period was called, came to an end. After being re-admitted to the Union, Southern lawmakers passed segregation laws and laws preventing blacks from voting, resulting in blacks being regarded as second-class citizens for decades to come. Another great change beginning in the 1870s was the settlement of the western territories by Americans. The population growth in the American West led to the creation of many new western states, and by 1912 all the land of the contiguous U.S. was part of a state, bringing the total to 48. As whites settled the West, however, conflicts occurred with the Amerindians. After several Indian Wars, the Amerindians were forcibly relocated to small reservations throughout the West and by 1914 whites were the dominant ethnic group in the American West. As the farming and cattle industries of the American West matured and new technology allowed goods to be refrigerated and brought to other parts of the country and overseas, people's diets greatly improved and contributed to increased population growth throughout the West. America's population greatly increased between 1870 and 1914, due largely to immigration. The U.S. had been receiving immigrants for decades but at the turn of the 20th century, the numbers greatly increased due partly to large population growth in Europe. Immigrants often faced discrimination, because many differed from most Americans in religion and culture. Despite this, most immigrants found work and enjoyed a greater degree of freedom than in their home countries. Major immigrant groups included the Irish, Italians, Russians, Scandinavians, Germans, Poles and Diaspora Jews. The vast majority, at least by the second generation, learned English, and adopted American culture, while at the same time contributing to that culture by, for example, introducing the celebration of ethnic holidays and foreign cuisine to America. These new groups also changed America's religious landscape. Although it remained mostly Protestant, Catholics especially, as well as Jews and Orthodox Christians, increased in number. The U.S. became a major military and industrial power during this time, gaining a colonial empire from Spain and surpassing Britain and Germany to become the world's major industrial power by 1900. Despite this, most Americans were reluctant to get involved in world affairs, and American presidents generally tried to keep the U.S. out of foreign entanglement. Europe: 1870–1914 The years between 1870 and 1914 saw the rise of Germany as the dominant power in Europe. By the late 19th century, Germany had surpassed Britain to become the world's greatest industrial power. It also had the mightiest army in Europe. From 1870 to 1871, Prussia was at war with France. Prussia won the war and gained two border territories, Alsace and Lorraine, from France. After the war, Wilhelm took the title kaiser from the Roman title caesar, proclaimed the German Empire, and all the German states other than Austria united with this new nation, under the leadership of Prussian Chancellor Otto von Bismarck. After the Franco-Prussian War, Napoleon III was dethroned and France was proclaimed a republic. During this time, France was increasingly divided between Catholics and monarchists and anticlerical and republican forces. In 1900, church and state were officially separated in France, although the majority of the population remained Catholic. France also found itself weakened industrially following its war with Prussia due to its loss of iron and coal mines following the war. In addition, France's population was smaller than Germany's and was hardly growing. Despite all this, France's strong sense of nationhood, among other things, kept the country together. Between 1870 and 1914, Britain continued to peacefully switch between Liberal and Conservative governments, and maintained its vast empire, the largest in world history. Two problems faced by Britain in this period were the resentment of British rule in Ireland and Britain's falling behind Germany and the United States in industrial production. British dominions: 1870–1914 The European populations of Canada, Australia, New Zealand and South Africa all continued to grow and thrive in this period and evolved democratic Westminster system parliaments. Canada united as a dominion of the British Empire under the Constitution Act, 1867 (British North America Acts). The colony of New Zealand gained its own parliament (called a "general assembly") and home rule in 1852. and in 1907 was proclaimed the Dominion of New Zealand. Britain began to grant its Australian colonies autonomy beginning in the 1850s and during the 1890s, the colonies of Australia voted to unite. In 1901 they were federated as an independent nation under the British Crown, known as the Commonwealth of Australia, with a wholly elected bicameral parliament. The Constitution of Australia had been drafted in Australia and approved by popular consent. Thus Australia is one of the few countries established by a popular vote. The Second Boer War (1899–1902) ended with the conversion of the Boer republics of South Africa into British colonies and these colonies later formed part of the Union of South Africa in 1910 with equal rights for the Boers, who dominated elections. From the 1850s, Canada, Australia and New Zealand had become laboratories of democracy. By the 1870s, they had already granted voting rights to their citizens in advance of most other Western nations. In 1893, New Zealand became the first self-governing nation to extend the right to vote to women and, in 1895, the women of South Australia also became the first to obtain the right to stand for Parliament. During the 1890s Australia also saw such milestones as the invention of the secret ballot, the introduction of a minimum wage and the election of the world's first Labor Party government, prefiguring the emergence of Social Democratic governments in Europe. The old age pension was established in Australia and New Zealand by 1900. From the 1880s, the Heidelberg School of art adapted Western painting techniques to Australian conditions, while writers like Banjo Paterson and Henry Lawson introduced the character of a new continent into English literature and antipodean artists such as the opera singer Dame Nellie Melba began to influence the European arts. Rival alliances The late 19th century saw the creation of two rival alliances in Europe. Germany, Italy, and Austria-Hungary formed the Triple Alliance. France and Russia also developed strong relations with one another, due to the financing of Russia's Industrial Revolution by French capitalists. Although it did not have a formal alliance, Russia supported the Slavic Orthodox nations of the Balkans and the Caucasus, which had been created in the 19th century after several wars and revolutions against the Ottoman Empire, which by now was in decline and ruled only parts of the southern Balkan Peninsula. This Russian policy, called Pan-Slavism, led to conflicts with the Ottoman and Austro-Hungarian Empires, which had many Slavic subjects. Franco-German relations were also tense in this period due to France's defeat and loss of land at the hands of Prussia in the Franco-Prussian War. Also in this period, Britain ended its policy of isolation from the European continent and formed an alliance with France, called the Entente Cordiale. Rather than achieve greater security for the nations of Europe, however, these alliances increased the chances of a general European war breaking out. Other factors that would eventually lead to World War I were the competition for overseas colonies, the military buildups of the period, most notably Germany's, and the feeling of intense nationalism throughout the continent. World War I When the war broke out, much of the fighting was between Western powers, and the immediate casus belli was an assassination. The victim was the heir to the Austro-Hungarian throne, Franz Ferdinand, and he was assassinated on 28 June 1914 by a Yugoslav nationalist named Gavrilo Princip in the city of Sarajevo, at the time part of the Austro-Hungarian Empire. Although Serbia agreed to all but one point of the Austrian ultimatum (it did not take responsibility in planning the assassination but was ready to hand over any subject involved on its territory), Austria-Hungary was more than eager to declare war, attacked Serbia and effectively began World War I. Fearing the conquest of a fellow Slavic Orthodox nation, Russia declared war on Austria-Hungary. Germany responded by declaring war on Russia as well as France, which it feared would ally with Russia. To reach France, Germany invaded neutral Belgium in August, leading Britain to declare war on Germany. The war quickly stalemated, with trenches being dug from the North Sea to Switzerland. The war also made use of new and relatively new technology and weapons, including machine guns, airplanes, tanks, battleships, and submarines. Even chemical weapons were used at one point. The war also involved other nations, with Romania and Greece joining the British Empire and France and Bulgaria and the Ottoman Empire joining Germany. The war spread throughout the globe with colonial armies clashing in Africa and Pacific nations such as Japan and Australia, allied with Britain, attacking German colonies in the Pacific. In the Middle East, the Australian and New Zealand Army Corps landed at Gallipoli in 1915 in a failed bid to support an Anglo-French capture of the Ottoman capital of Istanbul. Unable to secure an early victory in 1915, British Empire forces later attacked from further south after the beginning of the Arab revolt and conquered Mesopotamia and Palestine from the Ottomans with the support of local Arab rebels. The British Empire also supported an Arab revolt against the Ottomans that was centered in the Arabian Peninsula. 1916 saw some of the most ferocious fighting in human history with the Somme Offensive on the Western Front alone resulting in 500,000 German casualties, 420,000 British and Dominion casualties, and 200,000 French casualties. 1917 was a crucial year in the war. The United States had followed a policy of neutrality in the war, feeling it was a European conflict. However, during the course of the war many Americans had died on board British ocean liners sunk by the Germans, leading to anti-German feelings in the U.S. There had also been incidents of sabotage on American soil, including the Black Tom explosion. What finally led to American involvement in the war, however, was the discovery of the Zimmermann Telegram, in which Germany offered to help Mexico conquer part of the United States if it formed an alliance with Germany. In April, the U.S. declared war on Germany. The same year the U.S. entered the war, Russia withdrew. After the deaths of many Russian soldiers and hunger in Russia, a revolution occurred against the Czar, Nicholas II. Nicholas abdicated and a Liberal provisional government was set up. In October, Russian communists, led by Vladimir Lenin rose up against the government, resulting in a civil war. Eventually, the communists won and Lenin became premier. Feeling World War I was a capitalist conflict, Lenin signed a peace treaty with Germany in which it gave up a great deal of its Central and Eastern European lands. Although Germany and its allies no longer had to focus on Russia, the large numbers of American troops and weapons reaching Europe turned the tide against Germany, and after more than a year of fighting, Germany surrendered. The treaties which ended the war, including the famous Versailles Treaty dealt harshly with Germany and its former allies. The Austro-Hungarian Empire were completely abolished and Germany was greatly reduced in size. Many nations regained their independence, including Poland, Czechoslovakia, and Yugoslavia. The last Austro-Hungarian emperor abdicated, and two new republics, Austria and Hungary, were created. The last Ottoman sultan was overthrown by the Turkish nationalist revolutionary named Atatürk and the Ottoman homeland of Turkey was declared a republic. Germany's kaiser also abdicated and Germany was declared a republic. Germany was also forced to give up the lands it had gained in the Franco-Prussian War to France, accept responsibility for the war, reduce its military and pay reparations to Britain and France. In the Middle East, Britain gained Palestine, Transjordan (modern-day Jordan), and Mesopotamia as colonies. France gained Syria and Lebanon. An independent kingdom consisting of most of the Arabian peninsula, Saudi Arabia, was also established. Germany's colonies in Africa, Asia, and the Pacific were divided between the British and French Empires. The war had cost millions of lives and led many in the West to develop a strong distaste for war. Few were satisfied with, and many despised the agreements made at the end of the war. Japanese and Italians were angry that they had not been given any new colonies after the war, and many Americans felt the war had been a mistake. Germans were outraged at the state of their country following the war. Also, unlike many in the United States had hoped for, democracy did not flourish in the world in the post-war period. The League of Nations, an international organization proposed by American president Woodrow Wilson to prevent another great war from breaking out, proved ineffective, especially because the United States Senate stopped Wilson White House from ratifying U.S. membership. Interwar years: 1918–1939 United States in the interwar years After World War I, most Americans regretted getting involved in world affairs and desired a "return to normalcy". The 1920s were a period of economic prosperity in the United States. Many Americans bought cars, radios, and other appliances with the help of installment payments. Also, many Americans invested in the stock market as a source of income. Movie theaters sprang up throughout the country, although at first they did not have sound. Alcoholic beverages were outlawed in the United States and women were granted the right to vote. Although the United States was arguably the most powerful nation in the post-war period, Americans remained isolationist and elected several conservative presidents during this period. In October 1929 the New York stock market crashed, leading to the Great Depression. Many lost their life's savings and the resulting decline in consumer spending led millions to lose their jobs as banks and businesses closed. In the Midwestern United States, a severe drought destroyed many farmers' livelihoods. In 1932, Americans elected Franklin D. Roosevelt president. Roosevelt followed a series of policies which regulated the stock market and banks, and created many public works programs aimed at providing the unemployed with work. Roosevelt's policies helped alleviate the worst effects of the Depression, although by 1941 the Great Depression was still ongoing. Roosevelt also instituted pensions for the elderly and provided money to those who were unemployed. Roosevelt was also one of the most popular presidents in U.S. history, earning re-election in 1936, and also in 1940 and 1944, becoming the only U.S. president to serve more than two terms. Europe in the interwar years Europe was relatively unstable following World War I. Although many prospered in the 1920s, Germany was in a deep financial and economic crisis. Also, France and Britain owed the U.S. a great deal of money. When the United States went into Depression, so did Europe. There were perhaps 30 million people around the world unemployed following the Depression. Many governments helped to alleviate the suffering of their citizens and by 1937 the economy had improved although the lingering effects of the Depression remained. Also, the Depression led to the spread of radical left-wing and right-wing ideologies, like Communism and Fascism. In 1919–1921 Polish–Soviet War took place. After the Russian Revolution of 1917 Russia sought to spread communism to the rest of Europe. This is evidenced by the well-known daily order by marshal Tukhachevsky to his troops: "Over the corpse of Poland leads the road to the world's fire. Towards Wilno, Minsk, Warsaw go!". Poland, whose statehood had just been re-established by the Treaty of Versailles following the Partitions of Poland in the late 18th century achieved an unexpected and decisive victory at the Battle of Warsaw. In the wake of the Polish advance eastward, the Soviets sued for peace and the war ended with a ceasefire in October 1920. A formal peace treaty, the Peace of Riga, was signed on 18 March 1921. According to the British historian A. J. P. Taylor, the Polish–Soviet War "largely determined the course of European history for the next twenty years or more. [...] Unavowedly and almost unconsciously, Soviet leaders abandoned the cause of international revolution." It would be twenty years before the Bolsheviks would send their armies abroad to 'make revolution'. According to American sociologist Alexander Gella, "the Polish victory had gained twenty years of independence not only for Poland, but at least for an entire central part of Europe." In 1916, militant Irish republicans staged a rising and proclaimed a republic. The rising was suppressed after six days with leaders of the rising being executed. This was followed by the Irish War of Independence in 1919–1921 and the Irish Civil War (1922–1923). After the civil war, the island was divided. Northern Ireland remained part of the United Kingdom, while the rest of the island became the Irish Free State. In 1927, the United Kingdom renamed itself the United Kingdom of Great Britain and Northern Ireland. In 1918, the UK granted the right to vote to women. British dominions in the interwar years The relationship between Britain and its Empire evolved significantly over the period. In 1919, the British Empire was represented at the all-important Versailles Peace Conference by delegates from its dominions who had each suffered large casualties during the War. The Balfour Declaration at the 1926 Imperial Conference, stated that Britain and its dominions were "equal in status, in no way subordinate one to another in any aspect of their domestic or external affairs, though united by common allegiance to the Crown, and freely associated as members of the British Commonwealth of Nations". These aspects to the relationship were eventually formalised by the Statute of Westminster in 1931 – a British law which, at the request and with the consent of the dominion parliaments clarified the independent powers of the dominion parliaments, and granted the former colonies full legal freedom except areas where they chose to remain subordinate. Previously the British Parliament had had residual ill-defined powers, and overriding authority, over dominion legislation. It applied to the six dominions which existed in 1931: Canada, Australia, the Irish Free State, the Dominion of Newfoundland, New Zealand, and the Union of South Africa. Each of the dominions remained within the British Commonwealth and retained close political and cultural ties with Britain and continued to recognize the British monarch as head of their own independent nations. Australia, New Zealand, and Newfoundland had to ratify the statute for it to take effect. Australia and New Zealand did so in 1942 and 1947 respectively. Newfoundland united with Canada in 1949 and the Irish Free State came to an end in 1937, when the citizens voted by referendum to replace its 1922 constitution. It was succeeded by the entirely sovereign modern state of Ireland. Rise of totalitarianism The inter-war years saw the establishment of the first totalitarian regimes in world history. The first was established in Russia following the revolution of 1917. The Russian Empire was renamed the Union of Soviet Socialist Republics, or Soviet Union. The government controlled every aspect of its citizens' lives, from maintaining loyalty to the Communist Party to persecuting religion. Lenin helped to establish this state but it was brought to a new level of brutality under his successor, Joseph Stalin. The first totalitarian state in the West was established in Italy. Unlike the Soviet Union however, this would be a Fascist rather than a Communist state. Fascism is a less organized ideology than Communism, but generally it is characterized by a total rejection of humanism and liberal democracy, as well as very intense nationalism, with a government headed by a single all-powerful dictator. The Italian politician Benito Mussolini established the Fascist Party (from which Fascism derives its name) following World War I. Fascists won the support of many disillusioned Italians, who were angry over Italy's treatment following World War I. They also employed violence and intimidation against their political enemies. In 1922, Mussolini seized power by threatening to lead his followers on a march on Rome if he was not named prime minister. Although he had to share some power with the monarchy, Mussolini ruled as a dictator. Under his rule, Italy's military was built up and democracy became a thing of the past. One important diplomatic achievement of his reign, however, was the Lateran Treaty, between Italy and the Pope, in which a small part of Rome where St. Peter's Basilica and other Church property was located was given independence as Vatican City and the Pope was reimbursed for lost Church property. In exchange, the Pope recognized the Italian government. Another Fascist party, the Nazis, would take power in Germany. The Nazis were similar to Mussolini's Fascists but held many views of their own. Nazis were obsessed with racial theory, believing Germans to be part of a master race, destined to dominate the inferior races of the world. The Nazis were especially hateful of Jews. Another unique aspect of Nazism was its connection with a small movement that supported a return to ancient Germanic paganism. Adolf Hitler, a World War I veteran, became leader of the party in 1921. Gaining support from many disillusioned Germans, and by using intimidation against its enemies, the Nazi party had gained a great deal of power by the early 1930s. In 1933, Hitler was named Chancellor, and seized dictatorial power. Hitler built up Germany's military in violation of the Versailles Treaty and stripped Jews of all rights in Germany. Eventually, the regime Hitler created would lead to the Second World War. In Spain, a Republic was proclaimed in 1931 in the wake of the demise of the Bourbon monarchic regime and its dictatorial solution. In 1936, a military coup d'état against the republic started the Spanish Civil War, which ended in 1939 with the victory of the rebel side (supported by Fascist Italy and Nazi Germany), and with Francisco Franco as dictator. Second World War and its aftermath: 1939–1950 The late 1930s saw a series of violations of the Versailles Treaty by Germany, however, France and Britain refused to act. In 1938, Hitler annexed Austria in an attempt to unite all German-speakers under his rule. Next, he annexed a German-speaking area of Czechoslovakia. Britain and France agreed to recognize his rule over that land and in exchange Hitler agreed not to expand his empire further. In a matter of months, however, Hitler broke the pledge and annexed the rest of Czechoslovakia. Despite this, the British and French chose to do nothing, wanting to avoid war at any cost. Hitler then formed a secret non-aggression pact with the Soviet Union, despite the fact that the Soviet Union was Communist and Germany was Nazi. Also in the 1930s, Italy conquered Ethiopia. The Soviets too began annexing neighboring countries. Japan began taking aggressive actions towards China. After Japan opened itself to trade with the West in the mid-19th century, its leaders learned to take advantage of Western technology and industrialized their country by the end of the century. By the 1930s, Japan's government was under the control of militarists who wanted to establish an empire in the Asia-Pacific region. In 1937, Japan invaded China. In 1939, German forces invaded Poland, and soon the country was divided between the Soviet Union and Germany. France and Britain declared war on Germany, World War II had begun. The war featured the use of new technologies and improvements on existing ones. Airplanes called bombers were capable of travelling great distances and dropping bombs on targets. Submarine, tank and battleship technology also improved. Most soldiers were equipped with hand-held machine guns and armies were more mobile than ever before. Also, the British invention of radar would revolutionize tactics. German forces invaded and conquered the Low Countries and by June had even conquered France. In 1940 Germany, Italy and Japan formed an alliance and became known as the Axis Powers. Germany next turned its attention to Britain. Hitler attempted to defeat the British using only air power. In the Battle of Britain, German bombers destroyed much of the British air force and many British cities. Led by their prime minister, the defiant Winston Churchill, the British refused to give up and launched air attacks on Germany. Eventually, Hitler turned his attention from Britain to the Soviet Union. In June 1941, German forces invaded the Soviet Union and soon reached deep into Russia, surrounding Moscow, Leningrad, and Stalingrad. Hitler's invasion came as a total surprise to Stalin; however, Hitler had always believed sooner or later Soviet Communism and what he believed were the "inferior" Slavic peoples had to be wiped out. The United States attempted to remain neutral early in the war. However, a growing number feared the consequences of a Fascist victory. President Roosevelt began sending weapons and support to the British, Chinese, and Soviets. Also, the U.S. placed an embargo against the Japanese, as they continued their war with China and conquered many colonies formerly ruled by the French and Dutch, who were now under German rule. In 1941, Japan launched a surprise attack on Pearl Harbor, an American naval base in Hawaii. The U.S. responded by declaring war on Japan. The next day, Germany and Italy declared war on the United States. The United States, the British Commonwealth, and the Soviet Union now constituted the Allies, dedicated to destroying the Axis Powers. Other allied nations included Canada, Australia, New Zealand, South Africa and China. In the Pacific War, British, Indian and Australian troops made a disorganised last stand at Singapore, before surrendering on 15 February 1942. The defeat was the worst in British military history. Around 15,000 Australian soldiers alone became prisoners of war. Allied prisoners died in their thousands interned at Changi Prison or working as slave labourers on such projects as the infamous Burma Railway and the Sandakan Death Marches. Australian cities and bases – notably Darwin suffered air raids and Sydney suffered naval attack. U.S. General Douglas MacArthur, based in Melbourne, Australia became "Supreme Allied Commander of the South West Pacific" and the foundations of the post war Australia-New Zealand-United States Alliance were laid. In May 1942, the Royal Australian Navy and U.S. Navy engaged the Japanese in the Battle of the Coral Sea and halted the Japanese fleet headed for Australian waters. The Battle of Midway in June effectively defeated the Japanese navy. In August 1942, Australian forces inflicted the first land defeat on advancing Japanese forces at the Battle of Milne Bay in the Australian Territory of New Guinea. By 1942, German and Italian armies ruled Norway, the Low Countries, France, the Balkans, Central Europe, part of Russia, and most of North Africa. Japan by this year ruled much of China, Southeast Asia, Indonesia, the Philippines, and many Pacific Islands. Life in these empires was cruel – especially in Germany, where the Holocaust was perpetrated. Eleven million people – six million of them Jews – were systematically murdered by the German Nazis by 1945. From 1943 on, the Allies gained the upper hand. American and British troops first liberated North Africa from the Germans and Italians. Next they invaded Italy, where Mussolini was deposed by the king and later was killed by Italian partisans. Italy surrendered and came under Allied occupation. After the liberation of Italy, American, British, and Canadian troops crossed the English Channel and liberated Normandy, France, from German rule after great loss of life. The Western Allies were then able to liberate the rest of France and move towards Germany. During these campaigns in Africa and Western Europe, the Soviets fought off the Germans, pushing them out of the Soviet Union altogether and driving them out of Eastern and East-Central Europe. In 1945 the Western Allies and Soviets invaded Germany itself. The Soviets captured Berlin and Hitler committed suicide. Germany surrendered unconditionally and came under Allied occupation. The war against Japan continued however. American forces from 1943 on had worked their way across the Pacific, liberating territory from the Japanese. The British also fought the Japanese in such places as Burma. By 1945, the U.S. had surrounded Japan, however the Japanese refused to surrender. Fearing a land invasion would cost one million American lives, the U.S. used a new weapon against Japan, the atomic bomb, developed after years of work by an international team including Germans, in the United States. These atomic bombings of Hiroshima and Nagasaki combined with a Soviet invasion of many of Japan's occupied territories in the east, led Japan to surrender. After the war the U.S., Britain and the Soviet Union attempted to cooperate. German and Japanese military leaders responsible for atrocities in their regimes were put on trial and many were executed. The international organization the United Nations was created. Its goal was to prevent wars from breaking out as well as provide the people of the world with security, justice and rights. The period of post-war cooperation ended, however, when the Soviet Union rigged elections in the occupied nations of Central and Eastern Europe to allow for Communist victories. Soon, all of Eastern and much of Central Europe had become a series of Communist dictatorships, all staunchly allied with the Soviet Union. Germany following the war had been occupied by British, American, French, and Soviet forces. Unable to agree on a new government, the country was divided into a democratic west and Communist east. Berlin itself was also divided, with West Berlin becoming part of West Germany and East Berlin becoming part of East Germany. Meanwhile, the former Axis nations soon had their sovereignty restored, with Italy and Japan regaining independence following the war. World War II had cost millions of lives and devastated many others. Entire cities lay in ruins and economies were in shambles. However, in the Allied countries, the people were filled with pride at having stopped Fascism from dominating the globe, and after the war, Fascism was all but extinct as an ideology. The world's balance of power also shifted, with the United States and Soviet Union being the world's two superpowers. Fall of the Western empires: 1945–1999 Following World War II, the great colonial empires established by the Western powers beginning in early modern times began to collapse. There were several reasons for this. Firstly, World War II had devastated European economies and had forced governments to spend great deals of money, making the price of colonial administration increasingly hard to manage. Secondly, the two new superpowers following the war, the United States and Soviet Union were both opposed to imperialism, so the now weakened European empires could generally not look to the outside for help. Thirdly, Westerners increasingly were not interested in maintaining and even opposed the existence of empires. The fourth reason was the rise of independence movements following the war. The future leaders of these movements had often been educated at colonial schools run by Westerners where they adopted Western ideas like freedom, equality, self-determination and nationalism, and which turned them against their colonial rulers. The first colonies to gain independence were in Asia. In 1946, the U.S. granted independence to the Philippines, its only large overseas colony. In British India, Mahatma Gandhi led his followers in non-violent resistance to British rule. By the late 1940s Britain found itself unable to work with Indians in ruling the colony, this, combined with sympathy around the world for Gandhi's non-violent movement, led Britain to grant independence to India, dividing it into the largely Hindu country of India and the smaller, largely Muslim nation of Pakistan in 1947. In 1948 Burma gained independence from Britain, and in 1945 Indonesian nationalists declared Indonesian independence, which the Netherlands recognised in 1949 after a four-year armed and diplomatic struggle. Independence for French Indochina came only after a great conflict. After the withdrawal of Japanese forces from the colony following World War II, France regained control but found it had to contend with an independence movement that had fought against the Japanese. The movement was led by the Vietnamese Ho Chi Minh, leader of the Vietnamese Communists. Because of this, the U.S. supplied France with arms and support, fearing Communists would dominate South-east Asia. In the end though, France gave in and granted independence, creating Laos, Cambodia, Communist North Vietnam, and South Vietnam. In the Middle East, following World War II, Britain had granted independence to the formerly Ottoman territories of Mesopotamia, which became Iraq, Kuwait, and Transjordan, which became Jordan. France also granted independence to Syria and Lebanon. British Palestine, however, presented a unique challenge. Following World War I, when Britain gained the colony, Jewish and Arab national aspirations conflicted, followed by a proposal of the UN to divided Mandatory Palestine into a Jewish state and an Arab state. The Arabs objected, Britain withdrew and the Zionists declared the state of Israel on 14 May 1948. The other major center of colonial power, Africa, was freed from colonial rule following World War II as well. Egypt gained independence from Britain and this was soon followed by Ghana and Tunisia. One violent independence movement of the time was fought in Algeria, in which Algerian rebels went so far as to kill innocent Frenchmen. In 1962, however, Algeria gained independence from France. By the 1970s the entire continent had become independent of European rule, although a few southern countries remained under the rule of white colonial minorities. By the close of the 20th century, the European colonial Empires had ceased to exist as significant global entities. Sunset for the British Empire came when Britain's lease on the great trading port of Hong Kong was brought to end, and political control was transferred to the People's Republic of China in 1997. Soon after, in 1999, the transfer of sovereignty over Macau was concluded between Portugal and China, bringing to a close six centuries of Portuguese colonialism. Britain remained culturally linked to its former empire through the voluntary association of the Commonwealth of Nations, and 14 British Overseas Territories remained (formerly known as Crown colonies), consisting mainly of scattered island outposts. Currently, 15 independent Commonwealth realms retain the British monarch as their head of state. Canada, Australia and New Zealand emerged as vibrant and prosperous migrant nations. The once vast French colonial empire had lost its major possessions though a scattered territories remained as Overseas departments and territories of France. The shrunken Dutch Empire retained a few Caribbean islands as constituent countries of the Kingdom of the Netherlands. Spain had lost its overseas possessions, but its legacy was vast – with Latin culture remaining throughout South and Central America. Along with Portugal and France, Spain had made Catholicism a global religion. Of Europe's empires, only the Russian Empire remained a significant geo-political force into the late 20th century, having morphed into the Soviet Union and Warsaw Pact, which, drawing on the writings of the German Karl Marx, established a socialist economic model under Communist dictatorship, which ultimately collapsed in the early 1990s. Adaptations of Marxism continued as the stated inspiration for Governments in Central America and Asia into the 21st century – though only a handful survived the end of the Cold War. The end of the Western Empires greatly changed the world. Although many newly independent nations attempted to become democracies, many slipped into military and autocratic rule. Amid power vacuums and newly determined national borders, civil war also became a problem, especially in Africa, where the introduction of firearms to ancient tribal rivalries exacerbated problems. The loss of overseas colonies partly also led many Western nations, particularly in continental Europe, to focus more on European, rather than global, politics as the European Union rose as an important entity. Though gone, the colonial empires left a formidable cultural and political legacy, with English, French, Spanish, Portuguese, Russian and Dutch being spoken by peoples across far flung corners of the globe. European technologies were now global technologies – religions like Catholicism and Anglicanism, founded in the West, were booming in post colonial Africa and Asia. Parliamentary (or presidential) democracies, as well as rival Communist style one party states invented in the West had replaced traditional monarchies and tribal government models across the globe. Modernity, for many, was equated with Westernisation. Cold War: 1945–1991 From the end of World War II almost until the start of the 21st century, Western and world politics were dominated by the state of tensions and conflict between the world's two superpowers, the United States and the Soviet Union. In the years following World War II, the Soviets established satellite states throughout Central and Eastern Europe, including historically and culturally Western nations like Poland and Hungary. Following the division of Germany, the East Germans constructed the Berlin Wall, to prevent East Berliners from escaping to the "freedom" of West Berlin. The Berlin Wall would come to represent the Cold War around the world. Rather than revert to isolationism, the United States took an active role in global politics following World War II to halt Communist expansion. After the war, Communist parties in Western Europe increased in prestige and number, especially in Italy and France, leading many to fear the whole of Europe would become Communist. The U.S. responded to this with the Marshall Plan, in which the U.S. financed the rebuilding of Western Europe and poured money into its economy. The Plan was a huge success and soon Europe was prosperous again, with many Europeans enjoying a standard of living close that in the U.S. (following World War II, the U.S. became very prosperous and Americans enjoyed the highest standard of living in the world). National rivalries ended in Europe and most Germans and Italians, for example, were happy to be living under democratic rule, regretting their Fascist pasts. In 1949, the North Atlantic Treaty was signed, creating the North Atlantic Treaty Organization, or NATO. The treaty was signed by the United States, Canada, the Low Countries, Norway, Denmark, Iceland, Portugal, Italy, France, and Britain. NATO members agreed that if any one of them were attacked, they would all consider themselves attacked and retaliate. NATO would expand as the years went on, other nations joined, including Greece, Turkey, and West Germany. The Soviets responded with the Warsaw Pact, an alliance which bound Central and Eastern Europe to fight with the United States and its allies in the event of war. One of the first actual conflicts of the Cold War took place in China. Following the withdrawal of Japanese troops after World War II, China was plunged into civil war, pitting Chinese Communists against Nationalists, who opposed Communism. The Soviets supported the Communists while the Americans supported the Nationalists. In 1949, the Communists were victorious, proclaiming the People's Republic of China. However, the Nationalists continued to rule the island of Taiwan off the coast. With American guarantees of protection for Taiwan, China did not make an attempt to take over the island. A major political change in East Asia in this period was Japan's becoming a tolerant, democratic society and an ally of the United States. In 1950, another conflict broke out in Asia, this time in Korea. The peninsula had been divided between a Communist North and non-Communist South in 1948 following the withdrawal of American and Soviet troops. In 1950, the North Koreans invaded South Korea, wanting to united the land under Communism. The UN condemned the action, and, because the Soviets were boycotting the organization at the time and therefore had no influence on it, the UN sent forces to liberate South Korea. Many nations sent troops, but most were from America. UN forces were able to liberate the South and even attempted to conquer the North. However, fearing the loss of North Korea, Communist China sent troops to the North. The U.S. did not retaliate against China, fearing war with the Soviet Union, so the war stalemated. In 1953 the two sides agreed to a return to the pre-war borders and a de-militarization of the border area. The world lived in the constant fear of World War III in the Cold War. Seemingly any conflict involving Communism might lead to a conflict between the Warsaw pact countries and the NATO countries. The prospect of a third world war was made even more frightening by the fact that it would almost certainly be a nuclear war. In 1949 the Soviets developed their first atomic bomb, and soon both the United States and Soviet Union had enough to destroy the world several times over. With the development of missile technology, the stakes were raised as either country could launch weapons from great distances across the globe to their targets. Eventually, Britain, France, and China would also develop nuclear weapons. It is believed that Israel developed nuclear weapons as well. One major event that nearly brought the world to the brink of war was the Cuban Missile Crisis. In the 1950s a revolution in Cuba had brought the only Communist regime in the Western Hemisphere to power. In 1962, the Soviets began constructing missile sites in Cuba and sending nuclear missiles. Because of its close proximity to the U.S., the U.S. demanded the Soviets withdraw missiles from Cuba. The U.S. and Soviet Union came very close to attacking one another, but in the end came to a secret agreement in which the NATO withdrew missiles in exchange for a Soviet withdrawal of missiles from Cuba. The next great Cold War conflict occurred in Southeast Asia. In the 1960s, North Vietnam invaded South Vietnam, hoping to unite all of Vietnam under Communist rule. The U.S. responded by supporting the South Vietnamese. In 1964, American troops were sent to "save" South Vietnam from conquest, which many Americans feared would lead to Communist dominance in the entire region. The Vietnam War lasted many years, but most Americans felt the North Vietnamese would be defeated in time. Despite American technological and military superiority, by 1968, the war showed no signs of ending and most Americans wanted U.S. forces to end their involvement. The U.S. undercut support for the North by getting the Soviets and Chinese to stop supporting North Vietnam, in exchange for recognition of the legitimacy of mainland China's Communist government, and began withdrawing troops from Vietnam. In 1972, the last American troops left Vietnam and in 1975 South Vietnam fell to the North. In the following years Communism took power in neighboring Laos and Cambodia. By the 1970s global politics were becoming more complex. For example, France's president proclaimed France was a great power in and of itself. However, France did not seriously threaten the U.S. for supremacy in the world or even Western Europe. In the Communist world, there was also division, with the Soviets and Chinese differing over how Communist societies should be run. Soviet and Chinese troops even engaged in border skirmishes, although full-scale war never occurred. The last great armed conflict of the Cold War took place in Afghanistan. In 1979, Soviet forces invaded that country, hoping to establish Communism. Muslims from throughout the Islamic World travelled to Afghanistan to defend that Muslim nation from conquest, calling it a Jihad, or Holy War. The U.S. supported the Jihadists and Afghan resisters, despite the fact that the Jihadists were vehemently anti-Western. By 1989 Soviet forces were forced to withdraw and Afghanistan fell into civil war, with an Islamic fundamentalist government, the Taliban taking over much of the country. The late 1970s had seen a lessening of tensions between the U.S. and Soviet Union, called Détente. However, by the 1980s Détente had ended with the invasion of Afghanistan. In 1981, Ronald Reagan became President of the United States and sought to defeat the USSR by leveraging the United States capitalist economic system to outproduce the communist Russians. The United States military was in a state of low moral after its loss in the Vietnam War, and President Reagan began a huge effort to out-produce the Soviets in military production and technology. In 1985, a new Soviet leader, Mikhail Gorbachev took power. Gorbachev, knowing that the Soviet Union could no longer compete economically with the United States, implemented a number of reforms granting his citizens freedom of speech and introducing some capitalist reforms. Gorbachev and America's staunch anti-Communist president Ronald Reagan were even able to negotiate treaties limiting each side's nuclear weapons. Gorbachev also ended the policy of imposing Communism in Central and Eastern Europe. In the past Soviet troops had crushed attempts at reform in places like Hungary and Czechoslovakia. Now, however, Eastern Europe was freed from Soviet domination. In Poland the Round Table Talks between the government and the Solidarity-led opposition led to semi-free elections in 1989. Elections in Poland where anti-communist candidates won a striking victory sparked off a succession of peaceful anti-communist revolutions in Central and Eastern Europe known as the Revolutions of 1989. Soon, Communist regimes throughout Europe collapsed. In Germany, after calls from Reagan to Gorbachev to tear down the Berlin Wall, the people of East and West Berlin tore down the wall and East Germany's Communist government was voted out. East and West Germany unified to create the country of Germany, with its capital in the reunified Berlin. The changes in Central and Eastern Europe led to calls for reform in the Soviet Union itself. A failed coup by hard-liners led to greater instability in the Soviet Union, and the Soviet legislature, long subservient to the Communist Party, voted to abolish the Soviet Union in 1991. What had been the Soviet Union was divided into many republics. Although many slipped into authoritarianism, most became democracies. These new republics included Russia, Ukraine, and Kazakhstan. By the early 1990s, the West and Europe as a whole was finally free from Communism. Following the end of the Cold War, Communism largely died out as a major political movement. After the fall of USSR, the United States became the world's only superpower. Western countries: 1945–1980 United States: 1945–1980 Following World War II, there was an unprecedented period of prosperity in the United States. The majority of Americans entered the middle class and moved from the cities into surrounding suburbs, buying homes of their own. Most American households owned at least one car, as well as the relatively new invention, the television. Also, the American population greatly increased as part of the so-called "baby boom" following the war. For the first time following the war, large of numbers of non-wealthy Americans were able to attend college. Following the war, black Americans started what has become known as the civil rights movement in the United States. After roughly a century of second-class citizenship following the abolition of slavery, blacks began seeking full equality. This was helped by the 1954 decision by the Supreme Court, outlawing segregation in schools, which was common in the South. Martin Luther King Jr., a black minister from the South led many blacks and whites who supported their cause in non-violent protests against discrimination. Eventually, the Civil Rights Act and Voting Rights Act were passed in 1964, banning measures that had prevented blacks from voting and outlawing segregation and discrimination in the U.S. In politics, the Democratic and Republican parties remained dominant. In 1945, the Democratic party relied on Southerners, whose support went back to the days when Democrats defended a state's right to own slaves, and Northeasterners and industrial Mid-Westerners, who supported the pro-labor and pro-immigrant policies of the Democrats. Republicans tended to rely on middle-class Protestants from elsewhere in the country. As the Democrats began championing civil rights, however, Southern Democrats felt betrayed, began voting Republican. Presidents from this period were Harry Truman, Dwight Eisenhower, John F. Kennedy, Lyndon Johnson, Richard Nixon, Gerald Ford, and Jimmy Carter. The years 1945–1980 saw the expansion of federal power and the establishment of programs to help the elderly and poor pay for medical expenses. By 1980, many Americans had become pessimistic about their country. Despite its status as one of only two superpowers, the Vietnam War as well as the social upheavals of the 1960s and an economic downturn in the 1970s led America to become a much-less confident nation. Europe At the close of the war, much of Europe lay in ruins with millions of homeless refugees. A souring of relations between the Western Allies and the Soviet Union then saw Europe split by an Iron Curtain, dividing the continent between West and East. In Western Europe, democracy had survived the challenge of Fascism and began a period of intense rivalry with Eastern Communism, which was to continue into the 1980s. France and Britain secured themselves permanent positions on the newly formed United Nations Security Council, but Western European empires did not long survive the war, and no one Western European nation would ever again be the paramount power in world affairs. Despite these immense challenges however, Western Europe again rose as an economic and cultural powerhouse. Assisted first by the Marshall Plan of financial aid from the United States, and later through closer economic integration through the European Common Market, Western Europe quickly re-emerged as a global economic power house. The vanquished nations of Italy and West Germany became leading economies and allies of the United States. So marked was their recovery that historians refer to an Italian economic miracle and in the case of West Germany and Austria the Wirtschaftswunder (German for "economic miracle"). Facing a new power balance between the Soviet East and American West, Western European nations moved closer together. In 1957, Belgium, France, the Netherlands, West Germany, Italy and Luxembourg signed the landmark Treaty of Rome, creating the European Economic Community, free of customs duties and tariffs, and allowing the rise of a new European geo-political force. Eventually, this organization was renamed the European Union or (EU), and many other nations joined, including Britain, Ireland, and Denmark. The EU worked toward economic and political cooperation among European nations. Between 1945 and 1980, Europe became increasingly socialist. Most European countries became welfare states, in which governments provided a large number of services to their people through taxation. By 1980, most of Europe had universal health care and pensions for the elderly. The unemployed were also guaranteed income from the government, and European workers were guaranteed long vacation time. Many other entitlements were established, leading many Europeans to enjoy a very high standard of living. By the 1980s, however, the economic problems of the welfare state were beginning to emerge. Europe had many important political leaders during this time. Charles de Gaulle, leader of the French government in exile during World War II, served as France's president for many years. He sought to carve out for France a great power status in the world. Although Europe as a whole was relatively peaceful in this period, both Britain and Spain suffered from acts of terrorism. In Britain, The Troubles saw Irish republicans battle Unionists loyal to Britain. In Spain, ETA, a Basque separatist group, began committing acts of terror against Spaniards, hoping to gain independence for the Basques, an ethnic minority in north-eastern Spain. Both these terrorist campaigns failed, however. For Greece, Spain and Portugal, ideological battles between left and right continued and the emergence of parliamentary democracy was troubled. Greece experienced Civil War, coup and counter-coup into the 1970s. Portugal, since the 1930s under a quasi-Fascist regime and among the poorest nations in Europe, fought a rearguard action against independence movements in its empire, until a 1974 coup. The last authoritarian dictatorship in Western Europe fell in 1975, when Francisco Franco, dictator of Spain, died. Franco had helped to modernize the country and improve the economy. His successor, King Juan Carlos, transformed the country into a constitutional monarchy. By 1980, all Western European nations were democracies. British Empire and Commonwealth 1945–1980 Between 1945 and 1980, the British Empire was transformed from its centuries old position as a global colonial power, to a voluntary association known as the Commonwealth of Nations – only some of which retained any formal political links to Britain or its monarchy. Some former British colonies or protectorates disassociated themselves entirely from Britain. Britain The popular war time leader Winston Churchill was swept from office at the 1945 election and the Labour Government of Clement Attlee introduced a program of nationalisation of industry and introduced wide-ranging social welfare. Britain's finances had been ravaged by the war and John Maynard Keynes was sent to Washington to negotiate the massive Anglo-American loan on which Britain relied to fund its post-war reconstruction. India was granted Independence in 1947 and Britain's global influence rapidly declined as decolonisation proceeded. Though the USSR and United States now stood as the post war super powers, Britain and France launched the ill-fated Suez intervention in the 1950s, and Britain committed to the Korean War. From the 1960s The Troubles afflicted Northern Ireland, as British Unionist and Irish Republican paramilitaries conducted campaigns of violence in support of their political goals. The conflict at times spilled into Ireland and England and continental Europe. Paramilitaries such as the IRA (Irish Republican Army) wanted union with the Republic of Ireland while the UDA (Ulster Defence Association) were supporters of Northern Ireland remaining within the United Kingdom. In 1973, Britain entered the European Common Market, stepping away from imperial and commonwealth trade ties. Inflation and unemployment contributed to a growing sense of economic decline – partly offset by the exploitation of North Sea Oil from 1974. In 1979, the electorate turned to Conservative Party leader Margaret Thatcher, who became Britain's first female prime minister. Thatcher launched a radical program of economic reform and remained in power for over a decade. In 1982, Thatcher dispatched a British fleet to the Falkland Islands which successfully repelled an Argentine invasion of the British Territory, demonstrating that Britain could still project power across the globe. Canada Canada continued to evolve its own national identity in the post-war period. Although it was an independent nation, it remained part of the British Commonwealth and recognized the British monarch as the Canadian monarch as well. Following the war, French and English were recognized as co-equal official languages in Canada, and French became the only official language in the French-speaking province of Quebec. Referendums were held in both 1980 and 1995 in which Quebecers, however, voted not to secede from the union. Other cultural changes Canada faced were similar to those in the United States. Racism and discrimination largely disappeared in the post-war years, and dual-income families became the norm. Also, there was a rejection of traditional Western values by many in Canada. The government also established universal health care for its citizens following the war. Australia and New Zealand: 1945–1980 Following World War II, Australia and New Zealand enjoyed a great deal of prosperity along with the rest of the West. Both countries remained constitutional monarchies within the evolving Commonwealth of Nations and continued to recognise British monarchs as head of their own independent Parliaments. However, following British defeats by the Japanese in World War II, the post-war decline of the British Empire, and entry of Britain into the European Economic Community in 1973, the two nations re-calibrated defence and trade relations with the rest of the world. Following the Fall of Singapore in 1941, Australia turned to the United States for military aid against the Japanese Empire and Australia and New Zealand joined the United States in the ANZUS military alliance in the early 1950s and contributed troops to anti-communist conflicts in South-East Asia in the 1950s, 1960s and 1970s. The two nations also established multicultural immigration programs with waves of economic and refugee migrants establishing bases for large Southern European, East Asian, Middle Eastern, and South Pacific islander communities. Trade integration with Asia expanded, particularly through good post-war relations with Japan. The Maori and Aboriginal Australians had been largely dispossessed and disenfranchised during the 19th and early 20th centuries, but relations between the descendants of European settlers and the Indigenous peoples of Australia and New Zealand began to improve through legislative and social reform over the post-war period corresponding with the civil rights movement in North America. The Fraser government became a vocal critic of white-minority rule in Apartheid South Africa and Rhodesia, concluding the Gleaeagles Agreement in 1977. The arts also diversified and flourished over the period – with Australian cinema, literature and musical artists expanding their nation's profile internationally. The iconic Sydney Opera House opened in 1973 and Australian Aboriginal Art began to find international recognition and influence. Western culture: 1945–1980 The West went through a series of great cultural and social changes between 1945 and 1980. Mass media created a global culture that could ignore national frontiers. Literacy became almost universal, encouraging the growth of books, magazines and newspapers. The influence of cinema and radio remained, while televisions became near essentials in every home. A new pop culture also emerged with rock n roll and pop stars at its heart. Religious observance declined in most of the West. Protestant churches also began focusing more on social gospel rather than doctrine, and the ecumenical movement, which supported co-operation among Christian Churches. The Catholic Church changed many of its practices in the Second Vatican Council, including allowing masses to be said in the vernacular rather than Latin. The counterculture of the 1960s (and early 1970s) began in the United States as a reaction against the conservative government, social norms of the 1950s, the political conservatism (and perceived social repression) of the Cold War period, and the US government's extensive military intervention in Vietnam. With the abolition of laws treating most non-whites as second-class citizens, overt institutional racism largely disappeared from the West. Although the United States failed to secure the legal equality of women with men (by the failure of Congress to ratify the Equal Rights Amendment), women continued working outside the home, and by 1980 the double-income family became commonplace in Western society. Beginning in the 1960s, many began rejecting traditional Western values and there was a decline in emphasis on church and the family. Rock and roll music and the spread of technological innovations such as television dramatically altered the cultural landscape of western civilisation. The influential artists of the 20th century often belonged to the new technology artforms. Rock and roll emerged from the United States out of African-American music from the 1950s to become a quintessential 20th-century art form. Artists such as Chuck Berry and Little Richard developed the new genre in the United States. British rock and roll emerged later, with bands and artists like The Beatles, The Rolling Stones and Jimi Hendrix rising to unparalleled success during the 1960s. From Australia emerged the mega pop band The Bee Gees and hard rock band AC/DC, who carried the genre in new directions through the 1970s. These musical artists were icons of radical social changes which saw many traditional notions of western culture alter dramatically. Hollywood, California became synonymous with film during the 20th century and American Cinema continued a period of immense global influence in the West after World War II. American cinema played a role in adjusting community attitudes through the 1940s to 1980 with seminal works like John Ford's 1956 Western The Searchers, starring John Wayne, providing a sympathetic view of the Native American experience; and 1962's To Kill a Mockingbird, based on the Pulitzer Prize-winning novel by Harper Lee and starring Gregory Peck, challenging racial prejudice. The advent of television challenged the status of cinema and the artform evolved dramatically from the 1940s through the age of glamorous icons like Marilyn Monroe and directors like Alfred Hitchcock to the emergence of such directors as Stanley Kubrick, George Lucas and Steven Spielberg, whose body of work reflected the emerging Space Age and immense technological and social change. Western nations: 1980–present The 1980s were a period of economic growth in the West, though the 1987 Stock Market Crash saw much of the West enter the 1990s in a downturn. The 1990s and turn of the century in turn saw a period of prosperity throughout the West. The World Trade Organization was formed to assist in the organisation of world trade. Following the collapse of Soviet Communism, Central and Eastern Europe began a difficult readjustment towards market economies and parliamentary democracy. In the post Cold War environment, new co-operation emerged between the West and former rivals like Russia and China, but Islamism declared itself a mortal enemy of the West, and wars were launched in Afghanistan and the mid-East in response. The economic cycle turned again with the Great Recession, but amidst a new economic paradigm, the effect on the West was uneven, with Europe and United States suffering deep recession, but Pacific economies like Australia and Canada, largely avoiding the downturn – benefitting from a combination of rising trade with Asia, good fiscal management and banking regulation. In the early 21st century, Brasil, Russia, India and China (the BRIC nations) were re-emerging as drivers of economic growth from outside North America and Western Europe. In the early stages after the Cold War, Russian president Boris Yeltsin stared down an attempted restoration of Sovietism in Russia, and pursued closer relations with the West. Amid economic turmoil a class of oligarchs emerged at the summit of the Russian economy. Yeltsin's chosen successor, the former spy, Vladimir Putin, tightened the reins on political opposition, opposed separatist movements within the Russian Federation, and battled pro-Western neighbour states like Georgia, contributing to a challenging climate of relations with Europe and America. Former Soviet satellites joined NATO and the European Union, leaving Russia again isolated in the East. Under Putin's long reign, the Russian economy profited from a resource boom in the global economy, and the political and economic instability of the Yeltsin era quickly became a thing of the past. Elsewhere, both within and without the West, democracy and capitalism were in the ascendant – even Communist holdouts like mainland China and (to a lesser extent) Cuba and Vietnam, while retaining one party government, experimented with market liberalisation, a process which accelerated after the fall of European Communism, enabling the re-emergence of China as an alternative centre of economic and political power standing outside the West. Free trade agreements were signed by many countries. The European nations broke down trade barriers with one another in the EU, and the United States, Canada, and Mexico signed the North American Free Trade Agreement (NAFTA). Although free trade has helped businesses and consumers, it has had the unintended consequence of leading companies to outsource jobs to areas where labor is cheapest. Today, the West's economy is largely service and information-based, with most of the factories closing and relocating to China and India. European countries have had very good relations with each other since 1980. The European Union has become increasingly powerful, taking on roles traditionally reserved for the nation-state. Although real power still exists in the individual member states, one major achievement of the Union was the introduction of the Euro, a currency adopted by most EU countries. Australia and New Zealand continued their large multi-ethnic immigration programs and became more integrated in the Asia Pacific region. While remaining constitutional monarchies within the Commonwealth, distance has grown between them and Britain, spurred on by Britain's entry into the European Common Market. Australia and New Zealand have integrated their own economies via a free trade agreement. While political and cultural ties with North America and Europe remain strong, economic reform and commodities trade with the booming economies of Asia have set the South Pacific nations on a new economic trajectory with Australia largely avoiding a downturn in the Great Recession which unleashed severe economic loss through North America and Western Europe. Today Canada remains part of the Commonwealth, and relations between French and English Canada have continued to present problems. A referendum was held in Quebec, however, in 1980, in which Quebecers voted to remain part of Canada. In 1990, the white-minority government of the Republic of South Africa, led by F. W. de Klerk, began negotiations to dismantle the system of apartheid. South Africa held its first universal elections in 1994, which the African National Congress (ANC), led by Nelson Mandela, won by an overwhelming majority. The country has since rejoined the Commonwealth of Nations. Since 1991, the United States has been regarded as the world's only superpower. Politically, the United States is dominated by the Republican and Democratic parties. Presidents of the United States between 1980 and 2006 have been Ronald Reagan, George H. W. Bush, Bill Clinton, and George W. Bush. Since 1980, Americans have become far more optimistic about their country than they were in the 1970s. Since the 1960s, a large number of immigrants have been coming into the U.S., mostly from Asia and Latin America, with the largest single group being Mexicans. Large numbers from those areas have also been coming illegally, and the solution to this problem has produced much debate in the U.S. On 11 September 2001, the United States suffered the worst terrorist attack in its history. Four planes were hijacked by Islamic extremists and crashed into the World Trade Center, the Pentagon, and a field in Pennsylvania. The 2008 financial crisis was triggered by a liquidity shortfall in the United States banking system, and resulted in the collapse of large financial institutions, the bailout of banks by national governments, and downturns in stock markets throughout much of the West. The United States and Britain faced serious downturn, while Portugal, Greece, Ireland and Iceland faced major debt crises. Almost uniquely among Western nations, Australia avoided recession off the back of strong Asian trade and 25 years of economic reform and low levels of government debt. Evidence of the major demographic and social shifts which have taken place within Western society since World War II can be found with the elections of national level leaders: United States (Barack Obama was elected president in 2009, becoming the first African-American to hold that office), France (Nicolas Sarkozy, a president of France of Hungarian descent), Germany (Angela Merkel, the first female leader of that nation), and Australia (Julia Gillard, also the first female leader of that nation). Western nations and the world Following 1991, Western nations provided troops and aid to many war-torn areas of the world. Some of these missions were unsuccessful, like the attempt by the United States to provide relief in Somalia in the early 1990s. A very successful peace-making operation was conducted in the Balkans in the late 1990s, however. After the Cold War, Yugoslavia broke up into several countries along ethnic lines, and soon countries and ethnic groups within countries of the former Yugoslavia began fighting one another. Eventually, NATO troops arrived in 1999 and ended the conflict. Australian led a United Nations mission into East Timor in 1999 (INTERFET) to restore order during that nation's transition to democracy and independence from Indonesia. The greatest war fought by the West in the 1990s, however, was the Persian Gulf War. In 1990, the Middle Eastern nation of Iraq, under its brutal dictator Saddam Hussein, invaded the much smaller neighbouring country of Kuwait. After refusing to withdraw troops, the United Nations condemned Iraq and sent troops to liberate Kuwait. American, British, French, Egyptian and Syrian troops all took part in the liberation. The war ended in 1991, with the withdrawal of Iraqi troops from Kuwait and Iraq's agreement to allow United Nations inspectors to search for weapons of mass destruction in Iraq. The West had become increasingly unpopular in the Middle East following World War II. The Arab states greatly disliked the West's support for Israel. Many soon had a special hatred towards the United States, Israel's greatest ally. Also, partly to ensure stability on the region and a steady supply of the oil the world economy needed, the United States supported many corrupt dictatorships in the Middle East. In 1979, an Islamic revolution in Iran overthrew the pro-Western Shah and established an anti-Western Shiite Islamic theocracy. Following the withdrawal of Soviet troops from Afghanistan, most of the country came under the rule of a Sunni Islamic theocracy, the Taliban. The Taliban offered shelter to the Islamic terrorist group Al-Qaeda, founded by the extremist Saudi Arabian exile Osama bin Laden. Al-Qaeda launched a series of attacks on United States overseas interests in the 1990s and 2000. Following the September 11 attacks, however, the United States overthrew the Taliban government and captured or killed many Al Qaeda leaders, including Bin Laden. In 2003, the United States led a controversial war in Iraq, because Saddam had never accounted for all his weapons of mass destruction. By May of that year, American, British, Polish and troops from other countries had defeated and occupied Iraq. Weapons of mass destruction however, were never found afterwards. In both Afghanistan and Iraq, the United States and its allies established democratic governments. Following the Iraq war, however, an insurgency made up of a number of domestic and foreign factions has cost many lives and made establishing a government very hard. In March 2011, a multi-state coalition led by NATO began a military intervention in Libya to implement United Nations Security Council Resolution 1973, which was taken in response to threat made by the government of Muammar Gaddafi against the civilian population of Libya during the 2011 Libyan civil war. Western society and culture (since 1980) In general, Western culture has become increasingly secular in Northern Europe, North America, Australia and New Zealand. Nevertheless, in a sign of the continuing status of the ancient Western institution of the Papacy in the early 21st century, the Funeral of Pope John Paul II brought together the single largest gathering in history of heads of state outside the United Nations. It is likely to have been the largest single gathering of Christianity in history, with numbers estimated in excess of four million mourners gathering in Rome. He was followed by another non-Italian Benedict XVI, whose near-unprecedented retirement from the papacy in 2013 ushered in the election of the Argentine Pope Francis – the first pope from the Americas, the new demographic heartland of Catholicism. Personal computers emerged from the West as a new society changing phenomenon during this period. In the 1960s, experiment began on networks linking computers and from these experiments grew the World Wide Web. The internet revolutionised global communications through the late 1990s and into the early 21st century and permitted the rise of new social media with profound consequences, linking the world as never before. In the West, the internet allowed free access to vast amounts of information, while outside the democratic West, as in China and in Middle Eastern nations, a range of censorship and monitoring measures were instigated, providing a new socio-political contrast between east and west. Historiography Chicago historian William H. McNeill wrote The Rise of the West (1965) to show how the separate civilizations of Eurasia interacted from the very beginning of their history, borrowing critical skills from one another, and thus precipitating still further change as adjustment between traditional old and borrowed new knowledge and practice became necessary. He then discusses the dramatic effect of Western civilization on others in the past 500 years of history. McNeill took a broad approach organized around the interactions of peoples across the globe. Such interactions have become both more numerous and more continual and substantial in recent times. Before about 1500, the network of communication between cultures was that of Eurasia. The term for these areas of interaction differ from one world historian to another and include world-system and ecumene. His emphasis on cultural fusions influenced historical theory significantly. See also Outline of the history of Western civilization Role of Christianity in civilization History of Europe Eurocentrism Media Civilisation: A Personal View by Kenneth Clark (TV series), BBC TV, 1969 The Ascent of Man (TV series), BBC TV, 1973 References Further reading Cole, Joshua and Carol Symes. Western Civilizations (Brief Fifth Edition) (2 vol 2020) Kishlansky, Mark A. et al. A brief history of western civilization : the unfinished legacy (2 vol 2007) vol 1 online; also vol 2 online Perry, Marvin Myrna Chase, et al. Western Civilization: Ideas, Politics, and Society (2015) Rand McNally. Atlas of western civilization (2006) online Spielvogel, Jackson J. Western Civilization (10th ed. 2017) Bruce Thornton Greek Ways: How the Greeks Created Western Civilization Encounter Books, 2002 Tim Blanning The Pursuit of Glory: Europe 1648–1815 Penguin Books, 2008 Niall Ferguson Civilization. The West and the rest Penguin Press, 2011 Ian Kershaw To Hell and Back: Europe 1914–1949 Penguin Books, 2015 Richard J. Evans The Pursuit of Power: Europe 1815–1914 Penguin Books, 2017 Ian Kershaw To Hell and Back: Europe 1914–1949 Penguin Books, 2015 Ian Kershaw The Global Age: Europe 1950–2017 Penguin Books, 2020 Historiography Allardyce, Gilbert. "The rise and fall of the western civilization course." American Historical Review 87.3 (1982): 695–725. online Bavaj, Riccardo: "The West": A Conceptual Exploration , European History Online, Mainz: Institute of European History, 2011. Retrieved 28 November 2011. Bentley, Jerry H. "Cross-cultural interaction and periodization in world history." American Historical Review 101.3 (1996): 749–770. Douthit, Nathan. "The Dialectical Commons of Western Civilization and Global/World History." History Teacher 24#3 (1991), pp. 293–305, online. McNeill, William H. (1995). "The Changing Shape of World History". History and Theory. 34 (2): 8–26. doi:10.2307/2505432. JSTOR 2505432. Manning, Patrick. "The problem of interactions in world history." American Historical Review 101.3 (1996): 771–782. online Pincince, John. "Jerry Bentley World History, and the Decline of the 'West'" Journal of World History 25#4 (2014), pp. 631–43, online. Porciana, Haria, and Lutz Raphael, eds. Atlas of European Historiography: The Making of a Profession 1800–2005 ( Palgrave Macmillan, 2010) uses 80 maps to show how historians studied Europe. External links textbooks—online free to read Precision questioning (PQ), an intellectual toolkit for critical thinking and for problem solving, grew out of a collaboration between Dennis Matthies (1946- ) and Dr. Monica Worline, while both taught/studied at Stanford University. Precision questioning seeks to enable its practitioners with a highly structured, one-question/one-answer discussion format to help them: solve complex problems conduct deep analysis make difficult decisions PQ focuses on clearly expressing gaps in thinking by coupling a taxonomy of analytical questions with a structured call-and-response model to enable PQ practitioners to uncover weaknesses in thinking and to raise the intellectual level of a conversation. Those who use precision questioning (also called "PQers") describe PQ conversations as those analytical opportunities motivated by an attempt to get to precise answers, or to identify where no answer is available. However, when "drilling" into a topic, practitioners endeavor to avoid the use of personalization (blame or shame). Precision questioning holds to the ideal of meeting one's own needs for information while also respecting the intellectual integrity of the conversation-partner. Matthies, who taught at Stanford University's Center for Teaching and Learning (CTL) in the 1990s, developed several experimental courses that have subsequently become known to a wider public — including Precision Questioning, initially taught in the Stanford Philosophy Department. Proselytizing for precision questioning on a commercial basis continues via the Vervago company, co-founded by Matthies and Worline. Tens of thousands of people in universities and companies throughout the world have studied different versions of precision questioning. See also Critical Thinking Problem-solving Nonviolent Communication External links 1995 – Precision Questioning (Published through the Stanford University Bookstore) www.vervago.com == References == In political science, legitimacy is a concept which turns brute force into power. The right and acceptance of an authority, usually a governing law or a regime, at least formally, are impossible to be built on one's brute force, or to coerce people and force them to identify with a given group. Whereas authority denotes a specific position in an established government, the term legitimacy denotes a system of government—wherein government denotes "sphere of influence". An authority viewed as legitimate often has the right and justification to exercise power. Political legitimacy is considered a basic condition for governing, without which a government will suffer legislative deadlock(s) and collapse. In political systems where this is not the case, unpopular regimes survive because they are considered legitimate by a small, influential elite. In Chinese political philosophy, since the historical period of the Zhou dynasty (1046–256 BC), the political legitimacy of a ruler and government was derived from the Mandate of Heaven, and unjust rulers who lost said mandate therefore lost the right to rule the people. In moral philosophy, the term legitimacy is often positively interpreted as the normative status conferred by a governed people upon their governors' institutions, offices, and actions, based upon the belief that their government's actions are appropriate uses of power by a legally constituted government. The Enlightenment-era British social John Locke (1632–1704) said that political legitimacy derives from popular explicit and implicit consent of the governed: "The argument of the [Second] Treatise is that the government is not legitimate unless it is carried on with the consent of the governed." The German political philosopher Dolf Sternberger said that "[l]egitimacy is the foundation of such governmental power as is exercised, both with a consciousness on the government's part that it has a right to govern, and with some recognition by the governed of that right". The American political sociologist Seymour Martin Lipset said that legitimacy also "involves the capacity of a political system to engender and maintain the belief that existing political institutions are the most appropriate and proper ones for the society". The American political scientist Robert A. Dahl explained legitimacy as a reservoir: so long as the water is at a given level, political stability is maintained, if it falls below the required level, political legitimacy is endangered. Types Tradition, charisma and rational-legality Legitimacy is "a value whereby something or someone is recognized and accepted as right and proper". In political science, legitimacy has traditionally been understood as the popular acceptance and recognition by the public of the authority of a governing régime, whereby authority has political power through consent and mutual understandings, not coercion. The three types of political legitimacy described by German sociologist Max Weber, in "Politics as Vocation", are traditional, charismatic, and rational-legal: Traditional legitimacy derives from societal custom and habit that emphasize the history of the authority of tradition. Traditionalists understand this form of rule as historically accepted, hence its continuity, because it is the way society has always been. Therefore, the institutions of traditional government usually are historically continuous, as in monarchy and tribalism. Charismatic legitimacy derives from the ideas and personal charisma of the leader, a person whose authoritative persona charms and psychologically dominates the people of the society to agreement with the government's régime and rule. A charismatic government usually features weak political and administrative institutions, because they derive authority from the persona of the leader, and usually disappear without the leader in power. However, if the charismatic leader has a successor, a government derived from charismatic legitimacy might continue. Rational-legal legitimacy derives from a system of institutional procedure, wherein government institutions establish and enforce law and order in the public interest. Therefore, it is through public trust that the government will abide the law that confers rational-legal legitimacy. More recent scholarship distinguishes between multiple other types of legitimacy in an effort to draw distinctions between various approaches to the construct. These include empirical legitimacy versus normative legitimacy, instrumental versus substantive legitimacy, popular legitimacy, regulative legitimacy, and procedural legitimacy. Types of legitimacy draw distinctions that account for different sources of legitimacy, different frameworks for evaluating legitimacy, or different objects of legitimacy. Interactive dignity Legitimacy in conflict zones, where multiple authorities compete over authority and legitimacy, can rest on other sources. The theory of interactive dignity by Weigand shows that interactions are key for the construction of substantive legitimacy in such contexts. The aspect of an authority that most concerns people in the absence of other accountability mechanisms are its actions, particularly with regard to how authorities interact with them on a day-to-day basis. The value-based expectation people have with regard to such interactions is one of human dignity. People expect procedures to be fair and practices to be respectful, reflecting a serving rather than an extractive attitude. As long as authorities do not satisfy people's more immediate expectation of interactive dignity, people support and consider alternative authorities to be more legitimate. Forms Numinous legitimacy In a theocracy, government legitimacy derives from the spiritual authority of a god or a goddess. In ancient Egypt (c. 3150 BC), the legitimacy of the dominion of a Pharaoh (god–king) was theologically established by a doctrine that posited the pharaoh as the Egyptian patron god Horus, son of Osiris. Civil legitimacy The political legitimacy of a civil government derives from agreement among the autonomous constituent institutions—legislative, judicial, executive—combined for the national common good. In the United States, this issue has surfaced around how voting is impacted by gerrymandering, the United States Electoral College's ability to produce winners by minority rule and discouragement of voter turnout outside of swing states, and the repeal of part of the Voting Rights Act in 2013. Another challenge to the political legitimacy offered by elections is whether or not marginalized groups such as women or those who are incarcerated are allowed to vote. Civil legitimacy can be granted through different measures for accountability than voting, such as financial transparency and stake-holder accountability. In the international system another method for measuring civil legitimacy is through accountability to international human rights norms. In an effort to determine what makes a government legitimate, the Center for Public Impact launched a project to hold a global conversation about legitimacy stating, inviting citizens, academics and governments to participate. The organization also publishes case studies that consider the theme of legitimacy as it applies to projects in a number of different countries and cities including Bristol, Lebanon and Canada. "Good" governance vs "bad" governance The United Nations Human Rights Office of the High Commission (OHCHR) established standards of what is considered "good governance" that include the key attributes transparency, responsibility, accountability, participation and responsiveness (to the needs of the people). Input, output and throughput legitimacy Assessing the political legitimacy of a government can be done by looking at three different aspects of which a government can derive legitimacy. Fritz Scharpf introduced two normative criteria, which are output legitimacy, i.e. the effectiveness of policy outcomes for people and input legitimacy, the responsiveness to citizen concerns as a result of participation by the people. A third normative criterion was added by Vivien Schmidt, who analyzes legitimacy also in terms of what she calls throughput, i.e. the governance processes that happen in between input and output. Negative and positive legitimacy Derived from the concepts of positive freedom and negative freedom distinguished by Isaiah Berlin, Abulof distinguishes between negative political legitimacy (NPL), which is about the object of legitimation (answering what is legitimate), and positive political legitimacy (PPL), which is about the source of legitimation (answering who is the 'legitimator'). NPL is concerned with establishing where to draw the line between good and bad; PPL with who should be drawing it in the first place. From the NPL perspective, political legitimacy emanates from appropriate actions; from a PPL perspective, it emanates from appropriate actors. In the social contract tradition, Hobbes and Locke focused on NPL (stressing security and liberty, respectively), while Rousseau focused more on PPL ("the people" as the legitimator). Arguably, political stability depends on both forms of legitimacy. Instrumental and substantive legitimacy Weber's understanding of legitimacy rests on shared values, such as tradition and rational-legality. But policies that aim at (re-)constructing legitimacy by improving the service delivery or 'output' of a state often only respond to shared needs. Therefore, Weigand distinguishes substantive sources of legitimacy from more instrumental ones. Instrumental legitimacy rests on "the rational assessment of the usefulness of an authority ..., describing to what extent an authority responds to shared needs. Instrumental legitimacy is very much based on the perceived effectiveness of service delivery. Conversely, substantive legitimacy is a more abstract normative judgment, which is underpinned by shared values. If a person believes that an entity has the right to exercise social control, he or she may also accept personal disadvantages." Perceived legitimacy Establishing legitimacy is not simply transactional; service provision, elections and rule of law do not automatically grant legitimacy. State legitimacy rests on citizens' perceptions and expectations of the state, and these may be co-constructed between state actors and citizens. What legitimizes a state is also contextually specific. McCullough et al. (2020) show that in different countries, provision of different services build state legitimacy. In Nepal public water provision was most associated with state legitimacy, while in Pakistan it was health services. But it is not only states that can build legitimacy. Other authorities, such as armed groups in a conflict zones, may construct legitimacy more successfully than the state in certain strata of the population. Foundational and contingent legitimacy Political theorist Ross Mittiga has proposed an alternative typology, consisting of two parts: foundational and contingent legitimacy. According to Mittiga, foundational legitimacy (FL) "pertains to a government's ability to ensure the safety and security of its citizens," while contingent legitimacy (CL) obtains in situations in which governments "exercise[] power in acceptable ways." Mittiga specifies further that FL:...is bound up with a range of political capacities and actions including, among other things, being able to ensure continuous access to essential goods (particularly food, water, and shelter), prevent avoidable catastrophes, provide immediate and effective disaster relief, and combat invading forces or quell unjustified uprisings or rebellions. If a government cannot fulfill these basic security functions, it is not legitimate, if it is even a government at all. [p.3]On the other hand, Mittiga acknowledges that there is "extensive debate" about which factors are relevant to CL, but argues that, "[a]mong the most commonly defended factors" are "the presence of democratic rights and processes, consent, guarantees of equal representation, provision of core public benefits, protection of basic individual rights and freedoms, social justice, and observance of fairness principles." [pp. 4–5] Mittiga specifies further that "[m]ost contemporary theorists maintain that legitimacy [in the contingent sense] requires multiple of these factors—some of which are procedural and others substantive." According to Mittiga, what makes certain aspects of legitimacy "contingent" (as opposed to "foundational") is that they are affected by (1) "the problem of pluralism"—i.e., the idea that "any firm agreement on" which factor(s) matters (or matter most of all) "will remain elusive or at least always open to contestation and renegotiation"; (2) "the problem of partial displacement," which holds that "when new legitimation factors emerge," as they often have historically, "earlier ones may not entirely disappear but only become less salient, at least for sizable portions of the citizenry"; and (3) "the problem of exceptional circumstances," which is "the fact that even widely shared and seemingly stable CL factors are routinely relaxed or abandoned during emergencies, often without calling into question the basic legitimacy of the government." Mittiga summarizes the difference between these two types or levels or types of legitimacy as follows:The factors associated with CL condition the use of political power by specifying, for instance, what can or cannot be done or sacrificed, how decisions should be made, and who counts (and for how much). The answers to these questions often appear to us as moral universals; yet, in practice, they are the products of long and contentious historical processes. FL, on the other hand, does not vary between societies, generations, or circumstances. Ensuring safety and security is always the primary—though, in good states, under reasonably favorable conditions, not the exclusive—end of political power. Aristotle expresses something like this in insisting that the point of political society is to furnish the resources needed not just to live but to live well. Crudely put, FL is about living, CL about living well. And it is of course impossible to live well without living: after all, there can be no democracy of desolation, no fair social cooperation in conditions of extreme scarcity, no real rights when political stability is maintainable only through raw assertions of coercive power (if it can be maintained at all). In this sense, FL is necessarily prior to CL, and must be regarded as such in moments when trade-offs become a necessary part of the political calculus. [p.7] Sources Max Weber proposed that societies behave cyclically in governing themselves with different types of governmental legitimacy. That democracy was unnecessary for establishing legitimacy, a condition that can be established with codified laws, customs, and cultural principles, not by means of popular suffrage. That a society might decide to revert from the legitimate government of a rational–legal authority to the charismatic government of a leader; e.g., the Nazi Germany of Adolf Hitler, Fascist Italy under Benito Mussolini, and Francoist Spain under General Francisco Franco. The French political scientist Mattei Dogan's contemporary interpretation of Weber's types of political legitimacy (traditional, charismatic, legal-rational) proposes that they are conceptually insufficient to comprehend the complex relationships that constitute a legitimate political system in the 21st century. Moreover, Dogan proposed that traditional authority and charismatic authority are obsolete as forms of contemporary government; e.g., the Islamic Republic of Iran (est. 1979) rule by means of the priestly Koranic interpretations by the Ayatollah Ruhollah Khomeini. That traditional authority has disappeared in the Middle East; that the rule-proving exceptions are Islamic Iran and Saudi Arabia. Furthermore, the third Weber type of political legitimacy, rational-legal authority, exists in so many permutations no longer allow it to be limited as a type of legitimate authority. Forms of legitimate government In determining the political legitimacy of a system of rule and government, the term proper—political legitimacy—is philosophically an essentially contested concept that facilitates understanding the different applications and interpretations of abstract, qualitative, and evaluative concepts such as "art", "social justice", et cetera, as applied in aesthetics, political philosophy, the philosophy of history, and the philosophy of religion. Therefore, in defining the political legitimacy of a system of government and rule, the term "essentially contested concept" indicates that a key term (communism, democracy, constitutionalism, etc.) has different meanings within a given political argument. Hence, the intellectually restrictive politics of dogmatism ("My answer is right, and all others are wrong"), scepticism ("I don't know what is true, and I even doubt my own opinion"), and eclecticism ("Each meaning gives a partial view, so the more meanings the better") are inappropriate philosophic stances for managing a political term that has more than one meaning (see Walter Bryce Gallie). Establishing what qualifies as a legitimate form of government continues to be a topic of great philosophical controversy. Forms of legitimate government are posited to include: Communism, where the legitimacy of a Communist state derives from having won a civil war, a revolution, or from having won an election such as the Presidency of Salvador Allende (1970–73) in Chile; thus, the actions of the Communist government are legitimate, authorised by the people. In the early 20th century, Communist parties based the arguments supporting the legitimacy of their rule and government upon the claimed scientific nature of Marxism (see dialectical materialism). Constitutionalism, where the modern political concept of constitutionalism establishes the law as supreme over the private will, by integrating nationalism, democracy, and limited government. The political legitimacy of constitutionalism derives from popular belief and acceptance that the actions of the government are legitimate because they abide by the law codified in the political constitution. The political scientist Carl Joachim Friedrich (1901–1984) said that, in dividing political power among the organs of government, constitutional law effectively restrains the actions of the government (see checks and balances). Democracy, where government legitimacy derives from the popular perception that the elected government abides by democratic principles in governing, and thus is legally accountable to its people. Fascism, where in the 1920s and the 1930s it based its political legitimacy upon the arguments of traditional authority; respectively, the German National Socialists and the Italian Fascists claimed that the political legitimacy of their right to rule derived from philosophically denying the (popular) political legitimacy of elected liberal democratic governments. During the Weimar Republic (1918–1933), the political philosopher Carl Schmitt (1888–1985)—whose legal work as the "Crown Jurist of the Third Reich" promoted fascism and deconstructed liberal democracy—addressed the matter in Legalität und Legitimität (Legality and Legitimacy, 1932), an anti-democratic polemic treatise that asked: "How can parliamentary government make for law and legality, when a 49 per cent minority accepts as politically legitimate the political will of a 51 per cent majority?" Monarchy, where the divine right of kings establishes the political legitimacy of the rule of the monarch (king or queen); legitimacy also derives from the popular perception (tradition and custom) and acceptance of the monarch as the rightful ruler of nation and country. Contemporarily, such divine-right legitimacy is manifest in the absolute monarchy of the House of Saud (est. 1744), a royal family who have ruled and governed Saudi Arabia since the 18th century. Moreover, constitutional monarchy is a variant form of monarchic political legitimacy which combines traditional authority and legal–rational authority, by which means the monarch maintains nationalist unity (one people) and democratic administration (a political constitution). Theocracy, where deity establishes legitimacy. See also Further reading Schoon, Eric W. (2022). "Operationalizing Legitimacy." American Sociological Review. Weigand, Florian (2015). "Investigating the Role of Legitimacy in the Political Order of Conflict-torn Spaces." Security in Transition. Weigand, Florian (2022). Waiting for Dignity: Legitimacy and Authority in Afghanistan. Columbia University Press. ISBN 978-0-231-55364-3. == References == Meta-ethnicity is a level of commonality that is wider ("meta-") and more general (i.e., might differ on specifics) than ethnicity, but does not necessarily correspond to (and may actually transcend) nation or nationality. It is a relatively recent term (or neologism) occasionally used in academic literature or public discourse on ethnic studies. In colloquial discourse, it usually signifies a larger in-group of distinct ethnic groups who identify more closely with each other than they would with out-group ethnic groups. The groups within the in-group may be genetically and culturally related which reinforces the grouping. An early use—possibly the first published in English—was an article in a 1984 USSR Academy of Sciences publication discussing identity in Asia and Africa. Examples of use Some other examples: Gurharpal Singh, Ethnic Conflict in India: A Case-Study of Punjab (New York: Palgrave, 2000). Gurharpal Singh, "Against this dominant view of the nature of the Indian state, Singh argues that India should be seen as an 'ethnic democracy' in which Hinduism works as a meta-ethnicity and in which hegemonic control is exercised over ethnic minorities, particularly those living in the peripheral regions" in Christopher Shackle, Gurharpal Singh and Arvind-Pal Mandair eds., Sikh Religion, Culture and Ethnicity (Curson: 2001), p.155. "L. Byzov, however, believes that 'there has taken place within the Russian national consciousness one of the most radical changes ever: from a meta-ethnic sense of identity to a strictly ethnic identity' (Byzov 1996, 45)." "Geoffrey Fox, on the other hand, argues that 'Hispanic', with its emphasis on Spanish-language heritage as the foundation of meta-ethnicity, has no implied racial or class agendas and is simply preferred by most immigrants from Latin America." ... "Furthermore, these split-level processes of identity formation—the forging of ethnicity and meta-ethnicity—take place in regional contexts of unequal ethnic control over media and symbol systems." Peter Turchin introduces the concept "metaethnic frontier theory" in his 2003 book, Historical Dynamics: Why States Rise and Fall According to Hussain, Imtiaz, "At the beginning of the new century, Chinese people are living the construction of a metaethnicity of multiple identities." See also Multiethnic society Panethnicity References External links Discussion of "Meta-ethnicity" on H-Africa In philosophy, praxeology or praxiology (; from Ancient Greek πρᾶξις (praxis) 'deed, action' and -λογία (-logia) 'study of') is the theory of human action, based on the notion that humans engage in purposeful behavior, contrary to reflexive behavior and other unintentional behavior. French social philosopher Alfred Espinas gave the term its modern meaning, and praxeology was developed independently by two principal groups: the Austrian school, led by Ludwig von Mises, and the Polish school, led by Tadeusz Kotarbiński. Origin and etymology Coinage of the word praxeology (praxéologie) is often credited to Louis Bourdeau, the French author of a classification of the sciences, which he published in his Théorie des sciences. Plan de science intégrale in 1882: On account of their dual natures of specialty and generality, these functions should be the subject of a separate science. Some of its parts have been studied for a long time, because this kind of research, in which man could be the main subject, has always presented the greatest interest. Physiology, hygiene, medicine, psychology, animal history, human history, political economy, morality, etc. represent fragments of a science that we would like to establish, but as fragments scattered and uncoordinated have remained until now only parts of particular sciences. They should be joined together and made whole in order to highlight the order of the whole and its unity. Now you have a science, so far unnamed, which we propose to call Praxeology (from πραξις, action), or by referring to the influence of the environment, Mesology (from μεσος, environment). However, the term was used at least once previously (with a slight spelling difference), in 1608, by Clemens Timpler in his Philosophiae practicae systema methodicum: There was Aretology: Following that Praxiology: which is the second part of the Ethics, in general, commenting on the actions of the moral virtues. It was later mentioned by Robert Flint in 1904 in a review of Bourdeau's Théorie des sciences. The modern definition of the word was first given by Alfred V. Espinas (1844–1922), the French philosopher and sociologist; he was the forerunner of the Polish school of the science of efficient action. The Austrian school of economics was based on a philosophical science of the same kind. With a different spelling, the word was used by the English psychologist Charles Arthur Mercier (in 1911), and proposed by Knight Dunlap to John B. Watson as a better name for his behaviorism, but Watson rejected it. The Chinese physiologist of behavior Zing-Yang Kuo (b. 1898) adopted the term around 1935. It was also used by William McDougall (in 1928 and later). Previously the word praxiology, with the meaning Espinas gave to it, was used by Tadeusz Kotarbiński (in 1923). The Ukrainian economist Eugene Slutsky (1926) used it in his attempt to base economics on a theory of action. It was also used by Austrian economist Ludwig von Mises (1933), Russian Marxist Nikolai Bukharin (1888–1938) during the Second International Congress of History of Science and Technology in London (in 1931), and Polish scholar Oscar Lange (1904–1965) in 1959, and later. The Sicilian philosopher Carmelo Ottaviano was using the Italianised version, prassiologia, in his treatises starting from 1935, but in his own way, as a theory of politics. After the Second World War the use of the term praxeology spread widely. After the emigration of Mises to the US his pupil Murray Rothbard defended the praxeological approach. A revival of Espinas's approach in France was revealed in the works of Pierre Massé (1946), the cybernetician, Georges Théodule Guilbaud (1953), the Belgian logician, Leo Apostel (1957), the cybernetician, Anatol Rapoport (1962), Henry Pierron, psychologist and lexicographer (1957), François Perroux, economist (1957), the social psychologist, Robert Daval (1963), the well-known sociologist, Raymond Aron (1963) and the methodologists, Abraham Antoine Moles and Roland Caude (1965). Under the influence of Tadeusz Kotarbiński, praxeology flourished in Poland. A special "Centre of Praxeology" (Zaklad Prakseologiczny) was created under the organizational guidance of the Polish Academy of Sciences, with its own periodical (from 1962), called at first Materiały Prakseologiczne (Praxeological Papers), and then abbreviated to Prakseologia. It published hundreds of papers by different authors, and the materials for a special vocabulary edited by Professor Tadeusz Pszczolowski, the leading praxeologist of the younger generation. A sweeping survey of the praxeological approach is to be found in the paper by the French statistician Micheline Petruszewycz, "A propos de la praxéologie". Ludwig von Mises was influenced by several theories in forming his work on praxeology, including Immanuel Kant's works, Max Weber's work on methodological individualism, and Carl Menger's development of the subjective theory of value. Philosopher of science Mario Bunge published works of systematic philosophy that included contributions to praxeology.: 407 Austrian economics Austrian economics in the tradition of Ludwig von Mises relies heavily on praxeology in the development of its economic theories. Mises considered economics to be a sub-discipline of praxeology. Austrian School economists, following Mises, use praxeology and deduction, rather than empirical studies, to determine economic principles. According to these theorists, with the action axiom as the starting point, it is possible to draw conclusions about human behavior that are both objective and universal. For example, the notion that humans engage in acts of choice implies that they have preferences, and this must be true for anyone who exhibits intentional behavior. Advocates of praxeology also say that it provides insights for the field of ethics. Subdivisions In 1951, Murray Rothbard divided the subfields of praxeology as follows: A. The Theory of the Isolated Individual (Crusoe Economics) B. The Theory of Voluntary Interpersonal Exchange (Catallactics, or the Economics of the Market) 1. Barter 2. With Medium of Exchange a. On the Unhampered Market b. Effects of Violent Intervention with the Market c. Effects of Violent Abolition of the Market (Socialism) C. The Theory of War – Hostile Action D. The Theory of Games (Game theory) (e.g., von Neumann and Morgenstern) E. Unknown At the time, topics C, D, and E were regarded by Rothbard as open research problems. Criticisms Thomas Mayer has argued that, because praxeology rejects positivism and empiricism in the development of theories, it constitutes nothing less than a rejection of the scientific method. For Mayer, this invalidates the methodologies of the Austrian school of economics. Austrians argue that empirical data itself is insufficient to describe economics; that consequently empirical data cannot falsify economic theory; that logical positivism cannot predict or explain human action; and that the methodological requirements of logical positivism are impossible to obtain for economic questions. Ludwig von Mises in particular argued against empiricist approaches to the social sciences in general, because human events are unique and non-repeatable, whereas experiments in the physical sciences are necessarily reproducible. However, economist Antony Davies argues that because statistical tests are predicated on the independent development of theory, some form of praxeology is essential for model selection; conversely, praxeology can illustrate surprising philosophical consequences of economic models. Argentine-Canadian philosopher Mario Bunge dismissed von Mises's version of praxeology as "nothing but the principle of maximization of subjective utility—a fancy version of egoism".: 394 Bunge, who was also a fierce critic of pseudoscience, warned that when "conceived in extremely general terms and detached from both ethics and science, praxiology has hardly any practical value".: 394 See also References Further reading Austrian school Selgin, George A. (December 1988). "Praxeology and understanding: an analysis of the controversy in Austrian Economics". The Review of Austrian Economics. 2 (1): 19–58. CiteSeerX 10.1.1.378.1506. doi:10.1007/BF01539297. S2CID 15604266. Smith, George H. (2008). "Praxeology". In Hamowy, Ronald (ed.). The Encyclopedia of Libertarianism. Thousand Oaks, CA: SAGE Publications; Cato Institute. pp. 387–388. doi:10.4135/9781412965811.n239. ISBN 978-1412965804. LCCN 2008009151. OCLC 750831024. Polish school Gasparski, Wojciech W. (1992–). Praxiology: The International Annual of Practical Philosophy and Methodology. New Brunswick, NJ: Transaction Publishers. OCLC 611063114 Kotarbiński, Tadeusz (1965) [1955]. Praxiology: An Introduction to the Sciences of Efficient Action. Oxford; New York: Pergamon Press. ISBN 978-0080101101. OCLC 825097. Mulattos in Jamaica generally refers to Jamaicans of mixed African and European ancestry. Over time, the classification has come to include a broader range of ancestries—among them, significant contributions from Middle Eastern immigrants (predominantly from Syria, Lebanon, and neighboring regions) who arrived in the late 19th and early 20th centuries. Terminology Historically, the term "mulatto" was used in the Americas as a descriptor for individuals of mixed European and African descent. In Jamaica, early colonial documents, plantation records, and legal codes employed classifications—such as "mulatto", "quadroon", and "octoroon"—to denote differences in the degree of European ancestry. Over time, scholars have argued that such classifications are overly simplistic in the Jamaican context. While "mulatto" traditionally implies a mix of European and African ancestry, Jamaica's complex migratory history means that many individuals with Middle Eastern admixture have also been grouped under this category. This expanded understanding challenges the binary framework and calls for a more inclusive redefinition of mixed‑race identity in Jamaica. Historical background The Colonial Era and racial classification Jamaica's colonial history, beginning under Spanish rule and later as a British colony, was marked by plantation agriculture which relied on enslaved African labor. Interracial unions—whether consensual or coerced—between European planters and African women generated a growing mixed‑race population. Early legal and administrative records reveal that colonial authorities constructed a racial hierarchy which positioned mixed‑race individuals in an "in-between" space. They were often accorded certain privileges relative to enslaved Africans yet excluded from full participation in the white colonial elite. As European concepts of race became more formalized, colonial governors and merchant classes codified legal statuses tied to ancestry and social utility. These classifications, in part, laid the groundwork for later disputes over property rights and access to education. Emergence of the Mulatto elite By the late 18th and early 19th centuries, a distinctive group of free, mixed‑race Jamaicans—commonly known as the "mulatto elite"—emerged. Their ascent was facilitated by several factors: Factors 1) Access to education and commerce Many mulattoes gained better educational opportunities than enslaved Africans, often receiving private tutoring or schooling in Britain. This access enabled them to participate in trade, acquire wealth, and own land, positioning them as merchants, plantation managers, and skilled professionals. Daniel Livesay’s research highlights how elite mixed-race Jamaicans petitioned for exemptions from racial restrictions, allowing them to inherit wealth and enter commerce. The Jamaican Assembly’s 1761 Act limited inheritance for mixed-race individuals but still allowed some mulatto families to retain property. 2) Navigating dual worlds Mulatto elites were uniquely positioned to serve as intermediaries between European colonial authorities and the broader Jamaican population. Their familiarity with both European and African cultural norms allowed them to engage in diplomacy, business negotiations, and political advocacy. Livesay’s study on kinship privileges shows that family ties to white colonists often helped mulattoes gain legal exemptions and social advantages. Jamaican colonial records indicate that some mixed-race individuals were granted honorary "white" status, further integrating them into elite circles. 3) Legal privileges and property rights Certain legal provisions favored free people of color, securing advantageous inheritance and property rights. While restrictions existed, mulatto elites successfully petitioned for legal recognition, allowing them to own estates, engage in trade, and pass wealth to their descendants. The 1761 Jamaican Assembly Act capped inheritance for mixed-race individuals but allowed exceptions for those with strong family ties to white colonists. JSTOR research on Jamaican legal history confirms that mulatto elites strategically used legal petitions to secure property rights. Legacy and Modern Implications Since the 1938 labor uprisings, Jamaica’s trade unions have played a central role in securing workers’ rights—from establishing collective-bargaining standards to influencing minimum-wage legislation—and they remain key stakeholders in debates over economic policy and social welfare today. During colonial and early post-independence eras, a relatively small Mulatto elite parlayed their access to education and capital into leadership roles in commercial enterprises, university governance, and the civil service. Anthropological studies show that these families helped found premier schools and financial institutions, embedding their influence in Jamaica’s economic infrastructure. Yet the same colonial hierarchies that privileged lighter skin continue to inform social attitudes. Empirical surveys demonstrate that darker-skinned Jamaicans still face systematic disadvantages in schooling, employment opportunities, and access to household amenities—patterns consistent with a “color-class” stratification that echoes the old white-Brown-Black pyramid. Though there is no formal “Mulatto” party today, lighter-skinned Jamaicans remain overrepresented among political office-holders—especially within the Jamaica Labour Party (JLP). Electoral studies find that, even after racial labels fell out of official use, skin tone correlates strongly with the probability of attaining elected office and senior civil-service posts. That enduring color-class pyramid—white at the apex, Brown in the middle, and Black at the base—still casts a long shadow over political representation. Qualitative research on Jamaican political culture shows how phenotypical distinctions continue to shape who gets party nominations, campaign funding, and high-profile cabinet portfolios. The JLP itself was born in 1943 under Alexander Bustamante (of mixed European and African ancestry) and forged close ties to business-class interests—many of whom were lighter-skinned. That alliance endures: today’s JLP leadership ranks include a disproportionately high share of officials from historically “Brown” family backgrounds. By contrast, the People’s National Party (PNP), founded in 1938 by Norman Manley (also of mixed ancestry), has styled itself as more explicitly aligned with Black nationalist and democratic-socialist ideals—yet it too has featured prominent Mulatto figures (Manley’s own family among them) in its highest echelons. Legal policies favoring mixed-race individuals Inheritance and property laws Research into records dating from 1700 to 1761 shows that some mixed-race individuals successfully petitioned the colonial assembly to be recognized in a legal category comparable to that of whites. This recognition enabled them to secure property inheritance rights and expand their landholdings—a privilege rarely available to enslaved or darker-skinned freed Africans. Manumission legislation and post-emancipation policy Following emancipation (1834–1838) in the British Caribbean, legal frameworks such as those documented in the Laws of Jamaica, 1845–1967 facilitated the upward mobility of free people of color. These measures provided better access to manumission, education, and professional opportunities, further contributing to long-term wealth disparities that favored mixed-race individuals. Discriminatory underpinnings Despite these advantages, legal privileges granted to mixed-race individuals were founded on racial hierarchies. They were justified on the basis of a purported "civilizing" potential and often aimed at promoting controlled inter-ethnic cooperation while cementing imperial authority. Legacy of the mulatto elite These legal advantages boosted the economic and social standing of the mulatto elite yet also laid the foundation for enduring debates about racial bias and wealth disparity in contemporary Jamaica. These advantages entrenched a social hierarchy that favored lighter-skinned Jamaicans over those with fuller African features—a legacy that continues to influence wealth and social status on the island. A number of laws and legal practices during the 18th and 19th centuries in Jamaica helped cement the status of the mulatto elite: Inheritance and property laws: Research into records dating from 1700 to 1761 shows that some mixed‑race individuals successfully petitioned the colonial assembly to be recognized in a legal category comparable to that of whites. This recognition enabled them to secure property inheritance rights and expand their landholdings—a privilege rarely available to enslaved or darker‑skinned freed Africans. Manumission legislation and post‑emancipation policy: Following emancipation (which occurred around 1834–1838 in the British Caribbean), legal frameworks such as those documented in the Laws of Jamaica, 1845–1967 facilitated the upward mobility of free people of color. These measures provided better access to manumission, education, and professional opportunities, further contributing to long‑term wealth disparities that favored mixed‑race individuals. Discriminatory underpinnings: Despite these advantages, legal privileges granted to mixed‑race individuals were founded on racial hierarchies. They were justified on the basis of a purported "civilizing" potential and often aimed at promoting controlled inter‑ethnic cooperation while cementing imperial authority. These legal advantages boosted the economic and social standing of the mulatto elite yet also laid the foundation for enduring debates about racial bias and wealth disparity in contemporary Jamaica. Expanded scope of mixed heritage Middle Eastern contributions While traditional usage of "mulatto" focused on European African ancestry, significant immigration from the Middle East occurred during the late 19th and early 20th centuries. Immigrants from Syria, Lebanon, and neighboring regions came primarily as entrepreneurs and traders. Over time, intermarriage between these immigrants and Afro‑Jamaicans broadened the definition of mixed heritage in Jamaica. Consequently, many individuals traditionally labeled as "mulatto" may also possess Middle Eastern ancestry—a nuance that complicates the conventional binary framework. Scholars note that Middle Eastern immigrant communities successfully integrated into commerce and social networks, and their cultural practices have continued to influence the island's multicultural identity. Scholarly debate on racial classification Contemporary scholars argue that traditional racial categorizations oversimplify Jamaica's multifaceted identity. Researchers such as Reid and Mohammed (2014) suggest that a comprehensive understanding of Jamaican identity must recognize contributions from Middle Eastern, Asian, and indigenous influences. This reinterpreted framework better reflects the cultural and migratory complexities that have shaped the island's heritage. Social, economic, and cultural roles Racial stratification and social hierarchies Despite these advantages, mulatto elites faced structural limitations imposed by colonial racial hierarchies. Their legal privileges were often justified by European authorities as part of a “civilizing mission,” promoting controlled racial cooperation while preserving imperial dominance. These policies reinforced color-based stratification, ensuring that while mulattoes could accumulate wealth and power, they remained subordinate to British colonial elites. The legacy of Jamaica’s mulatto elite is evident in modern discussions of race, colorism, and economic inequality. Research into Jamaican social stratification reveals that lighter-skinned individuals continue to enjoy greater access to education, wealth, and social status, while darker-skinned Jamaicans face systemic barriers. These disparities are directly linked to colonial policies, which prioritized mixed-race individuals, shaping Jamaica’s long-term patterns of wealth accumulation and social privilege. Economic and political influence Economic empowerment allowed the mulatto elite to become central figures in commerce, industry, and politics. Their control over land and business—facilitated by favorable legal provisions—enabled them to help shape local governance and contribute significantly to Jamaica's early bourgeoisie. Many members of this group later played influential roles in the nation‑building process following independence in 1962. Cultural impact and identity formation Jamaica's mixed-race heritage has contributed to a diverse cultural landscape, reflected in the island's artistic, musical, and literary traditions. The integration of African, European, and Middle Eastern cultural influences has resulted in a distinct creole culture, evident in genres such as reggae and dancehall music, as well as visual arts and culinary practices. This cultural fusion highlights Jamaican identity while also contributing to discussions on colorism and social justice within the society. Relevance to nation‑building and contemporary debates At independence, Jamaica's national symbols—most notably the motto "Out of Many, One People"—were deliberately chosen to promote unity amid diversity. This inclusive narrative recognized the multifaceted nature of the island's heritage, reflecting the diverse contributions of mixed‑race populations with European, African, and Middle Eastern elements. However, the historical legal privileges that once favored the mulatto elite have also been implicated in modern debates over racial inequality and wealth disparities. The enduring legacy of these legal frameworks continues to shape discussions on identity, equity, and resource distribution in contemporary Jamaican society. Legal petitions by people of color in Jamaica Legal privileges & petitioning: Elite mixed-race Jamaicans in the 18th century petitioned the Jamaican Assembly to secure legal privileges, sometimes even being reclassified as "white" based on wealth and ancestry. However, increasing restrictions were imposed after Tacky’s Revolt (1760). Mixed-race identity in Jamaica vs. other Caribbean islands Racialization & colonial legacy: Jamaica’s racial identity is shaped by colonial-era classifications, where mixed-race individuals were often positioned between the white elite and the enslaved population. The government has historically downplayed racial divisions, promoting a "raceless" national identity. Social inequality & skin color: Studies show that lighter-skinned Jamaicans tend to have higher household amenities and educational attainment compared to darker-skinned individuals, even when controlling for class. Creolization & national identity: Jamaica’s "Out of Many, One People" motto reflects an ideology of racial mixing, but this has also been used to mask systemic inequalities. Historical legal provisions & wealth disparities in Jamaica Colonial wealth distribution: Jamaica was considered one of the richest British colonies in the 18th century, but wealth was concentrated among white plantation owners, while enslaved and free Black populations lived in extreme poverty. Legal framework & economic rights: Jamaica’s legal system historically favored elite landowners, and even today, economic rights are not fully enforceable under the Jamaican Constitution. Maroons & land ownership: The Maroons were granted land rights after signing peace treaties with the British, but later faced legal challenges that undermined their autonomy. Notable Jamaican Mulattos Bob Marley George William Gordon Norman Manley George Stiebel Yendi Phillipps Kaci Fennell Kamina Johnson Smith Donald Sangster David Coore Damian Marley See also Mulatto Casta African Diaspora in the Americas Jamaican Creole Postcolonialism == References == Seishitsu (正室) is the Japanese term of the Edo period for the official wife of high-ranking persons. The tennō, kugyō (court officials), shōgun and daimyōs often had several wives to ensure the birth of an heir. The seishitsu had a status above other wives, called sokushitsu (側室, concubine). The system dates back to the ritsuryō system in the Nara and Heian periods. At the time, the main wife was called chakusai (嫡妻). The last Japanese emperor to have official concubines was Emperor Meiji. Succession disputes between sons of the official wife and concubines were a constant source of internal, often armed conflict within houses (O-Ie Sōdō). See also midaidokoro Dishu system A pull-tight seal (or pull-up seal) is a disposable tamper-evident seal which restricts access to closed objects for those who do not have access. The tightening seal is needed in cases where the use of more sophisticated security measures are impossible or impractical. The seal is not established for the physical protection of the object which is sealed, but for indicating attempts to interfere. Layers of security The tightening seal (Pull-up type) uses multiple layers of security: an individual number; an individual logo; protection for the latching mechanism against direct access; the ability to control of a condition of the locking mechanism (collet); a material of collet production; labeling method (pad printing or laser marking). Any attempt to circumvent one of these protections must be visible easily and immediately. Access control The seal gives users the opportunity to provide access control to objects such as warehouses, vehicles, containers, gas stations, offices, vault cash in banks, cash collection bags, slot machines, lockers, luggage and many other objects. Also food (fish, meat, honey) and fur animals can be protected from forgery. Design The design of the pull-tight seal is configured of plastic in the form of arrows. They have a surface for marking an individual number and latching mechanism on the one hand, and a plastic cable on the other. Like a zip tie, the cable forms a loop during mill threading into the collet, which can not be separated without damaging the cable or the locking mechanism. Its integrity is monitored through a transparent cover. Advantages and disadvantages Advantages: the possibility of visual control of integrity; easy to install; high quality; can not be opened without damage; unique numbering; favorable price. Disadvantages: seal’s material is easily broken and can not serve as a fastening element. Security Tamper-Evident: Pull-tight seals are designed to provide visual evidence of tampering. Once they are properly secured, any attempt to remove or tamper with the seal will cause visible damage or indicate signs of interference. Adjustable Length: Pull-tight seals offer adjustable lengths, allowing them to fit a variety of applications. They can be pulled tightly around the item being secured, preventing unauthorized access or tampering. Unique Identification: Many pull-tight seals come with unique identification numbers or barcodes printed on them. This feature helps in tracking and verifying the seal's authenticity and can be useful for inventory management or security audits. Secure Closure: Pull-tight seals use a locking mechanism that securely fastens the seal in place. Once tightened, they are difficult to open without breaking or damaging the seal, ensuring the integrity of the sealed item. Versatile Application: Pull-tight seals can be used in various industries and applications, such as securing cargo containers, securing doors, locking storage units, sealing evidence bags, and more. They are widely used in logistics, transportation, healthcare, law enforcement, and other sectors that require tamper-proof security measures. Durable Materials: Pull-tight seals are typically made of strong and durable materials like high-density polyethylene (HDPE) or metal, which are resistant to tampering attempts such as cutting or pulling. One-Time Use: Pull-tight seals are generally designed for one-time use. Once they have been removed or broken, they cannot be resealed, providing an added layer of security by preventing reapplication or resealing attempts. Customization Options: Pull-tight seals can be customized with specific colors, logos, or text to enhance branding or identification. This customization can help differentiate between different security levels or indicate ownership. Compliance Standards: Pull-tight seals may comply with industry standards and regulations, such as ISO 17712 for high-security seals used in international shipping containers. Compliance with these standards ensures that the seals meet specific security requirements. It's important to note that the effectiveness of pull-tight seals relies on proper application and careful monitoring. Regular inspection and verification of the seals are necessary to ensure the integrity of the secured items. In Indonesia, virginity tests are performed as a requirement for joining the police and military force. Tests have been performed since 1965. It has since been condemned internationally and domestically by news outlets and human rights organisations. In August 2021, the Indonesian Army stated that it would no longer continue this practice for female recruits, and was formally announced by the Indonesian Army Chief of Staff General Andika Perkasa. Legal basis The 1999 human rights law in Indonesia bans discrimination against women. The UN Human Rights Office called for ban of virginity testing, viewing the practice to be medically unnecessary, often painful, humiliating, and traumatic. Despite international pressure, Jokowi's government has not initiated legislation to end virginity testing. Virginity testing in different fields Police Chief Police Regulation No. 5/2009 outlines the Guidelines for Police Candidates. Article 36 requires female applicants to an "obstetrics and gynecology" examination, without specifying further. Tests are conducted by the Police Medical and Health Center (Pusat Kedokteran dan Kesehatan) using the "two-finger test". The Indonesian National Police jobs website stated in 2014, that, "In addition to the medical and physical tests, women who want to be policewomen must also undergo virginity tests. So all women who want to become policewomen should keep their virginity." The official letter containing the order to stop the tests has been sent in 2014 and its gradually being implemented, however it may still be performed on regional areas. Military The Indonesian National Armed Forces also has been conducting this practice, as confirmed by its deputy head of the Health Center, Andriani. The test isn't only required for female applicants, but also for the fianceés of its personnel. They carry out the test to "ensure the health of the body and the spirit of these women." High-ranking officials have deemed the test to be relevant, as it measures "the personality and mentality of the person," and linked non-virgin women to bad habits as opposed to military personnel who are supposed to "protect the nation." Discontinuation In August 2021, Indonesian Army Chief of Staff General Andika Perkasa has officially announced that the Army will no longer conduct virginity testing for female recruits. The Navy and Air Force have already stopped these tests as well previously. Schools Several attempts have been made to make virginity testing compulsory in school applications across Indonesia. In 2010, the Regional Representative Council of Jambi made a recommendation to conduct virginity tests to students upon enrolling in junior high school and high school. The same recommendation was made by the Regional Representative Council of Jember in 2015. In 2013, the Education Department of Prabumulih, South Sumatra recommended conducting virginity tests to high school students, and even incorporating it to the regional budget plan. Sports In December 2019, a gymnast from East Java was banned from competing at the 2019 SEA Games in the Philippines due to virginity rumors. The controversy sparked protests from Indonesian activists and various women's rights organizations. Media coverage The Jakarta Post, first reported on 21 August 2013 about the planned virginity tests in Sumatra. The story was picked up by western outlets such as The Guardian and Huffington Post The plans were cancelled after protests. The Human Rights Watch first reported on November 17, 2014 about "painful and traumatic" virginity as a requirements for Indonesia's National Police and released a video of the women interviewed. This sparked an international outcry with extensive coverage. One day later Tedjo Edhy Purdijatno, (minister for politics, law, and security) confirmed that virginity test have long been mandatory for applicants of the military, namely the Indonesian National Armed Forces (TNI). Three days later, the head of the National Police, Moechgiyarto defended the test stating the women have to live up to high moral standards and would not accept candidates if they turn out to be "prostitutes". Response Doctors in Indonesia have stated that virginity testing doesn't have scientific basis. So when they are demanded to do virginity testing, they only describe the condition of the hymen. They are not legally and medically allowed to judge what has happened to the hymen. In May 2015, the European Commission declared virginity testing a "discriminatory and degrading practice," supporting Indonesian health minister Nila Moeloek, who has publicly opposed the tests, having "doubts on the necessity, accuracy and merits of such tests as a requirement to recruit young policewomen". In 2017, an Indonesian judge, Binsar Gultom, published a book that suggested virginity testing to soon-to-be-married couples, insisting that it could lower the nation's divorce rate. The book recommended "preventive and repressive measures from the government" to non-virgin fiancées. == References == The term world community is used primarily in political and humanitarian contexts to describe an international aggregate of nation states of widely varying types. In most connotations, the term is used to convey meanings attached to consensus or inclusion of all people in all lands and their governments. Politics World community often is a semi-personal rhetorical connotation that represents Humanity in a singular context as in "…for the sake of the World Community" or "…with the approval of the World Community". The term sometimes is used to reference the United Nations or its affiliated agencies as bodies of governance. Other times it is a generic term with no explicit ties to states or governments but retaining a political connotation. Humanitarianism In terms of human needs, humanitarian aid, human rights, and other discourse in the humanities, the world community is akin to the conceptual Global village aimed at the inclusion of non-aligned countries, aboriginal peoples, the Third World into the connected world via the communications infrastructure or at least representative ties to it. Economics In terms of the World economy, the world community is seen by some economists as an inter-dependent system of goods and services with semi-permeable boundaries and flexible sets of import/export rules. Proponents of Globalization may tend to establish or impose more rigidity to this framework. Controversy has arisen as to whether this paradigm will strengthen or weaken the world as a community. See World Trade Organization Ecology When considering Sustainable development and Ecology, the inter-dependence angle generally expands quickly to a Global context. In this paradigm, the planet as a whole represents a single Biome and the World's population represents the Ecological succession in a singular eco-system. This also can be recognized as the World Community. Religion Many religions have taken on the challenge of establishing a world community through the propagation of their doctrine, faith and practice. In the Baháʼí Faith, ʻAbdu'l-Bahá, successor and son of Baháʼu'lláh, produced a series of documents called the Tablets of the Divine Plan. Now in a book form, after their publication in 1917 and their 'unveiling' in New York in 1919, these tablets contained an outline and a plan for the expansion of the Baháʼí community into Asia, Asia minor, Europe and the Americas, indeed, throughout the planet. The formal implementation of this plan, known as 'Abdu'l-Baha's Divine Plan, began in 1937 in the first of a series of sub-plans, the Seven Year Plan of 1937 to 1944. Shoghi Effendi, the leader of the Baháʼí community until 1957 and then the Universal House of Justice from 1963, were instrumental in the organization and design of future sub-plans. This led to the creation and establishment of a world community, with members of the faith estimated to have reached 5 to 6 million in the early 21st century, while also being the second most geographically widespread religion in the world (missing only North Korea and the Vatican City State). In Buddhism "the conventional Sangha of monks has been entrusted by the Buddha with the task of leading all people in creating the ideal world community of noble disciples or truly civilized people." A Benedictine monk, Friar John Main, inspired the World Community for Christian meditation through the practice of meditation centered around the Maranatha mantra, meaning "Come Lord." The Lutheran Church in America had issued a social statement - World Community: Ethical Imperatives in an Age of Interdependence Adopted by the Fifth Biennial Convention, Minneapolis, Minnesota, June 25-July 2, 1970. Since then The Evangelical Lutheran Church in America has formed and retained the statement as a 'historical document'. World peace The term world community is often used in the context of establishing and maintaining world peace through a peace process or through a resolvable end to local-regional wars and global-world wars. Many social movements and much political theory deals with issues revolving around the institutionalization of the process of propagating the ideal of a world community. A world community is one which has a global vision, is established throughout the world, that is it has a membership that exists in most of the countries on the planet and that involves the participation of its members in a variety of ways. See also World anthem, a theoretical song that represents Earth Community Global village International community Moral syncretism References Willard, Charles Arthur. Liberalism and the Problem of Knowledge: A New Rhetoric for Modern Democracy, University of Chicago Press, 1996. Tragicomedy is a literary genre that blends aspects of both tragic and comic forms. Most often seen in dramatic literature, the term can describe either a tragic play which contains enough comic elements to lighten the overall mood or a serious play with a happy ending. Tragicomedy, as its name implies, invokes the intended response of both the tragedy and the comedy in the audience, the former being a genre based on human suffering that invokes an accompanying catharsis and the latter being a genre intended to be humorous or amusing by inducing laughter. In theatre Classical precedent There is no concise formal definition of tragicomedy from the classical age. It appears that the Greek philosopher Aristotle had something like the Renaissance meaning of the term (that is, a serious action with a happy ending) in mind when, in Poetics, he discusses tragedy with a dual ending. In this respect, a number of Greek and Roman plays, for instance Alcestis, may be called tragicomedies, though without any definite attributes outside of plot. The word itself originates with the Roman comic playwright Plautus, who coined the term (tragicomoedia in Latin) somewhat facetiously in the prologue to his play Amphitryon. The character Mercury, sensing the indecorum of the inclusion of both kings and gods alongside servants in a comedy, declares that the play had better be a "tragicomoedia":I will make it a mixture: let it be a tragicomedy. I don't think it would be appropriate to make it consistently a comedy, when there are kings and gods in it. What do you think? Since a slave also has a part in the play, I'll make it a tragicomedy...—Plautus, Amphitryon Renaissance revivals Italy Two figures helped to elevate tragicomedy to the status of a regular genre, by which is meant one with its own set of rigid rules. First was Giovanni Battista Giraldi Cinthio, a dramatist working in the mid-sixteenth century who developed a treatise on drama modeled on Roman comedies and tragedies as opposed to early Greek-based treatises that became the model for Italian dramatists at the time. He argued for a version of tragicomedy where a tragic story was told with a happy or comic ending (tragedia a lieto fine), which he thought were better suited for staged performances as opposed to tragedies with unhappy endings which he thought were better when read. Even more important was Giovanni Battista Guarini. Guarini's Il Pastor Fido, published in 1590, provoked a fierce critical debate in which Guarini's spirited defense of generic innovation eventually carried the day. Guarini's tragicomedy offered modulated action that never drifted too far either to comedy or tragedy, mannered characters, and a pastoral setting. All three became staples of continental tragicomedy for a century and more. England In England, where practice ran ahead of theory, the situation was quite different. In the sixteenth century, "tragicomedy" meant the native sort of romantic play that violated the unities of time, place, and action, that glibly mixed high- and low-born characters, and that presented fantastic actions. These were the features Philip Sidney deplored in his complaint against the "mungrell Tragy-comedie" of the 1580s, and of which Shakespeare's Polonius offers famous testimony: "The best actors in the world, either for tragedy, comedy, history, pastoral, pastoral-comical, historical-pastoral, tragical-historical, tragical-comical-historical-pastoral, scene individuable, or poem unlimited: Seneca cannot be too heavy, nor Plautus too light. For the law of writ and the liberty, these are the only men." Some aspects of this romantic impulse remain even in the work of more sophisticated playwrights: Shakespeare's last plays, which may well be called tragicomedies, have often been called romances. By the early Stuart period, some English playwrights had absorbed the lessons of the Guarini controversy. John Fletcher's The Faithful Shepherdess, an adaptation of Guarini's play, was produced in 1608. In the printed edition, Fletcher offered a definition of the term, stating that: "A tragi-comedie is not so called in respect of mirth and killing, but in respect it wants deaths, which is enough to make it no tragedy, yet brings some neere it, which is inough to make it no comedie." Fletcher's definition focuses primarily on events: a play's genre is determined by whether or not people die in it, and in a secondary way on how close the action comes to a death. But, as Eugene Waith showed, the tragicomedy Fletcher developed in the next decade also had unifying stylistic features: sudden and unexpected revelations, outré plots, distant locales, and a persistent focus on elaborate, artificial rhetoric. Some of Fletcher's contemporaries, notably Philip Massinger and James Shirley, wrote popular tragicomedies. Richard Brome also essayed the form, but with less success. And many of their contemporary writers, ranging from John Ford to Lodowick Carlell to Sir Aston Cockayne, made attempts in the genre. Tragicomedy remained fairly popular up to the closing of the theaters in 1642, and Fletcher's works were popular in the Restoration as well. The old styles were cast aside as tastes changed in the eighteenth century; the "tragedy with a happy ending" eventually developed into melodrama, in which form it still flourishes. Landgartha (1640) by Henry Burnell, the first play by an Irish playwright to be performed in an Irish theatre, was explicitly described by its author as a tragicomedy. Critical reaction to the play was universally hostile, partly it seems because the ending was neither happy nor unhappy. Burnell in his introduction to the printed edition of the play attacked his critics for their ignorance, pointing out that as they should know perfectly well, many plays are neither tragedy nor comedy, but "something between". Later developments Criticism that developed after the Renaissance stressed the thematic and formal aspects of tragicomedy, rather than plot. Gotthold Ephraim Lessing defined it as a mixture of emotions in which "seriousness stimulates laughter, and pain pleasure." Tragicomedy's affinity with satire and "dark" comedy have suggested a tragicomic impulse in modern theatre with Luigi Pirandello who influenced many playwrights including Samuel Beckett and Tom Stoppard. Also it can be seen in absurdist drama. Friedrich Dürrenmatt, the Swiss dramatist, suggested that tragicomedy was the inevitable genre for the twentieth century; he describes his play The Visit (1956) as a tragicomedy. Tragicomedy is a common genre in post-World War II British theatre, with authors as varied as Samuel Beckett, Tom Stoppard, John Arden, Alan Ayckbourn and Harold Pinter writing in this genre. Vladimir Nabokov's postmodern 1962 novel Pale Fire is a tragicomedy preoccupied with Elizabethan drama. Postmodern tragicomedy American writers of the metamodernist and postmodernist movements have made use of tragicomedy and/or gallows humor. A notable example of a metamodernist tragicomedy is David Foster Wallace's 1996 magnum opus, Infinite Jest. Wallace writes of comedic elements of living in a halfway house (i.e. "some people really do look like rodents"), a place steeped in human tragedy and suffering. Tragicomedy in media Films including Life is Beautiful, Mary and Max, Parasite, Jojo Rabbit, The Banshees of Inisherin, Beau Is Afraid, Robot Dreams, and Memoir of a Snail have been described as tragicomedies. Television series including Succession, Killing Eve, Breaking Bad, Better Call Saul, Fleabag, I May Destroy You, BoJack Horseman, South Park, Steven Universe Future, Moral Orel, Barry, Young Sheldon, Made for Love, and The White Lotus have also been described as tragicomedies. See also Comedy drama Outrapo Shakespearean problem play Theatre of the Absurd References External links Tragicomedy from Ancient Greece to Shakespeare Post-war British drama A weapon, arm, or armament is any implement or device that is used to deter, threaten, inflict physical damage, harm, or kill. Weapons are used to increase the efficacy and efficiency of activities such as hunting, crime (e.g., murder), law enforcement, self-defense, warfare, or suicide. In a broader context, weapons may be construed to include anything used to gain a tactical, strategic, material, or mental advantage over an adversary or enemy target. While ordinary objects such as rocks and bottles can be used as weapons, many objects are expressly designed for the purpose; these range from simple implements such as clubs and swords to complicated modern firearms, tanks, missiles and biological weapons. Something that has been repurposed, converted, or enhanced to become a weapon of war is termed weaponized, such as a weaponized virus or weaponized laser. History The use of weapons has been a major driver of cultural evolution and human history up to today since weapons are a type of tool that is used to dominate and subdue autonomous agents such as animals and, by doing so, allow for an expansion of the cultural niche, while simultaneously other weapon users (i.e., agents such as humans, groups, and cultures) are able to adapt to the weapons of enemies by learning, triggering a continuous process of competitive technological, skill, and cognitive improvement (arms race). Prehistoric The use of objects as weapons has been observed among chimpanzees, leading to speculation that early hominids used weapons as early as five million years ago. However, this cannot be confirmed using physical evidence because wooden clubs, spears, and unshaped stones would have left an ambiguous record. The earliest unambiguous weapons to be found are the Schöningen spears, eight wooden throwing spears dating back more than 300,000 years. At the site of Nataruk in Turkana, Kenya, numerous human skeletons dating to 10,000 years ago may present evidence of traumatic injuries to the head, neck, ribs, knees, and hands, including obsidian projectiles embedded in the bones that might have been caused by arrows and clubs during conflict between two hunter-gatherer groups. But the interpretation of warfare at Nataruk has been challenged due to conflicting evidence. Ancient history The earliest ancient weapons were evolutionary improvements of late Neolithic implements, but significant improvements in materials and crafting techniques led to a series of revolutions in military technology. The development of metal tools began with copper during the Copper Age (about 3,300 BC) and was followed by the Bronze Age, leading to the creation of the Bronze Age sword and similar weapons. During the Bronze Age, the first defensive structures and fortifications appeared as well, indicating an increased need for security. Weapons designed to breach fortifications followed soon after, such as the battering ram, which was in use by 2500 BC. The development of ironworking around 1300 BC in Greece had an important impact on the development of ancient weapons. It was not the introduction of early Iron Age swords, however, as they were not superior to their bronze predecessors, but rather the domestication of the horse and widespread use of spoked wheels by c. 2000 BC. This led to the creation of the light, horse-drawn chariot, whose improved mobility proved important during this era. Spoke-wheeled chariot usage peaked around 1300 BC and then declined, ceasing to be militarily relevant by the 4th century BC. Cavalry developed once horses were bred to support the weight of a human. The horse extended the range and increased the speed of attacks. Alexander's conquest saw the increased use of spears and shields in the Middle East and Western Asia as a result Greek culture spread which saw many Greek and other European weapons be used in these regions and as a result many of these weapons were adapted to fit their new use in war In addition to land-based weaponry, warships, such as the trireme, were in use by the 7th century BC. During the first First Punic War, the use of advanced warships contributed to a Roman victory over the Carthaginians. Post-classical history European warfare during post-classical history was dominated by elite groups of knights supported by massed infantry. They were involved in mobile combat and sieges, which involved various siege weapons and tactics. Knights on horseback developed tactics for charging with lances, providing an impact on the enemy formations, and then drawing more practical weapons (such as swords) once they entered melee. By contrast, infantry, in the age before structured formations, relied on cheap, sturdy weapons such as spears and billhooks in close combat and bows from a distance. As armies became more professional, their equipment was standardized, and infantry transitioned to pikes. Pikes are normally seven to eight feet in length and used in conjunction with smaller sidearms (short swords). In Eastern and Middle Eastern warfare, similar tactics were developed independent of European influences. The introduction of gunpowder from Asia at the end of this period revolutionized warfare. Formations of musketeers, protected by pikemen, came to dominate open battles, and the cannon replaced the trebuchet as the dominant siege weapon. The Ottoman used the cannon to destroy much of the fortifications at Constantinople which would change warfare as gunpowder became more available and technology improved Modern history Early modern The European Renaissance marked the beginning of the implementation of firearms in western warfare. Guns and rockets were introduced to the battlefield. Firearms are qualitatively different from earlier weapons because they release energy from combustible propellants, such as gunpowder, rather than from a counterweight or spring. This energy is released very rapidly and can be replicated without much effort by the user. Therefore, even early firearms such as the arquebus were much more powerful than human-powered weapons. Firearms became increasingly important and effective during the 16th–19th centuries, with progressive improvements in ignition mechanisms followed by revolutionary changes in ammunition handling and propellant. During the American Civil War, new applications of firearms, including the machine gun and ironclad warship, emerged that would still be recognizable and useful military weapons today, particularly in limited conflicts. In the 19th century, warship propulsion changed from sail power to fossil fuel-powered steam engines. Since the mid-18th century North American French-Indian war through the beginning of the 20th century, human-powered weapons were reduced from the primary weaponry of the battlefield to yielding gunpowder-based weaponry. Sometimes referred to as the "Age of Rifles", this period was characterized by the development of firearms for infantry and cannons for support, as well as the beginnings of mechanized weapons such as the machine gun. Artillery pieces such as howitzers were able to destroy masonry fortresses and other fortifications, and this single invention caused a revolution in military affairs, establishing tactics and doctrine that are still in use today. World War I An important feature of industrial age warfare was technological escalation – innovations were rapidly matched through replication or countered by another innovation. World War I marked the entry of fully industrialized warfare as well as weapons of mass destruction (e.g., chemical and biological weapons), and new weapons were developed quickly to meet wartime needs. The technological escalation during World War I was profound, including the wide introduction of aircraft into warfare and naval warfare with the introduction of aircraft carriers. Above all, it promised the military commanders independence from horses and a resurgence in maneuver warfare through the extensive use of motor vehicles. The changes that these military technologies underwent were evolutionary but defined their development for the rest of the century. Interwar This period of innovation in weapon design continued in the interwar period (between WWI and WWII) with the continuous evolution of weapon systems by all major industrial powers. The major armament firms were Schneider-Creusot (based in France), Škoda Works (Czechoslovakia), and Vickers (Great Britain). The 1920s were committed to disarmament and the outlawing of war and poison gas, but rearmament picked up rapidly in the 1930s. The munitions makers responded nimbly to the rapidly shifting strategic and economic landscape. The main purchasers of munitions from the big three companies were Romania, Yugoslavia, Greece, and Turkey – and, to a lesser extent, Poland, Finland, the Baltic States, and the Soviet Union. Criminalizing poison gas Realistic critics understood that war could not really be outlawed, but its worst excesses might be banned. Poison gas became the focus of a worldwide crusade in the 1920s. Poison gas did not win battles, and the generals did not want it. The soldiers hated it far more intensely than bullets or explosive shells. By 1918, chemical shells made up 35 percent of French ammunition supplies, 25 percent of British, and 20 percent of American stock. The "Protocol for the Prohibition of the Use in War of Asphyxiating, Poisonous, or Other Gases and of Bacteriological Methods of Warfare", also known as the Geneva Protocol, was issued in 1925 and was accepted as policy by all major countries. In 1937, poison gas was manufactured in large quantities but not used except against nations that lacked modern weapons or gas masks. World War II and postwar Many modern military weapons, particularly ground-based ones, are relatively minor improvements to weapon systems developed during World War II. World War II marked perhaps the most frantic period of weapon development in the history of humanity. Massive numbers of new designs and concepts were fielded, and all existing technologies were improved between 1939 and 1945. The most powerful weapon invented during this period was the nuclear bomb; however, many other weapons influenced the world, such as jet aircraft and radar, but were overshadowed by the visibility of nuclear weapons and long-range rockets. Nuclear weapons Since the realization of mutual assured destruction (MAD), the nuclear option of all-out war is no longer considered a survivable scenario. During the Cold War in the years following World War II, both the United States and the Soviet Union engaged in a nuclear arms race. Each country and their allies continually attempted to out-develop each other in the field of nuclear armaments. Once the joint technological capabilities reached the point of being able to ensure the destruction of the Earth by 100 fold, a new tactic had to be developed. With this realization, armaments development funding shifted back to primarily sponsoring the development of conventional arms technologies for support of limited wars rather than total war. Types By user – what person or unit uses the weapon Personal weapons (or small arms) – designed to be used by a single person. Light weapons – 'man-portable' weapons that may require a small team to operate. Heavy weapons – artillery and similar weapons larger than light weapons (see SALW). Crew served weapons – larger than personal weapons, requiring two or more people to operate correctly. Fortification weapons – mounted in a permanent installation or used primarily within a fortification. Mountain weapons – for use by mountain forces or those operating in difficult terrain. Vehicle-mounted weapons – to be mounted on any type of combat vehicle. Railway weapons – designed to be mounted on railway cars, including armored trains. Aircraft weapons – carried on and used by some type of aircraft, helicopter, or other aerial vehicle. Naval weapons – mounted on ships and submarines. Space weapons – are designed to be used in or launched from space. Autonomous weapons – are capable of accomplishing a mission with limited or no human intervention. By function – the construction of the weapon and the principle of operation Antimatter weapons (theoretical) – would combine matter and antimatter to cause a powerful explosion. Archery weapons – operate by using a tensioned string and a bent solid to launch a projectile. Artillery – firearms capable of launching heavy projectiles over long distances. Biological weapons – spread biological agents, causing disease or infection. Blunt instruments – designed to break or fracture bones, produce concussions, create organ ruptures, or crush injuries. Chemical weapons – poison people and cause reactions. Edged and bladed weapons – designed to pierce or cut through skin, muscle, or bone and cause internal or external bleeding. Energy weapons – rely on concentrating forms of energy to attack, such as lasers or sonic attacks. Explosive weapons – use a physical explosion to create a blast, concussion, or spread shrapnel. Firearms – use a chemical charge to launch projectiles. Improvised weapons – common objects reused as weapons, such as crowbars and kitchen knives. Incendiary weapons – cause damage by fire. Loitering munitions – designed to loiter over a battlefield, striking once a target is located. Magnetic weapons – use magnetic fields to propel projectiles or focus particle beams. Missiles – rockets that are guided to their target after launch. (Also a general term for projectile weapons.) Non-lethal weapons – designed to subdue without killing. Nuclear weapons – use radioactive materials to create nuclear fission or nuclear fusion detonations Rockets – self-propelled projectiles. Suicide weapons – exploit the willingness of their operators not surviving the attack. By target – the type of target the weapon is designed to attack Anti-aircraft weapons – target missiles and aerial vehicles in flight. Anti-fortification weapons – designed to target enemy installations. Anti-personnel weapons – designed to attack people, either individually or in numbers. Anti-radiation weapons – target sources of electronic radiation, particularly radar emitters. Anti-satellite weapons – target orbiting satellites. Anti-ship weapons – target ships and vessels on water. Anti-submarine weapons – target submarines and other underwater targets. Anti-tank weapons – designed to defeat armored targets. Area denial weapons – target territory, making it unsafe or unsuitable for enemy use or travel. Hunting weapons – weapons used to hunt game animals. Infantry support weapons – designed to attack various threats to infantry units. Siege engines – designed to break or circumvent heavy fortifications in siege warfare. Manufacture of weapons The arms industry is a global industry that involves the sale and manufacture of weaponry. It consists of a commercial industry involved in the research and development, engineering, production, and servicing of military material, equipment, and facilities. Many industrialized countries have a domestic arms industry to supply their own military forces, and some also have a substantial trade in weapons for use by their citizens for self-defense, hunting, or sporting purposes. Contracts to supply a given country's military are awarded by governments, making arms contracts of substantial political importance. The link between politics and the arms trade can result in the development of a "military–industrial complex", where the armed forces, commerce, and politics become closely linked. According to research institute SIPRI, the volume of international transfers of major weapons in 2010–2014 was 16 percent higher than in 2005–2009, and the arms sales of the world's 100 largest private arms-producing and military services companies totaled $420 billion in 2018. Legislation The production, possession, trade, and use of many weapons are controlled. This may be at a local or central government level or by international treaty. Examples of such controls include: Gun laws All countries have laws and policies regulating aspects such as the manufacture, sale, transfer, possession, modification, and use of small arms by civilians. Countries that regulate access to firearms will typically restrict access to certain categories of firearms and then restrict the categories of persons who may be granted a license for access to such firearms. There may be separate licenses for hunting, sport shooting (a.k.a. target shooting), self-defense, collecting, and concealed carry, with different sets of requirements, permissions, and responsibilities. Arms control laws International treaties and agreements place restrictions on the development, production, stockpiling, proliferation, and usage of weapons, from small arms and heavy weapons to weapons of mass destruction. Arms control is typically exercised through the use of diplomacy, which seeks to impose such limitations upon consenting participants, although it may also comprise efforts by a nation or group of nations to enforce limitations upon a non-consenting country. Arms trafficking laws Arms trafficking is the trafficking of contraband weapons and ammunition. What constitutes legal trade in firearms varies widely, depending on local and national laws. In 2001, the United Nations had made a protocol against the manufacturing and trafficking of illicit arms. This protocol made governments dispose illegal arms, and to licence new firearms being produced, to ensure them being legitimate. It was signed by 122 parties. Lifecycle problems There are a number of issues around the potential ongoing risks from deployed weapons, the safe storage of weapons, and their eventual disposal when they are no longer effective or safe. Ocean dumping of unused weapons such as bombs, ordnance, landmines, and chemical weapons has been common practice by many nations and has created hazards. Unexploded ordnance (UXO) are bombs, land mines, naval mines, and similar devices that did not explode when they were employed and still pose a risk for many years or decades. Demining or mine clearance from areas of past conflict is a difficult process, but every year, landmines kill 15,000 to 20,000 people and severely maim countless more. Nuclear terrorism was a serious concern after the fall of the Soviet Union, with the prospect of "loose nukes" being available. While this risk may have receded, similar situations may arise in the future. In science fiction Strange and exotic weapons are a recurring feature or theme in science fiction. In some cases, weapons first introduced in science fiction have now become a reality. Other science fiction weapons, such as force fields and stasis fields, remain purely fictional and are often beyond the realms of known physical possibility. At its most prosaic, science fiction features an endless variety of sidearms, mostly variations on real weapons such as guns and swords. Among the best-known of these are the phaser used in the Star Trek television series, films, and novels, and the lightsaber and blaster featured in the Star Wars movies, comics, novels, and TV series. In addition to adding action and entertainment value, weaponry in science fiction sometimes becomes a theme when it touches on deeper concerns, often motivated by contemporary issues. One example is science fiction that deals with weapons of mass destruction like doomsday devices. See also References Weapon Specialist – Weapon Expert. Bevic Huynh, New York, 2012.. Retrieved July 26, 2012. External links The dictionary definition of weapon at Wiktionary Quotations related to Weapon at Wikiquote Media related to Weapons at Wikimedia Commons The various ethnolinguistic groups found in the Caucasus, Central Asia, Europe, the Middle East, North Africa and/or South Asia demonstrate differing rates of particular Y-DNA haplogroups. In the table below, the first two columns identify ethnolinguistic groups. Subsequent columns represent the sample size (n) of the study or studies cited, and the percentage of each haplogroup found in that particular sample. (Data from studies conducted before 2004 may be inaccurate or a broad estimate, due to obsolete haplogroup naming systems – e.g. the former Haplogroup 2 included members of the relatively unrelated haplogroups known later as Haplogroup G and macrohaplogroup IJ [which comprises haplogroups I and J].) See also References External links World Haplogroups Maps Phylogeography of Y-Chromosome Haplogroup I Reveals Distinct Domains of Prehistoric Gene Flow in Europe Phylogeographic Analysis of Haplogroup E3b (E-M215) Y Chromosomes Reveals Multiple Migratory Events Within and Out Of Africa Y-chromosome haplogroup N dispersals from south Siberia to Europe A Synthesis of Haplogroup R2 High-Resolution Phylogenetic Analysis of Southeastern Europe (SEE) Traces Major Episodes of Paternal Gene Flow Among Slavic Populations A prehistory of Indian Y chromosomes: Evaluating demic diffusion scenarios - Appears to be Indian nationalistic declined ... Polarity and Temporality of High-Resolution Y-Chromosome Distributions in India Identify Both Indigenous and Exogenous Expansions and Reveal Minor Genetic Influence of Central Asian Pastoralists Mitochondrial DNA and Y-Chromosome Variation in the Caucasus YunusbaevBB Map and tree based upon the current YCC 2003 tree Archived 2006-02-18 at the Wayback Machine [3] Frequencies of Haplogroup I and its Subhaplogroups An only child is a person with no siblings, by birth or adoption. Overview Throughout history, only-children were relatively uncommon. From around the middle of the 20th century, birth rates and average family sizes fell sharply for a number of reasons, including perceived concerns about human overpopulation and more women having their first child later in life due to birth control and women in the workforce. The proportion of families in the United States with only-children increased during the Great Depression but fell during the Post–World War II baby boom. After the Korean War ended in 1953, the South Korean government suggested citizens each have one or two children to boost economic prosperity, which resulted in significantly reduced birth rates and a larger number of only-children in the country. From 1980 to 2015, the one-child policy in the People's Republic of China restricted most parents to having only one child, although it was subject to local relaxations and individual circumstances (for instance, when twins were conceived). Families may have an only child for a variety of reasons, including: personal preference, family planning, financial and emotional or physical health issues, desire to travel, stress in the family, educational advantages, late marriage, stability, focus, time constraints, fears over pregnancy, advanced age, illegitimate birth, infertility, divorce, and death of a sibling or parent. The premature death of one parent also contributed to a small percentage of marriages producing just one child until around the mid-20th century, not to mention the then-rare occurrence of divorce. Only-children are sometimes said to be more likely to develop precocious interests (from spending more time with adults) and to feel lonely. Sometimes they compensate for the aloneness by developing a stronger relationship with themselves or developing an active fantasy life that includes imaginary friends. Stereotypes In Western countries, only-children can be the subject of a stereotype that equates them with "spoiled brats". G. Stanley Hall was one of the first commentators to give only-children a bad reputation when he referred to their situation as "a disease in itself". Even today, only-children are commonly stereotyped as "spoiled, selfish, and bratty". While many only-children receive a lot of attention and resources for their development, it is not clear that, as a class, they are overindulged or differ significantly from children with siblings. Susan Newman, a social psychologist at Rutgers University and the author of Parenting an Only Child, says that this is a myth. "People articulate that only children are spoiled, they're aggressive, they're bossy, they're lonely, they're maladjusted", she said. "There have been hundreds and hundreds of research studies that show that only children are no different from their peers." However, differences have been found. Research involving teacher ratings of U.S. children's social and interpersonal skills has scored only-children lower in self-control and interpersonal skills. While a later study failed to find evidence that this continued through middle and high school, a further study showed that deficits persisted until at least the fifth grade. Overall, most findings do not support the negative view of only-children, though there are differences. In China, perceived behavioral problems in only-children have been called the Little Emperor Syndrome, and the lack of siblings has been blamed for a number of social ills such as materialism and crime. However, recent studies do not support these claims, and show no significant differences in personality between only-children and children in larger families. The one-child policy, which ended in 2015, was speculated to be the underlying cause of forced abortions, female infanticide, underreporting of female births, and has been suggested as a possible cause behind China's increasing number of crimes and gender imbalance. The popular media often posit that it is more difficult for only-children to cooperate in a conventional family environment, as they have no competitors for the attention of their parents and other relatives. It is suggested that confusion arises about the norms of ages and roles and that a similar effect exists in understanding during relationships with other peers and youth, all throughout life. Furthermore, it is believed that many feel that their parents place extra pressure and expectations on the only child, and that often, only-children are perfectionists. Only-children are noted to have a tendency to mature faster. Some psychologists believe in the “only child syndrome," though there is very little evidence to back it up. “Only child syndrome” is the idea that in adulthood, those who have had no siblings are more likely to have less developed social skills and antisocial tendencies that have carried on from childhood. Scientific research A 1987 quantitative review of 141 studies on 16 different personality traits failed to support the opinion, held by theorists including Alfred Adler, that only-children are more likely to be maladjusted due to pampering. The study found no evidence of any greater prevalence of maladjustment in only-children. The only statistically significant difference discovered was that only-children possessed a higher achievement motivation, which Denise Polit and Toni Falbo attributed to their greater share of parental resources, expectations, and scrutiny exposing them to a greater degree of reward, and greater likelihood of punishment for falling short. A second analysis by the authors revealed that only-children, children with only one sibling, and first-borns in general score higher on tests of verbal ability than later-borns and children with multiple siblings. A large (n=8,689) study found no evidence for the idea that only children are more narcissistic than children with siblings. Toni Falbo & Denise Polit, in their research of only children, gathered 115 studies to address information and evidence for personality, intelligence, adaptability, and relationships with peers and their parents. According to their findings, only-children surpassed all others in each category except for children who were in similar circumstances to them, such as first borns. One of their biggest findings was that the parent-child relationship was positively stronger compared to those children with siblings. Due to this relationship being significantly present in an only child's life, it correlated to developmental outcomes, showing that only-children were not at a developmental disadvantage. According to the Resource Dilution Model, parental resources (e.g. time to read to the child) are important in development. Because these resources are finite, children with many siblings are thought to receive fewer resources. However, the Confluence Model suggests there is an opposing effect from the benefits to the non-youngest children of tutoring younger siblings, though being tutored does not make up the reduced share of parental resources. This provides one explanation for the poorer performance on tests of ability of only-children compared to first-borns, commonly seen in the literature, though explanations such as the increased and earlier likelihood of experiencing parental separation or loss for last-born and only children have also been suggested, as this may be the cause of their very status. In his book Maybe One, the environmental campaigner Bill McKibben argues in favor of a voluntary one-child policy on the grounds of climate change and overpopulation. He reassures the reader with a narrative constructed from interviews with researchers and writers on only-children, combined with snippets from the research literature, that this would not be harmful to child development. He argues that most cultural stereotypes are false, that there are not many differences between only-children and other children, and where there are differences, they are favorable to the only child. Most research on only-children has been quantitative and focused on the behavior of only-children and on how others, for example teachers, assess that behavior. Bernice Sorensen, in contrast, used qualitative methods in order to elicit meaning and to discover what only-children themselves understand, feel, or sense about their lives that are lived without siblings. Her research showed that during their life span, only children often become more aware of their only-child status and are very much affected by society's stereotype of the only-child, whether or not the stereotype is true or false. She argues in her book, Only Child Experience and Adulthood, that growing up in a predominantly sibling society affects only-children and that their lack of sibling relationships can have an important effect on both the way they see themselves and others and how they interact with the world. The latest research by Cameron et al. (2011) controls for endogeneity associated with being only-children. Parents that choose to have only one child could differ systematically in their characteristics from parents who choose to have more than one child. The paper concludes that "those who grew up as only children as a consequence of the [one-child] policy [in China] are found to be less trusting, less trustworthy, less likely to take risks, and less competitive than if they had had siblings. They are also less optimistic, less conscientious, and more prone to neuroticism". Furthermore, according to Professor Cameron, it was found that "greater exposure to other children in childhood – for example, frequent interactions with cousins and/or attending childcare – was not a substitute for having siblings". In his book Born to Rebel, Frank Sulloway provides evidence that birth order influences the development of the "big five personality traits" (also known as the Five Factor Model). Sulloway suggests that firstborns and only-children are more conscientious, more socially dominant, less agreeable, and less open to new ideas compared to later-borns. However, his conclusions have been challenged by other researchers, who argue that birth order effects are weak and inconsistent. In one of the largest studies conducted on the effect of birth order on the Big Five, data from a national sample of 9,664 subjects found no association between birth order and scores on the NEO PI-R personality test. Similarly, a large study (n = 8,689) from 2020 did not find any evidence for the hypothesis that only children are more narcissistic than non-only children. See also == References == Vetting is the process of performing a background check on someone before offering them employment, conferring an award, or doing fact-checking prior to making any decision. In addition, in intelligence gathering, assets are vetted to determine their usefulness. Etymology To vet was originally a horse-racing term, referring to the requirement that a horse be checked for health and soundness by a veterinarian before being allowed to race. Thus, it has taken the general meaning "to check". It is a figurative contraction of veterinarian, which originated in the mid-17th century. The colloquial abbreviation dates to the 1860s; the verb form of the word, meaning "to treat an animal," came a few decades later—according to the Oxford English Dictionary, the earliest known usage is 1891—and was applied primarily in a horse-racing context ("He vetted the stallion before the race," "You should vet that horse before he races", etc.). By the early 1900s, vet had begun to be used as a synonym for evaluate, especially in the context of searching for flaws. Political selection Candidates for political office are often thoroughly vetted. United States Vice presidential nominees In the United States, following longstanding convention, a party's presidential nominee is expected to choose a vice presidential candidate to accompany them on their ticket. The practical reason for this is to ensure that presidential electors who are pledged to vote for a particular candidate for president can also be pledged to vote for a particular and separate candidate for vice president, thus making it highly likely that a clear majority of electors will elect political allies for president and vice president in accordance with the procedure set forth in the Twelfth Amendment. As a rule, in modern presidential elections, no person will be seriously considered for the vice presidential nomination without first undergoing a thorough evaluation by a team of advisers acting on behalf of the nominee. In later stages of the vetting process, the team will examine such items as a prospective vice presidential candidate's finances, personal conduct, and previous coverage in the media. The hurried vetting that preceded the selection by Republican nominee John McCain of his running mate Sarah Palin in 2008 was seen by many political observers as a mistake. Transitional justice Vetting is also a term used in the field of transitional justice. When countries undergo a transition process—after a period of armed conflict or authoritarian rule—they must determine what to do with public employees who perpetrated human rights abuses. They also must examine and revise the institutional structures that allowed such abuses to occur. Vetting is the set of processes for assessing the integrity of individuals (such as their adherence to relevant human rights standards) in order to determine their suitability for public employment. Countries transitioning to democracy and peace often utilize such processes to ensure that abusive or incompetent public employees are excluded from future public service. See also Online vetting – Vetting of people's online presence Security clearance – Permission to access restricted information Background check – Process of identification of a person, for security concerns Due diligence – Standard of care before entering into a contract with another party Security vetting in the United Kingdom Notes and references External links International Center for Transitional Justice (ICTJ); Pablo de Greiff and Alexander Mayer-Rieckh. (2007): "Justice as Prevention: Vetting Public Employees in Transitional Societies" Vermont Square is a neighborhood in Los Angeles, California, within the South Los Angeles region. The Vermont Square Branch library, a designated Historic–Cultural Monument, is located in the community. History The name Vermont Square appeared in newspaper ads in 1909, advertising the community as "the largest subdivision ever put on the market in Los Angeles". In the 1920s, the neighborhood was home to lower-middle-class white families. After World War II, African Americans began moving into the community. In the 1980s, Latino families began moving in. As late as 1969, the name Vermont Square was still being used by local businesses. In 1996, the community got a LANI (Los Angeles Neighborhood Initiative) grant to install trees, streetlights and bus shelters. In the 1997, in an effort to distinguish the area from South Central Los Angeles, residents of Vermont Square met with historian Gregory Fischer to discuss neighborhood signage. Fischer had helped design historic signage for the Victoria Park neighborhood. Vermont Square signage is installed on Vermont Avenue at King Boulevard. In December 2000, Vermont Square Park was refurbished at a cost of $20,000. Lights were repaired, trees were trimmed, sandboxes graded and gazebos freshly painted. On April 19, 2002, the Vermont Square Community Garden was dedicated, with Councilperson Jan Perry in attendance. Funded by an $80,000 grant from the S.Mark Taper Foundation, it was the first community garden in South Los Angeles. Geography In 1997, The Los Angeles Times defined the neighborhood as a 3-mile area, approximately bounded by Martin Luther King Jr. Boulevard on the north, Hoover Street on the east, Slauson Avenue on the south and Arlington Avenue on the west. Demographics A total of 42,284 people lived in Vermont Square's 2.54 square miles, according to the 2000 U.S. census—averaging 17,798 people per square mile, among the highest population densities in the city as a whole. Population was estimated at 47,555 in 2008. The median age was 26, considered young when compared to the city as a whole. The percentages of young residents, aged birth to 18, were among the county's highest. Within the neighborhood, Latinos made up 58.5% of the population, with black people at 39.2%, whites 1.4%, Asian 1.1%, and other 1.8%. Mexico and El Salvador were the most common places of birth for the 38.5% of the residents who were born abroad, an average percentage of foreign-born when compared with the city or county as a whole. The $29,904 median household income in 2008 dollars was considered low for the city and county. The percentage of households earning $20,000 or less was high, compared to the county at large. The average household size of 3.4 people was high for the city. Renters occupied 63.2% of the housing units, and homeowners occupied the rest. In 2000, there were 2,519 families headed by single parents, or 26.7%, a rate that was high for the county and the city. Vermont Square residents with a four-year college degree amounted to 5.3% of the population aged 25 and older in 2000, which was a low figure when compared with the city and the county at large; the percentage of those residents with less than a high school diploma was high for the county. Education LAUSD has 12 schools within Vermont Square. They are: Manual Arts Senior High School, LAUSD, 4131 South Vermont Avenue, a high school just across the street from the Los Angeles Memorial Coliseum Manual Arts Community Adult School, LAUSD, 4131 South Vermont Avenue Barack Obama Global Preparatory Charter Academy, LAUSD, 1708 West 46th Street Lou Dantzler Preparatory Charter Middle School, LAUSD, 5029 South Vermont Avenue Global Education Academy, LAUSD charter, 4141 South Figueroa Street Menlo Avenue Elementary School, LAUSD, 4156 Menlo Avenue Normandie Avenue Elementary School, LAUSD, 4506 South Raymond Avenue Dr. James Edward Jones Primary Center, LAUSD, 1017 West 47th Street Garr Academy of Math and Entrepreneurial Studies, LAUSD charter, 5101 South Western Avenue Fifty-Second Street Elementary School, LAUSD, 816 West 51st Street Western Avenue Elementary School, LAUSD, 1724 West 53rd Street Lou Dantzler Preparatory Charter Elementary School, LAUSD, 1260 West 36th Street Parks and Libraries 49th Street Park - 670 E. 49th Street. It features a children's play area and benches. Vermont Square Park - 1248 West 47th Street. It is opposite the Vermont Square branch library. It contains barbecue pits, basketball courts, a children's play area and picnic tables. The Vermont Square Branch library - 1201 W. 48th Street. The oldest branch library in the Los Angeles Public Library system, it was built in 1913 with a grant from Andrew Carnegie and is one of three surviving Carnegie libraries in Los Angeles. It is a designated a Historic–Cultural Monument and listed on the National Register of Historic Places. To direct visitors, there is city-installed signage on Vermont Avenue at 48th Street [7], on King Boulevard at Budlong Avenue, and on Normandie Avenue at 48th Street. Notable people Will H. Kindig, City Council member Maxine Waters, member of Congress Jackson Pollock, Painter Schoolboy Q, Rapper See also References External links Vermont Square crime map and statistics Animal Management in Rural and Remote Indigenous Communities (AMRRIC) is an Australian organisation that works alongside Aboriginal and Torres Strait Islander peoples to coordinate and facilitate sustainable, culturally-sensitive, professional animal health programs. AMRRIC supports desexing and dog health programs to improve the situation for dogs, their owners and communities. By improving the health and welfare of companion animals, AMRRIC contributes to improved community health. AMRRIC works with a range of stakeholders and partners. It trains locally employed animal management workers, provides education programs and supports research into disease and disease prevention. AMRRIC is a non-profit organisation based in Darwin, Northern Territory, operating nationally across Australia. AMRRIC receives funding from the Australian Federal Government's Department of Families, Housing, Community Services and Indigenous Affairs and from the Northern Territory Government, and also relies on private and philanthropic donations. Activities AMRRIC is an independent organisation working in all areas of animal management in remote communities, including dog health and welfare, policy, research, education and capacity building. It runs programs which address animal management in a way which is sustainable, culturally sensitive and agreed upon by all parties. AMRRIC has a range of programs and projects, including: Dog health programs which are veterinarian-led desexing programs, including anti-parasite treatment. The program reduces the problems associated with large unmanaged populations of dogs, this includes zoonotic diseases and roaming packs of dogs which show threatening behaviour towards or attacks on humans. Animal Management Worker program which provides training, resources and employment to local Indigenous people that enables them to take responsibility for their animals’ health and welfare. Animal Management Workers assist in the running of dog health programs. Education programs designed for Indigenous school students, community members, environmental health practitioners, animal management workers and government and non-government organisations about all aspects of animal health and welfare in remote Indigenous communities. Including guidebooks for veterinarians conducting programs on Indigenous communities Developing animal health and welfare policy with government partners which is relevant to remote Indigenous communities Research programs including the Cancer Genome Project in Cambridge, UK, and its work on Canine Transmissible Venereal Tumour, a common disease in dogs in remote Australian communities. Understanding the issues Living conditions in remote Indigenous communities across Australia impact on animal and human health. There are many challenges when managing large dog populations within poorly resourced Indigenous communities. The impact of dog health on the human community is evidenced in the zoonotic diseases passed from animals to humans. In some Indigenous communities in Western Australia gastrointestinal diseases are a major problem in young children. In Western Australia, hospitalisation for gastroenteritis was 7 times higher in Aboriginal children than Non-Aboriginal children. Dogs have always been part of Indigenous communities in Australia and have many roles including, hunters, companions and guard dogs. Although beliefs about the spiritual significance of dogs vary in from community to community, dogs occupy an important place in culture and the community. It is essential to understand the strong cultural tradition of living with companion animals when developing and delivering animal health and management programs. The problem of unmanaged dogs When there are no veterinary services available to a remote community, large unmanaged populations of dogs flourish. These are some of the problem associated with unmanaged dog populations: Overpopulation - uncontrolled breeding, unwanted dogs and large numbers of dogs often roaming in packs Zoonoses - the transference of disease from animals to humans, these include scabies, giardia, hookworm Noise - associated with barking, fighting or mating Litter - mess from scavenging such as overturned bins, scraps, faeces, etc. Danger and threatening behaviour - "cheeky dogs" biting or attacking children, dogs chasing vehicles Loss - dogs stealing food from storage or young children; dogs attacking livestock See also Animal welfare and rights in Australia == References == A list of torture methods and devices includes: Psychological torture methods Blackmail Chinese water torture Humiliation Subjection to periods of interrogation Music torture Mock execution Forced nudity Seclusion Pharmacological torture Exploitation of phobias; e.g., mock execution, leaving arachnophobes in a room full of spiders Sensory deprivation Sensory overload Sleep deprivation Solitary confinement / isolation Threat of severe disfigurement Tickle torture Waterboarding White room torture Physical torture methods Instruments of torture Note that the line between "torture method" and "torture device" is often blurred, particularly when a specifically named implement is but one component of a method. Also, many devices that can be used for torture have mainstream uses, completely unrelated to torture. Medieval and early modern instruments of torture Chair of torture Appearance There are many variants of the chair, though they all have one thing in common: spikes cover the back, arm-rests, seat, leg-rests, and foot-rests. The number of spikes in one of these chairs ranges from 500 to 1,500. Use To avoid movement, the victim's wrists were tied to the chair or, in one version, two bars pushed the arms against arm-rests for the spikes to penetrate the flesh even further. In some versions, there were holes under the chair's bottom where the torturer placed coal to cause severe burns while the victim still remained conscious. In other versions, there were weights that would be placed on the victim's thighs or feet. In a special version, there were spikes on the headrest, and the executioner pushed the victim's head against it. This instrument's strength lies primarily in the psychological fear caused to the victims. They would often use the victim's fear to coerce them into confessing by forcing the victim to watch someone else be tortured with this instrument. The time of death greatly varied ranging from a few hours to a day or more. No spikes penetrated any vital organ, and the wounds were closed by the spikes themselves which delayed blood loss greatly. The rack Origins The rack was first used in antiquity and it is unclear exactly from which civilization it originated, though some of the earliest examples are from Greece. Arrian's Anabasis of Alexander states that Alexander the Great had the pages who conspired in his assassination, and their mentor, his court historian Callisthenes, tortured on the rack in 328 BC. Appearance The rack is a torture device that consists of an oblong, rectangular, usually wooden frame, slightly raised from the ground, with a roller at one, or both, ends, having at one end a fixed bar to which the legs were fastened, and at the other a movable bar to which the hands were tied. The victim's feet are fastened to one roller, and the wrists are chained to the other. Use The torturer turned the handle, causing the ropes to pull the victim's arms. Eventually, the victim's bones were dislocated with a loud crack, caused by snapping cartilage, ligaments or bones. If the torturer kept turning the handles, the limbs would eventually be torn off. This method was mostly used to extract confessions. Not confessing meant that the torturer could stretch more. Sometimes, torturers forced their victim to watch other people be tortured with this device to implant psychological fear. Many knights from the Knights Templar were tortured with the rack. The limbs collected from this and other punishments of the time were "emptied by the hundreds". Sometimes, this method was limited to dislocating a few bones, but the torturer often went too far and rendered the legs or arms (sometimes both) useless. In the late Middle Ages, some new variants of this instrument appeared. They often had spikes that penetrated the victim's back - as the limbs were pulled apart, so was his or her spinal cord increasing not only in physical pain but the psychological one of being handicapped at best, too. Brazen bull Origins The Brazen Bull was invented in Ancient Greece, by Perillos of Athens as a hollow bull-shaped statue in which victims were roasted alive over a fire. Perillos proposed his idea of a more painful means of execution to Phalaris, the tyrant of Akraga. Phalaris liked the idea of the Brazen Bull, and so it was made. Once finished, Phalaris ordered it to be tested on Perillos himself. Perillos was removed from the Bull before he died, but was later killed by Phalaris when he threw Perillos off a hill. The Greeks had specially engineered tubes to make the screams of the victims sound like the noise of a bull. Appearance The Bull was made wholly of brass, and was hollow with a door in the side. Use When a victim was placed inside the brazen bull, they were slowly burned to death. The device gradually became more sophisticated, until the Greeks invented a complex system of tubes in order to make the victim's screams sound more like an infuriated bull, and also made it so the smoke from it rose in clouds of incense. This torture is similar to being boiled alive. Even though this torture was not used during the Middle Ages as it was used earlier by the Greek and Romans, a simple form of boiling was still used in Central Europe, without the use of the bull. Pear of anguish Appearance A pear shaped instrument, consisting of four leaves that slowly separated from each other as the torturer turned the screw at the top. Use There is no contemporary first-hand account of those devices or their use. An early mention of a spring-loaded gagging device is in F. de Calvi's L'Inventaire général de l'histoire des larrons ("General inventory of the history of thieves"), written in 1639, which attributes the invention to a robber named Capitaine Gaucherou de Palioly in the days of Henry of Navarre. Further mentions of the device appear in the 19th century. They are also mentioned in Grose's Dictionary of the Vulgar Tongue (1811) as "Choak Pears," and described as being "formerly used in Holland." They were also discussed in a book by Eldridge and Watts, superintendent of police and chief inspector of the detective bureau in Boston, Massachusetts (1897). While accepting that ordinary pear-shaped gags exist, they observed that contemporary robbers used no such device as Palioly's Pear and cast doubt upon its very existence in the first place, saying that "fortunately for us this 'diabolical invention' appears to be one of the lost arts, if, indeed, it ever existed outside of de Calvi's head. There is no doubt, however, of the fashioning of a pear-shaped gag which has been largely used in former days by robbers in Europe, and may still be employed to some extent. This is also known as the 'choke-pear', though it is far less marvellous and dangerous than the pear of Palioly." Though there is little or no evidence of its being used by bandits, there are a number of examples of ornate and elaborate, pear-shaped devices with three or four leaves or lobes, driven by turning a key that rotates the central screw thread, which spreads the leaves. These are generally held in museums devoted to the subject of torture, and are described as instruments of torture by distension or evisceration. Some, but not all, have small spikes of uncertain purpose at the bottom of each leaf. However, these devices do not seem to match the descriptions given by Calvi or the 19th century sources. Dunking Use This was a form of punishment that was mainly reserved for supposed witches. The victim was tied to a chair which was elevated by ropes above a pond or barrel/vat of water. The victim was then lowered into the water until completely underwater/submerged. The chair was raised if the victim was about to pass out or to give the victim a chance to confess. Often, some form of a plug, or more simply, a piece of fruit, was placed in the victim's mouth and nose beforehand, so they couldn't get a good breath before being dunked. If the victim confessed they would most likely be killed. This method was widely used during the Spanish Inquisition and in England and France. The victim was usually intermittently submerged for many hours until he or she revealed information or death had occurred. Ordeal by water began with the witch-hunts of the 16th and 17th centuries. King James VI of Scotland (later also James I of England) claimed in his Daemonologie that water was so pure an element that it repelled the guilty. While supposed witches were commonly tortured using this method, thieves and murderers could be subjected to it in order to extract a confession. This was more common when other more sophisticated torture devices were not present. Dunking was also used as punishment for common scolds. Boiling Use In England, statute 22 passed in 1532 by Henry VIII, made boiling a legal form of capital punishment. It began to be used for murderers who used poisons after the Bishop of Rochester's cook, Richard Rice, gave a number of people poisoned porridge, resulting in two deaths in February 1532. Boiling to death was employed again in 1542 for a woman who also used poison. It was also used for counterfeiters, swindlers and coin forgers during the Middle Ages. A large cauldron was filled with water, oil, tar, tallow or molten lead. The liquid was then boiled. Sometimes the victim would be placed in the cauldron before it was boiled so as to be cooked slowly. Or they would be placed, usually head first, into the already boiling liquid. This was more frequently a way to execute a prisoner rather than to extract a confession. Exposure Types Freezing to death In the winter, the naked victim was forced to stand outside in full view of everyone. Slowly, the torturer poured water on the victim's head which eventually became frozen, making them die slowly and painfully. Sometimes the body was left for the whole winter to terrify the population and dissuade any further crimes. Live burial As its name implies, this method consists of exposing a victim to the elements. The victim could be buried up to their neck letting any animals, insects or other people kill them slowly. Restraint The gibbet, a large iron basket with holes large enough for arms and legs, would be hung from a pole with a person inside it. During hot days, the metal would heat, causing pain. During cold days and nights, the chill, as well as lack of protection from the wind, could easily sap a victim's body heat. The holes in the grating were also big enough to allow carrion birds, and the occasional rat, to enter and pluck at a victim's skin and eyes. Rat torture A cheap and effective way to torture someone was with the use of rats. One of the first documented utilizations of the method was by Diederik Sonoy. There were many variants, but the most common was to force a rat to gnaw through a victim's body (usually the intestines) in order to escape, as follows: The victim would be completely restrained and tied to the ground or a bed, and the torturer would cut slits into the victim's stomach. The torturer would then use a bowl to trap rats on the victim's stomach, then place hot coal on top of the bowl; the rats would then get hot, and after a few seconds would enter the victim's stomach. Gnawing the intestines usually resulted in a few hours of pain for the victim. This almost always resulted in death. See also List of methods of capital punishment (torture devices) Peine forte et dure Torture chamber Torture Museums Rüdesheim am Rhein § Museums (Mediaeval Torture Museum) Turcas == References == Jealousy in religion examines how the scriptures and teachings of various religions deal with the topic of jealousy. Religions may be compared and contrasted on how they deal with two issues: concepts of divine jealousy, and rules about the provocation and expression of human jealousy. Divine jealousy Greek mythology The gods and goddesses of ancient Greek mythology were no strangers to romantic jealousy. No god or goddess illustrates this better than Hera. Hera was the wife of Zeus. Zeus, the leader of the gods on Mt. Olympus, frequently took lovers in addition to Hera. Hera in turn exacted jealous revenge against her romantic rivals. The examples below come from the Wikipedia article on Hera: Leto – When Hera discovered that Leto was pregnant and that Hera's husband, Zeus, was the father, she banned Leto from giving birth on "terra-firma", or the mainland, or any island at sea. Alternatively, Hera kidnapped Ilithyia, the goddess of childbirth, to prevent Leto from going into labor. The other gods forced Hera to let her go. Callisto/Arcas – A follower of Artemis, Callisto took a vow to remain a virgin. But Zeus fell in love with her and disguised himself as Artemis in order to lure her into his embrace. Hera then turned Callisto into a bear out of revenge. Semele/Dionysus – In one of various birth myths of him, Dionysus was a son of Zeus by a mortal woman. A jealous Hera again attempted to kill the child, this time by sending Titans to rip Dionysus to pieces after luring the baby with toys. Though Zeus drove the Titans away with his thunderbolts but only after the Titans ate everything but the heart, which was saved, variously, by Athena, Rhea, or Demeter. Io – Hera almost caught Zeus with a mistress named Io, a fate avoided by Zeus turning Io into a beautiful white heifer. However, Hera was not completely fooled and demanded Zeus give her the heifer as a present. Once Io was given to Hera, she placed her in the charge of Argus Panoptes to keep her separated from Zeus. Hermes freed her on Zeus' orders. Lamia – Lamia was a queen of Libya, whom Zeus loved. Hera turned her into a monster and murdered their children. Or, alternately, she killed Lamia's children and the grief turned her into a monster. Lamia was cursed with the inability to close her eyes so that she would always obsess over the image of her dead children. Zeus, or the other gods, would frequently intervene to undo some of the damage caused by Hera's vengeance. However, the message in these stories seems clear—provoking divine jealousy can result in terrible suffering. Judaism The concept of divine jealousy in Judaism stems from the concept of monotheism. One of the most well known assertions of monotheism in Judaism is the Shema. The Shema proclaims: "Hear, O Israel: The Lord is our God; The Lord is one." (Deuteronomy 6:4, World English Bible) Reciting the Shema affirms an individual's faith in one God. Since there is only one God, worship of multiple gods wrongly gives to false gods what belongs to the one true God. Worship of multiple gods constitutes a form of spiritual infidelity against the one God. The one God responds to this infidelity with jealousy. For example, the second of the Ten Commandments states: "You shall not make for yourselves an idol, nor any image of anything that is in the heavens above, or that is in the earth beneath, or that is in the water under the earth: you shall not bow yourself down to them, nor serve them, for I, Yahweh your God, am a jealous God, visiting the iniquity of the fathers on the children, on the third and on the fourth generation of those who hate me, and showing loving kindness to thousands of those who love me and keep my commandments." (Exodus 20:4–6, World English Bible) This prohibition is later repeated in the verse: "...for you shall worship no other god: for the Lord, whose name is Jealous (Kanna), is a jealous God (El Kanna)." (Exodus 34:14, World English Bible) Divine jealousy in Judaism thus refers to how the one God responds to humans worshipping multiple gods. Humans are prohibited from worshipping multiple gods and provoking the jealousy of the one true God. Christianity Christianity has adopted the concept of divine jealousy from Judaism. There is only one true God, who becomes jealous when people worship other gods. The prohibition against worshipping other gods in the Ten Commandments is widely accepted in Christianity. However, the Christian concept of divine jealousy is not identical to the Judaic concept of divine jealousy. Paul the Apostle has extended the concept of divine jealousy to include accepting false doctrines. Paul writes: "For I am jealous over you with a godly jealousy. For I married you to one husband, that I might present you as a pure virgin to Christ. But I am afraid that somehow, as the serpent deceived Eve in his craftiness, so your minds might be corrupted from the simplicity that is in Christ. For if he who comes preaches another Jesus, whom we did not preach, or if you receive a different spirit, which you did not receive, or a different 'good news', which you did not accept, you put up with that well enough." (2 Corinthians 11:2–4, World English Bible) Just as the Ten Commandments asserts that God is jealous when His people worship other gods, Paul claims to be jealous when the churches he founded turn away from the doctrines he taught about Christ. This is just an analogy, however. Paul does not claim be the equal of God. He instead suggests it is Christ (the groom) who has reason to be jealous when his bride (the church) turns to false doctrines about Him. Paul makes the same argument with respect to doctrinal interpretation of the sacrament of communion: "You can’t both drink the cup of the Lord and the cup of demons. You can’t both partake of the table of the Lord, and of the table of demons. Or do we provoke the Lord to jealousy?" (10:21–22, World English Bible) Islam A hadith attributed to Abu Hurairah reports: Sunni muslims tend to view this as Sahih. Human jealousy Judaism The Judaic scriptures warn people not to provoke jealousy by committing adultery. The jealous spouse may exact revenge. "He who commits adultery with a woman is void of understanding. He who does it destroys his own soul. He will get wounds and dishonor. His reproach will not be wiped away. For jealousy arouses the fury of the husband. He won’t spare in the day of vengeance. He won’t regard any ransom, neither will he rest content, though you give many gifts." (Book of Proverbs 6:32–35, World English Bible) The destructive potential of romantic jealousy may underlie the strong prohibitions against actions that can provoke it. Two of the Ten Commandments prohibit feelings and actions that could potentially provoke romantic jealousy. The tenth commandment says "You shall not covet your neighbor's wife," and the seventh commandment says "You shall not commit adultery." (Exodus 20: 14–17, World English Bible). The punishment for committing adultery was death, both for the adulteress and the adulterer. The destructive potential of male romantic jealousy may also underlie a ritual in the Mosaic laws that test the sexual fidelity of a wife. (Numbers 5:11–30, World English Bible) The ritual is triggered when a husband becomes jealous over a real or suspected sexual affair, because his wife has repeatedly been in seclusion with another man, and he has warned her not to continue to see this man. The husband takes the wife, called a sotah, to the temple priests. The temple priests mix a drink composed of holy water, dust from the temple floor, part of a meal offering, and a parchment with God's Name on it. The wife drinks the mixture. If the wife has been sexually unfaithful, the drink will cause her to die: the flesh will fall off her thighs and her belly will bloat. The male adulterer dies, as well. If the wife has been sexually faithful, no harm will come to her and she will bear a beautiful child in the near future. The outcomes of this ritual are designed to appease the husband's jealousy and prove the wife's innocence, since there are no witnesses. If the wife does not become ill, the husband can take satisfaction in the wife's fidelity and look forward to a new child. This is a ritual of ancient Judaism. Because of the destruction of the Temple, Modern Jewish people do not practice all the rituals of ancient Judaism, such as the one just described. Christianity Many Christian writings do not clearly distinguish jealousy and envy. Only a few verses in the New Testament mention jealousy, and many of these verses appear to refer to envy rather than romantic rivalry: "You have heard that it was said, You shall not commit adultery. But I say to you, anyone who looks on a woman to lust after her has committed adultery with her already in his heart." (Matthew 5:27–28) "But if you have bitter jealousy and selfish ambition in your heart, don’t boast and don’t lie against the truth. This wisdom is not that which comes down from above, but is earthly, sensual, and demonic. For where jealousy and selfish ambition are, there is confusion and every evil deed. (Epistle of James 3:14–16, World English Bible) "Now the works of the flesh are obvious, which are: adultery, sexual immorality, uncleanness, lustfulness, idolatry, sorcery, hatred, strife, jealousies, outbursts of anger, rivalries, divisions, heresies, envyings, murders, drunkenness, orgies, and things like these; of which I forewarn you, even as I also forewarned you, that those who practice such things will not inherit the Kingdom of God." (Galatians 5:19–21, World English Bible) "For insofar as there is jealousy, strife, and factions among you, aren’t you fleshly, and don’t you walk in the ways of men?" (1 Corinthians 3:3, World English Bible) "Let us walk properly, as in the day; not in reveling and drunkenness, not in sexual promiscuity and lustful acts, and not in strife and jealousy." (Epistle to the Romans 13:13, World English Bible) These verses indicate early Christians viewed envy as inconsistent with their faith. The New Advent Catholic encyclopedia equates jealousy with envy. It describes envy as contrary to the Golden Rule taught by Jesus and contrary to the spirit of solidarity that should permeate all humanity—especially the Christian community. Jealousy, at least in the form of envy, is incompatible with the principles of Christian faith. Islam According to some Muslim scholars being jealous is akin to being displeased with what God has given and not given to certain people. Thus, these scholars advise dealing with jealousy by being grateful (shukr) for what one has, and being patient (sabr) while waiting for what one desires. Buddhism In Buddhism, the term irshya is commonly translated as either envy or jealousy. Irshya is defined as a state of mind in which one is highly agitated to obtain wealth and honor for oneself, but unable to bear the excellence of others. See also Attachment in adults Emotion Jealousy Jealousy sociology Jealousy in art Monogamy Open marriage == References == Redemptive violence is defined as a belief that "violence is a useful mechanism for control and order", or, alternately, a belief in "using violence to rid and save the world from evil". The French Revolution involved violence that was depicted as redemptive by revolutionaries, and decolonization theorist Frantz Fanon was an advocate of redemptive violence. Pacifism rejects the idea that violence can be redemptive. Myth The myth of redemptive violence is the story of the victory of order over chaos by means of violence. It is the ideology of conquest, the gods favour those who conquer. Today’s common understanding of the Myth of Redemptive Violence was put forward by American scholar and theologian Walter Wink in his book, The Powers that Be: Theology for a New Millenium, wherein he defines the Myth of Redemptive Violence as “the belief that violence saves, that war brings peace, that might makes right.” Domination system Redemptive violence is the means by which the powers that be support the Domination System; another term coined by Walter Wink. The domination system is described as a network of oppressive relations such as classism, racism, and sexism and the role that violence plays in preserving them. “It is characterized by unjust economic relations, oppressive political relations, biased race relations, patriarchal gender relations, hierarchical power relations, and the use of violence to maintain them all.” In early history The myth of redemptive violence can be traced all the way back into biblical times. For instance, the Babylonian creation story from 1250 BCE follows the same blueprint as virtually every story of redemptive violence put forward today. In this story, known as the Enuma Elish, the god Marduk, defeats the god Tiamat in a fierce battle, and then creates the world using her body. He then uses the blood of another slain god, Qingu to create humans. Depictions of redemptive violence can also be seen in various art forms throughout early human history. In modern culture In describing the Myth of Redemptive Violence, Walter Wink points to the popular 1950s cartoon, Popeye and Bluto, describing the basic plot that is repeated in nearly every episode. “In a typical segment, Bluto abducts a screaming and kicking Olive Oyl, Popeye’s girlfriend. When Popeye attempts to rescue her, the massive Bluto beats his diminutive opponent to a pulp, while Olive Oyl helplessly wrings her hands. At the last moment, as our hero oozes to the floor, and Bluto is trying, in effect, to rape Olive Oyl, a can of spinach pops from Popeye’s pocket and spills into his mouth. Transformed by this gracious infusion of power, he easily demolishes the villain and rescues his beloved. The format never varies. Neither party ever gains any insight or learns from these encounters. They never sit down and discuss their differences. Repeated defeats do not teach Bluto to honour Olive Oyl’s humanity, and repeated pummellings do not teach Popeye to swallow his spinach before the fight.” Wink points out that no matter what happens, Popeye continues to use violence as the only means of solving problems because he never learns that there is another option. He sees violence as a necessity; as the only possible way to solve a problem, and never learns that any other method would solve the problem. See also References Further reading Beaumont, Thomas E. (2020). "The Phenomenology of Redemptive Violence". Alternatives: Global, Local, Political. 45 (4): 184–199. doi:10.1177/0304375421999175. S2CID 232233904. Bielefeld, Shelley (2021). "Cashless welfare transfers and Australia's First Nations: redemptive or repressive violence?". Griffith Law Review. 30 (4): 597–620. doi:10.1080/10383441.2021.1996891. S2CID 242072134. Cothran, Boyd (2014). Remembering the Modoc War: Redemptive Violence and the Making of American Innocence. UNC Press Books. ISBN 978-1-4696-1861-6. Dalton, Jacob P. (2012). "Sometimes Love Don't Feel Like It Should: Redemptive Violence in Tantric Buddhism". Sins and Sinners: Perspectives from Asian Religions. Brill. ISBN 978-90-04-23200-6. Dalton, Russell W. (2015). "(Un)Making Violence Through Media Literacy and Theological Reflection: Manichaeism, Redemptive Violence, and Hollywood Films". Religious Education. 110 (4): 395–408. doi:10.1080/00344087.2015.1063963. S2CID 141818053. Duong, Kevin (2020). The Virtues of Violence: Democracy Against Disintegration in Modern France. Oxford University Press. ISBN 978-0-19-005841-8. Forbes, Ella (1998). ""By My Own Right Arm": Redemptive Violence and the 1851 Christiana, Pennsylvania Resistance". The Journal of Negro History. 83 (3): 159–167. doi:10.2307/2649012. JSTOR 2649012. S2CID 141075568. Harison, Casey (2011). "Redemptive violence and stuttering across the Atlantic: The Who's "My Generation" and Herman Melville's Billy Budd in historical perspective". Atlantic Studies. 8 (1): 49–68. doi:10.1080/14788810.2011.539787. S2CID 161895257. Lavender, Wayne (2015). The Worldview of Redemptive Violence in the US. Springer. ISBN 978-1-137-47911-2. McDowell, John C. (2010). ""Wars Not Make One Great": Redeeming the Star Wars Mythos from Redemptive Violence Without Amusing Ourselves to Death". The Journal of Religion and Popular Culture. 22 (1): 4. doi:10.3138/jrpc.22.1.004. Morris, Barry (1992). "Frontier colonialism as a culture of terror". Journal of Australian Studies. 16 (35): 72–87. doi:10.1080/14443059209387119. Stevens, Ellie Emslie (2017). "Eschatological Inversions in Isaiah and Dante: From Malicious to Redemptive Violence". Annali d'Italianistica. 35: 51–70. ISSN 0741-7527. JSTOR 26570505. Ingleby, Johnathan. “Confronting the Domination System - JSTOR.” JSTOR, Sage Publications Ltd., www.jstor.org/stable/43052707. Accessed 29 Oct. 2023. Integrated National Security Enforcement Teams (INSET; French: Équipes intégrées de la sécurité nationale, EISN) are Canadian counterterrorist, counter-foreign interference, and counter-espionage units operating under the auspices of Public Safety Canada. These federal investigative teams were formed in 2002 in response to the September 11 attacks. Canadian provinces will have National Security Enforcement Sections (NSES) instead of an INSET in places that do not have a large city. Organization INSET units are made up of personnel from the Royal Canadian Mounted Police (RCMP), Canada Border Services Agency (CBSA), Canadian Security Intelligence Service (CSIS), and police forces at the municipal and provincial levels. These units are tasked with investigating criminal national security matters domestically and internationally. INSET units are known to operate in and around Toronto, Vancouver, Ottawa, Montreal, and Edmonton. Mandate The mandate of INSET units is as follows: Increase the capacity to collect, share and analyze intelligence among partners, with respect to targets (individuals) that are a threat to national security. To create an enhanced enforcement capacity to bring such targets to justice. Enhance partner agencies' collective ability to combat national security threats and meet specific mandate responsibilities. History INSET units were established in 2002 after the September 11 attacks. Toronto, Montreal, Ottawa, and Vancouver NSES units were changed to be INSETs in response to the attacks. An INSET unit operating in Toronto played a major role in the capture of 17 terror suspects on June 2, 2006. In 2012, INSETs were tasked to secure Albertan energy infrastructure from all attacks. On May 6, 2022, the RCMP announced that an INSET unit arrested an unnamed individual with officers from the Ontario Provincial Police Provincial Anti-Terrorism Section (OPP PATS) and the Windsor Police Service for contributing to terrorist activity. In April 2022, several unnamed RCMP officers resigned from an INSET unit after they were ordered to arrest a suspect by the Canadian Security Intelligence Service without being told why. On January 24, 2024, the Edmonton Police Service (EPS) is reportedly investigating a shooting incident at Edmonton City Hall concerning a Canadian Corps of Commissionaires (CCC) employee named Bezhani Sarvar alongside an INSET unit. See also Konrad Shourie == References == Beatniks were members of a social movement in the mid-20th century, who subscribed to an anti-materialistic lifestyle. They rejected the conformity and consumerism of mainstream American culture and expressed themselves through various forms of art, such as literature, poetry, music, and painting. They also experimented with spirituality, drugs, sexuality, and travel. The term "beatnik" was coined by San Francisco Chronicle columnist Herb Caen in 1958, as a derogatory label for the followers of the Beat Generation, a group of influential writers and artists who emerged during the era of the Silent Generation's maturing, from as early as 1946, to as late as 1963, but the subculture was at its most prevalent in the 1950s. This lifestyle of anti-consumerism may have been influenced by their generation living in extreme poverty in the Great Depression during their formative years, seeing slightly older people serve in WWII and being influenced by the rise of left-wing politics and the spread of Communism. The name was inspired by the Russian suffix "-nik", which was used to denote members of various political or social groups. The term "beat" originally was used by Jack Kerouac in 1948 to describe his social circle of friends and fellow writers, such as Allen Ginsberg, William S. Burroughs, and Neal Cassady. Kerouac said that "beat" had multiple meanings, such as "beaten down", "beatific", "beat up", and "beat out". He also associated it with the musical term "beat", which referred to the rhythmic patterns of jazz, a genre that influenced many beatniks. Beatniks often were stereotyped as wearing black clothing, berets, sunglasses, and goatees, and speaking in hip slang that incorporated words like "cool", "dig", "groovy", and "square". They frequented coffeehouses, bookstores, bars, and clubs, where they listened to jazz, read poetry, discussed philosophy, and engaged in political activism. Some of the most famous beatnik venues were the Six Gallery in San Francisco, where Ginsberg first read his poem "Howl" in 1955; the Gaslight Cafe in New York City, where many poets performed; and the City Lights Bookstore, also in San Francisco, where Kerouac's novel On the Road was published in 1957. Beatniks also traveled across the country and abroad, seeking new experiences and inspiration. Some of their destinations included Mexico, Morocco, India, Japan, and France. Beatniks had a significant impact on American culture and society as they challenged the norms and values of their time. They influenced many aspects of art, literature, music, film, fashion, and language. They also inspired many social movements and subcultures that followed them, such as the hippies, the counterculture, the New Left, the environmental movement, and the LGBT movement. Some of the more notable figures who were influenced by or associated with beatniks include Bob Dylan, The Beatles, Andy Warhol, Ken Kesey, and Timothy Leary. Beatniks have been portrayed or parodied in many works of fiction, such as The Many Loves of Dobie Gillis, A Charlie Brown Christmas, The Munsters, The Flintstones, The Simpsons, and SpongeBob SquarePants. History In 1948, Jack Kerouac introduced the phrase "Beat Generation", generalizing from his social circle to characterize the underground, anti-conformist youth gathering in New York City at that time. The name came up in conversation with John Clellon Holmes, who published an early Beat Generation novel titled Go (1952), along with the manifesto This Is the Beat Generation in The New York Times Magazine. In 1954, Nolan Miller published his third novel Why I Am So Beat (Putnam), detailing the weekend parties of four students. "Beat" came from underworld slang—the world of hustlers, drug addicts, and petty thieves—from which Allen Ginsberg and Kerouac sought inspiration. "Beat" was slang for "beaten down" or "downtrodden". However, to Kerouac and Ginsberg, it also had a spiritual connotation, as in "beatitude". Other adjectives discussed by Holmes and Kerouac were "found" and "furtive". Kerouac felt he had identified (and was the embodiment of) a new trend analogous to the influential Lost Generation. In "Aftermath: The Philosophy of the Beat Generation", Kerouac criticized what he saw as a distortion of his visionary, spiritual ideas: The Beat Generation, that was a vision that we had, John Clellon Holmes and I, and Allen Ginsberg in an even wilder way, in the late Forties, of a generation of crazy, illuminated hipsters suddenly rising and roaming America, serious, bumming and hitchhiking everywhere, ragged, beatific, beautiful in an ugly graceful new way—a vision gleaned from the way we had heard the word "beat" spoken on street corners on Times Square and in the Village, in other cities in the downtown city night of postwar America—beat, meaning down and out but full of intense conviction. We'd even heard old 1910 Daddy Hipsters of the streets speak the word that way, with a melancholy sneer. It never meant juvenile delinquents, it meant characters of a special spirituality who didn't gang up but were solitary Bartlebies staring out the dead wall window of our civilization ... Kerouac explained what he meant by "beat" at a Brandeis Forum, "Is There A Beat Generation?", on November 8, 1958, at New York's Hunter College Playhouse. The seminar's panelists were Kerouac, James A. Wechsler, Princeton anthropologist Ashley Montagu and author Kingsley Amis. Wechsler, Montagu, and Amis wore suits, while Kerouac was clad in black jeans, ankle boots and a checkered shirt. Reading from a prepared text, Kerouac reflected on his beat beginnings: It is because I am Beat, that is, I believe in beatitude and that God so loved the world that He gave His only begotten son to it ... Who knows, but that the universe is not one vast sea of compassion actually, the veritable holy honey, beneath all this show of personality and cruelty? Kerouac's statement was later published as "The Origins of the Beat Generation" (Playboy, June 1959). In that article, Kerouac noted how his original beatific philosophy had been ignored amid maneuvers by several pundits, among them San Francisco newspaper columnist Herb Caen, to alter Kerouac's concept with jokes and jargon: I went one afternoon to the church of my childhood and had a vision of what I must have really meant with "Beat"...the vision of the word Beat as being to mean beatific...People began to call themselves beatniks, beats, jazzniks, bopniks, buggies, and finally, I was called the "avatar" of all this. In light of what he considered beat to mean and what beatnik had come to mean, Kerouac said to a reporter "I'm not a beatnik. I'm a Catholic", showing the reporter a painting of Pope Paul VI and saying "You know who painted that? Me." Stereotype In her memoir Minor Characters, Joyce Johnson described how the stereotype was absorbed into American culture: "Beat Generation" sold books, sold black turtleneck sweaters and bongos, berets and dark glasses, sold a way of life that seemed like dangerous fun—thus to be either condemned or imitated. Suburban couples could have beatnik parties on Saturday nights and drink too much and fondle each other's wives. Kerouac biographer Ann Charters noted that the term "Beat" was appropriated to become a Madison Avenue marketing tool: The term caught on because it could mean anything. It could even be exploited in the affluent wake of the decade's extraordinary technological inventions. Almost immediately, for example, advertisements by "hip" record companies in New York used the idea of the Beat Generation to sell their new long-playing vinyl records. Lee Streiff, an acquaintance of many members of the movement who went on to become one of its chroniclers, believed that the news media saddled the movement for the long term with a set of false images: Reporters are not generally well-versed in artistic movements, or the history of literature or art. And most are certain that their readers, or viewers, are of limited intellectual ability and must have things explained simply, in any case. Thus, the reporters in the media tried to relate something that was new to already preexisting frameworks and images that were only vaguely appropriate in their efforts to explain and simplify. With a variety of oversimplified and conventional formulas at their disposal, they fell back on the nearest stereotypical approximation of what the phenomenon resembled, as they saw it. And even worse, they did not see it clearly and completely at that. They got a quotation here and a photograph there—and it was their job to wrap it up in a comprehensible package—and if it seemed to violate the prevailing mandatory conformist doctrine, they would also be obliged to give it a negative spin as well. And in this, they were aided and abetted by the Poetic Establishment of the day. Thus, what came out in the media: from newspapers, magazines, TV, and the movies, was a product of the stereotypes of the 30s and 40s—though garbled—of a cross between a 1920s Greenwich Village bohemian artist and a Bop musician, whose visual image was completed by mixing in Daliesque paintings, a beret, a Vandyck beard, a turtleneck sweater, a pair of sandals, and set of bongo drums. A few authentic elements were added to the collective image: poets reading their poems, for example, but even this was made unintelligible by making all of the poets speak in some kind of phony Bop idiom. The consequence is, that even though we may know now that these images do not accurately reflect the reality of the Beat movement, we still subconsciously look for them when we look back to the 50s. We have not even yet completely escaped the visual imagery that has been so insistently forced upon us. Etymology The origin of the word "beatnik" is traditionally ascribed to Herb Caen from his column in the San Francisco Chronicle on April 2, 1958, where he wrote "Look magazine, preparing a picture spread on S.F.'s Beat Generation (oh, no, not AGAIN!), hosted a party in a No. Beach house for 50 Beatniks, and by the time word got around the sour grapevine, over 250 bearded cats and kits were on hand, slopping up Mike Cowles' free booze. They're only Beat, y'know, when it comes to work ..." It is claimed that Caen coined the term by adding the Yiddish suffix -nik (ultimately borrowed into Yiddish from Slavic languages) to Beat as in the Beat Generation. Nik, as a suffix was also in vogue due to the Sputnik, the first satellite to orbit the planet, in 1957. The suffix came to be used in colloquial synthetics such as Nogoodnik, etc. An earlier source from 1954, or possibly 1957 after the launch of Sputnik, is ascribed to Ethel (Etya) Gechtoff, the well-known owner of a San Francisco Art Gallery. Objecting to the term, Allen Ginsberg wrote to The New York Times to deplore "the foul word beatnik", commenting, "If beatniks and not illuminated Beat poets overrun this country, they will have been created not by Kerouac but by industries of mass communication which continue to brainwash man." Beat culture In the vernacular of the period, "Beat" referred to Beat culture, attitude and literature; while "beatnik" referred to a stereotype found in cartoon drawings and (in some cases at worst) twisted, sometimes violent media characters. In 1995, film scholar Ray Carney wrote about the authentic beat attitude as differentiated from stereotypical media portrayals of the beatnik: Much of Beat culture represented a negative stance rather than a positive one. It was animated more by a vague feeling of cultural and emotional displacement, dissatisfaction, and yearning, than by a specific purpose or program ... It was many different, conflicting, shifting states of mind. Since 1958, the terms Beat Generation and Beat have been used to describe the antimaterialistic literary movement that began with Kerouac in the 1940s and continued into the 1960s. The Beat philosophy of antimaterialism and soul searching influenced 1960s musicians such as Bob Dylan, the early Pink Floyd and The Beatles. Music and fashion However, the soundtrack of the beat movement was the modern jazz pioneered by saxophonist Charlie Parker and trumpeter Dizzy Gillespie, which the media dubbed bebop. Jack Kerouac and Allen Ginsberg spent much of their time in New York jazz clubs such as the Royal Roost, Minton's Playhouse, Birdland and the Open Door, "shooting the breeze" and "digging the music". Charlie Parker, Dizzy Gillespie and Miles Davis rapidly became what Ginsberg dubbed "secret heroes" to this group of aesthetes. The Beat authors borrowed much from the jazz/hipster slang of the 1940s, peppering their works with words such as "square", "cats", "cool" and "dig". At the time the term "beatnik" was coined, a trend existed among young college students to adopt the stereotype. Men emulated the trademark look of bebop trumpeter Dizzy Gillespie by wearing goatees, horn-rimmed glasses and berets, rolling their own cigarettes, and playing bongos. Fashions for women included black leotards and long, straight, unadorned hair, in a rebellion against the middle-class culture of beauty salons. Marijuana use was associated with the subculture, and during the 1950s, Aldous Huxley's The Doors of Perception further influenced views on drugs. By 1960, a small "beatnik" group in Newquay, Cornwall, England (including a young Wizz Jones) had attracted the attention and abhorrence of their neighbours for growing their hair beyond shoulder length, resulting in a television interview with Alan Whicker on BBC television's Tonight series. Philosophy and religion The Beat philosophy was generally countercultural and antimaterialistic, and stressed the importance of bettering one's inner self over material possessions. Some Beat writers, such as Gary Snyder, began to delve into Eastern religions such as Buddhism and Taoism. Politics tended to be liberal, left-wing and anti-war, with support for causes such as desegregation (although many of the figures associated with the original Beat movement, particularly Kerouac, embraced libertarian and conservative ideas). An openness to African American culture and arts was apparent in literature and music, notably jazz. While Caen and other writers implied a connection with communism, no obvious or direct connection occurred between Beat philosophy, as expressed by the literary movement's leading authors, and that of the communist movement, other than the antipathy both philosophies shared towards capitalism. Those with only a superficial familiarity with the Beat movement often saw this similarity and assumed the two movements had more in common. The Beat movement introduced Asian religions to Western society. These religions provided the Beat generation with new views of the world and corresponded with its desire to rebel against conservative middle-class values of the 1950s, old post-1930s radicalism, mainstream culture, and institutional religions in America. By 1958, many Beat writers published writings on Buddhism. This was the year Jack Kerouac published his novel The Dharma Bums, whose central character (whom Kerouac based on himself) sought Buddhist contexts for events in his life. Allen Ginsberg's spiritual journey to India in 1963 also influenced the Beat movement. After studying religious texts alongside monks, Ginsberg deduced that what linked the function of poetry to Asian religions was their mutual goal of achieving ultimate truth. His discovery of Hindu mantra chants, a form of oral delivery, subsequently influenced Beat poetry. Beat pioneers who followed a Buddhism-influenced spiritual path felt that Asian religions offered a profound understanding of human nature and insights into the being, existence and reality of mankind. Many of the Beat advocates believed that the core concepts of Asian religious philosophies had the means of elevating American society's consciousness, and these concepts informed their main ideologies. Notable Beat writers such as Kerouac, Ginsberg, and Gary Snyder were drawn to Buddhism to the extent that they each, at different periods in their lives, followed a spiritual path in their quests to provide answers to universal questions and concepts. As a result, the Beat philosophy stressed the bettering of the inner self and the rejection of materialism, and postulated that East Asian religions could fill a religious and spiritual void in the lives of many Americans. Many scholars speculate that Beat writers wrote about Eastern religions to encourage young people to practice spiritual and sociopolitical action. Progressive concepts from these religions, particularly those regarding personal freedom, influenced youth culture to challenge capitalist domination, break their generation's dogmas, and reject traditional gender and racial rules. Art "Beatnik art" is the direction of contemporary art that originated in the United States as part of the beat movement in the 1960s. The movement itself, unlike the so-called "Lost Generation" did not set itself the task of changing society, but tried to distance itself from it, while at the same time trying to create its own counter-culture. The art created by artists was influenced by jazz, drugs, occultism, and other attributes of beat movement. The scope of the activity was concentrated in the cultural circles of New York City, Los Angeles, San Francisco and North Carolina. Prominent representatives of the trend were artists Wallace Berman, Jay DeFeo, Jess Collins, Robert Frank, Claes Oldenburg and Larry Rivers. The culture of the beat generation has become a kind of intersection for representatives of the creative intellect of the United States associated with visual and performing art, which are usually attributed to other areas and trends of artistic expression, such as assemblage, happening, funk art and Neo-Dadaism. They made efforts to destroy the wall between art and real life, so that art would become a living experience in cafes or jazz clubs, and not remain the prerogative of galleries and museums. Many works of artists of the movement were created on the verge of various types of art. Artists wrote poetry and poets painted, something like this can describe the processes taking place within the framework of the movement. Performances were a key element in the art of beats, whether it was the Theatrical Event of 1952 at Black Mountain College or Jack Kerouac typing in 1951 the novel On the Road on a typewriter in a single session on a single roll of 31-meter long paper. Representatives of the movement were united by hostility to traditional culture with its conformism and brightly degenerate commercial component. They also did not like the approach of traditional culture to hushing up the dark side of American life – violence, corruption, social inequality, racism. They tried through art to create a new way of life based on the ideals of rebellion and freedom. Critics highlight the artist Wallace Berman as the main representative of the movement. In his work concentrated many of the characteristic features of hipsters, especially in his collages made on photocopied photographs, which are a mixture of elements of pop art and mysticism. Among other artists and works, one can single out the work The Rose by the artist Jay DeFeo, the work on which was carried out for seven years, a huge painting-assembly weighing about a ton with a width of up to 20 centimeters. Beatniks in media Possibly the first film portrayal of the Beat society was in the 1950 noir film D.O.A, directed by Rudolph Maté. In the film the main character goes to a loud San Francisco bar, where one woman shouts to the musicians: "Cool! Cool! Really cool!" One of the characters says, "Man, am I really hip", and another replies, "You're from nowhere, nowhere!" Lone dancers are seen moving to the beat. Some are dressed with accessories and have hairstyles that one would expect to see in much later films. Typical 1940s attire is mixed with beatnik clothing styles, particularly in one male who has a beatnik hat, long hair, and a mustache and goatee, but is still wearing a dress suit. The bartender refers to a patron as "Jive Crazy" and talks of the music driving its followers crazy. He then tells one man to "Calm down, Jack!" and the man replies, "Oh don't bother me, man. I'm being enlightened!". The scene also demonstrates the connection to and influence of 1940s genres of African American music such as bebop on the emergence of Beat culture. The featured band "Jive" is all-black, while the customers who express their appreciation for the music in a jargon that would come to characterize the stereotype of Beat culture are young white hipsters. The 1953 Dalton Trumbo film Roman Holiday starring Audrey Hepburn and Gregory Peck features a supporting character played by Eddie Albert who is a stereotypical beatnik, appearing five years before the term was coined. He has an Eastern European surname, Radovich, and is a promiscuous photographer who wears baggy clothes, a striped T-shirt and a beard, which is mentioned four times in the screenplay. The character Maynard G. Krebs, played on TV by Bob Denver in The Many Loves of Dobie Gillis (1959–63), solidified the stereotype of the indolent non-conformist beatnik, which contrasted with the aggressively rebellious Beat-related images presented by popular film actors of the early and mid-1950s, notably Marlon Brando and James Dean. The Beat Generation (1959) associated the movement with crime and violence, as did The Bloody Brood (1959) and The Beatniks (1960). An episode of The Addams Family titled "The Addams Family Meets a Beatnik," broadcast January 1, 1965, features a young biker/beatnik who injures himself in an accident, and ends up staying with the Addams family. Harry Connick Jr. portrays Dean, a beatnik, in Brad Bird's film The Iron Giant (1999). Beatnik books Alan Bisbort's survey Beatniks: A Guide to an American Subculture was published by Greenwood Press in 2009 as part of the series Greenwood Press Guides to Subcultures and Countercultures. The book includes a timeline, a glossary and biographical sketches. Others in the Greenwood series: Punks, Hippies, Goths and Flappers. Tales of Beatnik Glory: Volumes I and II by Ed Sanders is, as its name suggests, a collection of short stories, and a definitive introduction to the beatnik scene as lived by its participants. The author, who went on to found The Fugs, lived in the beatnik epicenter of Greenwich Village and the Lower East Side in the late 1950s and early 1960s. Among the humor books, Beat, Beat, Beat was a 1959 Signet paperback of cartoons by Phi Beta Kappa Princeton graduate William F. Brown, who looked down on the movement from his position in the TV department of the Batten, Barton, Durstine & Osborn advertising agency. Suzuki Beane (1961), by Sandra Scoppettone with Louise Fitzhugh illustrations, was a Bleecker Street beatnik spoof of Kay Thompson's Eloise series (1956–1959). In the 1960s comic book, the Justice League of America's sidekick Snapper Carr was portrayed as a stereotypical beatnik, down to his lingo and clothes. The DC Comics character Jonny Double is portrayed as a beatnik. Museums In San Francisco, Jerry and Estelle Cimino operate their Beat Museum, which began in 2003 in Monterey, California and moved to San Francisco in 2006. Ed "Big Daddy" Roth used fiberglass to build his Beatnik Bandit in 1960. Today, this car is in the National Automobile Museum in Reno, Nevada. See also Beat Generation Beatitude Cool Generation Gap Hippie Moody Street Irregulars Silent Generation Subcultures of the 1950s Yves Saint Laurent (designer) References Sources Charters, Ann (ed.). The Portable Beat Reader. Penguin Books. New York. 1992. ISBN 0-670-83885-3 (hc); ISBN 0-14-015102-8 (pbk) Nash, Catherine. "The Beat Generation and American Culture." (PDF file) Phillips, Lisa (ed). Beat Culture and the New America: 1950–1965. New York: Whitney Museum of Art and Paris: Flammarion, 1995. External links The Beat Museum Kerouac.net John Sinclair, "The Last Beatnik Warrior Poet" Philament: "Beat Etymologies" This is the Beat Generation Beatniks on film and TV Beats In Kansas – the Beat Generation in the Heartland San Francisco beatniks and hipsters, 1950s, links to Denver and Neal Cassady sites Unrest, also called disaffection, is a sociological phenomenon, including: Civil disorder Domestic terrorism Industrial unrest Labor unrest Rebellion Riot Strike action State of emergency Notable historical instances of unrest 19th century Luddites 1978–79 Winter of Discontent (UK) 1989 Purple Rain Revolt (South Africa) 2003 Maldives civil unrest 2004 Unrest in Kosovo 2005 Belize unrest May 2005 unrest in Uzbekistan Arab Spring Post-coup unrest in Egypt (2013–14) 2014 pro-Russian conflict in Ukraine 2022 Kazakh unrest Film and television Unrest (2006 film) Unrest (2017 film) See also All pages with titles containing Unrest Political cognition refers to the study of how individuals come to understand the political world, and how this understanding leads to political behavior. Some of the processes studied under the umbrella of political cognition include attention, interpretation, judgment, and memory. Most of the advancements in the area have been made by scholars in the fields of social psychology, political science, and communication studies. History In the early 20th century, the psychological study of cognition encountered significant push back from behaviorism. According to behaviorists, if social psychology was to be considered a serious science, it should study observable and measurable phenomena. Since the processes of the mind are not observable and thus are hard to measure, behaviorist believed that these were not worth studying. However, as Gestalt psychology was introduced to the US by European immigrants, the dominance of the behaviorist approach began to declined. Questions related to perception, judgment, impression formation, and attitude change began to attract more researchers. In the 1950s, the development of new methodological tools ignited the Cognitive Revolution. In 1984, Susan Fiske and Shelley E. Taylor published the first social cognition book, Social Cognition. Early theories of social cognition Naïve scientist First proposed by Fritz Heider in 1958, the Naïve scientist model of cognition conceptualizes individuals as actors with limited information that want to derive an accurate understanding of the world. Much of the work done within this model focused on examining how people perceive and explain why others behave the way they do. This work served as the basis for the development of modern theories of attribution, advanced independently by Harold Kelley and Bernard Weiner. Kelley's attribution theory included the interaction between three variables: consistency, consensus, and distinctiveness. This interaction was summarized in Kelley's Covariation model, also known as Kelley's cube. Consistency refers to whether the person exhibit the behavior across time. The more the person exhibits the behavior across time, the more representative this behavior is of the person. Consensus refers to whether other individuals exhibits the same behavior when presented in the same situation. If many individuals exhibit the same behavior, then the behavior is less informativeness of the person. Distinctiveness refers to whether the person exhibits similar behavior in other situations. The more the person exhibits the behavior in other situation, the less the behavior is representative of the individual. Cognitive miser The cognitive miser model argues that, when individuals are attempting to understand the world, they tend to prefer methods that allow them to reduce the amount of cognitive work required to process information. This preference for efficiency leads to the development of biases and heuristics. In the past, political psychologists have identified a wide range of biases and heuristics—such as partisan heuristics and the Black utility heuristic—that people use to make political decisions. Motivated reasoning Motivated reasoning is a cognitive phenomena that occurs when an individual changes a peripheral attitude that is inconsistent with a more central element of the self. The purpose of these cognitive biases is to maintain a positive sense of self-esteem. In the past, they have been referred to as cognitive adaptions and positive illusions. Motivated reasoning has been extensively studied in political psychology. One of the most significant contributions of this area of research is the identification of cases in which voters adopt their preferred candidates' or party' policy positions. Study of public opinion In political science, the study of political cognition was facilitated by the emergence of survey research and a growing interest in understanding how individual make voting decisions. In the 1930s, however, the explosion of commercial polling agencies facilitated the collection of data at the individual level. The availability of this new type of data increased an interest in understanding what individuals know about politics, what attitudes individual have towards political objects, and how individuals make political decisions. In 1940, Paul Lazarsfeld, Bernard Berelson, and Hazel Gaudet carried out one of the earliest studies examining how individual-level factors influence political decisions. The study took place in Erie County, Ohio. Lazarsfeld, Berelson, and Gaudet were interested in identifying what sources of information influence an individual's political attitudes during an electoral campaign. They found that, among those who were less interested in politics, had not decided who to support, or change their voting intentions during the campaign, personal influences—such as the opinion of a friend or a family member—played a more significant role than the media. In 1948, by Bernard Berelson, Paul F. Lazarsfeld, and William N. McPhee carried out a similar study in Elmira, New York. Both the Erie County and the Elmira studies had been significantly influential to the study of American public opinion. Their findings have been replicated repeatedly and still explain the ways people develop political attitudes today. The biggest influence of these studies, however, was their methodological approach. These two studies were the first studies that followed an individual's political attitudes and voting intentions throughout a campaign with an interview-reinterview approach. In 1944, the National Opinion Research Center (NOR) at the University of Chicago was the first organization ever to collect panel data at a national level in the United States. In the elections of 1944 and 1948, the Survey Research Center of the University of Michigan performed similar panel-data studies at the national level. These studies were characterized by unstructured questions that allowed participants to express what they knew about politics, as well as what attitudes they had towards political actors and policies. Major theories of voting behavior (1950s–1980s) The rational voter In his 1957 book, An Economic Theory of Democracy, Anthony Downs argues that individuals are rational voters—i.e., they decide who to support by calculating which candidate will maximize the benefits they receive from the government, while minimizing the costs. This rational calculation is performed by taking into consideration the individual's interest, what the party in office has done in the past, and what the party in office and the party out of office could do in the future.: 138 Downs defines political parties as coalitions of political elites, whose primary goal is to be elected to office. Because they know voters behave rationally, parties adopt the policy positions of most voters to maximize their chances of being elected into office. The interaction between the rational behavior of voters and the rational behavior of the political elite facilitates the development of a two-party system when voters are normally distributed along the liberal-conservative spectrum. The reason for this is that each party will try to maximize the number of voters it can appeal to while still maintaining significant distinctions from the other party. This results in a party heuristic: voters start to consistently support the party that is closest to their beliefs along this liberal-conservative lines. Since its publication, the theory of rational voter has encountered numerous empirical challenges, as research suggests that the average voter is not equipped with the necessary information to make rational decisions as defined by Downs. Specifically, most American voters are unable to think in ideological terms—i.e., to articulate their political positions using coherent belief systems. Drawing from social cognition theories, some scholars have argued voters might be still able to make rational decisions even if they are incapable of putting their perceptions, beliefs, and rationales into the formal language of political elites. Specifically, these critics believe that, instead of expecting the average voter to present high levels of political sophistication, political scientists should take into account individual-level variations of information acquisition and processing. They propose that partisan biases motivate individuals to seek out and reject particular sets of information that then lead to candidate evaluations, and then voting. Thus, these critics advance a theory of rational voter that incorporates both cognitive processes and economic utility calculations. Party attachment In 1960, Angus Campbell, Philip E. Converse, Warren E. Miller, and Donald E. Stokes published The American Voter. Unlike most prior work, The American Voter was the first book to systematically analyze quantitative data at the national level from three presidential elections (Truman-Dewey in 1948, Eisenhower-Stevenson in 1952, and Eisenhower-Stevenson in 1956). This data was collected by the Survey Research Center of the University of Michigan. The theoretical framework derived from these studies is thus known as Michigan model. The American Voter is also one of the first works to ever look for observable implications of the rational choice theory of voting behavior—a body of work that claims voters are aware of political events, have well-developed political attitudes, and thus are able to aligned their votes with the candidate that is closest to their political dispositions. Lastly, this book was also one of the first works to incorporate a social psychological perspective to the study of politics. Taken as a system, these [attitudinal] variables were seen to constitute a field of forces operating on the individual as he deliberates over his vote decision.: 16 As described by the quote above, the authors of this seminal work that a Lewinian view of the political world. They conceptualize attitudes towards political objects as field forces that led an individual to decide who to support in an election. According to Campbell and colleagues, the most significant of these forces is partisan identification, which the authors defined as a psychological attachment to a party. These psychological attachments are developed early in life and remain stable throughout adulthood. Today, partisan identification is still the strongest and most reliable predictor of vote choice. According to Campbell and colleagues, these partisan attachments function as lenses that paint the way people perceive political information about issues and political actors. Specifically, voters accept and endorse information that is consistent with their partisan beliefs, and reject information that is inconsistent with their partisan views. Additionally, since most voters do not have the time to acquire and process all political information available, they use these partisan attachments as heuristics, or shortcuts, when deciding who to support. Political information and voter sophistication Ideology In his widely known book chapter, "The Nature of Belief Systems", Philip E. Converse examines the nature of abstract political thought among American voters. Converse defined a belief system as a set of idea-elements that were interconnected by logical, psychological, or social constraints. These belief systems could vary in terms of how central certain idea-elements are to the belief system relative to other idea-elements. The centrality of each idea-element influences whether an individual changes her belief based on external changes in the political world. Idea-elements that are peripheral to an individual's belief system are more likely to change than idea-elements that are central to an individual's belief system. To perform this study, Converse relied on the analysis of open-ended questions. He specifically examines two things. First, he examines what types of political information is associated with what type of political attitudes. Second, he examines whether voters are able to provide an abstract reason to explain this association by referring to the liberal-conservative spectrum. He found that most voters do not think in coherent ideological terms. He divided voters into five types based on their ideological sophistication: Ideologues, Near Ideologues, Group Interest, Nature of the Times, and No issue content. About less than 10% of the voters are considered ideologues or near ideologues. Converse's findings provided further evidence against the theory of rational voting, as voters seem not to be aware of political events, seem to lack well-developed political attitudes, and thus might be unequipped to make rational and informed decisions. Political attitudes In 1992, political scientist John Zaller published his book, The Nature and Origins of Mass Opinion. In this work, the author examines the processes by which people develop and report their political opinions. According to Zaller, the study of political opinion must be understood through the lens of political awareness and political values, which is summarized in his Receive-Accept-Sample (RAS) model. This model contends that individuals receive political information, decide what to accept and store in memory based on their political values, and when asked to express their opinions about a topic they use a sample of whatever relevant information is accessible in memory to construct their opinions on the spot. Since most people do not have a direct experience with the political world, they often rely on the political elite—which includes both politicians and the media—to acquire political information. Zaller argues that voters vary greatly in terms of political awareness either because of a lack of interest in politics or because of a lack of time to pay attention. Consequently, the average voter tends to score low on measures of political knowledge. Zaller observes that this lack of political information is associated with the high level of attitude instability that is exhibited among voters. According to Zaller, this instability is a sign of voters constructing their opinion statements on the spot based on relevant information that happens to be available in memory, rather than the complete in-existence of an attitude (as suggested by Converse) or measurement error. When voters receive information from the political elite, they almost never receive a complete and neutral account of facts. They receive an oversimplified version of the relevant information that often comes with a political frame, which interacts with the voter's predispositions. If the information is consistent with the voter's prior beliefs, then the information is accepted and stored in memory. If the information is inconsistent with the voter's prior beliefs, then the information is not accepted. In political science, Zaller's work has been instrumental in the examination of two major types of evaluations: 1) on-line evaluations; and 2) memory-based evaluations. On-line evaluation model asserts that individuals update their evaluations of political objects every time they acquire new information. The memory-based evaluation model asserts that individuals construct their evaluations on the spot based on information available in the working memory. Because most voters fail to recall the content of information they are exposed to during a campaign, many political scientists believed that voters rely on memory-based processes to make political judgments. On the contrary, other scholars believed that voters do update their evaluations of political objects as they acquire new information, but these updates take the form of affective evaluations. Specifically, when voters receive political information—either from a political campaign or from the media—the voter processes that information and turns it into an affective evaluation that is then stored in memory. These stored affective evaluations are then used to make political decisions along with memory-based information. This process is known as the dual processing model. Priming and agenda setting Priming is a cognitive process that occurs when a stimulus changes the attitudinal or behavioral response of an individual. This process is facilitated by the activation of information related to the stimulus in the working memory with or even without the individual's awareness. Psychologists have tested this by priming different cognitive styles in the lab and then having participants read articles about political issues. The thought styles can shift people's support for or away from particular policies. In the study of politics, priming effects have also been studied with relation to the media and political campaigns. In 1987, Shanto Iyengar and Donald Kinder published News That Matters: Televised and American Opinion. This work reported the results of a series experiments designed to assess the role of the media on political attitudes. They found that the primarily role of the media is to set the agenda for political evaluations. According to the authors, the media is able to achieve this by priming—or in this case, by making more salient—certain political issues. These salient issues are then used to make political evaluations. Additional work has illustrated that priming only occurs among topics in which the voter has already well-established predispositions. Persuasion Most research about political persuasion has taken place within the context of campaign effect. Early work finds that campaigns that use various forms of personal contact (e.g., canvassing and telephone calls) to deliver information are more effective than campaigns that use non-personal contact (e.g., mailing information) at mobilizing voters. Contemporary research suggests that whenever persuasion—defined as an attitude change—is achieved, its effects are relatively small and fade away rapidly. Additionally, well-developed political predisposition are not easily persuadable, while less developed attitudes move around fairly easily. Social identities and inter-group relations Linked fate: black utility heuristic In his 1994 book, Behind the Mule: Race and Class in African-American Politics, Michael Dawson argues African American voters use evaluations of their group-level interests as short-cuts to determine the policy positions, vote choice, and political engagement that would safeguard their individual-level interest. According to Dawson, this political heuristic was developed as a consequence of the historical oppression of African Americans, which facilitated the development of the belief that individual interest was linked to the racial group interest among African Americans. Consequently, this black utility heuristic is known as linked fate. Dawson argues that racial issues override class-based differences which results in the political homogeneity of African Americans. Additional work suggests that other groups—including Whites, Asian Americans, Latinxs, and women—also exhibit linked fate. Other researchers have advocated for the revision of the current linked fate measure, as it seems to be inconsistently associated with group identification and with political engagement. See also Neuropolitics – branch of science that investigates the relationship of neuroscience in the political realmPages displaying wikidata descriptions as a fallback Political psychology – Interdisciplinary study of the relationship between political and psychological processes Voting behavior – How voters decide how to vote Public opinion – Aggregate of individual attitudes or beliefs held by the adult population Attitude (psychology) – Concept in psychology and communication studies Social cognition – Study of cognitive processes involved in social interactions References External links "election studies". American National Election Study. "Survey Research Center". University of Michigan. Discrimination against people from rural areas, also called rural discrimination, represents a confrontation between rural and urban populations, manifesting in various dimensions of daily life, including social, cultural, labor, and economic aspects. These circumstances arise within a framework of behaviors characterized by contempt, stigmatization, rejection, mockery and ridicule, among other adverse and negative attitudes directed toward individuals who were either born or raised in a rural setting, such as a farm or a small village. These discriminatory behaviors can appear against an individual or a group of individuals just because of their origin, as well as because of their manners, habits, traditions or idiosyncrasies that reveal a difference with urban people or an urban group, can be classified as a type of cultural shock. Rural people in an urban environment As part of rural-to-urban migration, inhabitants of rural origin face challenging and disadvantageous conditions when arriving in a large urban agglomeration. The knowledge they have about agriculture or livestock farming becomes obsolete in these spaces of large cities, where the labor field is largely destined for services. Discrimination based on whether individuals are perceived as coming from urban or rural areas can vary significantly depending on the context. For example, in a metropolitan area with a population of 10 million, a city with 1 million residents may be viewed as a "town," with all the negative connotations that this would bring from the perspective of the discriminator. Similarly, in a city of 1 million inhabitants, a city with a population of 300,000 may be regarded as lesser in status, and this pattern continues across different population sizes. Given this heterogeneity, researchers generally use case study methodology to conduct analyses on this topic. Additionally, perceptions of rural dwellers —both positive and negative— vary significantly across different countries. This encompasses a range of perspectives, from negative stereotypes to idealized representations of rural life and communities. The term "urban narcissism" or also called "geographic narcissism" refers to the tendency of individuals residing in large cities to perceive themselves as more advanced, sophisticated, or superior compared to those from smaller towns or rural areas, solely based on their urban background. Intersectionality Rural discrimination doesn’t affect everyone in the countryside the same way. Intersectionality helps to understand how people in rural areas can face multiple, overlapping forms of exclusion based on more than just where they live. There are various factors that directly influence discrimination, such as social class, ethnic group, gender, sexual orientation, and other situations, such as age and disabilities. See also Rural diversity Rural women Rural LGBTQ people == References == The term "person of color" (pl.: people of color or persons of color; abbreviated POC) is used to describe any person who is not considered "white". In its current meaning, the term originated in, and is associated with, the United States. From the 2010s, however, it has been adopted elsewhere in the Anglosphere (often as person of colour), including relatively limited usage in the United Kingdom, Canada, Australia, Ireland, and South Africa. In the United States, the term is involved in the various definitions of non-whiteness, including African Americans, Asian Americans, Native Americans, Pacific Islander Americans, multiracial Americans, and some Latino Americans, though members of these communities may prefer to view themselves through their cultural identities rather than color-related terminology. The term, as used in the United States, emphasizes common experiences of systemic racism, which some communities have faced. The term may also be used with other collective categories of people such as "communities of color", "men of color" (MOC), "women of color" (WOC), or "librarians of color". The acronym "BIPOC" refers to "black, indigenous, and other people of color" and aims to emphasize the historic oppression of black and indigenous people. The term "colored" was originally equivalent in use to the term "person of color" in American English, but usage of the appellation "colored" in the Southern United States gradually came to be restricted to "Negroes", and is now considered a racial pejorative. Elsewhere in the world, and in other dialects of English, the term may have entirely different connotations, however; for example, in South Africa, "Coloureds" refers to multiple multiracial ethnic groups and is sometimes applied to other groups in Southern Africa, such as the Basters of Namibia. History The American Heritage Guide to Contemporary Usage and Style cites usage of "people of colour" as far back as 1796. It was initially used to refer to light-skinned people of mixed African and European heritage. French colonists used the term gens de couleur ("people of color") to refer to people of mixed African and European ancestry in the Americas who were not enslaved. In South Carolina and other parts of the Deep South, this term was used to distinguish between slaves who were mostly "black" or "Negro" and free people who were primarily "mulatto" or "mixed race". After the American Civil War, "colored" was used as a label almost exclusively for black Americans, but the term eventually fell out of favor by the mid-20th century. Although American activist Martin Luther King Jr. used the term "citizens of color" in 1963, the phrase in its current meaning did not catch on until the late 1970s. In the late 20th century, the term "person of color" was introduced in the United States in order to counter the condescension implied by the terms "non-white" and "minority", and racial-justice activists in the U.S., influenced by radical theorists such as Frantz Fanon, popularized it at this time. By the late 1980s and early 1990s, it was in wide circulation. Both anti-racist activists and academics sought to move the understanding of race beyond the black–white dichotomy then prevalent. The phrase "women of color" was developed and introduced for wide use by a group of black women activists at the National Women's Conference in 1977. The phrase was used as a method of communicating solidarity between non-white women that was, according to Loretta Ross, not based on "biological destiny" but instead a political act of naming themselves. In the twenty-first century, use of the term and the categorization continued to proliferate: for example, the Joint Council of Librarians of Color (JCLC), a recurring conference of the American Library Association, which uses the "of color" designation for its five ethnic affiliate associations. They include: the Black Caucus of the American Library Association, the American Indian Library Association, the Asian Pacific American Librarians Association, the Chinese American Librarians Association, and REFORMA: The National Association to Promote Library & Information Services to Latinos and the Spanish Speaking. Political significance According to Stephen Satris of Clemson University, there are two main racial divides in the United States. The first is the "black–white" delineation; the second racial delineation is the one "between whites and everyone else", with whites being "narrowly construed" and everyone else being called "people of color". Because the term "people of color" includes vastly different people with only the common distinction of not being white, it draws attention to the perceived fundamental role of racialization in the United States. Joseph Tuman of San Francisco State University argues that the term "people of color" is attractive because it unites disparate racial and ethnic groups into a larger collective in solidarity with one another. Use of the term "person of color", especially in the United States, is often associated with the social-justice movement. Style guides from the American Heritage Guide to Contemporary Usage and Style, the Stanford Graduate School of Business, and Mount Holyoke College all recommend the term "person of color" over other alternatives. Unlike "colored", which historically referred primarily to black people and is often considered offensive, "person of color" and its variants refer inclusively to all non-European peoples—often with the notion that there is political solidarity among them—and, according to one style guide, "are virtually always considered terms of pride and respect". Criticism Many critics of the term, both white and non-white, object to its lack of specificity and find the phrase racially offensive. It has been argued that the term lessens the focus on individual issues facing different racial and ethnic groups, particularly African Americans whom Michael Holzman argues ignores the specific legacy of historical disadvantage in the United States. Preserving "whiteness" as an intact category while lumping every other racial group into an indiscriminate category ("of color") replicates the marginalization that the term was intended to counter. Other commentators state that the term "people of color" is a misnomer and an arbitrary term in which people who are white are mislabeled as people of color. People of color also encompasses various heterogeneous groups which have little in common, with some arguing that American culture as a whole does not deliberate on economic inequality or issues of class. Political scientist Angelo Falcón argues that the use of broad terms like "person of color" is offensive because it aggregates diverse communities and projects "a false unity" that "obscure[s] the needs of Latinos and Asians". Citing the sensitivity of the issue, Falcón suggested that there should be "a national summit of Black, Latino and Asian community leaders" to discuss "how can the problem of the so-called 'black/white binary' be tackled in the way it respects the diversity it ignores and helps build the broader constituency for racial social justice that is needed in the country" and to "open the way for a perhaps much-needed resetting of relations between these historically-discriminated against communities that can lead to a more useful etymology of this relationship". Daniel Lim has also been criticized for the term for centering whiteness, framing non-white identities in relation to it and implying that whiteness is the default, race-neutral category. This positioning suggests that race is only relevant for non-white people, reinforcing the idea that whiteness is the norm and other identities are deviations. Critics argue this dynamic marginalizes non-white groups even as the term seeks to unite them, and some individuals express discomfort with having their identities defined in relation to whiteness rather than independently. Comedian George Carlin described "people of color" as "an awkward, bullshit, liberal-guilt phrase that obscures meaning rather than enhancing it", adding, "What should we call white people? 'People of no color'?" The use of the phrase person of color to describe white Hispanic and Latino Americans and Spaniards has been criticized as inaccurate. The United States census denotes the term "Latino" as a pan-ethnic label, rather than a racial category. Although many Latinos may qualify as being "people of color", the indiscriminate labeling of all Latinos as "people of color" obscures the racial diversity that exists within the Latino population itself, and, for this reason, some commentators have found the term misleading. BIPOC The acronym BIPOC, referring to "black, indigenous, (and) people of color", first appeared around 2013. By June 2020, it was, according to Sandra Garcia of The New York Times, "ubiquitous in some corners of Twitter and Instagram", as racial justice awareness grew in the United States in the wake of the murder of George Floyd. The term aims to emphasize the historic oppression of black and indigenous people, which is argued to be superlative and distinctive in U.S. history at the collective level. The BIPOC Project promotes the term in order "to highlight the unique relationship to Whiteness that Indigenous and Black (African Americans) people have, which shapes the experiences of and relationship to white supremacy for all people of color within a U.S. context". The term BIPOC does not appear to have originated in the black and Indigenous American communities, as it had been adopted much more widely among white Democrats than among people of color in a 2021 national poll. Asian and Latino Americans have often been confused as to whether the term includes them. The centering of black and Indigenous people in the acronym has been criticized as an unnecessary, unfounded, and divisive ranking of the oppression faced by the communities of color. The acronym's purposeful and definitional assertion that the historical and present-day suffering experienced by black and Indigenous people is more significant in kind or degree than that of other non-white groups has been described as casting communities of color in an oppression Olympics that obscures intersectional characteristics, similarities, and opportunities for solidarity in the struggle against racism. Critics argue that the systems of oppression foundational to U.S. history were not limited to the slavery and genocide suffered by black and Indigenous Americans, but also included the Asian-American and Latino-American experiences of oppression under the Chinese Exclusion Act and the doctrine of manifest destiny. Noting that "Black and Indigenous people are not at the center of every contemporary racial issue", other commentators have found it problematic that the ascendancy of the term coincided with the pronounced rise in anti-Asian hate crimes during the COVID-19 pandemic. By rendering Asian Americans as an unnamed "remnant", critics argue that the acronym renders the racial discrimination they experience invisible, thereby perpetuating harmful model minority and perpetual foreigner stereotypes. Some critics advocate a return to "POC" for its emphasis on coalition-building, while others call for a contextual approach that names "the groups actually included and centered in the arguments themselves". The term has also been criticized for being redundant. See also Anglo-Indian Black, Asian and minority ethnic Colorism Discrimination based on skin color Model minority Oppression Olympics Perpetual foreigner Political blackness Political correctness Race Race and ethnicity in the United States Statistext Visible minority References External links Adam, Mohammed (2020). "Adam: Why the term 'people of colour' is offensive to so many". ottawacitizen. Benavides, Lucía (February 9, 2020). "Why Labeling Antonio Banderas A 'Person Of Color' Triggers Such A Backlash". NPR. Retrieved April 24, 2020. Bland, Trinity (April 14, 2020). "Opinion: The term 'people of color' fails to be properly inclusive of the black community". Falcon, Angelo (April 3, 2018). "Latinos and the 'Of Color' Problem". AL DÍA News. Fowler, Yara Rodrigues (November 5, 2020). "Yara Rodrigues Fowler | White Latinos · LRB 5 November 2020". LRB Blog. "Readers React: The problem with 'people of color': It implies whiteness is the default". Opinion. Los Angeles Times. May 4, 2019. Laborde, Antonia (January 17, 2020). "Is Spain's Antonio Banderas an 'actor of color'?". EL PAÍS. Lamuye, Adebola (July 31, 2017). "I am no 'person of colour', I am a black African woman". The Independent. Retrieved January 23, 2018. Lind, Michael (July 4, 2016). "How to Fix America's Identity Crisis". Politico. Retrieved April 26, 2025. Holzman, Michael (September 19, 2015). "The Misnomer Called 'People of Color'". dropoutnation. Archived from the original on December 26, 2022. Retrieved January 25, 2021.{{cite web}}: CS1 maint: bot: original URL status unknown (link) Khan, Razib (September 17, 2020). "How Brahmins lead the fight against white privilege". Unherd. Retrieved February 3, 2021. Shoneye, Tolani (April 22, 2018). "As a black woman, I hate the term 'people of colour'". Young, Damon (2020). "The Phrase 'People of Color' Needs to Die". GQ. Domestic violence occurs across the world, in various cultures, and affects people across society, at all levels of economic status; however, indicators of lower socioeconomic status (such as unemployment and low income) have been shown to be risk factors for higher levels of domestic violence in several studies. In the United States, according to the Bureau of Justice Statistics in 1995, women reported a six times greater rate of intimate partner violence than men. However, studies have found that men are much less likely to report victimization in these situations. While some sources state that gay and lesbian couples experience domestic violence at the same frequency as heterosexual couples, other sources report that domestic violence rates among gay, lesbian and bisexual people might be higher but more under-reported. By demographic Against women Domestic violence against women has been occurring for centuries. Domestic violence is deemed as any and all physical, sexual, and verbal assaults towards an individual's body, sense of self, or sense of trust. It was not considered a world-wide issue or considered an issue in most countries until the 1980s. A study was conducted by the World Health Organization (WHO) in 1997 and determined that 5-20% of lost healthy lives between women aged 15–44 was due to domestic violence. Domestic violence has since been an issue recognized by most UN countries and further studies have been conducted to include specific domestic violence statistics per country and ways to decrease rates. According to various national surveys, the percentage of women who were ever physically assaulted by an intimate partner varies substantially by country: Barbados (30%), Canada (29%), Egypt (34%), New Zealand (35%), Switzerland (21%), United States (33%). Some surveys in specific places report figures as high as 50–70% of women who were ever physically assaulted by an intimate partner. Others, including surveys in the Philippines and Paraguay, report figures as low as 10%. Statistics published in 2004 show that the rate of domestic violence victimisation for Indigenous women in Australia may be 40 times the rate for non-Indigenous women. 80% of women surveyed in rural Egypt said that beatings were common and often justified, particularly if the woman refused to have sex with her husband. Up to two-thirds of women in certain communities in Nigeria's Lagos State say they are victims to domestic violence. In Turkey 42% of women over 15 have suffered physical or sexual violence. In India, around 70% of women are victims of domestic violence. Between 1993 and 2001, U.S. women reported intimate partner violence almost seven times more frequently than men (a ratio of 20:3). Statistics for the year 1994 showed that more than five times as many females reported being victimized by an intimate than did males. Pregnancy Domestic violence during pregnancy can be missed by medical professionals because it often presents in non-specific ways. A number of countries have been statistically analyzed to calculate the prevalence of this phenomenon: UK prevalence: 3.4% USA prevalence: 3.2–33.7% Ireland prevalence: 12.5% Rates are higher in teenagers Severity and frequency increase postpartum (10% antenatally vs. 19% postnatally); 21% at three months post partum There are a number of presentations that can be related to domestic violence during pregnancy: delay in seeking care for injuries; late booking, non-attenders at appointments, self-discharge; frequent attendance, vague problems; aggressive or over-solicitous partner; burns, pain, tenderness, injuries; vaginal tears, bleeding, STDs; and miscarriage. Domestic violence against a pregnant woman can also affect the fetus and can have lingering effects on the child after birth. Physical abuse is associated with neonatal death (1.5% versus 0.2%), and verbal abuse is associated with low birth weight (7.6% versus 5.1%). Against men Due to social stigmas regarding male victimization, men who are victims of domestic violence face an increased likelihood of being overlooked by healthcare providers. While much attention has been focused on domestic violence against women, researchers showed that also domestic violence that men experience from other men needs also attention. The issue of victimization of men by women has been contentious, due in part to studies which report drastically different statistics regarding domestic violence. Severe perpetration of physical violence tends to be committed by men, and victimization reports generally show women being more likely to experience domestic violence than men. A 2013 review of the literature that combined perpetration and victimization reports indicate that, worldwide, most studies only look at female victimization. The review examined studies from five continents and the correlation between a country's level of gender inequality and rates of domestic violence. The authors found that when partner abuse is defined broadly to include emotional abuse, any kind of hitting, and who hits first, partner abuse is relatively even. They also stated if one examines who is physically harmed and how seriously, expresses more fear, and experiences subsequent psychological problems, domestic violence is significantly gendered toward women as victims. Sherry Hamby argues that victimization reports are more reliable than perpetration reports and therefore studies showing women being more likely to suffer domestic violence than men are the accurate ones. A 2016 meta-analysis indicated that the only risk factors for the perpetration of intimate partner violence that differ by gender are witnessing intimate partner violence as a child, alcohol use, male demand, and female withdrawal communication patterns. Among LGBT people Some sources state that gay and lesbian couples experience domestic violence at the same frequency as heterosexual couples, while other sources state domestic violence among gay and lesbian couples might be higher than among heterosexual couples, that gay, lesbian, and bisexual individuals are less likely to report domestic violence that has occurred in their intimate relationships than heterosexual couples are, or that lesbian couples experience domestic violence less than heterosexual couples do. By contrast, some researchers commonly assume that lesbian couples experience domestic violence at the same rate as heterosexual couples, and have been more cautious when reporting domestic violence among gay male couples. In a survey by the Canadian Government, some 19% of lesbian women reported being victimized by their partners. Other research reports that lesbian relationships exhibit substantially higher rates of physical aggression. Against children The U.S Department of Health and Human Services reports that for each year between 2000 and 2005, "female parents acting alone" were most common perpetrators of child abuse. When it comes to domestic violence towards children involving physical abuse, research in the UK by the NSPCC indicated that "most violence occurred at home" (78%). Forty to sixty percent of men and women who abuse other adults also abuse their children. Girls whose fathers batter their mothers are 6.5 times more likely to be sexually abused by their fathers than are girls from non-violent homes. In China in 1989, 39,000 baby girls died during their first year of life because they did not receive the same medical care that would be given to a male child. In Asia alone, about one million children working in the sex trade are held in slavery-like conditions. Between teenagers Teen dating violence is a pattern of controlling behavior by one teenager over another teenager in the context of a dating relationship. While there are many similarities to "traditional" domestic violence, there are also some differences. Teens are much more likely than adults to become isolated from their peers as a result of controlling behavior by their romantic partner. Also, for many teens the abusive relationship may be their first dating experience, and so they may lack a "normal" dating experience with which to compare it. While teenagers are trying to establish their sexual identities, they are also confronting violence in their relationships and exposure to technology. Studies document that teenagers are experiencing significant amounts of dating or domestic violence. Depending on the population studied and the way dating violence is defined, between 9 and 35% of teens have experienced domestic violence in a dating relationship. When a broader definition of abuse that encompasses physical, sexual, and emotional abuse is used, one in three teen girls is subjected to dating abuse." Additionally, a significant number of teens are victims of stalking by intimate partners. Although involvement with romantic relationships is a critical aspect of adolescence, these relationships also present serious risks for teenagers. Unfortunately, adolescents in dating relationships are at greater risk of intimate partner violence than any other age group. Approximately one-third of adolescent girls are victims of physical, emotional, or verbal abuse from a dating partner. Estimates of sexual victimization range from 14% to 43% of girls and 0.3% to 36% for boys. According to the Center for Disease Control, in 2009, nearly 10% of students nationwide had been intentionally hit, slapped, or physically hurt by their boyfriend or girlfriend. Twenty-six percent of girls in a relationship reported being threatened with violence or experiencing verbal abuse; 13% reported being physically hurt or hit. Measuring Measures of the incidence of violence in intimate relationships can differ markedly in their findings depending on the measures used. Care is needed when using domestic violence statistics to ensure that both gender bias and under-reporting issues do not affect the inferences that are drawn from the statistics. Some researchers, such as Michael P. Johnson, suggest that where and how domestic violence is measured also affects findings, and caution is needed to ensure statistics drawn from one class of situations are not applied to another class of situations in a way that might have fatal consequences. Other researchers, such as David Murray Fergusson, counter that domestic violence prevention services, and statistics that they produce, target the extreme end of domestic violence and preventing child abuse rather than domestic violence between couples. Europe A 1992 Council of Europe study on domestic violence against women found that one in four women experience domestic violence over their lifetimes and between 6 and 10% of women suffer domestic violence in a given year. In the European Union, DV is a serious problem in the Baltic States. These three countries – Estonia, Latvia, and Lithuania – have also lagged behind most post-communist countries in their response to DV. The problem in these countries is severe, and in 2013 a DV victim won a European Court of Human Rights case against Lithuania. United Kingdom The British Crime Survey for 2006–2007 reported that 0.5% of people (0.6% of women and 0.3% of men) reported being victims of domestic violence during that year and 44.3% of domestic violence was reported to the police. According to the survey, 312,000 women and 93,000 men were victims of domestic violence. The Northern Ireland Crime Survey for 2005 reported that 13% of people (16% of women and 10% of men) reported being victims of domestic violence at some point in their lives. The National Study of Domestic Abuse for 2005 reported that 213,000 women and 88,000 men reported being victims of domestic violence at some point in their lives. According to the study, one in seven women and one in sixteen men were victims of severe physical abuse, severe emotional abuse, or sexual abuse. There are more than 1 million violent crimes against women and girls recorded by police in 2022/23 accounting for just under 20 per cent of all police-recorded crime. The prevalence of sexual assault against women aged 16 to 59 in England and Wales was 4.3 per cent in 2023-24, up from 3.4 per cent in 2009-10, as the NAO said. France France has experienced domestic violence across both sexes for decades without the government addressing the issue. The Gender-Based Violence Against Women in Contemporary France determined the number of deaths relating to domestic violence from 2010 to 2014. 653 deaths in this time period were female victims at the hand of their male partner and 125 deaths were male victims by their female partners. These findings were determined after a research into domestic violence toward women compared to men was conducted by the French government, also known as the Istanbul Convention. Since these findings were published, the domestic violence in France has lowered due to passing the Law for Equality between Women and Men. Germany In Germany, domestic violence is a serious issue for women. According to Sexual Violence against Women in Germany, there is significant domestic violence toward women at any given time. The Criminological Research Institute of Lower Saxony conducted research involving women who resided in Germany. It was found that out of 4450 women, 5.4% will experience domestic violence during their lifetime. The German government has attempted to decrease the amount of domestic violence victims among women. Various programs have been implemented, including information campaign to make women aware of different risks like multiple partners or substance abuse that will lead to an increased chance of experiencing domestic violence. North America Canada In Canada, the Assembly of First Nations evaluation of the Canada Prenatal Nutrition Program conducted by CIET offers an inclusive and relatively unbiased national estimate. It documented domestic violence in a random sample of 85 First Nations across Canada: 22% (523 of 2,359) of mothers reported suffering abuse in the year prior to being interviewed; of these, 59% reported physical abuse. Results of studies which estimate the prevalence of domestic violence vary significantly, depending on specific wording of survey questions, how the survey is conducted, the definition of abuse or domestic violence used, the willingness or unwillingness of victims to admit that they have been abused and other factors. For instance, Straus (2005) conducted a study which estimated that the rate of minor assaults by women in the United States was 78 per 1,000 couples, compared with a rate for men of 72 per 1,000 and the severe assault rate was 46 per 1,000 couples for assaults by women and 50 per 1,000 for assaults by men. Neither difference is statistically significant. He claimed that since these rates were based exclusively on information provided by women respondents, the near-equality in assault rates could not be attributed to a gender bias in reporting. One analysis found that "women are as physically aggressive or more aggressive than men in their relationships with their spouses or male partners". However, studies have shown that women are more likely to be injured. Archer's meta-analysis found that women in the United States suffer 65% of domestic violence injuries. A Canadian study showed that 7% of women and 6% of men were abused by their current or former partners, but female victims of spousal violence were more than twice as likely to be injured as male victims, three times more likely to fear for their life, twice as likely to be stalked, and twice as likely to experience more than ten incidents of violence. However, Straus notes that Canadian studies on domestic violence have simply excluded questions that ask men about being victimized by their wives. According to a 2004 survey in Canada, the percentages of males being physically or sexually victimized by their partners was 6% versus 7% for women. However, females reported higher levels of repeated violence and were more likely than men to experience serious injuries; 23% of females versus 15% of males were faced with the most serious forms of violence including being beaten, choked, or threatened with or having a gun or knife used against them. Also, 21% of women versus 11% of men were likely to report experiencing more than 10 violent incidents. Women who often experience higher levels of physical or sexual violence from their current partner were 44%, compared with 18% of men to suffer from an injury. Cases in which women are faced with extremely abusive partners result in the females having to fear for their lives due to the violence. In addition, statistics show that 34% of women feared for their lives, and 10% of men feared for theirs. Some studies show that lesbian relationships have similar levels of violence as heterosexual relationships. United States Approximately 1.3 million women and 835,000 men report being physically assaulted by an intimate partner annually in the United States. In the United States, domestic violence is the leading cause of injury to women between the ages of 15 and 44. Victims of DV are offered legal remedies, which include the criminal law, as well as obtaining a protection order. The remedies offered can be both of a civil nature (civil orders of protection and other protective services) and of a criminal nature (charging the perpetrator with a criminal offense). People perpetrating DV are subject to criminal prosecution, most often under assault and battery laws. Russia In Russia, according to a representative of the Russian Ministry of Internal Affairs, one in four families experiences domestic violence. Domestic violence is not a specific criminal offense: it can be charged under various crimes of the criminal code (e.g. assault), but in practice cases of domestic violence turn into criminal cases only when they involve severe injuries, or the victim has died. For more details see Domestic violence in Russia. Asia In Turkey 42% of women over 15 have suffered physical or sexual violence. Fighting the prevalence of domestic violence in Kashmir has brought Hindu and Muslim activists together. According to some Islamic clerics and women's advocates, women from Muslim-majority cultures often face extra pressure to submit to domestic violence, as their husbands may manipulate Islamic law to exert their control. One study found that half of Palestinian women have been the victims of domestic violence. A study on Bedouin women in Israel found that most have experienced DV, most accepted it as a decree from God, and most believed they were to blame themselves for the violence. The study also showed that the majority of women were not aware of existing laws and policies which protect them: 60% said they did not know what a restraining order was. In Iraq husbands have a legal right to "punish" their wives. The criminal code states at Paragraph 41 that there is no crime if an act is committed while exercising a legal right; examples of legal rights include: "The punishment of a wife by her husband, the disciplining by parents and teachers of children under their authority within certain limits prescribed by law or by custom". In Jordan, part of article 340 of the Penal Code states that "he who discovers his wife or one of his female relatives committing adultery and kills, wounds, or injures one of them, is exempted from any penalty." This has twice been put forward for cancellation by the government, but was retained by the Lower House of the Parliament, in 2003: a year in which at least seven honor killings took place. Article 98 of the Penal Code is often cited alongside Article 340 in cases of honor killings. "Article 98 stipulates that a reduced sentence is applied to a person who kills another person in a 'fit of fury'". The Human Rights Watch found that up to 90% of women in Pakistan were subject to some form of maltreatment within their own homes. Honor killings in Pakistan are a very serious problem, especially in northern Pakistan. In Pakistan, honour killings are known locally as karo-kari. Karo-kari is a compound word literally meaning "black male" (Karo) and "black female" (Kari). Domestic violence in India is widespread, and is often related to the custom of dowry. Honor killings are more common in some regions of India, particularly in northern regions of the country. Honor killings have been reported in the states of Punjab, Rajasthan, Haryana, Uttar Pradesh, and Bihar, as a result of people marrying without their family's acceptance, and sometimes for marrying outside their caste or religion. Africa A UN report compiled from a number of different studies conducted in at least 71 countries found domestic violence against women to be most prevalent in Ethiopia. Up to two-thirds of women in certain communities in Nigeria's Lagos State say they are victims to domestic violence. 80% of women surveyed in rural Egypt said that beatings were common and often justified, particularly if the woman refused to have sex with her husband. Oceania Australia Statistics published in 2004, show that the rate of domestic violence victimisation for Indigenous women in Australia may be 40 times the rate for non-Indigenous women. Findings from the 2006 Australian Bureau of Statistics Personal Safety Survey show that among the female victims of physical assault, 31% were assaulted by a current or previous partner. Among male victims, 4.4% were assaulted by a current or previous partner. Thirty percent of people who had experienced violence by a current partner since the age of 15 were male, and seventy percent were female. References External links World Report on Violence Against Children, Secretary-General of the United Nations Hidden in Plain Sight: A statistical analysis of violence against children, UNICEF South Robertson is an area on the Westside of Los Angeles that is served by the South Robertson neighborhood council. It contains the following city neighborhoods: Beverlywood, Castle Heights, Cheviot Hills, Crestview, La Cienega Heights and Reynier Village. The area is notable as a center for the Jewish community. Geography Boundaries According to the South Robertson Neighborhood Council's map, South Robertson is bounded roughly by the Santa Monica Freeway and Venice Boulevard on the south, La Cienega Boulevard on the east, Gregory Way (to Robertson) on the north, Whitworth (from Robertson to Roxbury) on the north, Roxbury and Beverwil on the west. The Mapping L.A. project of the Los Angeles Times, identifies a geographically similar area called Pico-Robertson. Its street borders are: north, Gregory Way and Pico Boulevard; northeast, LeDoux Road and Olympic and San Vicente Boulevards, roughly Beverly Glen Drive; east, La Cienega Boulevard; south, Airdrome Street and Hillcrest Country Club; west, Robertson Boulevard, Beverly Green Drive and S. Roxbury Drive. Neighborhoods in the South Robertson Neighborhood Council area The following neighborhoods are within the boundaries established by the neighborhood council: Beverlywood, Castle Heights, Cheviot Hills, Crestview, La Cienega Heights and Reynier Village. Population 2000 The 2000 U.S. census counted 18,019 residents in the 1.03-square-mile Pico-Robertson neighborhood—an average of 17,468 people per square mile, among the highest population densities for the city. In 2008, the city estimated that the population had increased to 19,253. The median age for residents was 36, older than the city at large; the percentages of residents aged 19 to 34 and 65 and older were among the county's highest. The neighborhood was considered "not especially diverse" ethnically, with a high percentage of white people. The breakdown was whites, 73.5%; Latinos, 7.3%; Asians, 5.7%; blacks, 5.6%; and others, 7.9%. Iran (37.2%) and Israel (5.7%) were the most common places of birth for the 34.6% of the residents who were born abroad—about the same percentage as in the city at large. The median yearly household income in 2008 dollars was $63,356, an average figure for Los Angeles. The average household size of 2.1 people was low for Los Angeles. Renters occupied 73.1% of the housing stock and house- or apartment owners held 26.9%. Education The following public schools are within the council area: Canfield, Crescent Heights, Shenandoah, and Castle Heights elementary schools Los Angeles Center for Enriched Studies Emerson Middle School Hamilton High School Jewish community The neighborhood features more than thirty certified kosher restaurants, including delis, Chinese, Italian and Mexican restaurants, a donut shop, a frozen yogurt shop, bakeries, and butchers. The community features four men's mikvahs and one woman's mikvah, the largest known as the Los Angeles Mikvah. There are several Jewish day schools located in the Pico-Robertson area. The Chabad community operates four schools, Bais Chaya Mushka and Bais Chana, both of which are on Pico Boulevard, as well as the newly relocated Cheder Menachem on La Cienega. Yeshiva University High School has campuses on both South Robertson Boulevard and West Pico Boulevard. The community overall has a wide variety of Jewish denominational groups. Over the past two decades, the Orthodox community has grown to become the largest Jewish denomination in the area. This is evident in the growth of the Chabad community. According to Chabad, the Hasidic movement has eleven centers in the immediate Pico-Robertson area, including the two high schools, boys cheder, day school, six synagogues, and a community center. Minyan Finder reports over twenty synagogues operating in the area. In 1993, the neighborhood became home to the Los Angeles Museum of Tolerance. See also Neighborhood councils of Los Angeles == References == Deculturalization is the process by which an ethnic group is forced to abandon its language, culture, and customs. It is the destruction of the culture of a dominated group and its replacement with the culture of the dominating group. Deculturalization is a slow process due to its extensive goal of fully replacing the subordinate ethnic group's culture, language, and customs. This term is often confused with assimilation and acculturation. Methods of deculturalization Geographical segregation Forbidding education to the dominated group Forceful replacing of language Superior culture's curriculum in schools Instructors are from the dominant group Avoiding the dominated group's culture in curriculum Deculturalization in the United States African Americans The enslavement of African Americans during the 18th and 19th centuries in the United States is a form of deculturalization. Slavery in the United States made the African Americans dependent on their owners allowing for the owners to exploit them. The owners removed their African names, did not allow them to read, and did not allow them to practice their culture and language. Deculturalization of African Americans stems back to When the African American slaves were forbidden access to education due to fear of a slave revolt against the slaveholders. A series of court cases occurred in the United States helping deculturalization of African Americans as wells as there were cases that went against deculturalization. For example, the addition of the 14th Amendment to the United States Constitution, the Dred Scott v. Sandford decision, Brown v. Board of Education, Plessy v. Ferguson, and countless others. After the Civil War (United States) segregated education continued and was a struggle to integrate fully and completely. While integration was achieved, the textbooks that the African American students learn from are bias and contain material from the dominant, Anglo-American culture. Latin Americans The deculturalization of Latinos can refer back to the Mexican–American War and The Treaty Of Guadalupe Hidalgo. Once the United States won California, Texas, New Mexico, Arizona, Nevada, Utah, and parts of Wyoming and Colorado Mexicans who were living in these areas were removed from their lands. Their identity in the United States changed constantly from Mexican to White and vice versa until the word Hispanic was created to refer to these Mexican Americans. By simply using the word Hispanic to refer to the Mexican Americans and later the Latin-American immigrants refers to the conqueror's culture-the Spanish culture. Latinos in the United States also had segregated schooling. In schools they were given second-hand material from the wealthy, Anglo schools. When Latinos were being integrated, they as well as the African Americans, were being taught from bias, Anglo-cultured, Anglo-praising textbooks. Latinos did have a win to have bilingual education. While they were allowed to have bilingual education, the primary, enforced language is the English one. In some schools Latinos were corporally punished for speaking Spanish in the classroom. In some universities, Latinos were also forced to take many speech classes in order to remove the accents of the Latinos when they spoke English. While that is not seen evidently in schools anymore, the education system continues to enforce English, Anglo-American customs, culture and language as the dominant one. Asian Americans Asian Americans began to be deculturalized by not being allowed to be naturalized, the Chinese-Exclusion Act, Japanese Internment, forbidding land ownership, and enforcing the Anglo-culture onto them. The Naturalization Act of 1790 did not allow for the Chinese along with other Asians to become naturalized, because the naturalization process was limited only to the Anglo community in the United States. In terms of schooling, in some cases Asian Americans were denied an education entirely. It was not until the 1900s when Asian Americans were allowed to receive an education through the implementation of certain provisions. In 1855, the Chan Yong case fortified that the Chinese are not "white" therefore ineligible for citizenship due to the Naturalization Act of 1790. Also, in 1922, the court case Ozawa v. United States, the Japanese man understood he was not allowed to be naturalized due to the former act, but asked for the Japanese to be considered white, but was denied the request. The Ozawa v. United States shows how some Asians would rather refer to themselves as white than as Japanese or their individual ethnic group, because of the advantages that being "white" bring. The enforced Anglo-American culture upon the Asians and using them during the Cold War as a model minority that the United States is not racist, it is the individual's fault allows for deculturalization to be successful. Jade Snow Wong is a Chinese-American writer who was used by the American government to travel to the Asian world and show how an Asian can succeed in America. Indigenous Americans Once the first settlement in Jamestown 1607 occurred, the Pre-American deculturalization process began. When the English came to America they looked to the Native Americans as "pagans" and "savages". Native Americans believed that the land was not property, a thing to be claimed and owned. Once the English settlers arrived this was one of the major culture difference that needed to be extinguished. The idea of private property and ownership was enforced upon the Native Americans. While even those who accepted it, because they understood the consequences, their lands were taken away. They wanted to impose the traditional "Christian" nuclear family as well among the Native Americans. In order to gain success the colonists made Native American Educational Programs. Christian missionaries such as John Eliot learned the Native American language in order to convert them into Christianity began the segregation among the "pagans" and the "holy". The Native Americans were exploited. There was a cultural genocide and simply genocide against the Native Americans. From the Trail of Tears to the appropriation of their designs in order to gain capital, corporate gains. See also Cultural genocide Forced assimilation Institutional racism Americanization (immigration) Acculturation Linguistic discrimination Language death == References == A sign is an object, quality, event, or entity whose presence or occurrence indicates the probable presence or occurrence of something else. A natural sign bears a causal relation to its object—for instance, thunder is a sign of storm, or medical symptoms a sign of disease. A conventional sign signifies by agreement, as a full stop signifies the end of a sentence; similarly the words and expressions of a language, as well as bodily gestures, can be regarded as signs, expressing particular meanings. The physical objects most commonly referred to as signs (notices, road signs, etc., collectively known as signage) generally inform or instruct using written text, symbols, pictures or a combination of these. The philosophical study of signs and symbols is called semiotics; this includes the study of semiosis, which is the way in which signs (in the semiotic sense) operate. Nature Semiotics, epistemology, logic, and philosophy of language are concerned about the nature of signs, what they are and how they signify. The nature of signs and symbols and significations, their definition, elements, and types, is mainly established by Aristotle, Augustine, and Aquinas. According to these classic sources, significance is a relationship between two sorts of things: signs and the kinds of things they signify (intend, express or mean), where one term necessarily causes something else to come to the mind. Distinguishing natural signs and conventional signs, the traditional theory of signs (Augustine) sets the following threefold partition of things: all sorts of indications, evidences, symptoms, and physical signals, there are signs which are always signs (the entities of the mind as ideas and images, thoughts and feelings, constructs and intentions); and there are signs that have to get their signification (as linguistic entities and cultural symbols). So, while natural signs serve as the source of signification, the human mind is the agency through which signs signify naturally occurring things, such as objects, states, qualities, quantities, events, processes, or relationships. Human language and discourse, communication, philosophy, science, logic, mathematics, poetry, theology, and religion are only some of fields of human study and activity where grasping the nature of signs and symbols and patterns of signification may have a decisive value. Communication takes place without words but via the mind as a result of signs and symbols; They communicate/pass across/ messages to the human mind through their pictorial representation. Types The word sign has a variety of meanings in English, including: Sun signs in astrology Sign or signing, in communication: communicating via hand gestures, such as in sign language. Gang signal Sign, in Tracking (hunting): also known as Spoor (animal); trace evidence left on the ground after passage. A signboard. A sign, in common use, is an indication that a previously observed event is about to occur again Sign, in divination and religion: an omen, an event or occurrence believed to foretell the future Sign, in ontology and spirituality: a coincidence or surprising event thought to reveal divine will; see synchronicity Sign (linguistics): a combination of a concept and a sound-image described by Ferdinand de Saussure In mathematics, the sign of a number tells whether it is positive or negative. Also, the sign of a permutation tells whether it is the product of an even or odd number of transpositions. Signedness, in computing, is the property that a representation of a number has one bit, the sign bit, which denotes whether the number is non-negative or negative. A number is called signed if it contains a sign bit, otherwise unsigned. See also signed number representation Sign, in biology: an indication of some living thing's presence Medical sign, in medicine: objective evidence of the presence of a disease or disorder, as opposed to a symptom, which is subjective Sign (semiotics): the basic unit of meaning Information sign: a notice that instructs, advises, informs or warns people Traffic sign: a sign that instructs drivers; see also stop sign, speed limit sign, cross walk sign Sign, in a writing system: a basic unit. Similar terms which are more specific are character, letter or grapheme Commercial signage, including flashing signs, such as on a retail store, factory, or theatre Signature, in history: a handwritten depiction observed on a document to show authorship and will For marketing or advocacy purposes, a signage refers to the collective use of signs to convey a message. Christianity St. Augustine was the first man who synthesized the classical and Hellenistic theories of signs. For him a sign is a thing which is used to signify other things and to make them come to mind (De Doctrina Christiana (hereafter DDC) 1.2.2; 2.1.1). The most common signs are spoken and written words (DDC 1.2.2; 2.3.4-2.4.5). Although God cannot be fully expressible, Augustine gave emphasis to the possibility of God's communication with humans by signs in Scripture (DDC 1.6.6). Augustine endorsed and developed the classical and Hellenistic theories of signs. Among the mainstream in the theories of signs, i.e., that of Aristotle and that of Stoics, the former theory filtered into the works of Cicero (106-43 BC, De inventione rhetorica 1.30.47-48) and Quintilian (circa 35–100, Institutio Oratoria 5.9.9-10), which regarded the sign as an instrument of inference. In his commentary on Aristotle's De Interpretatione, Ammonius said, "according to the division of the philosopher Theophrastus, the relation of speech is twofold, first in regard to the audience, to which speech signifies something, and secondly in regard to the things about which the speaker intends to persuade the audience." If we match DDC with this division, the first part belongs to DDC Book IV and the second part to DDC Books I-III. Augustine, although influenced by these theories, advanced his own theological theory of signs, with whose help one can infer the mind of God from the events and words of Scripture. Books II and III of DDC enumerate all kinds of signs and explain how to interpret them. Signs are divided into natural (naturalia) and conventional (data); the latter is divided into animal (bestiae) and human (homines); the latter is divided into non-words (cetera) and words (verba); the latter is divided into spoken words (voces) and written words (litterae); the latter is divided into unknown signs (signa ignota) and ambiguous signs (signa ambigua); both the former and the latter are divided respectively into particular signs (signa propria) and figurative signs (signa translata), among which the unknown figurative signs belong to the pagans. In addition to exegetical knowledge (Quintilian, Institutio Oratoria 1.4.1-3 and 1.8.1-21) which follows the order of reading (lectio), textual criticism (emendatio), explanation (enarratio), and judgment (iudicium), one needs to know the original language (Hebrew and Greek) and broad background information on Scripture (DDC 2.9.14-2.40.60). Augustine's understanding of signs includes several hermeneutical presuppositions as important factors. First, the interpreter should proceed with humility, because only a humble person can grasp the truth of Scripture (DDC 2.41.62). Second, the interpreter must have a spirit of active inquiry and should not hesitate to learn and use pagan education for the purpose of leading to Christian learning, because all truth is God's truth (DDC 2.40.60-2.42.63). Third, the heart of interpreter should be founded, rooted, and built up in love which is the final goal of the entire Scriptures (DDC 2.42.63). The sign does not function as its own goal, but its purpose lies in its role as a signification (res significans, DDC 3.9.13). God gave signs as a means to reveal himself; Christians need to exercise hermeneutical principles in order to understand that divine revelation. Even if the Scriptural text is obscure, it has meaningful benefits. For the obscure text prevents us from falling into pride, triggers our intelligence (DDC 2.6.7), tempers our faith in the history of revelation (DDC 3.8.12), and refines our mind to be suitable to the holy mysteries (DDC 4.8.22). When interpreting signs, the literal meaning should first be sought, and then the figurative meaning (DDC 3.10.14-3.23.33). Augustine suggests the hermeneutical principle that the obscure Scriptural verse is interpreted with the help of plain and simple verses, which formed the doctrine of "scriptura scripturae interpres" (Scripture is the Interpreter of Scripture) in the Reformation Era. Moreover, he introduces the seven rules of Tyconius the Donatist to interpret the obscure meaning of the Bible, which demonstrates his understanding that all truth belongs to God (DDC 3.3.42-3.37.56). In order to apply Augustine's hermeneutics of the sign appropriately in modern times, every division of theology must be involved and interdisciplinary approaches must be taken. See also References External links The dictionary definition of sign at Wiktionary Progress is movement towards a perceived refined, improved, or otherwise desired state. It is central to the philosophy of progressivism, which interprets progress as the set of advancements in technology, science, and social organization efficiency – the latter being generally achieved through direct societal action, as in social enterprise or through activism, but being also attainable through natural sociocultural evolution – that progressivism holds all human societies should strive towards. The concept of progress was introduced in the early-19th-century social theories, especially social evolution as described by Auguste Comte and Herbert Spencer. It was present in the Enlightenment's philosophies of history. As a goal, social progress has been advocated by varying realms of political ideologies with different theories on how it is to be achieved. Measuring progress Specific indicators for measuring progress can range from economic data, technical innovations, change in the political or legal system, and questions bearing on individual life chances, such as life expectancy and risk of disease and disability. GDP growth has become a key orientation for politics and is often taken as a key figure to evaluate a politician's performance. However, GDP has a number of flaws that make it a bad measure of progress, especially for developed countries. For example, environmental damage is not taken into account nor is the sustainability of economic activity. Wikiprogress has been set up to share information on evaluating societal progress. It aims to facilitate the exchange of ideas, initiatives and knowledge. HumanProgress.org is another online resource that seeks to compile data on different measures of societal progress. Our World in Data is a scientific online publication, based at the University of Oxford, that studies how to make progress against large global problems such as poverty, disease, hunger, climate change, war, existential risks, and inequality. The mission of Our World in Data is to present "research and data to make progress against the world’s largest problems". The Social Progress Index is a tool developed by the International Organization Imperative Social Progress, which measures the extent to which countries cover social and environmental needs of its citizenry. There are fifty-two indicators in three areas or dimensions: Basic Human Needs, and Foundations of Wellbeing and Opportunities which show the relative performance of nations. Indices that can be used to measure progress include: Scientific progress Scientific progress is the idea that the scientific community learns more over time, which causes a body of scientific knowledge to accumulate. The chemists in the 19th century knew less about chemistry than the chemists in the 20th century, and they in turn knew less than the chemists in the 21st century. Looking forward, today's chemists reasonably expect that chemists in future centuries will know more than they do. From the 18th century through late 20th century, the history of science, especially of the physical and biological sciences, was often presented as a progressive accumulation of knowledge, in which true theories replaced false beliefs. Some more recent historical interpretations, such as those of Thomas Kuhn, tend to portray the history of science in terms of competing paradigms or conceptual systems in a wider matrix of intellectual, cultural, economic and political trends. These interpretations, however, have met with opposition for they also portray the history of science as an incoherent system of incommensurable paradigms, not leading to any scientific progress, but only to the illusion of progress. Whether other intellectual disciplines make progress in the same way as the sciences is a matter of debate. For example, one might expect that today's historians know more about global history than their ancient counterparts (consider the histories of Herodotus). Yet, knowledge can be lost through the passage of time, or the criteria for evaluating what is worth knowing can change. Similarly, there is considerable disagreement over whether fields such as philosophy make progress - or even whether they aim at accumulating knowledge in the same way as the sciences. Social progress Aspects of social progress, as described by Condorcet, have included the disappearance of slavery, the rise of literacy, the lessening of inequalities between the sexes, reforms of harsh prisons and the decline of poverty. The social progress of a society can be measured based on factors such as its ability to address fundamental human needs, help citizens improve their quality of life, and provide opportunities for citizens to succeed. Social progress is often improved by increases in GDP, although other factors are also relevant. An imbalance between economic and social progress hinders further economic progress, and can lead to political instability. Where there is an imbalance between economic growth and social progress, political instability and unrest often arise. Lagging social progress also holds back economic growth in these and other countries that fail to address human needs, build social capital, and create opportunity for their citizens. Status of women How progress improved the status of women in traditional society was a major theme of historians starting in the Enlightenment and continuing to today. British theorists William Robertson (1721–1793) and Edmund Burke (1729–1797), along with many of their contemporaries, remained committed to Christian- and republican-based conceptions of virtue, while working within a new Enlightenment paradigm. The political agenda related beauty, taste, and morality to the imperatives and needs of modern societies of a high level of sophistication and differentiation. Two themes in the work of Robertson and Burke—the nature of women in 'savage' and 'civilized' societies and 'beauty in distress'—reveals how long-held convictions about the character of women, especially with regard to their capacity and right to appear in the public domain, were modified and adjusted to the idea of progress and became central to modern European civilization. Classics experts have examined the status of women in the ancient world, concluding that in the Roman Empire, with its superior social organization, internal peace, and rule of law, allowed women to enjoy a somewhat better standing than in ancient Greece, where women were distinctly inferior. The inferior status of women in traditional China has raised the issue of whether the idea of progress requires a thoroughgoing rejection of traditionalism—a belief held by many Chinese reformers in the early 20th century. Historians Leo Marx and Bruce Mazlish asking, "should we in fact abandon the idea of progress as a view of the past," answer that there is no doubt "that the status of women has improved markedly" in cultures that have adopted the Enlightenment idea of progress. Modernization Modernization was promoted by classical liberals in the 19th and 20th centuries, who called for the rapid modernization of the economy and society to remove the traditional hindrances to free markets and free movements of people. During the Enlightenment in Europe social commentators and philosophers began to realize that people themselves could change society and change their way of life. Instead of being made completely by gods, there was increasing room for the idea that people themselves made their own society—and not only that, as Giambattista Vico argued, because people made their own society, they could also fully comprehend it. This gave rise to new sciences, or proto-sciences, which claimed to provide new scientific knowledge about what society was like, and how one may change it for the better. In turn, this gave rise to progressive opinion, in contrast with conservational opinion. The social conservationists were skeptical about panaceas for social ills. According to conservatives, attempts to radically remake society normally make things worse. Edmund Burke was the leading exponent of this, although later-day liberals like Friedrich Hayek have espoused similar views. They argue that society changes organically and naturally, and that grand plans for the remaking of society, like the French Revolution, National Socialism and Communism hurt society by removing the traditional constraints on the exercise of power. The scientific advances of the 16th and 17th centuries provided a basis for Francis Bacon's book the New Atlantis. In the 17th century, Bernard le Bovier de Fontenelle described progress with respect to arts and the sciences, saying that each age has the advantage of not having to rediscover what was accomplished in preceding ages. The epistemology of John Locke provided further support and was popularized by the Encyclopedists Diderot, Holbach, and Condorcet. Locke had a powerful influence on the American Founding Fathers. The first complete statement of progress is that of Turgot, in his "A Philosophical Review of the Successive Advances of the Human Mind" (1750). For Turgot, progress covers not only the arts and sciences but, on their base, the whole of culture—manner, mores, institutions, legal codes, economy, and society. Condorcet predicted the disappearance of slavery, the rise of literacy, the lessening of inequalities between the sexes, reforms of harsh prisons and the decline of poverty. John Stuart Mill's (1806–1873) ethical and political thought demonstrated faith in the power of ideas and of intellectual education for improving human nature or behavior. For those who do not share this faith the idea of progress becomes questionable. Alfred Marshall (1842–1924), a British economist of the early 20th century, was a proponent of classical liberalism. In his highly influential Principles of Economics (1890), he was deeply interested in human progress and in what is now called sustainable development. For Marshall, the importance of wealth lay in its ability to promote the physical, mental, and moral health of the general population. After World War II, the modernization and development programs undertaken in the Third World were typically based on the idea of progress. In Russia the notion of progress was first imported from the West by Peter the Great (1672–1725). An absolute ruler, he used the concept to modernize Russia and to legitimize his monarchy (unlike its usage in Western Europe, where it was primarily associated with political opposition). By the early 19th century, the notion of progress was being taken up by Russian intellectuals and was no longer accepted as legitimate by the tsars. Four schools of thought on progress emerged in 19th-century Russia: conservative (reactionary), religious, liberal, and socialist—the latter winning out in the form of Bolshevist materialism. The intellectual leaders of the American Revolution, such as Benjamin Franklin, Thomas Paine, Thomas Jefferson and John Adams, were immersed in Enlightenment thought and believed the idea of progress meant that they could reorganize the political system to the benefit of the human condition; both for Americans and also, as Jefferson put it, for an "Empire of Liberty" that would benefit all mankind. In particular, Adams wrote “I must study politics and war, that our sons may have liberty to study mathematics and philosophy. Our sons ought to study mathematics and philosophy, geography, natural history and naval architecture, navigation, commerce and agriculture in order to give their children a right to study painting, poetry, music, architecture, statuary, tapestry and porcelain.” Juan Bautista Alberdi (1810–1884) was one of the most influential political theorists in Argentina. Economic liberalism was the key to his idea of progress. He promoted faith in progress, while chiding fellow Latin Americans for blind copying of United States and Europe models. He hoped for progress through promotion of immigration, education, and a moderate type of federalism and republicanism that might serve as a transition in Argentina to true democracy. In Mexico, José María Luis Mora (1794–1850) was a leader of classical liberalism in the first generation after independence, leading the battle against the conservative trinity of the army, the church, and the hacendados. He envisioned progress as both a process of human development by the search for philosophical truth and as the introduction of an era of material prosperity by technological advancement. His plan for Mexican reform demanded a republican government bolstered by widespread popular education free of clerical control, confiscation and sale of ecclesiastical lands as a means of redistributing income and clearing government debts, and effective control of a reduced military force by the government. Mora also demanded the establishment of legal equality between native Mexicans and foreign residents. His program, untried in his lifetime, became the key element in the Mexican Constitution of 1857. In Italy, the idea that progress in science and technology would lead to solutions for human ills was connected to the nationalism that united the country in 1860. The Piedmontese Prime Minister Camillo Cavour envisaged the railways as a major factor in the modernization and unification of the Italian peninsula. The new Kingdom of Italy, formed in 1861, worked to speed up the processes of modernization and industrialization that had begun in the north, but were slow to arrive in the Papal States and central Italy, and were nowhere in sight in the "Mezzogiorno" (that is, Southern Italy and Sicily). The government sought to combat the backwardness of the poorer regions in the south and work towards augmenting the size and quality of the newly created Italian army so that it could compete on an equal footing with the powerful nations of Europe. In the same period, the government was legislating in favour of public education to fight the great problem of illiteracy, upgrade the teaching classes, improve existing schools, and procure the funds needed for social hygiene and care of the body as factors in the physical and moral regeneration of the race. In China, in the 20th century the Kuomintang or Nationalist party, which ruled from the 1920s to the 1940s, advocated progress. The Communists under Mao Zedong adopted different models and their ruinous projects caused mass famines. After Mao's death, however, the new regime led by Deng Xiaoping (1904–1997) and his successors aggressively promoted modernization of the economy using capitalist models and imported western technology. This was termed the "Opening of China" in the West, and more broadly encompasses Chinese economic reform. Among environmentalists, there is a continuum between two opposing poles. The one pole is optimistic, progressive, and business-oriented, and endorses the classic idea of progress. For example, bright green environmentalism endorses the idea that new designs, social innovations and green technologies can solve critical environmental challenges. The other is pessimistic in respect of technological solutions, warning of impending global crisis (through climate change or peak oil, for example) and tends to reject the very idea of modernity and the myth of progress that is so central to modernization thinking. Similarly, Kirkpatrick Sale, wrote about progress as a myth benefiting the few, and a pending environmental doomsday for everyone. An example is the philosophy of Deep Ecology. Philosophy Sociologist Robert Nisbet said that "No single idea has been more important than ... the Idea of Progress in Western civilization for three thousand years", and defines five "crucial premises" of the idea of progress: value of the past nobility of Western civilization worth of economic/technological growth faith in reason and scientific/scholarly knowledge obtained through reason intrinsic importance and worth of life on earth Sociologist P. A. Sorokin said, "The ancient Chinese, Babylonian, Hindu, Greek, Roman, and most of the medieval thinkers supporting theories of rhythmical, cyclical or trendless movements of social processes were much nearer to reality than the present proponents of the linear view." Unlike Confucianism and to a certain extent Taoism, that both search for an ideal past, the Judeo-Christian-Islamic tradition believes in the fulfillment of history, which was translated into the idea of progress in the modern age. Therefore, Chinese proponents of modernization have looked to western models. According to Thompson, the late Qing dynasty reformer, Kang Youwei, believed he had found a model for reform and "modernisation" in the Ancient Chinese Classics. Philosopher Karl Popper said that progress was not fully adequate as a scientific explanation of social phenomena. More recently, Kirkpatrick Sale, a self-proclaimed neo-luddite author, wrote exclusively about progress as a myth, in an essay entitled "Five Facets of a Myth". Iggers (1965) says that proponents of progress underestimated the extent of man's destructiveness and irrationality, while critics misunderstand the role of rationality and morality in human behavior. In 1946, psychoanalyst Charles Baudouin claimed modernity has retained the "corollary" of the progress myth, the idea that the present is superior to the past, while at the same time insisting that it is free of the myth: The last two centuries were familiar with the myth of progress. Our own century has adopted the myth of modernity. The one myth has replaced the other. Men ceased to believe in progress; but only to pin their faith to more tangible realities, whose sole original significance had been that they were the instruments of progress. This exaltation of the present ... is a corollary of that very faith in progress which people claim to have discarded. The present is superior to the past, by definition, only in a mythology of progress. Thus one retains the corollary while rejecting the principle. There is only one way of retaining a position of whose instability one is conscious. One must simply refrain from thinking. A cyclical theory of history was adopted by Oswald Spengler (1880–1936), a German historian who wrote The Decline of the West in 1920. World War I, World War II, and the rise of totalitarianism demonstrated that progress was not automatic and that technological improvement did not necessarily guarantee democracy and moral advancement. British historian Arnold J. Toynbee (1889–1975) felt that Christianity would help modern civilization overcome its challenges. The Jeffersonians said that history is not exhausted but that man may begin again in a new world. Besides rejecting the lessons of the past, they Americanized the idea of progress by democratizing and vulgarizing it to include the welfare of the common man as a form of republicanism. As Romantics deeply concerned with the past, collecting source materials and founding historical societies, the Founding Fathers were animated by clear principles. They saw man in control of his destiny, saw virtue as a distinguishing characteristic of a republic, and were concerned with happiness, progress, and prosperity. Thomas Paine, combining the spirit of rationalism and romanticism, pictured a time when America's innocence would sound like a romance, and concluded that the fall of America could mark the end of "the noblest work of human wisdom". Historian J. B. Bury wrote in 1920: To the minds of most people the desirable outcome of human development would be a condition of society in which all the inhabitants of the planet would enjoy a perfectly happy existence. ... It cannot be proved that the unknown destination towards which man is advancing is desirable. The movement may be Progress, or it may be in an undesirable direction and therefore not Progress. ... The Progress of humanity belongs to the same order of ideas as Providence or personal immortality. It is true or it is false, and like them it cannot be proved either true or false. Belief in it is an act of faith. In the postmodernist thought steadily gaining ground from the 1980s, the grandiose claims of the modernizers are steadily eroded, and the very concept of social progress is again questioned and scrutinized. In the new vision, radical modernizers like Joseph Stalin and Mao Zedong appear as totalitarian despots, whose vision of social progress is held to be totally deformed. Postmodernists question the validity of 19th-century and 20th-century notions of progress—both on the capitalist and the Marxist side of the spectrum. They argue that both capitalism and Marxism overemphasize technological achievements and material prosperity while ignoring the value of inner happiness and peace of mind. Postmodernism posits that both dystopia and utopia are one and the same, overarching grand narratives with impossible conclusions. Some 20th-century authors refer to the "Myth of Progress" to refer to the idea that the human condition will inevitably improve. In 1932, English physician Montague David Eder wrote: "The myth of progress states that civilization has moved, is moving, and will move in a desirable direction. Progress is inevitable... Philosophers, men of science and politicians have accepted the idea of the inevitability of progress." Eder argues that the advancement of civilization is leading to greater unhappiness and loss of control in the environment. The strongest critics of the idea of progress complain that it remains a dominant idea in the 21st century, and shows no sign of diminished influence. As one fierce critic, British historian John Gray (b. 1948), concludes: Faith in the liberating power of knowledge is encrypted into modern life. Drawing on some of Europe's most ancient traditions, and daily reinforced by the quickening advance of science, it cannot be given up by an act of will. The interaction of quickening scientific advance with unchanging human needs is a fate that we may perhaps temper, but cannot overcome... Those who hold to the possibility of progress need not fear. The illusion that through science humans can remake the world is an integral part of the modern condition. Renewing the eschatological hopes of the past, progress is an illusion with a future. Recently the idea of progress has been generalized to psychology, being related with the concept of a goal, that is, progress is understood as "what counts as a means of advancing towards the end result of a given defined goal." Antiquity Historian J. B. Bury said that thought in ancient Greece was dominated by the theory of world-cycles or the doctrine of eternal return, and was steeped in a belief parallel to the Judaic "fall of man," but rather from a preceding "Golden Age" of innocence and simplicity. Time was generally regarded as the enemy of humanity which depreciates the value of the world. He credits the Epicureans with having had a potential for leading to the foundation of a theory of progress through their materialistic acceptance of the atomism of Democritus as the explanation for a world without an intervening deity. For them, the earliest condition of men resembled that of the beasts, and from this primitive and miserable condition they laboriously reached the existing state of civilisation, not by external guidance or as a consequence of some initial design, but simply by the exercise of human intelligence throughout a long period. Robert Nisbet and Gertrude Himmelfarb have attributed a notion of progress to other Greeks. Xenophanes said "The gods did not reveal to men all things in the beginning, but men through their own search find in the course of time that which is better." Islamic era With the rise of the Umayyad and Abbasid caliphates and later Ottoman Empire, progress in the Islamic civilizations was characterized by a system of translating books (particularly Greek philosophy books in the Abbasid era) of various cultures into local languages (often Arabic and Persian), testing and refining their scientific or philosophical theories and claims, and then building upon them with their own Islamic ideas, theologies, ontologies, and scientific experimental results. The Round city of Baghdad was characterized as a model and example of progress for the region, where peoples of every religion and race sent their top students to study at its famous international academy called the House of Wisdom. Islamic Spain was also famed as a center of learning in Europe, where Jews and Christians flocked to Muslim halaqas, eager to bring the latest knowledge back to their countries in Europe, which later sparked the European Renaissance due the Muslim scholars' finesse in adapting classical knowledge (such as Greek philosophy) to Abrahamic contexts. Muslim rulers viewed knowledge, including both scientific and philosophical knowledge, as a key to power, and promoted learning, scientific inquiry, and patronization of scholars. Renaissance During the Medieval period, science was to a large extent based on Scholastic (a method of thinking and learning from the Middle Ages) interpretations of Aristotle's work. The Renaissance changed the mindset in Europe, which induced a revolution in curiosity about nature in general and scientific advance, which opened the gates for technical and economic advance. Furthermore, the individual potential was seen as a never-ending quest for being God-like, paving the way for a view of man based on unlimited perfection and progress. Age of Enlightenment (1650–1800) In the Enlightenment, French historian and philosopher Voltaire (1694–1778) was a major proponent of progress. At first Voltaire's thought was informed by the idea of progress coupled with rationalism. His subsequent notion of the historical idea of progress saw science and reason as the driving forces behind societal advancement. Immanuel Kant (1724–1804) argued that progress is neither automatic nor continuous and does not measure knowledge or wealth, but is a painful and largely inadvertent passage from barbarism through civilization toward enlightened culture and the abolition of war. Kant called for education, with the education of humankind seen as a slow process whereby world history propels mankind toward peace through war, international commerce, and enlightened self-interest. Scottish theorist Adam Ferguson (1723–1816) defined human progress as the working out of a divine plan, though he rejected predestination. The difficulties and dangers of life provided the necessary stimuli for human development, while the uniquely human ability to evaluate led to ambition and the conscious striving for excellence. But he never adequately analyzed the competitive and aggressive consequences stemming from his emphasis on ambition even though he envisioned man's lot as a perpetual striving with no earthly culmination. Man found his happiness only in effort. Some scholars consider the idea of progress that was affirmed with the Enlightenment, as a secularization of ideas from early Christianity, and a reworking of ideas from ancient Greece. Romanticism and 19th century In the 19th century, Romantic critics charged that progress did not automatically better the human condition, and in some ways could make it worse. Thomas Malthus (1766–1834) reacted against the concept of progress as set forth by William Godwin and Condorcet because he believed that inequality of conditions is "the best (state) calculated to develop the energies and faculties of man". He said, "Had population and food increased in the same ratio, it is probable that man might never have emerged from the savage state." He argued that man's capacity for improvement has been demonstrated by the growth of his intellect, a form of progress which offsets the distresses engendered by the law of population. German philosopher Friedrich Nietzsche (1844–1900) criticized the idea of progress as the 'weakling's doctrines of optimism,' and advocated undermining concepts such as faith in progress, to allow the strong individual to stand above the plebeian masses. An important part of his thinking consists of the attempt to use the classical model of 'eternal recurrence of the same' to dislodge the idea of progress. Iggers (1965) argues there was general agreement in the late 19th century that the steady accumulation of knowledge and the progressive replacement of conjectural, that is, theological or metaphysical, notions by scientific ones was what created progress. Most scholars concluded this growth of scientific knowledge and methods led to the growth of industry and the transformation of warlike societies into industrial and pacific ones. They agreed as well that there had been a systematic decline of coercion in government, and an increasing role of liberty and of rule by consent. There was more emphasis on impersonal social and historical forces; progress was increasingly seen as the result of an inner logic of society. Marxist theory (late 19th century) Marx developed a theory of historical materialism. He describes the mid-19th-century condition in The Communist Manifesto as follows: The bourgeoisie cannot exist without constantly revolutionizing the instruments of production, and thereby the relations of production, and with them the whole relations of society. Conservation of the old modes of production in unaltered form, was, on the contrary, the first condition of existence for all earlier industrial classes. Constant revolutionizing of production, uninterrupted disturbance of all social conditions, everlasting uncertainty, and agitation distinguish the bourgeois epoch from all earlier ones. All fixed, fast frozen relations, with their train of ancient and venerable prejudices and opinions, are swept away, all new-formed ones become antiquated before they can ossify. All that is solid melts into air, all which is holy is profaned, and man is at last compelled to face with sober senses his real condition of life and his relations with his kind. Furthermore, Marx described the process of social progress, which in his opinion is based on the interaction between the productive forces and the relations of production: No social order is ever destroyed before all the productive forces for which it is sufficient have been developed, and new superior relations of production never replace older ones before the material conditions for their existence have matured within the framework of the old society. Capitalism is thought by Marx as a process of continual change, in which the growth of markets dissolve all fixities in human life, and Marx argues that capitalism is progressive and non-reactionary. Marxism further states that capitalism, in its quest for higher profits and new markets, will inevitably sow the seeds of its own destruction. Marxists believe that, in the future, capitalism will be replaced by socialism and eventually communism. Many advocates of capitalism such as Schumpeter agreed with Marx's analysis of capitalism as a process of continual change through creative destruction, but, unlike Marx, believed and hoped that capitalism could essentially go on forever. Thus, by the beginning of the 20th century, two opposing schools of thought—Marxism and liberalism—believed in the possibility and the desirability of continual change and improvement. Marxists strongly opposed capitalism and the liberals strongly supported it, but the one concept they could both agree on was progress, which affirms the power of human beings to make, improve and reshape their society, with the aid of scientific knowledge, technology and practical experimentation. Modernity denotes cultures that embrace that concept of progress. (This is not the same as modernism, which was the artistic and philosophical response to modernity, some of which embraced technology while rejecting individualism, but more of which rejected modernity entirely.) See also References Further reading External links United Nations Economic and Social Development Islam, Modernity and the Concept of Progress Minorities At Risk (MAR) is a university-based research project that monitors and analyzes the status and conflicts of 283 politically-active communal groups in many countries throughout the world from 1945 to 2006. Those minorities included have been deemed politically significant, meaning that the group collectively suffers or benefits from systematic discriminatory treatment at the hands of other societal groups and the group is the foundation of political mobilization and collective action in defense or promotion of self-defined interests. MAR seeks to identify where the groups are located, what they do, and what happens to them. The project is designed to provide information in a standardized format that aids comparative research and contributes to the understanding of conflicts involving relevant groups. The MAR project was initiated by Ted Robert Gurr in 1986 and has been based at the University of Maryland's Center for International Development and Conflict Management (CIDCM) since 1988. Dataset and requirements Scholars have found the dataset useful for studying ethnic protest, mobilization, and rebellion; however, it does not include all minority groups across the world. There are specific requirements that shape inclusion and statistics of groups analyzed, namely: MAR includes groups only in countries with a population (within the year of interest) greater than 500,000; MAR includes groups only if in the year of interest they numbered at least 100,000 or, if fewer, exceeded 1% of the population of at least one country in which they resided; MAR includes groups separately in each country in which they meet the general criteria. For example, it profiles Kurds separately in Turkey, Iraq, and Iran; MAR includes advantaged minorities like the Sunni Arabs of Iraq and the Overseas Chinese of Southeast Asia, but excludes advantaged majorities; MAR excludes refugee and immigrant groups unless and until they are regarded by outside observers as permanent residents; MAR counts and codes groups at the highest level within-country level of aggregation that is politically meaningful. For example, all Hispanics in the U.S. are profiled as a single group because they are usually regarded and treated by Anglo-Americans as one collectivity; and, MAR estimates membership in a group using the widest demographic definition, even though not all people who nominally are members of a group necessarily identify with it. Group types MAR groups are categorized into six groups which refer to the populations’ past and current struggles on the basis of racial/historical/ethnical variances from the majority population of their country. 1. Ethnonationalist: regionally concentrated peoples with a history of organized political autonomy with their own state, traditional ruler, or regional government who have supported political movements for autonomy at some time since 1945. 2. Indigenous: conquered descendants of earlier inhabitants of a region who live primarily in conformity with traditional social, economic, and cultural customs that are sharply distinct from those of dominant groups. 3. Ethnoclass: ethnically or culturally distinct, usually descended from slaves or immigrants, most of whom occupy a distinct social and economic stratum or niche. 4. Communal Contender: culturally distinct peoples, tribes, or clans in heterogeneous societies who hold or seek a share in state power. -Disadvantaged: subject to some degree of political, economic, or cultural discrimination but lack offsetting advantages -Advantaged: those with political advantage over other groups in their society -Dominant: those with a preponderance of both political and economic power 5. Religious Sect: communal groups that differ from others principally in their religious beliefs and related cultural practices, and whose political status and activities are centered on the defense of their beliefs. 6. National Minority: segments of a trans-state people with a history of organized political autonomy whose kindred control of an adjacent state, but who constitute a minority in the state in which they reside. Phases There are five phases completed thus far for the dataset. Phase I covered 227 communal groups, which met the criteria for classification as a minority at risk for the years 1945–1990; Phase II covered 275 groups from 1990–96; Phase III covered 275 groups from 1996–1999; Phase IV covered 283 groups from 1998–2003; and phase V covered 283 groups from 2003–2006. Also available is the Minorities at Risk Organizational Behavior (MAROB) which began in 2005 with the purpose of answering fundamental questions focusing on the identification of those factors that motivate some members of ethnic minorities to become radicalized, to form activist organizations, and to move from conventional means of politics and protest into violence and terrorism. How it is coded The data is coded by-hand by undergraduate and graduate students under the direct supervision of directors. The information is culled from a variety of sources; journalistic accounts, government reports, group orgs, scholarly material. Coders rely upon multiple sources for each code assigned as often as possible. Variables are broken down into four categories: Group Characteristics, Group Status, External Support, and Group Conflict Behavior. For all of the phases, there are a varying number of variables for which have been coded. Phase V includes 71 core variables and 282 groups. There are codebooks for every phase of recorded data. The variables range from descriptive traits referring to their population and location in the world, to their active protests and rebellions, to the discriminatory practices that affect them. The data allows viewers to see trends by years, country, or conflict. Such an analysis can be done over many years and at both the group and country level. For example, here are the calculated means for three variables, including rebellion, protest, and political discrimination, for African Americans, Hispanics, Native Americans, and Native Hawaiians within the United States. The means for the years 1985–1993 can then be compared to the country's average as well as the overall average for all groups included in the database. African Americans: 1.0 (political discrimination), 3.33 (protest), 0 (rebellion) Hispanics: 1.0 (political discrimination), 3.00 (protest), 1.11 (rebellion) Native Americans: 1.00 (political discrimination), 2.56 (protest), 0 (rebellion) Native Hawaiians: 0 (political discrimination), 2.33 (protest), 0 (rebellion) United States aggregate level: 0.75 (political discrimination), 2.81 (protest), 0.28 (rebellion) Overall average for the database: 1.86 (political discrimination), 1.37 (protest), 0.92 (rebellion) The codebooks allow for an understanding of what each number means (e.g., a 1 for political discrimination represents neglect/remedial policies and a 3 for protest represents small demonstrations numbering less than 10,000 actors). External links MAR Project Minority Group Assessments—MAR Project website == References == Data Colada is a blog dedicated to investigative analysis and replication of academic research, focusing in particular on the validity of findings in the social sciences. It is known for its advocacy against problematic research practices such as p-hacking, and for publishing evidence of data manipulation and research misconduct in several prominent cases, including celebrity professors Dan Ariely and Francesca Gino. Data Colada was established in 2013 by three behavioral science researchers: Uri Simonsohn, a professor at ESADE Business School, Barcelona/Spain (as of 2023), Leif Nelson, a professor at the University of California, Berkeley, and Joe Simmons, a professor at the University of Pennsylvania. History Around 2011, Simmons, Nelson and Simonsohn "bonded over the false, ridiculous, and flashy findings that the field [of behavioral sciences] was capable of producing", such as a paper by Cornell psychologist Daryl Bem that had supposedly found evidence for clairvoyance. They reacted by publishing an influential 2011 paper about false positive results in psychology, illustrating the problem with a parody research finding that supposedly showed that listening to the Beatles song "When I’m Sixty-Four" made experimental subjects one and a half years younger. The "Data Colada" blog was launched two years later, in 2013, carrying the tagline "Thinking about evidence, and vice versa", becoming what The New York Times described as "a hub for nerdy discussions of statistical methods — and, before long, various research crimes and misdemeanors". In particular, the three researchers objected to the then widespread practice of cherry-picking data and attempts to make insignificant results appear statistically credible, especially an approach for which they coined the term p-hacking in a 2014 paper. Notable findings Apart from calling out faulty, but presumably well-intended research practices, Data Colada also published evidence of data manipulations and research misconduct. These include studies about the concept of the moral high ground by psychologist Lawrence Sanna, and research by Flemish psychologist Dirk Smeesters. According to The New Yorker, after Data Colada published their work, the careers of Sanna and Smeesters "came to an unceremonious end". In 2021, Data Colada discovered fabricated data in a 2012 field study published in PNAS by Lisa L. Shu, Nina Mazar, Francesca Gino, Dan Ariely, and Max H. Bazerman. All of the study's authors agreed with their assessment and the paper was retracted. The authors also agreed that Ariely was the only author who had access to the data prior to transmitting it in its fraudulent form to Mazar, the analyst. Ariely denied manipulating the data, but Excel metadata showed that he created the spreadsheet and was the last to edit it. He also admitted to having mislabeled all of the values in an entire column of the data in an e-mail to Mazar shortly after he initially sent her the data. Ariely has stated that someone at the insurance agency that provided the data must have fabricated it. Reception Data Colada's work is credited with contributing awareness to the replication crisis, the idea that many research results in the social sciences are difficult or impossible to reproduce. Data Colada is also recognized for helping to establish better research practices, such as the sharing of replication data. The Nobel-prize winning psychologist Daniel Kahneman described Data Colada in 2023 as "heroes of mine" and expressed his regret about previously endorsing research findings that the blog later showed were faulty. Brian Nosek of the Center for Open Science applauded Data Colada for having "done an amazing job of developing new methodologies to interrogate the credibility of research." On the other hand, as summarized by The New Yorker, "Data Colada's harshest critics saw the young men as jealous upstarts who didn’t understand the soft artistry of the social sciences". Psychologist Norbert Schwarz accused Data Colada and other reformers of engaging in a "witch hunt," while psychologist Daniel Gilbert denounced what he called the "replication police" as "shameless little bullies". Francesca Gino lawsuit In 2021, researcher Zoé Ziani and another collaborator alerted Data Colada about problems replicating work by Harvard University behavioral scientist Francesca Gino. Later that year, the Data Colada team contacted Harvard about anomalies in four papers by Gino. Harvard subsequently conducted its own internal investigation with the help of an outside firm, which discovered additional data alterations besides the cases raised by Data Colada. In June 2023, Harvard Business School placed Gino on unpaid administrative leave after the internal investigation determined she had falsified data in her research. Around the same time, Data Colada published four blog posts detailing evidence that the four papers (all of which had been retracted or set to be retracted at that point), and possibly others by Gino, "contain fake data." Gino subsequently filed a defamation suit against Harvard, Harvard Business School Dean Srikant Datar, and the three members of Data Colada for $25 million, alleging that they had conspired to damage her reputation with false accusations, and that the penalties against her amounted to gender-based discrimination under Title IX. Gino accused Harvard and the Data Colada team of having "worked together to destroy my career and reputation despite admitting they have no evidence proving their allegations." The lawsuit raised concerns about chilling effects. Open science proponent Simine Vazire raised over $370,000 to help cover the legal fees of Data Colada. On September 11, 2024, the judge dismissed all of Gino's claims against the Data Colada defendants (defamation and other claims), and dismissed Gino's defamation and certain other claims (such as violation of privacy) against the Harvard University defendants, while allowing some breach of contract claims against Harvard to continue. References External links Official site Criminal tradition refers to the cultural transmission of criminal values, behaviors, and norms within certain communities. These traditions are often passed from one generation to the next, particularly in environments where crime has become normalized. Like customs in mainstream society, criminal traditions influence how individuals interact, make decisions, and view authority. Theories and Development This concept of a criminal tradition is very much associated with the sociological studies of Clifford R. Shaw and Henry D. McKay. They coined the concept of cultural transmission as a theory that showed that certain urban neighborhoods usually grow to develop these persistent patterns of behavior of crime. From this study, it has been found that such tradition continues even after population shifts by virtue of the social setting reinforcing these criminal values. Edwin H. Sutherland's theory on differential association is among the major theories in this field. According to Sutherland, criminal behavior is acquired through social interaction. Thus, the criminal values of young people, for instance, are derived from associating with other people already possessing this belief. This theory underscored the teaching of criminal behavior rather than an inheritance of it from the home. Research by Shaw and McKay on the concept of cultural transmission indicates that a criminal tradition or subculture does exist in areas of larger cities. According to their studies Criminal tradition arises and is maintained in areas of instability, and the values, norms, and behaviors of the participants in the criminal tradition are viewed as normal by these people. Population categories subject to criminal traditions Criminal traditions are closest to adolescents and young people who are most likely to imitate the behavior of their social group with poor morality put down by neither individual authority. Rather, these traditions would be formed from group norms. This depersonalization makes them more effective, especially in closed settings such as correctional facilities or youth gangs. There it can become embedded in one's manner of daily life, making it difficult to rein in criminal behavior. Researchers often differentiate between these two types of criminal traditions: Historical traditions: They came to us from the criminal environment of the distant past and transformed into modern ones, taking into account the conditions of life of society and the relations dominating in it; Contemporary Traditions: New traditions that have arisen and established in a criminal environment in modern conditions and have no analogues in the past. At the same time, there is a transformation of existing criminal traditions and the emergence of new ones. The reason for this is the changes in the social, economic, legal and other spheres. World criminal tradition Criminal traditions are also present in other parts of the world. For example, in South Africa, numbers gangshave their own set of rules and codes, which are passed down within the gang culture. These traditions influence not only the behavior of the members but also the ways in which gangs operate, both inside and outside of prison environments. Alternative opinions Some researchers think that not everything is bad. Some subcultures may discourage behavior in the interest of order and cleanliness. A hygiene code that might otherwise be considered outside the law in certain communities-as in some prisons-enforces a mode of cleanliness that maintains sanitary standards in the living environment for inmates. Such factors prevent an easy view of all criminal traditions remaining antisocial or harmful. Russian criminal tradition In Russian criminal tradition adherents of the criminal (criminal) tradition are characterized by active participation in the life of the "thieves' community"; Living on tangible assets obtained by criminal means; Propaganda of "thieves' customs and traditions, as well as criminal way of life; Compulsion to keep a word not only before the "brother", but also the criminal-criminal world; Organization of collection of "Obshchak" funds and control over their use; Guardianship and assistance to detainees and convicts, the so-called "vagabonds" and "honest prisoners"; Compliance with the decisions of "gatherings"; Demanding of "brotherhood" and control over their compliance; Organization of counteraction to state bodies. Sources Shaw, C. R., & McKay, H. D. (1942). Juvenile Delinquency and Urban Areas. University of Chicago Press. Sutherland, E. H. (1947). Principles of Criminology (4th ed.). J.B. Lippincott. CRIMINAL RUSSIA - ESSAYS ON CRIME IN THE SOVIET UNION, Author V. CHALIDZE, 1977 A Sociological Technique in Clinical Criminology by Saul D Alinskiy Kapsis, R. E. (1978). RESIDENTIAL SUCCESSION AND DELINQUENCY A Test of Shaw and McKay’s. Theory of Cultural Transmission. Williams, F. P. III, & McShane, M. D. (Eds.). (2007). Criminological Theory: Selected Classic Readings. Tradition == References == A democratic intervention is a military intervention by external forces with the aim of assisting democratization of the country where the intervention takes place. Examples include intervention in Afghanistan and Iraq. Democratic intervention has occurred throughout the mid-twentieth century, as evidenced in the Empire of Japan, Nazi Germany and the Kingdom of Italy after World War II, where democracies were imposed by military intervention. Democratic intervention can be facilitated by the mechanisms of military aggression but can also involve non-aggressive methods. The legal grounds for democratic intervention remain disputed and surround the tension between narrow legislative interpretations and the weak binding nature of international law regimes. States engage in democratic intervention for a variety of reasons, ranging from national interests to international security. Proponents of democratic intervention acknowledge the superiority of democracies to autocratic regimes in facets of peace, economics and human rights. Criticisms of democratic intervention surround the infringement of state sovereignty of the country where the intervention takes place and the failure of democratic intervention to consider a nation's cultural complexities. Methods The various methods that may be used to enact policies of democratic intervention range from military aggression to non-violent means. They include: Military aggression: the use of military force to forcefully impose democratization on another country via occupation or other means. Proxy warfare: the funding, co-ordination and incitement of pro-democratic armed groups against their non-democratic opponents and their non-democratic allies. Economic sanctions: the imposition of commercial and financial penalties against non-democratic states with the intent of inspiring democratization. Information warfare: the use of information and communication technology to spread propaganda and disinformation in non-democratic states to instigate pro-democratic movements. Foreign aid: the provision of resources to encourage and support the transition from autocracy to democracy. History The occupation of Japan and Germany following the victory of the Allies in World War II are early examples of democratic intervention. In Japan during the years 1947 to 1952, the wartime cabinet was removed via military aggression and was replaced by a cabinet acceptable to the Allies and one that would implement the Potsdam Declaration which called for the promotion of a Parliamentary Democracy. The United States Government took an active role in shaping the policies of the democratic intervention into Japan. The United States Government encouraged democratic reform by sending billions of dollars in foreign aid in exchange for increased democratization. General MacArthur outlawed cultural practices such as prostitution for being against the ideals of democracy and imposed the promotion of democratic values in all facets of education. The United States played an influential role in the forced democratization and denazification of post-Nazi Germany in the years following 1945. Democratic intervention into West Germany consisted of foreign Allied military presence, censorship of all German media outlets and persecution of anti-democratic, Nazi sympathizing individuals and movements. 21st century implementation Iraq (2003–2011) Democratic intervention in Iraq started in 2003. Intervention into Iraq was partly inspired by democratic principles and the assumption that installing a democracy in Iraq would inspire further democratization in the Middle East. It involved the overthrowing of Saddam Hussein’s authoritarian regime via the use of military force and replacing it with a parliamentary democracy. The US led Coalition Provisional Authority helped instigate the drafting process of the constitution for Iraq's new democracy. In the year 2005, the US assisted Iraq in holding multi-party democratic elections which in turn materialized the inclusion of the once oppressed Kurdish and Shia populations. The new government was headed by Prime Minister Nouri Al Maliki. Up until 2011, United States military presence continued after authority had been given to the post-invasion government to protect the newly built democratic institutions from violent insurgencies from the Sunni minority. In response to the rise of the Islamic State of Iraq and the Levant (ISIL), the US militarily intervened in 2014 to prevent the spread of violent terrorism and to maintain Iraq's democratic state institutions. Afghanistan (2001–2022) In Afghanistan, the intervening body responsible for supporting the democratization of the Post-Taliban government was the International Security Assistance Force (ISAF). This was a NATO led coalition responsible for training Afghan National Security Forces to protect and develop democratic institutions as well as combat non-democratic Islamist forces such as the Taliban and Al Qaeda. The ISAF was active during the years 2001 to 2014 and were heavily supported by the United States. While the ISAF was disbanded in 2014, the United States continued to play an active role in defending state institutions and democratic bodies from Islamic extremism through the Resolute Support Mission which was founded in 2015. Northern Syria (2015–present) A Western coalition composed of the United States, France and other allies intervened in the Syrian civil war in support of the Syrian Democratic Forces (SDF) which was founded in 2015. The SDF is an alliance of Syria and Kurdish militias committed to the development of a secular, democratic state in North Syria. Western intervention was composed of ammunition supplies, air raids, ground offensives and military personnel against Islamist anti-democratic forces such as Al Nusra and ISIL. Legal grounds Chapter VII of the UN Charter allows the Security Council to militarily intervene if there is a 'threat to the peace, breach of the peace or act of aggression.' It does not explicitly provide an avenue for intervention on the grounds of democratization. Article 2(4) outlaws the use of force and provides no suggestion nor does it imply that there may be motives underpinning military aggression that may nullify the article. The article stipulates that force that undermines and opposes the political independence of a state is illegal. This domain of illegality is broader than cross-border military action and outlaws both the use of force and the threat of its use for any political means, including democratization. Ian Hurd, Associate Professor at the Northwestern University Department of Political science affirms that a strict legal interpretation of international law prohibits any legal ground for democratic intervention undertaken by states. Cases for legality suggests that the ban on any form of military intervention in Article 2(4) has lost legal force after repeated violations by states. Thomas Franck, in 1970, suggests that the twenty-five-year history post UN Charter, which has seen various uses of force violating international law, signals that the illegality of intervention into states has eroded beyond recognition. This argument was reinforced by Michael Glennon who conceded that the general banning of the use force has collapsed. Frank references the general notion in international legal scholarship that rules lose their force if they are frequently violated, demonstrating that democratic intervention could be legally acceptable on that basis. Causes The Bueno de Mesquita model suggests that given governments are concerned with their own domestic actors who can depose them, states that engage in democratic intervention do so to maximize their political survival. The model demonstrates that states will pursue democratic intervention if it will be perceived by electorates to further national interests or benefit a gago portion of the world population. George Downs, political scientist at New York University provides an example of this model. Downs demonstrates that the buildup of democratic institutions in the post-war occupation of Japan and Germany was underpinned by the belief that their citizens would not politically support the emergence of militarism thus preventing another world war. The World Bank proposed that the notion of democratic peace induces democratic intervention. Democracies rarely engage in interstate warfare, as confirmed by studies conducted by foreign policy analysts Rakerud and Hegre. Their statistical studies estimate that two democracies have a 57% lower probability of engaging in interstate warfare than a mixed pair and a 35% lower probability than a non-democratic pair. Levy argues that this relationship has been labelled as one of the closest principles to an empirical law in world politics. German sociologist Erich Weede disagrees with that notion but nonetheless concedes that the relationship is very strong. Views held by states Western liberal democracies have historically and publicly supported democratic intervention on the basis that it would reduce military conflicts between modern nation states. In 1992, James Baker, Secretary of State in the George H.W. Bush Administration, stated ‘real democracies do not go to war with one another’. Former US President Bill Clinton affirmed the United States acceptance of this by stating in his State of the Union address ‘Democracies don't attack each other.’ The United Kingdom has also echoed these exact sentiments as former Prime Minister Margaret Thatcher was quoted to have said ‘Democracies do not attack each other.’ Non-western states however, like China, have not encouraged the promotion of democracy and have intervened at the UN Security Council against enforced democratization proposals, as well as humanitarian interventions. China regularly Vetoed military intervention against the autocratic Syrian Arab Republic during the years 2011 to 2016. Proponents Scholars in the field of democratic development such as Larry Diamond, Juan J. Linz, and Seymour Martin Lipse affirm “Democracy was imposed on Germany, Italy, and Japan, and surprisingly took hold and endured”. Gary Dempsey, a foreign-policy analyst at Cato Institute, states that Western intervention was responsible for turning Japan and Germany into self-sustaining peaceful, democracies. Liberal proponents see democratic intervention in a mutual relationship with other liberal values such as economic interdependence, international law and effective organization. Russel and Oneal also believe enforcing these values strengthens both international and domestic peace, and as a result, also strengthens the case for increased democratic intervention. Holmes in his book 'Passions and Constraint: On the Theory of Liberal Democracy' proposes that democratic intervention would enhance the lives of those living in the target state through the provision of what he sees as widely supported values such as individual liberty, freedom of speech and freedom of expression. Huntington, former director of Harvard's Centre for International Affairs, also proposes the imposition of liberal democracies would result less death as a result of civil unrest or government instigated violence. His studies have confirmed this trend as they find only 0.14% of a population living under a democracy die annually, which is lower than the 1.48% figure characterizing totalitarian regimes. Rudolph Rummel, founder of the democratic peace theory, reasons that Huntington's statistical trend stems from the sentiment that should a government be committed to democratic principles, critics of the state would not need to stage violent insurgencies, but rather, engage in the electoral system. Economic benefits of democratic institutions have also been cited by proponents. In a 1996 Wall Street Journal analysis, transitions from autocracy to democracy in the years 1980–1993 have seen GDP increases top 4.32%. Strobe Talbott, former US Deputy Secretary of State, argued that the political legitimacy afforded by democracies, even if imposed by intervention, allows states to better implement economic reform that would be accepted by their electorate. Criticisms Normative criticism The social theorist David Beetham states that democratic intervention infringes the sovereignty of the target state. He proposes that the intervening state risks being an illegitimate occupying force, which in turns incurs resistance from within the target state resulting in intensified insecurity. Beetham also argues that intervention may not be able to achieve democratization if outweighed by the other interests of the intervening state. De Mesquita and Downs proposed that the US military and hegemonic interests are reasons as to why democratic interventions into Iraq and Afghanistan failed to produce self-sustaining and effective democracies. Democratic intervention is also scrutinized by Beetham for the loss of human life it has caused. US intervention into Iraq had resulted in hundreds of thousands of Iraqis killed or wounded with four million displaced internally or living refugees in other countries. Destabilizing effect Democratic intervention is also criticized on the basis that it ignores the social conditions and dynamics that define the target state. Feffer and Junes of Foreign Policy in Focus suggest a failure to understand the communities and a unified sense of nationhood would result in democratic intervention stirring and inciting civil unrest, as demonstrated in the sectarian conflicts which were considered a symptom of the US intervention into Iraq. German and Japanese revisionist critics have also stated that enforced democratization policies resulted in negative outcomes such as ‘distorting cultural perspectives’ and compare democratic intervention to colonialism. Military intervention is counterproductive The concept of democratic peace is confusing cause and effect, according to the territorial peace theory. Historical studies show that peace is a necessary prerequisite for democracy. War leads to authoritarianism and disregard for democracy, while peace and stable borders of a country motivates tolerance and support for democratic ideals. Military interventions are likely to impede a transition to democracy rather than promoting it. In fact, almost all attempts to impose democracy with military means have failed. Democracy may improve peaceful relations between non-neighbor countries and between countries that are already at peace, but not between neighbor countries involved in territorial conflict. == References == The Brahma marriage (Sanskrit: ब्राह्मविवाह, romanized: Brāhmavivāha) is a righteous form of marriage in described in Hindu texts. It refers to the marriage of one's daughter to a man of good conduct, learned in the Vedas, and invited by oneself. Originally intended only for the Brahmins, a Brahma marriage is where a boy is able to get married once he has completed his education in the first stage of life, the Brahmacharya. Brahma marriage holds the supreme position of the eight types of Hindu matrimony. When the parents of a boy seek a suitable bride, they consider her family background, and the girl's father would ensure that his daughter's prospective groom is a scholar, one who is well-versed in the Vedas. This form of marriage is described in the Manusmriti. Description This form of marriage held that the son born of a marriage redeems the sins of ten ancestors, ten descendants, and himself. In the Mahabharata, it is found that the Kshatriyas practiced the Brahma form of marriage, although as suggested by its name, it was mostly practiced by Brahmins. The Brahma marriage is most common type of arranged marriage nowadays, wherein the father of a girl (bride) marries her to a man (groom) of good conduct. == References == In ancient Rome, the plebeians or plebs were the general body of free Roman citizens who were not patricians, as determined by the census, or in other words "commoners". Both classes were hereditary. Etymology The precise origins of the group and the term are unclear, but may be related to the Greek, plēthos, meaning masses. In Latin, the word plebs is a singular collective noun, and its genitive is plebis. Plebeians were not a monolithic social class. In ancient Rome In the annalistic tradition of Livy and Dionysius, the distinction between patricians and plebeians was as old as Rome itself, instituted by Romulus' appointment of the first hundred senators, whose descendants became the patriciate. Modern hypotheses date the distinction "anywhere from the regal period to the late fifth century" BC. The 19th-century historian Barthold Georg Niebuhr believed plebeians were possibly foreigners immigrating from other parts of Italy. This hypothesis, that plebeians were racially distinct from patricians, however, is not supported by the ancient evidence. Alternatively, the patriciate may have been defined by their monopolisation of hereditary priesthoods that granted ex officio membership in the senate. Patricians also may have emerged from a nucleus of the rich religious leaders who formed themselves into a closed elite after accomplishing the expulsion of the kings. In the early Roman Republic, there are attested 43 clan names, of which 10 are plebeian with 17 of uncertain status. There existed an aristocracy of wealthy families in the regal period, but "a clear-cut distinction of birth does not seem to have become important before the foundation of the Republic". The literary sources hold that in the early Republic, plebeians were excluded from magistracies, religious colleges, and the Senate. However, some scholars doubt that patricians monopolised the magistracies of the early republic, as plebeian names appear in the lists of Roman magistrates back to the fifth century BC. It is likely that patricians, over the course of the first half of the fifth century, were able to close off high political office from plebeians and exclude plebeians from permanent social integration through marriage. Plebeians were enrolled into the curiae and the tribes; they also served in the army and also in army officer roles as tribuni militum. Conflict of the Orders The Conflict of the Orders (Latin: ordo meaning "social rank") refers to a struggle by plebeians for full political rights from the patricians. According to Roman tradition, shortly after the establishment of the Republic, plebeians objected to their exclusion from power and exploitation by the patricians. The plebeians were able to achieve their political goals by a series of secessions from the city: "a combination of mutiny and a strike". Ancient Roman tradition claimed that the Conflict led to laws being published, written down, and given open access starting in 494 BC with the law of the Twelve Tables, which also introduced the concept of equality before the law, often referred to in Latin as libertas, which became foundational to republican politics. This succession also forced the creation of plebeian tribunes with authority to defend plebeian interests. Following this, there was a period of consular tribunes who shared power between plebeians and patricians in various years, but the consular tribunes apparently were not endowed with religious authority. In 445 BC, the lex Canuleia permitted intermarriage among plebeians and patricians. There was a radical reform in 367–6 BC, which abolished consular tribunes and "laid the foundation for a system of government led by two consuls, shared between patricians and plebeians" over the religious objections of patricians, requiring at least one of the consuls to be a plebeian. And after 342 BC, plebeians regularly attained the consulship. Debt bondage was abolished in 326, freeing plebeians from the possibility of slavery by patrician creditors. By 287, with the passage of the lex Hortensia, plebiscites – or laws passed by the concilium plebis – were made binding on the whole Roman people. Moreover, it banned senatorial vetoes of plebeian council laws. And also around the year 300 BC, the priesthoods also were shared between patricians and plebeians, ending the "last significant barrier to plebeian emancipation". The veracity of the traditional story is profoundly unclear: "many aspects of the story as it has come down to us must be wrong, heavily modernised... or still much more myth than history". Substantial portions of the rhetoric put into the mouths of the plebeian reformers of the early Republic are likely imaginative reconstructions reflecting the late republican politics of their writers. Contradicting claims that plebs were excluded from politics from the fall of the monarchy, plebeians appear in the consular lists during the early fifth century BC. The form of the state may also have been substantially different, with a temporary ad hoc "senate", not taking on fully classical elements for more than a century from the republic's establishment. Noble plebeians The completion of plebeian political emancipation was founded on a republican ideal dominated by nobiles, who were defined not by caste or heredity, but by their accession to the high offices of state, elected from both patrician and plebeian families. There was substantial convergence in this class of people, with a complex culture of preserving the memory of and celebrating one's political accomplishments and those of one's ancestors. This culture also focused considerably on achievements in terms of war and personal merit. Throughout the Second Samnite War (326–304 BC), plebeians who had risen to power through these social reforms began to acquire the aura of nobilitas ("nobility", also "fame, renown"), marking the creation of a ruling elite of nobiles. From the mid-4th century to the early 3rd century BC, several plebeian–patrician "tickets" for the consulship repeated joint terms, suggesting a deliberate political strategy of cooperation. No contemporary definition of nobilis or novus homo (a person entering the nobility) exists; Mommsen, positively referenced by Brunt (1982), said the nobiles were patricians, patrician whose families had become plebeian (in a conjectural transitio ad plebem), and plebeians who had held curule offices (e.g., dictator, consul, praetor, and curule aedile). Becoming a senator after election to a quaestorship did not make a man a nobilis, only those who were entitled to a curule seat were nobiles. However, by the time of Cicero in the post-Sullan Republic, the definition of nobilis had shifted. Now, nobilis came to refer only to former consuls and the direct relatives and male descendants thereof. The new focus on the consulship "can be directly related to the many other displays of pedigree and family heritage that became increasingly common after Sulla" and with the expanded senate and number of praetors diluting the honour of the lower offices. A person becoming nobilis by election to the consulate was a novus homo (a new man). Marius and Cicero are notable examples of novi homines (new men) in the late Republic, when many of Rome's richest and most powerful men – such as Lucullus, Marcus Crassus, and Pompey – were plebeian nobles. Later history In the later Republic, the term lost its indication of a social order or formal hereditary class, becoming used instead to refer to citizens of lower socio-economic status. By the early empire, the word was used to refer to people who were not senators (of the empire or of the local municipalities) or equestrians. Life Much less is known about the plebeians than the patricians in Ancient Rome, as most could not write, and thus could not record what happened in their daily life. Childhood and education The average plebeian did not come into a wealthy family; the politically active nobiles as a whole comprised a very small portion of the whole population. The average plebeian child was expected to enter the workforce at a young age. Education was limited to what their parent would teach them, which consisted of only learning the very basics of writing, reading and mathematics. Wealthier plebeians were able to send their children to schools or hire a private tutor. Family life Throughout Roman society at all levels including plebeians, the paterfamilias (oldest male in the family) held ultimate authority over household manners. Sons could have no authority over fathers at any point in their life. Women had a subservient position in the family to fathers and husbands. Living quarters Plebeians who lived in the cities were referred to as plebs urbana. Plebeians in ancient Rome lived in three or four-storey buildings called insula, apartment buildings that housed many families. These apartments usually lacked running water and heat. These buildings had no bathrooms and was common for a pot to be used. The quality of these buildings varied. Accessing upper floors was done via a staircase from the street they were built on. Sometimes these were built around a courtyard and of these, some were built around a courtyard containing a cistern. Lower floors were of higher quality while the higher ones were less so. By the beginning of the Roman Empire, the insulae were deemed to be so dangerous because of a risk to collapse that Emperor Augustus passed a law limiting the height of the buildings to 18 metres (59 ft) but it appeared this law was not closely followed as buildings appeared that were six or seven floors high. Plebeian apartments had frescoes and mosaics on them to serve as decorations. Rents for housing in cities was often high because of the amount of demand and simultaneously low supply. Rents were higher in Rome than other cities in Italy along with other provincial cities. The owner of the insulae did not attend to duties regarding it and instead used an insularius who was most often an educated slave or a freedman instead. Their job was to collect rent from tenants, manage disputes between individual tenants and be responsible for maintenance. Not all plebeians lived in these conditions, as some wealthier plebs were able to live in single-family homes, called a domus. Another type of housing that existed was diversorias (lodging houses) Tabernae which were made of timber frames and wicker walls open to streets with the exception of shutters being one to two floors high with tightly packed spaces. Attire Plebeian men wore a tunic, generally made of wool felt or inexpensive material, with a belt at the waist, as well as sandals. Meanwhile, women wore a long dress called a stola. Roman fashion trends changed very little over the course of many centuries. However, hairstyles and facial hair patterns changed as initially early plebeian men had beards before a clean shaven look became more popular during the Republican era before having facial hair was popularized again by Emperor Hadrian in the 2nd century AD. Some plebeian women would wear cosmetics made from charcoal and chalk. Romans generally wore clothes with bright colors and did wear a variety of jewelry. Meals Since meat was very expensive, animal products such as pork, beef and veal would have been considered a delicacy to plebeians. Instead, a plebeian diet mainly consisted of bread and vegetables. Common flavouring for their food included honey, vinegar and different herbs and spices. A well-known condiment to this day known as garum, which is a fish sauce, was also largely consumed. Apartments often did not have kitchens in them so families would get food from restaurants and/or bars. Recreation and entertainment One popular outlet of entertainment for Roman plebeians was to attend large entertainment events such as gladiator matches, military parades, religious festivals and chariot races. As time went on, politicians increased the number of games in an attempt to win over votes and make the plebeians happy. A popular dice game among plebeians was called alea. Financial status Plebeians who resided in urban areas had to often deal with job insecurity, low pay, unemployment and high prices along with underemployment. A standard workday lasted for 6 hours although the length of the hours varied as Romans divided the day into 12 daytime hours and 12 nighttime hours; with the hours being determined based on the seasons. Cicero wrote in the late republican period that he estimated the average laborer working in the city of Rome earned 6 1/2 denarii a day which was 5 times what a provincial worker would make. By middle of the 1st century AD this number was higher because of inflation but however the high cost of living in the city of Rome kept the value of real wages down. Some plebeians would sell themselves into slavery or their children in order to have access to wealthy households and to them hopefully advance socially along with getting a chance to have an education. Another way plebeians would try to advance themselves was by joining the military which became easier after the Marian reforms as soldiers were expected to pay for their own weapons. By joining the military they could get a fixed salary, share of war loot along with a pension and an allotted land parcel. There was also the reward of getting citizenship for non-citizens. Potential recruits needed to meet a variety of requirements as well which included: being male, at least 172 centimetres (5 ft 8 in) tall, enlist before one was 35, having a letter of recommendation and completing training. Derivatives United States military academies In the U.S. military, plebes are freshmen at the U.S. Military Academy, U.S. Naval Academy, Valley Forge Military Academy and College, the Marine Military Academy, the U.S. Merchant Marine Academy, Georgia Military College (only for the first quarter), and California Maritime Academy. Philippine Military Academy Since the construction of Philippine Military Academy, the system and traditions were programmed the same as the United States Military Academy. First Year Cadets in PMA are called Plebes or Plebos (short term for Fourth Class Cadets) because they are still civilian antiques and they are expected to master first the spirit of Followership. As plebes, they are also expected to become the "working force (force men or "porsmen") in the Corps of Cadets. British and Commonwealth usage In British, Irish, Australian, New Zealand and South African English, the back-formation pleb, along with the more recently derived adjectival form plebby, is used as a derogatory term for someone considered unsophisticated, uncultured, or lower class. In popular culture The British comedy show Plebs followed plebeians during ancient Rome. In Margaret Atwood's novel Oryx and Crake, there is a major class divide. The rich and educated live in safeguarded facilities while others live in dilapidated cities referred to as the "pleeblands". See also Bread and circuses – Figure of speech referring to a superficial means of appeasement Capite censi – Lowest class of citizens of ancient Rome Plebeian Council – Principal assembly of the Roman RepublicPages displaying short descriptions of redirect targets Proletariat – Class of wage-earners Plebgate (aka Plodgate or Gategate), a 2012 British political scandal involving the use of the word as a slur References Citations Sources Further reading Ferenczy, Endre (1976). From the Patrician State to the Patricio-Plebeian State. Amsterdam: A.M. Hakkert. Horsfall, Nicholas (2003). The Culture of the Roman Plebs. London: Duckworth. Millar, Fergus (2002). The Crowd In Rome In the Late Republic. Ann Arbor: University of Michigan Press. Mitchell, Richard E. (1990). Patricians and plebeians: The origin of the Roman state. Ithaca: Cornell University Press. Morstein-Marx, Robert (2004). Mass Oratory and Political Power in the Late Roman Republic. doi:10.1017/CBO9780511482878. ISBN 9780511482878. Mouritsen, Henrik (2001). Plebsand Politics in the Late Roman Republic. doi:10.1017/CBO9780511482885. ISBN 9780511482885. Raaflaub, Kurt A., ed. (2005). Social Struggles in Archaic Rome. doi:10.1002/9780470752753. ISBN 9780470752753. Vanderbroeck, Paul J. J. (1987). Popular leadership and collective behavior in the late Roman Republic (ca. 80–50 B.C.). Amsterdam: Gieben. Vishnia, Rachel Feig (1996). State, Society, and Popular Leaders In Mid-Republican Rome 241–167 BC. London: Routledge. Williamson, Caroline (2005). The Laws of the Roman People. doi:10.3998/mpub.15992. ISBN 9780472110537. External links Smith's Dictionary of Greek and Roman Antiquities, article Plebs Livius.org: Plebs Texts on Wikisource: "Plebeians". Collier's New Encyclopedia. 1921. "Plebeians". Encyclopedia Americana. 1920. "Plebs". Encyclopædia Britannica (11th ed.). 1911. A security operations center (SOC) is responsible for protecting an organization against cyber threats. SOC analysts perform round-the-clock monitoring of an organization’s network and investigate any potential security incidents. If a cyberattack is detected, the SOC analysts are responsible for taking any steps necessary to remediate it. It comprises the three building blocks for managing and enhancing an organization's security posture: people, processes, and technology. Thereby, governance and compliance provide a framework, tying together these building blocks. A SOC within a building or facility is a central location from which staff supervises the site using data processing technology. Typically, a SOC is equipped for access monitoring and control of lighting, alarms, and vehicle barriers. SOC can be either internal or external. In latter case the organization outsources the security services, such monitoring, detection and analysis, from a Managed Security Service Provider (MSSP). This is typical to small organizations which don't have the resources to hire, train, and technically equip cybersecurity analysts. IT An information security operations center (ISOC) is a dedicated site where enterprise information systems (web sites, applications, databases, data centers and servers, networks, desktops and other endpoints) are monitored, assessed, and defended. The United States government The Transportation Security Administration in the United States has implemented security operations centers for most airports that have federalized security. The primary function of TSA security operations centers is to act as a communication hub for security personnel, law enforcement, airport personnel and various other agencies involved in the daily operations of airports. SOCs are staffed 24-hours a day by SOC watch officers. Security operations center watch officers are trained in all aspects of airport and aviation security and are often required to work abnormal shifts. SOC watch officers also ensure that TSA personnel follow proper protocol in dealing with airport security operations. The SOC is usually the first to be notified of incidents at airports such as the discovery of prohibited items/contraband, weapons, explosives, hazardous materials as well as incidents regarding flight delays, unruly passengers, injuries, damaged equipment and various other types of potential security threats. The SOC in turn relays all information pertaining to these incidents to TSA federal security directors, law enforcement and TSA headquarters. See also National SIGINT Operations Centre == References == The following outline is provided as an overview of and topical guide to transgender topics. The term "transgender" is multi-faceted and complex, especially where consensual and precise definitions have not yet been reached. While often the best way to find out how people identify themselves is to ask them, not all persons who might be thought of as falling under the transgender 'umbrella' identify as such. Transgender can also be distinguished from intersex, a term for people born with physical sex characteristics "that do not fit typical binary notions of male or female bodies". Books and articles written about transgender people or culture are often outdated by the time they are published, if not already outdated at the time of composition, due to inappropriate and/or outdated questions or premises. Psychology, medicine, and social sciences research, aid, or otherwise interact with or study transgender people. Each field starts from a different point of view, offers different perspectives, and uses different nomenclature. This difference is mirrored by the attitude of transgender people regarding transgender issues, which can be seen in the articles listed below. People and behaviour Transgender Trans man Trans woman Transgender youth List of transgender people Transsexual Non-binary List of people with non-binary gender identities Genderfluidity Gender neutrality Androgyny Gender bender Gender variance Packing (phallus) Tucking Chest binding Breast prostheses Shemale Third gender Transsexual pornography Other gender non-conforming behaviour Cross-dressing Transvestism Dual-role transvestism Drag Drag queen Drag king Faux queen En travesti Pantomime dame Feminization (activity) Transvestic fetishism In non-Western cultures Akava'ine (Cook Islands) Bakla (Philippines) Bissu (Indonesia) Calabai (Indonesia) Eunuch Fakaleiti (Tonga) Fa'afafine (Samoa) Femminiello (Neapolitan) Galli (ancient Rome) Hijra (South Asia) Kathoey (Thailand) Khanith (Arabia) Khawal (Egypt) Koekchuch (Siberia) Köçek (Turkey) Māhū (Hawaii) Maknyah (Malaysia) Meti (Nepal) Mudoko dako (the Langi in Uganda) Mukhannathun (Arabia) Muxe (Mexico) Newhalf ("ニューハーフ") (Japan) Toms and dees (Thailand) Tom-Dee identity (Thailand) Balkan sworn virgins (Balkan) Takatāpui (Māori) Travesti (Brazil) Two-Spirit (North American Natives) Waria (Indonesia) Winkte (Native American) Basic terms Sex–gender distinction Gender Agender Transfeminine Transmasculine Bigender Cisgender Gender binary Gender blind Gender identity Gender dysphoria Gender euphoria Gender role Real-life experience (transgender) Gender variance Non-binary Pangender Terminology of transgender anatomy Third gender Trigender Sex Sex assignment Assigned female at birth Assigned male at birth Sexual characteristics Sex organ or primary sexual characteristics Secondary sex characteristics Sex-determination system Intersex Disorders of sex development Hermaphrodite Endosex Sexual orientation and behaviour Sexual orientation and behaviour are independent from gender identity; since both are often mentioned together or even confused, some relevant topics are mentioned here. The first article elaborates on this question. Sexual orientation Transgender sexuality Gynephilia and androphilia LGBTQ culture also contains a section on transgender Sexuality and gender identity-based cultures Sexual identity LGBTQ Gay Lesbian Bisexuality Pansexuality Asexuality Heterosexuality Other Heteronormativity Cisnormativity LGBTQ Discrimination against non-binary gender people Queer Transgender Day of Remembrance International Transgender Day of Visibility Transphobia Trans panic defense Violence against transgender people Law and rights Legal status of transgender people Legal recognition of non-binary gender Legal status of gender-affirming healthcare Name change List of transgender-rights organizations List of transgender political office-holders List of transgender politicians in Australia List of transgender public officeholders in the United States Yogyakarta Principles Transgender history By country Argentina Transgender rights in Argentina Australia List of transgender politicians in Australia Transgender rights in Australia Re Kevin – validity of marriage of transsexual Brazil Transgender rights in Brazil ADPF 787 Transgender history in Brazil Canada An Act to amend the Canadian Human Rights Act and the Criminal Code Transgender rights in Canada China Transgender in China Finland Transgender history in Finland Germany Transgender rights in Germany Transgender people in Nazi Germany India Rights of Transgender Persons Bill, 2014 Transgender rights in Tamil Nadu Iran Transgender rights in Iran Ireland Transgender rights in Ireland New Zealand Transgender rights in New Zealand Singapore Transgender people in Singapore South Africa Alteration of Sex Description and Sex Status Act, 2003 South Korea Transgender people in South Korea Turkey March against Homophobia and Transphobia United Kingdom Anti-transgender movement in the United Kingdom Transgender rights in the United Kingdom Gender Recognition Act 2004 Gender Recognition Panel United States Compton's Cafeteria riot Gender identity under Title IX History of transgender people in the United States List of transgender public officeholders in the United States Transgender disenfranchisement in the United States Transgender legal history in the United States Transgender rights in the United States Transphobia in the United States Discrimination Anti-gender movement United Kingdom Bathroom bill Deadnaming Discrimination against non-binary people Discrimination against transgender men History of violence against LGBT people in the United States List of people killed for being transgender TERF (acronym) Trans exclusionary radical feminist Transgender genocide Transgender inequality Transmisogyny Transphobia Violence against transgender people Medicine Gender dysphoria Gender dysphoria in children Sexual relationship disorder Sexual maturation disorder Ego-dystonic sexual orientation Body integrity identity disorder World Professional Association for Transgender Health Standards of Care for the Health of Transgender and Gender Diverse People Transgender health care Misinformation Puberty blocker Transgender voice therapy Gender-affirming hormone therapy Feminizing hormone therapy Masculinizing hormone therapy Gender-affirming surgery Gender-affirming surgery (male-to-female) Breast augmentation Orchiectomy Trachea shave Vulvoplasty Vaginoplasty Voice feminization Gender-affirming surgery (female-to-male) Male chest reconstruction Facial masculinization surgery Phalloplasty Metoidioplasty Scrotoplasty Classification and causes Causes of transsexuality Classification of transsexual people Sexual diversity studies Anima (Jung) Feminism Transfeminism Feminist views on transgender and transsexual people Gender studies Queer studies Queer theory Scholars Judith Butler Leslie Feinberg Eve Kosofsky Sedgwick Susan Stryker Social transition Closeted Coming out Detransition Rapid-onset gender dysphoria controversy Passing (gender) Questioning (sexuality and gender) Gender transition Real-life experience (transgender) Society Art Transgender art and artists include: Transgender literature New Media Art: Sandy Stone (artist) – ACT Lab Shu Lea Cheang – Brandon Performance: S. Bear Bergman Kate Bornstein Micha Cárdenas Willi Pape Music: Transgender representation in hip hop music Butterfly Music Transgender Chorus Genesis P-Orridge Ryan Cassata Laura Jane Grace Sophie Cavetown (musician) List of Transgender Woman Musicians Photography: Claude Cahun Loren Cameron Yishay Garbasz Film: Barbara Hammer – Lover Other Media Transgender publications Media portrayals of transgender people Film and television List of transgender characters in film and television Cross-dressing in film and television Comics Assigned Male Claudine...! Wandering Son Rain Books Herma by MacDonald Harris (ISBN 0-689-11179-7) the tale of a hermaphrodite as the central character, who is transformed from an opera singer (female) to an aviator (male) at the turn of the 20th century into World War One. Last Exit To Brooklyn by Hubert Selby, Jr. one of the stories revolve around a group of transvestites, led by a girl named Georgette. Masculinities Without Men? (ISBN 0-7748-0997-3) by Jean Bobby Noble Armistead Maupin's Tales of the City series includes a transgender person as a central character. Luna (ISBN 0-316-73369-5) by Julie Anne Peters Whipping Girl by Julia Serano Becoming, a gender flip book (ISBN 1935613006) by Yishay Garbasz a flip book with images of the artist one year before and one year after her gender affirmation surgery. Sport Transgender people in sports Transgender people in ice hockey Sex verification in sports Religion Transgender people and religion Christianity and transgender people Military service Transgender people and military service Transgender personnel in the South Korean military Transgender personnel in the United States military Gender-variant people or behaviour Many other terms describe gender-variant people or behaviour, without the people being described necessarily being transgender: Amazon Eunuch Butch and femme Tomboy Religion The cult of Aphroditus, the androgynous Amathusian Aphrodite in Greek mythology. Galli, the transgender priests of the Phrygian goddess Cybele and her consort Attis. The Sisters of Perpetual Indulgence, a group of (mostly) gay male nuns who take vows to promulgate universal joy and expiate stigmatic guilt. Skoptsy, religious sect in early 20th Century imperial Russia that practiced castration and mastectomies. Miscellaneous Homelessness among LGBT youth in the United States International Day Against Homophobia, Transphobia and Biphobia Stonewall riots Transgender flag LGBT people in prison Neuroqueer theory == References == Wars or conflicts can break out between different groups in some ant species for a variety of reasons. These violent confrontations typically involve entire colonies, sometimes allied with each other, and can end in a stalemate, the complete destruction of one of the belligerents, the migration of one of the groups, or, in some cases, the establishment of cordial relations between the different combatants or the adoption of members of the losing group. For some species of ants, this is even a deliberately undertaken strategy, as they require capturing pupae from other species to ensure the continuity of their colony. Thus, there are specific biological evolutions in certain species intended to give them an advantage in such conflicts. In some of these confrontations, ants can adopt ritualized behavior, even governed by certain implicit rules, for example by organizing duels between the most important ants of each colony or choosing a specific location for a battle. They should not be confused with social conflicts inside the same colony or supercolony of ants. These conflicts are not simply internal to ants, which can fight each other even within the same species, but also involve other animals, particularly other eusocial insects like termites or wasps. In the early 21st century, with the rapid spread of many species into new habitats facilitated by human colonization, significant wars are being waged between different supercolonies. Terminology The use of the term "war", found in scientific literature, is an anthropocentric analogy, derived from human wars. Causes and prevalence Causes The reasons that can lead ant colonies to clash are varied and depend on the species, locations, and contexts. For a number of them, such as leafcutter ants Atta laevigata, wood ants of the genus Formica, certain species of the genus Carebara, or giant ants Dinomyrmex gigas, it is a matter of territory covered and thus the available food for the different colonies. It can also be related to issues of overpopulation of the same species in the same area at certain times of the year. In other cases, some species aim to capture the pupae of an opposing group to use them in their own colony later. Prevalence It is difficult to assess the prevalence of this type of behavior in ants, given the significant diversity of species, behaviors, and different situations. Some species undergo specific evolutions with the sole purpose of engaging in these conflicts, such as Polyergus rufescens, which have sickle-shaped mandibles. The emergence of supercolonies from the 19th century, facilitated by human movements, has certainly reinforced these behaviors in the affected ants. It also seems to depend on the context in which the ants find themselves. For instance, within the same species, a colony facing external threats from another ant colony can produce up to twice as many soldier larvae as a colony not experiencing the same pressures. Some species are almost exclusively on defensive strategies, such as Camponotus ligniperdus, which are peaceful and occupy a small territory but defend it fiercely against any incursion, even against more dangerous or deadly species. Process and outcomes Process In general, there are two main ways ants conduct these conflicts. On the one hand, some species use specific ants that are more powerful and whose primary function is to fight. On the other hand, colonies increase the number of available fighters and send large numbers of individuals into battle. In some species, conflict is ritualized, for example through limited duels undertaken by the individuals most capable of combat, but phenomena of battles are also common. In the genus Formica, such battles are commonplace and can involve tens of thousands of individuals, and they are sometimes ritualized, with the respective groups withdrawing at nightfall only to return the next day to the same locations to resume the battle. The bodies of dead or injured ants are then brought back to the colony, where they are eaten. In other species, such as within the genus Carebara, ants arrange themselves in specific formations before the battle, like phalanxes, and advance against each other. They also regularly sacrifice workers, whose role is to try to hinder, injure, and attack enemy majors, before their own majors join the battlefield and can intervene. In other cases, particularly among ants that aim to capture larvae or pupae, colonies use chemical weapons, such as olfactory propaganda, to try to enter the targeted colonies as discreetly as possible. Outcomes Generally, wars between ants are costly for the groups, which must allocate a significant portion of their production to the war effort, to the detriment of forming workers, for example. These wars can result in the death of tens of thousands of individuals within a few hours; for wood ants of the genus Formica, there are regularly 10,000 casualties per day during the spring. For these ants, the war ends either when the opposing colony is destroyed or when the available prey is sufficient again for the needs of the colonies, which have then lost thousands of members. Estimates from 2016 on certain ant species show a loss of about a third of the total colony population in case of victory. For some species, such as Crematogaster mimosae, victory over an opposing colony usually results in the flight or death of the opposing queen, but the victorious colony often adopts the surviving ants of the losing colony, likely a way to avoid and mitigate the significant resource loss due to the war effort. In a few rare cases, the queen of the losing colony is herself adopted by the victorious colony, and the two merge. Supercolonies With the development of ant supercolonies, which follows human expansion into new areas, groups of dozens, hundreds, or even thousands of colonies engage in large-scale conflicts against other species. For example, around San Diego in the 2010s, millions of ants died each month in significant battles between the supercolony formed by Argentine ants and three other supercolonies present in the area. == References == The General Toll Switching Plan was a systematic nationwide effort by the American Telephone and Telegraph Company (AT&T) of organizing the telephone toll circuits and cable routes of the nation, and of streamlining the operating principles and technical infrastructure for connecting long-distance telephone calls in North America. This involved the design of a hierarchical system of toll-switching centers, a process that had found substantial maturity by 1929. The switching plan was principally operated by the Long Lines division of the Bell System in cooperation with independent telephone companies under the decree of the Kingsbury Commitment, reached with the United States government in 1913. The General Toll Switching Plan was a system manually operated by long-distance telephone operators. It was the forerunner of an automated system called Nationwide Operator Toll Dialing that was begun in 1943, which eventually led to Direct Distance Dialing (DDD) within the framework of the North American Numbering Plan decades later. Long-distance telephone service In the same manner that early telephone users developed an increasing desire to talk to each other, and expected the service to reach farther out and increase the number of participating private and business customers locally, so did users in different towns and cities desire to call each other. However, the state of the technology initially had limitation in distance. Telephone companies developed on a local basis, usually one company for each community. As the technology of telephones and of line construction improved, logistical problems stood in the way of rapid expansion. Solutions had to be found for organizational structure, management, business relationships, collaboration, to operate telephone services between or across multiple communities. Theodore Newton Vail, General Manager of the American Bell Telephone Company, created the vision for this endeavor. The first long-distance experiment was the Boston–New York telephone line. Vail suggested that a company separate from the four or five affected local telephone companies should be responsible for the construction and operation of the line. Approved in 1880, he directed American Bell Telephone to incorporate a new entity in New York, the Inter-State Telephone Company for the construction of the first section starting in Boston, with the grant of a license by American Bell. An additional company was formed in Connecticut to complete the line to New York. The Boston–New York toll line opened in 1884, Building additional long-distance lines would require enormous amounts of financing. However, the Massachusetts legislature rejected an application to increase the corporate capitalization of American Bell. With the experience of incorporating Inter-State Telephone in New York, the company formed a new subsidiary in New York City, the American Telephone and Telegraph Company, on March 3, 1885, with Theodore Vail as its first president. The new company had the mandate to construct and operate long-distance telephone lines, and would negotiate and facilitate inter-connections to local telephones companies under the umbrella of the Bell System. This original purpose of the business would later be delegated to a division called Long Lines. By 1892, the company's long-distance network reached Chicago, but the New York–Chicago line did not become commercially successful until after 1900, and the invention of the loading coil. Universal service In 1907, Theodore Vail became president of the American Telephone and Telegraph Company for the second time, having left the company previously in 1887. Immediately, he steered the company into a new direction. He refined a vision of service, shaped new goals for supporting technological progress, and reorganized the company to facilitate his ideas. He envisioned universal telephone service as a public utility, and the future of the American telephone industry as a unified system of companies under the lead of American Telephone and Telegraph. Such a nationwide network required technical standards that were understood and accepted by all cooperating participants in the industry. The Western Electric Company, the Bell System's sole and dedicated manufacturing unit, which was previously not permitted by company policy to sell outside the Bell System, was now directed to advertise and sell its products to the general market, so that independent operators could buy compatible apparatus. Vail organized a distinct research division within Western Electric, the later Bell Laboratories, to focus on basic research and development to solve the problems encountered in improving the technology of telephony. These efforts and this vision were communicated to the public by marketing campaigns under the slogan One System — One Policy — Universal Service. Kingsbury Commitment In 1913, AT&T settled pending anti-trust challenges in the Kingsbury Commitment. On December 19, 1913, in a letter by Nathan C. Kingsbury to the U.S. Attorney General, AT&T conceded to restrictions in the acquisition of independent companies, and agreed to the divestiture of Western Union. AT&T's telephone operations thereby essentially became a government-sanctioned natural monopoly, because an essential feature of this commitment was that independent telephony operators were permitted to "secure for their subscribers toll service over the lines of the companies in the Bell System." removing the barriers to a nationwide telephone system that would have no competitors. Local toll networks While the Bell System had a specialized division, Long Lines, to interconnect the local telephone networks of its Associated Companies, no such unifying driving force existed in the independent telephone industry. Telephone companies negotiated interconnection with neighboring businesses and built localized toll networks that addressed the regional needs of their customers. The interconnection agreements with the Bell System provided access beyond these networks. Long-haul and transcontinental transmission Long-distance toll lines for transmission of telephone calls were almost entirely open-wire pair installations early in the 20th century. By 1911, the Long Lines network had reached from New York as far west as Denver, using loading coil circuits, but this distance was the limit for communication without amplification. The research efforts at Western Electric, committed to by Vail in c. 1909, into the principles of the electron tube recently invented by Lee de Forest, the Audion, and its efficient manufacture made it possible to build signal repeaters that extended the transmission distance of toll lines. In 1914, AT&T succeeded in the first transcontinental transmission line spanning between the Atlantic Ocean and the Pacific Ocean. This connected a large customer base in the far west beyond the Rocky Mountains to the AT&T Long Lines network. Open wire, while marginally increasing in installations in the 1920s, was increasingly supplemented with cable routes, experiencing dramatic growth, but also with carrier transmission, a new development which multiplexed multiple communication channels, at times 200 or 300 circuits, on the same physical cable medium. By 1925, the extent and quality of transmission lines in the nation was good enough, so that telephone subscribers could place telephone calls to almost anywhere in the continental United States. However, set-up times for calls were typically still long, and callers often had to hang up after ordering a call with an operator, who called them back when the circuit was established. In 1925, the average time to establish connections was still over seven minutes, but this improved to about two and one half minutes by 1929. The extensions of the nationwide interconnections led to rapid increase in traffic. In 1915, less than a quarter billion toll messages had been carried in the Bell System. Over the next fifteen years, this more than quadrupled to over one trillion. A increasing fraction of this traffic was for the long-haul routes in the network, between the largest cities in the nation and via the transcontinental routes. As a consequence, the build-out of long-haul plant was emphasized in investments, resulting in better quality circuits for long-haul transmissions. Due to the investments in the plant, the average speed of establishing connections was steadily decreasing throughout the 1920s, AT&T was able to effect several rate cuts in long-distance service in just a few years. However, the growth also caused major construction problems in the layout of the cable plant. By 1930, the long-haul business was handled by about 2,500 toll centers, out of 6,400 central offices in the United States and eastern Canada. The long-haul build-out in the United States was paralleled by the construction of the Trans-Canada Telephone System, having been planned immediately after the formation of the Telephone Association of Canada in 1921. A systematic approach was needed to limit the number of intermediate toll offices that relayed the calls across the country, to further reduce set-up time, and to establish technical parameters for interchange points to assure a certain level of circuit quality. In 1929, the results of this research outlined a new fundamental layout of the toll plant (circuit) and the routing of toll calls. This first continental toll switching system became known as the General Toll Switching Plan. Hierarchical interconnections During the growth of telephone service since the first installations of telephone exchanges and the development of advanced manual and automatic switching systems, approximately 2,500 switching systems had been established in the nation that had trunks for connecting to other communities. The systems were designated as toll centers to which all local calls were routed that had to be connected to another toll center closer to the destination of each long distance call. The technological improvements since the transcontinental transmission line of 1914 required a new methodology and plan of managing the traffic. The purpose of this plan was to provide systematically a basic layout of the plant compatible with the highest practicable standards of service achievable within given economic goals. The layout of cabling and major toll centers needed optimization in the number of switching steps required along a given route to connect any two telephones on the continent. The General Toll Switching Plan organized these local toll centers into geographical groups associated with region within which all toll centers forwarded calls to destinations outside their territory to a Primary Outlet. The Primary Outlet was responsible for establishing an optimal route either to another Primary Outlet or to a Regional Center. Regional Centers were toll-switches responsible for a yet larger geographic region each. The Regional Centers were strategically located across the nation and each maintained cable routes to all other Regional Centers. In addition, they connected to some Primary Outlets in other regions as well, as traffic demanded, or for alternate routing in case of congestion or technical failures. See also Original North American area codes Routing in the PSTN Transmission line == References == Former Iranian president Mohammad Khatami introduced the idea of Dialogue Among Civilizations as a response to Samuel P. Huntington's theory of a Clash of Civilizations. The term was initially used by Austrian philosopher Hans Köchler who in 1972, in a letter to UNESCO, had suggested the idea of an international conference on the "dialogue between different civilizations" (dialogue entre les différentes civilisations) and had organized, in 1974, a first international conference on the role of intercultural dialogue ("The Cultural Self-comprehension of Nations") with the support and under the auspices of Senegalese President Léopold Sédar Senghor. History One of the first places where Dialogue Among Civilizations took place was in Isfahan, Iran at the Safa Khaneh Community that was established in 1902. Safa Khaneh was a place that Haj Aqa Nourollah and his older brother made. It was a place where Muslims and Christians talked about their religions with each other. It was one of the first interfaith centres in the world. Later a magazine was published based on the dialogues between Muslims and Christians in the Safa Khaneh and it was released in Iran, India and England. The founder of Safa Khaneh, Haj Aqa Nouroollah was one of the leaders of the Constitution Era in Iran. His house has become a museum named Constitution house of Isfahan. Introduction The page dedicated to the United Nations Year of Dialog Among Civilizations introduces the idea as follows: What is diversity? What can people do to open the lines of communication and redefine the meaning of diversity? How can we better understand diversity? What is the overall perception of diversity? These were the questions the General Assembly grappled with in 1998, when the year 2001 was announced as the United Nations Year of Dialogue Among Civilizations. What does a dialogue among civilizations mean? One could argue that in the world there are two groups of civilizations – one which perceives diversity as a threat and the other which sees it as an opportunity and an integral component for growth. The Year of Dialogue Among Civilizations was established to redefine diversity and to improve dialogue between these two groups. Hence, the goal of the Year of Dialogue Among Civilizations is to nurture a dialogue which is both preventive of conflicts – when possible – and inclusive in nature. To do this, Governments, the United Nations system and other relevant international and non-governmental organizations were invited by the United Nations General Assembly to plan and implement cultural, educational and social programmes to promote the concept of the dialogue among civilizations. The Vision Here are some excerpts from the vision of the Foundation for Dialogue among Civilizations: ... The act of dialogue among cultures and civilizations faces multiple theoretical and practical questions. Fundamental questions regarding civilization and culture, and the intellectual and scientific preoccupations in this regard should not be underestimated. I would like, however, to emphasize that the main objective for this initiative of dialogue among cultures and civilizations is in fact to initiate a new paradigm in international relations and those among human beings in our contemporary world. This necessity will be clearer when we compare it with the other paradigms which currently form the basis of international relations. It is through a fundamental and structural critique of these paradigms that the raison d'être for this new paradigm is identified. We cannot invite people and governments to the paradigm of dialogue of cultures and civilizations without learning lessons from history, without thoroughly investigating the reasons behind major world disasters in the twentieth century and their continuation in the current one, and without passing judgment on the existing dominant paradigm which is based on a dialogue of power and glorification of might. Dialogue among civilizations, viewed from an ethical perspective, is in fact an invitation to discard what might be termed the power oriented will, in favour of a love oriented one. In this case, the result of dialogue will be empathy and compassion. And the interlocutors will primarily be thinkers, leaders, artists and all benevolent intellectuals who are the true representatives of their respective cultures and civilizations. Relying on shared principles, objectives, and threats in order to find shared solutions is a major step towards changing the existing situation and isolating the extremists who, by sanctifying violence and force, have spoilt the world for all its inhabitants regardless of their culture or civilization. ... The Mission Here are some excerpts from the mission of the Foundation for Dialogue among Civilizations: ... The foundation aims to build upon the successes of the United Nations year and further implement the recommendations of the relevant UN resolutions. ... The Foundation believes that dialogue among civilisations is conducive to mutual understanding, tolerance, peaceful coexistence and international cooperation and security. Strategic objectives of the Foundation for Dialogue Among Civilisations include: promoting and facilitating the peaceful resolution of conflicts and/disputes reconciling tensions between cultures, countries and religions promoting and facilitating the much needed dialogue between Muslim societies and other societies around the world contributing to academic research and enriching the wider debate around peace in the world Activities The Foundation for Dialogue Among Civilisations will fulfill its objectives through: the organization of diverse cultural, artistic, and scientific events including debates, fora, symposia and seminars designed to encourage exchange between cultures and civilisations in the spirit of the Foundation maintaining, and when needed, initiating regular communication with experts in the field as well as with all other foundations or associations with similar or complementary objectives the publication of articles and reports resulting from research carried out by the Foundation's committees and debates at its workshops The Foundation welcomes proposals of cooperation, contribution and support by individuals and institutions with transparent activities in favour of dialogue, reconciliation and peace. Contrasting view: The Clash of Civilizations In 1993, Huntington provoked great debate among international relations theorists with the interrogatively-titled "The Clash of Civilizations?", a controversial, oft-cited article published in Foreign Affairs magazine. Its description of post–Cold War geopolitics contrasted with the controversial End of History thesis advocated by Francis Fukuyama. Huntington expanded "The Clash of Civilizations?" to book length and published it as The Clash of Civilizations and the Remaking of World Order in 1996. The article and the book posit that post–Cold War conflict would most frequently and violently occur because of cultural rather than ideological differences. That, whilst in the Cold War, conflict likely occurred between the Capitalist West and the Communist Bloc East, it now was most likely to occur between the world's major civilizations — identifying seven, and a possible eighth: (i) Western, (ii) Latin American, (iii) Islamic, (iv) Sinic (Chinese), (v) Hindu, (vi) Orthodox, (vii) Japanese, and (viii) the African. This cultural organization contrasts the contemporary world with the classical notion of sovereign states. To understand current and future conflict, cultural rifts must be understood, and culture — rather than the State — mern(?) nations will lose predominance if they fail to recognize the irreconcilable nature of cultural tensions. In Eurasia the great historic fault lines between civilizations are once more aflame. This is particularly true along the boundaries of the crescent-shaped Islamic bloc of nations, from the bulge of Africa to central Asia. Violence also occurs between Muslims, on the one hand, and Orthodox Serbs in the Balkans, Jews in Israel, Hindus in India, Buddhists in Burma and Catholics in the Philippines. Islam has bloody borders. Critics (for example, in Le Monde Diplomatique) called The Clash of Civilizations and the Remaking of World Order the theoretical legitimization of American-led Western aggression against China and the world's Islamic cultures. Nevertheless, this post–Cold War shift in geopolitical organization and structure requires that the West internally strengthens itself culturally, by abandoning the imposition of its ideal of democratic universalism and its incessant military interventionism. Other critics argued that Huntington's taxonomy is simplistic and arbitrary, and does not take account of the internal dynamics and partisan tensions within civilizations. Huntington's influence upon U.S. policy has been likened to that of British historian A.J. Toynbee's controversial religious theories about Asian leaders in the early twentieth century. Personal Representative of the Secretary-General for the UN Year of Dialogue Among Civilizations has said: History does not kill. Religion does not rape women, the purity of blood does not destroy buildings and institutions do not fail. Only individuals do those things. Former UN Assistant Secretary-General Giandomenico Picco was appointed the Personal Representative to the Secretary-General for the United Nations Year of Dialogue Among Civilizations in 1999 in order to facilitate discussions on diversity, through organizing conferences, seminars and disseminating information and scholarly materials. Having served the United Nations for two decades, Mr. Picco is most recognized for participating in UN efforts to negotiate the Soviet withdrawal from Afghanistan and in bringing an end to the Iran-Iraq war. He believes that people should take responsibility for who they are, what they do, what they value, and what they believe in. Related comments "A basic change in political ethics is required for the realization of the proposal, The dialog among civilizations." (UNESCO 1999) "In order to understand the meaning of the phrase dialogue among civilizations as defined here, one has no choice but to closely pay attention to a number of points one of which is the relationship between a politician and an artist, and the other is the relationship between ethics and politics." (Khatami, UNESCO 1999) The Daniel Pearl Dialogue for Muslim-Jewish Understanding is a series of personal yet public conversations between Daniel Pearl's father, Professor Judea Pearl, President of the Daniel Pearl Foundation, and Dr. Akbar Ahmed, Chair of Islamic Studies at American University. The program grew out of Professors Ahmed and Pearl's shared concern about the deterioration of relationships between Muslim and Jewish communities around the world, and their strong belief that reconciliation between these two Abrahamic faiths can be achieved through frank and respectful dialogue. The discussions range from theological issues, historical perceptions to current events. In 2006 Professors Ahmed and Pearl were awarded the first annual Purpose Prize "in recognition of [their] simple, yet innovative approach to solving one of society's most pressing problems." Professor Judea Pearl is a well-known computer scientist, and the President of the Daniel Pearl Foundation. "Dear President Khatami...I welcome your call for a dialogue between Islamic and Judeo-Christian civilizations because I believe that tensions between these two great world civilizations represent the most significant foreign policy challenge for the world community as we enter the twenty-first century." Excerpt from "An American Citizen Replies" (letter by Anthony J. Dennis to Iranian President Khatami dated August 18, 2000) published in the book Letters to Khatami: A Reply To The Iranian President's Call For A Dialogue Among Civilizations. To date, this book, published and released on July 1, 2001, is the only published reply the now-former Iranian President Khatami has ever received from the West in response to Khatami's call for such a dialogue in an exclusive, hour-long interview on CNN with CNN Foreign Correspondent Christiane Amanpour broadcast in North America on January 7, 1998. See also Alliance of Civilizations Centre for Dialogue Dialogue of Civilizations Fethullah Gülen Institute for Interreligious Dialogue Interfaith dialogue KAICIID Dialogue Centre Parliament of the World's Religions World Against Violence and Extremism Notes References Hans Köchler, Philosophical Foundations of Civilizational Dialogue. International Seminar on Civilizational Dialogue (3rd: 15–17 September 1997: Kuala Lumpur), BP171.5 ISCD. Kertas kerja persidangan / conference papers. Kuala Lumpur: University of Malaya Library, 1997. Hans Köchler, Unity in Diversity: The Integrative Approach to Intercultural Relations. UN Chronicle, Vol. XLIX, No. 3, September 2012. Ankerl, Guy (2000). Global communication without universal civilization. INU societal research. Vol. 1: Coexisting contemporary civilizations : Arabo-Muslim, Bharati, Chinese, and Western. Geneva: INU Press. ISBN 2-88155-004-5. External links Foundation for Dialogue among Civilizations in Geneva, the official website which includes various news and speeches United Nations: Background of Dialogue Among Civilizations UNESCO's contribution to Dialogue Among Civilizations United Nations Year of Dialogue Among Civilizations UNESCO's actions for the Dialogue among Civilizations UN Chronicle 2006, Interview with Khatami, five years after the UN 2001 Year of Dialogue Among Civilizations International Progress Organization Daniel Pearl Foundation Archived 2019-05-03 at the Wayback Machine An Overview of Sino-Tibetan Dialogue World Public Forum "Dialogue of Civilizations" Archived 2019-07-28 at the Wayback Machine Australia Wide was a rural-focused half-hour soft news programme produced by the ABC in Sydney. The programme was, up until mid-2007, produced by the corporation's New Media and Digital Services division in Brisbane. It was shown weekdays on the digital-only channel ABC2 at 4.00pm, 7.00pm, and 7.30am, and can also be viewed at ABC Online. The program follows a daily theme covering a range of topics and issues. Earth Works, shown on Mondays, focusses on the 'real world' and environment, Gen Next, shown on Tuesdays concentrates on the interests and issues of young people, especially those in rural areas. On Wednesdays the five winners of the 2005 "Video Lives" competition present video diaries of their lives and communities, while on Thursdays Arts About showcases the artistic talents and endeavours of Australians living in rural and regional areas. Outta Here on Fridays follows sport and recreational activities. Content for the program comes from a number of sources, including reports from state and territory ABC News bulletins, reports from programs on the Australia Network, and ABC Local Radio presenters. Over summer, Australia Wide Summertime screens 10 minutes of news and weather and replaces the second section with short documentaries from a variety of sources, shown only on ABC2. History Australia Wide stemmed from ABC Local Radio's Radio Pictures program, in which local radio presenters from rural Western Australia were given video equipment and training to produce local stories. This program produced a four-part series for ABC TV in 1998. In 2000, it was intended that the concept would be turned into a regular program, shown on ABC TV. A pilot was produced, however the series did not go ahead. Video equipment was still given to a number of additional Local Radio studios, along with training and additional staffing hours. A few more short series of "Radio Pictures" were produced, and video appeared on ABC Online. In 2003, the regular series idea was turned into the week-daily "Australia Wide", also including content from regular ABC News reports. It ran for fifteen minutes and was only available on ABC Online, produced by the ABC's New Media and Digital Services division in Brisbane. In March 2005, ABC2 launched, and Australia Wide became available to a wider audience. ABC ran a competition in late 2005 called "Video Lives" to find new documentary making talent. Following a restructure of the ABC in 2007, production was moved from Brisbane to Sydney, where the program was to be produced by the corporation's News and Current Affairs division. There have been a number of presenters over the years, including Kerrin Binnie and Emma Renwick. On 6 August 2016 Yassmin Abdel-Magied presented the show for the first time, and continued in this role until the cancellation of the show following the ABC restructure was announced in late May 2017, with the last episode to be aired on 1 July. References External links Australia Wide ABC Video on Demand: Australia Wide Carnography (also carno) refers to excessive or extended scenes of carnage, violence, and gore in media such as film, literature, and images. The term carnography—a portmanteau of the words carnage and pornography—was used as early as 1972 in Time magazine's review of David Morrell's book First Blood, upon which the Rambo film series is based. Rambo was later called "carnography" as well. The term refers to an obsession with the human body that "suggests a connection between horror and pornography", often relating to hardcore horror films. Carnography is considered taboo and a disreputable genre. It has been described as "nastily impure work", "splatter-obsessed hard core horror", and "watching flesh fly". Carnographic horror films have a "superfluous plot" in which characters are "initiated, only to be discarded", and the gore seems to be the only reason the film exists. Pornography and carnography share the feature of close, intimate physical contact, whether it be to caress or to attack. See also Splatter film Ero guro == References == Ethel Knight Kelly (born Ethel Knight Mollison, 28 January 1875 – 22 September 1949) was a Canadian–Australian actress, writer, and social leader. She appeared in a number of plays and wrote four books. Early life Kelly was born in Saint John, New Brunswick, Canada. She was the elder daughter of Margaret Millen Mollison and William Knight Mollison. She married Edmund Canston Moore in New York City on 12 September 1894. The marriage was brief; one source states that Edmund died less than a year after the wedding. Career She began her acting career with Olga Nethersole in 1894, and went on to appear in plays that included Cyrano de Bergerac and The Taming of the Shrew. She acted with a company headed by Augustin Daly and with George Holland's Stock Company. She appeared on Broadway in Beaucaire in 1901. J. C. Williamson brought her to Australia for the play Are You a Mason? in 1903. Later that year she appeared in Madame Butterfly. She largely left acting after her second marriage, but she still sometimes performed in matinée shows. She appeared in The School for Scandal in 1917, and in her own play, Swords and Tea, in 1918. Also in 1918, she appeared in the silent film Cupid Camouflaged, credited as Mrs. T. H. Kelly. Her first book was an account of her travels in India, titled Frivolous Peeps at India and published in 1911. In 1922, she became editor of the women's page of Smith's Weekly. In 1925 she published her first novel, Why the Sphinx Smiles. It was followed by Zara in 1927. She wrote a memoir, Twelve Milestones, which was published in 1929. Personal life While in Australia she met businessman Thomas Herbert Kelly, the brother of Willie and Frederick Kelly. They married on 29 August 1903. They had two sons and two daughters. They remained married until his death in 1948. From 1925 to 1934, she lived primarily in Florence, Italy, with her daughters. While in Italy she converted to Catholicism. She returned to Australia in 1934. Kelly was an active fundraiser for hospitals and other charities, and was a prominent hostess for Sydney social events. She died on 22 September 1949 at her home in Darlinghurst. She was survived by one of her sons and both of her daughters. References Burns, Nelson (25 September 1949). "From Stage to Society". The Sunday Mail. Brisbane. p. 8. "Late Mrs. T. H. Kelly". The National Advocate. Bathurst. 23 September 1949. p. 1. "Mrs. Kelly, Social Leader, Dead". The Sydney Morning Herald. 23 September 1949. p. 7. External links Media related to Ethel Knight Kelly at Wikimedia Commons The International Social Survey Programme (ISSP) is a collaboration between different nations conducting surveys covering topics which are useful for social science research. The ISSP researchers develop questions which are meaningful and relevant to all countries which can be expressed in an equal manner in different languages. The results of the surveys provide a cross-national and cross-cultural perspective to individual national studies. By 2021, 58 countries have already taken part in the ISSP. History The ISSP was founded in 1984 by research organizations from four countries: Zentrum für Umfragen, Methoden, und Analysen (ZUMA), Mannheim, Germany, now GESIS – Leibniz Institute for the Social Sciences National Opinion Research Center (NORC), University of Chicago, Chicago, Illinois, United States. Social and Community Planning Research (SCPR), London, United Kingdom, now National Centre for Social Research, NatCen Research School of Social Sciences (RSSS), Australian National University, now School of Demography Canberra, Australia. Four different Social Surveys included a common module each year: The British Social Attitudes Survey (BSA) in the UK The General Social Survey (GSS) in the USA The ALLBUS or German General Social Survey (GGSS) in Germany and The Surveys by the Research School of Social Sciences Since then social science institutions from 58 different countries included a 15-minute supplement to their national surveys. The membership to the ISSP is institutional and by country. One or more than one institute in a country can co-operate on ISSP research (cf. France and Spain). The common module surveyed by the member institutions also contains an extensive common core of background variables. The modules focus on one specific topic each year and were planned to be repeated more or less every five to ten years. When it comes to the researchers choice of topics, the relevance of the area of social sciences in the year of the survey is taken into account. Given this, the ISSP deliveries data sets are helpful for both Cross-sectional studies and Time series analysis. Over time the set of modules has grown towards more diverse topics. The latest additions were Leisure Time and Sports in 2007 as well as Health and Health Care in 2011. Organisation The ISSP is a self-funding organisation with an emphasis on democratic decision making stated in its working principles. To accomplish this principle it has set up several groups and committees. These groups either consist of member organisations as a whole or include some particular social scientists. There are: The ISSP secretariat (2021-2024): FORS - Swiss Centre for Expertise in the Social Sciences, Switzerland The ISSP archive (GESIS Data Archive for the Social Sciences, Germany) Methodology research groups The ISSP sub-groups drawn up within the ISSP Drafting groups for modules The ISSP Standing Committee Most of the members of these groups are elected democratically at the General Assembly. These meetings of delegates from every member state of the ISSP are held in May or June in changing locations all around the world. The General Assemblies also serve the function of discussing modules, which are to be completed the same year or begun and surveyed the upcoming one. The delegates also discuss the topics of upcoming modules. The ISSP also gives importance to the way member organisations implement their surveys. The organisation's principles are published in its ethical statement and its working principles. Methodology The methodological work in the ISSP is coordinated by a Methodology Committee, consisting of six members elected at the General Meeting. It co-ordinates the work of six groups addressing different areas of cross-cultural methods, all concerned with issues of equivalence: demography, non-response, weighting, mode effects, questionnaire design and translation. Modules by year The datasets from the different modules conducted by participating ISSP member states can be downloaded at the GESIS Archive page. All these links lead to the official GESIS – Leibniz Institute for the Social Sciences homepage, where the data is provided openly for research purposes. Modules by topic Source: Members (1984–2021) References External links Official website French ISSP Website GESIS ISSP Archive International Journal of Sociology special issues on ISSP data Bibliography Davis, James A., and Roger Jowell. "Measuring national differences: an introduction to the International Social Survey Programme (ISSP)." British Social Attitudes: Special International Report, edited by Roger Jowell, Sharon Witherspoon, and Lindsay Brook. Aldershot: Gower (1989): 1-13. Smith, Tom W. "The international social survey program." International Journal of Public Opinion Research 4.3 (1992): 1992. Max Haller, Roger Jowell et Tom Smith (dir.), Charting the Globe: The International Social Survey Programme, 1984-2009, London, Routledge, 2009 (ISBN 978-0-415-49192-1) A food swamp is an urban environment with an abundance of several non-nutritious food options such as corner stores or fast-food restaurants. The term was coined in 2009 by Donald Rose and his colleagues at the University of Michigan in a report on food access in New Orleans. The concept is comparable to that of a food desert. It is generally believed that those in a food desert have poor local access to nutritious food sources, while those in a food swamp have few grocery stores but easy local access to non-nutritious food. However, areas that have adequate access to healthy food options while still having an overwhelming amount of unhealthy food available are also considered food swamps. Food swamps may even be more widespread that food deserts, as suggested by some research, or overlap with food deserts as they exist in various regions around the world. One definition gives a general ratio of four unhealthy options for each healthy option. The term was first coined by researchers conducting longitudinal studies of the link between increased access to grocery stores and rising obesity rates. This study found that even with new access to local grocery stores, the proportion of convenience stores and fast food to a single grocery store did not shift food choices nor obesity rates. This indicates a distinction between food swamps and food deserts. According to researchers, food swamps are better measures for obesity rates. Food swamps are associated with varying health outcomes across different demographic groups, with Black and Brown communities experiencing disproportionately poorer health indicators. Food marketing, accessibility, and health outcomes Marketing and behavior Researchers are investigating if there are links between marketing practices and food swamp prevalence. Food swamps are characterized by an abundance of unhealthy food options, often accompanied by targeted advertising. For example, the visibility of high-calorie foods has been identified as one of the marketing tools using psychology in food swamps to encourage the consumption of these prevalent unhealthy food items. Food swamps also impact food choice. Exposure to fast food marketing has been found to be associated with people having a greater preference for the brands they see being marketed. Evidence shows that children often select menu items based on promotional materials or images for food. Accessibility The presence of a food swamp can impact the behavior of individuals living in the designated area. The expensive cost of food can limit people's ability to get healthy food, which can cause many people feel like they have to rely on the cheaper and more non-nutritious options in their surrounding area. One of the factors that may better explain this occurrence is the surrounding neighborhood of a food swamp. In many areas designated as food swamps, there exists a lack of transportation options to grocery stores that carry a more nutritious array of food options. This means that food options are still severely limited, especially when the nearest convenience store, bodega, or fast food restaurant is at walking distance and consumes the least amount of time. This leaves populations with less ability to travel outside of a swamp. Health Food swamps have positive, statistically significant effects on adult obesity rates, especially in those areas where the majority of residents do not have access to personal or public transportation, and have disproportionate health impacts on low-income minorities. This environment is found in areas with strong corporate or industrial influence and is becoming a global phenomenon. Research also suggests a positive correlation between obesity rates and the ratio of unhealthy to healthy food options. This is a consequence of fast-food options available in food swamps containing a high number of calories but a lower number of nutrients. This effect on obesity rates has reportedly also been associated with higher obesity-related cancer mortality rates. Some data also suggests that young adults living in close proximity to fast-food restaurants demonstrated higher incidence of type 2 diabetes. This has resulted in higher hospitalization rates for people with diabetes who live in a food swamp designated area. These dire health outcomes expose communities to vulnerabilities such as less optimal immune system functions that can impact the body's ability to combat sickness and disease. Demographic disparities Race and ethnicity In 2011, fast food access was assessed and it was found that fast-food restaurants are more likely to be located in areas with higher concentrations of ethnic minorities than whites in the United States. Racial-ethnic minorities have been shown to more frequently reside near unhealthy fast-food retailers than others. A survey study in 2020 found that non-Hispanic Black Americans are more likely to report living in a food swamp than other ethnic groups. Obesity rates have been found to be higher among African American and Latino populations compared to white populations, reflecting racial disparities in health outcomes. In Baltimore, Maryland, researchers found that young African Americans girls who were determined to be living in food swamps ate more snack foods and desserts than those who did not live in areas not categorized as food swamps, increasing their risk for obesity. Socioeconomic status Generally, differential barriers faced by low-income communities are mentioned as important influences on food swamp characterization. A 2017 study based on cross-sectional data from the 2009 United States Department of Agriculture suggested that food swamps can exist across communities with varying socioeconomic circumstances. However, the study also indicated that food swamps are more likely to be prevalent in counties with uneven distributions of income. In 2019, a study in Edmonton, Canada also suggested low-income groups are have been found to more likely live in areas that have an abundance of unhealthy fast-food options. The results observed across various locations suggest that further investigation is needed to clarify the correlation between socioeconomic status and the prevalence of food swamps. Global phenomena Some narratives purport that food deserts occur primarily in North America. However, food swamps have been identified in areas outside of the continental Americas. Studies find that in Mexico, food swamps are more of a concern than food deserts with regard to developing obesity prevention interventions, relating a higher access to obesity-creating foods to weight gain as opposed to simply a lack of other nutrients as a food desert term would imply. Additionally, there have been studies in Sub-Saharan Africa that indicate similar health outcomes in areas with excessive unhealthy foods that can be classified as food swamps. There has also been work done to typify urban food environments as food swamps in Melbourne, Australia based on their ranks in the accessibility of healthy foods. Current debates Definitions and measurements The term food swamp is relatively new as urban areas have progressed over time in modern society. Thus, governments and public health agencies are developing strategies to analyze and qualify areas by the term. New measurement tools have been developed to further improve analysis so that food swamps can be more appropriately and accurately designated. Some researchers qualify food swamps using a tool called the Modified Retail Food Environmental Index, or mRFEI. Created by the Centers for Disease Control and Prevention, mRFEI measures whether or not quality of food options within census tracts at retail stores constitute a food swamp. The use of grocers as the metric for food swamps is very common, often being compared by their number in a county or zip code to the density of a population. Other methods include use of geographic information systems like ArcGIS to geocode addresses. This helps researchers to understand to food locations around people's homes. Other measurement types include those considering broader factors such as geographic location and customer perceptions of their living and food environments . Government databases can be used to identify convenience food. A Brazilian methodology uses population to define food swamps by comparing the number of locations that provide processed foods to every 10,000 inhabitants. Methods used to define food swamps are varying and can still be refined or more universally agreed upon. A 2020 study conducted in The Bronx and Upper East Side of New York City noted suggested that food swamp classification should be broadened to include more than only retail food stores or restaurants. It has also been recommended that future measurement could also include surveys of perceived dietary quality among various groups, which have been found to align with the outcomes of objective classification tools. Controversy of term The term “food swamp” has endured some criticism on account of its referral to wetlands with a negative connotation. Critics have raised the point that while swamps have positive influences on ecosystems such as by detoxifying water and supporting biodiversity, food swamps exclusively cause problems for human health and the environment. Proposed solutions Many solutions have been proposed to address or eliminate food swamps. Researchers have proposed solutions such as introducing policies that address factors like the built environment, which consists of those man-made surroundings of a person's life. This also has included limiting the amount of fast-food establishments in an area, or incentivizing the distribution of healthy food options in an area. Municipal responsibilities have been particularly noted as an avenue for change. Researchers have suggested that local governments should introduce policies such as new zoning laws that limit the number of possible unhealthy food outlets and incentivize the presence of healthy food retailers. Lowering obesity rates is not dependent on the elimination of fast-food options but rather a more equal rate of unhealthy to healthy food options. It has also been suggested that having more community gardens and walkable neighborhoods could address this phenomenon. The Food Trust is an American nonprofit group that works to eliminate food swamps by ensuring access to food that is affordable and nutritious. They support programs which encourage Supplemental Nutrition Assistance Program, or SNAP, recipients to buy healthy food. Other efforts have been attempted outside of the United States as well. Some communities have attempted to implement a strategy called "Drying out" which involves multiple levels of intervention, ranging from top-down policy changes to attempts to shift individual health behaviors and habits. Such policies would seek to decrease a community's exposure to high-calorie food options. For example, in the early 2010s, Mexico began imposing strategies in schools to address food access. This included taxing sugar-sweetened beverages and prohibiting sodas. Some researchers have called for policies such as these to be researched in order to better understand implementation and replicate similar results. Smaller-scale advocacy efforts have also been suggested. In an effort to center the community, it has been suggested that researcher gather support and data from community members to gauge their own values surrounding food. Additionally, the need to examine food quality itself rather than its mere availability is also a matter of concern in food swamp intervention. See also Criticism of fast food Food choice Food desert Health equity Nutrition Obesity == References == Reactive ethnicity is the phenomenon where actions intended to limit or ban a practice cause people to continue the practice in protest. Jeffery Reitz has used this term to explain why the French restrictions on traditional Islamic veils are provoking even unveiled Muslim women to wear Islamic veils. See also Backfire effect Civil disobedience Identity politics References External links Reactive Ethnicity and Anticipated Discrimination among American Muslims in Southeastern Michigan "Reactive Ethnicity" or "Assimilation"? Further reading Rumbaut, Rubén G. (April 11, 2008). "Reaping What You Sow: Immigration, Youth, and Reactive Ethnicity". Applied Developmental Science. 12 (2): 108–111. doi:10.1080/10888690801997341. ISSN 1088-8691. S2CID 144390794. Diehl, Claudia; Schnell, Rainer (December 1, 2006). ""Reactive Ethnicity" or "Assimilation"? Statements, Arguments, and First Empirical Evidence for Labor Migrants in Germany". International Migration Review. 40 (4): 786–816. doi:10.1111/j.1747-7379.2006.00044.x. ISSN 0197-9183. S2CID 9046538. Çelik, Çetin (July 15, 2015). "'Having a German passport will not make me German': reactive ethnicity and oppositional identity among disadvantaged male Turkish second-generation youth in Germany". Ethnic and Racial Studies. 38 (9): 1646–1662. doi:10.1080/01419870.2015.1018298. ISSN 0141-9870. S2CID 144301973. Herda, Daniel (July 3, 2018). "Reactive Ethnicity and Anticipated Discrimination among American Muslims in Southeastern Michigan". Journal of Muslim Minority Affairs. 38 (3): 372–391. doi:10.1080/13602004.2018.1524136. ISSN 1360-2004. S2CID 150306874. Jiang, Qiaolei; Rajiv, George; Chib, Arul (January 1, 2015). "Silent but Brewing: Reactive Ethnicity and Interculturality among Chinese Students in Singapore". Journal of Intercultural Communication. Weilenmann, Markus (January 1, 2000). "Reactive ethnicity: some thoughts on political psychology based on the developments in Burundi, Rwanda and South-Kivu". Journal of Psychology in Africa. 10 (1): 01–25. ISSN 1433-0237. Portes, Alejandro, and Bryan Lagae. "Immigration, social change, and reactive ethnicity in the second generation." US Latinization: Education and the New Latino South (2017): 251–271. ISBN 9781438464992. St-Hilaire, Aonghas (January 1, 2001). "Ethnicity, assimilation and nation in plural Suriname". Ethnic and Racial Studies. 24 (6): 998–1019. doi:10.1080/01419870120077940. ISSN 0141-9870. PMID 19177691. S2CID 35306759. Heath, Anthony (January 2, 2014). "Introduction: Patterns of generational change: convergent, reactive or emergent?". Ethnic and Racial Studies. 37 (1): 1–9. doi:10.1080/01419870.2014.844845. ISSN 0141-9870. S2CID 144713423. Political demography is the study of the relationship between politics and population change. Population change is driven by classic demographic mechanisms – birth, death, age structure, and migration. However, in political demography, there is always scope for assimilation as well as boundary and identity change, which can redraw the boundaries of populations in a way that is not possible with biological populations. Typically, political-demographic projections can account for both demographic factors and transitions caused by social change. A notable leader in the area of sub-state population projection is the World Population Program of the International Institute of Applied Systems Analysis (IIASA) in Laxenburg, Austria. Some of the issues which are studied in the context of political demography are: surges of young people in the developing world, significantly increasing aging in the developed world, and the impact of increasing urbanization. Political demographers study issues like population growth in a political context. A population's growth is impacted by the relative balance of variables like mortality, fertility and immigration. Many of the present world's most powerful nations are aging quickly, largely as a result of major decreases in fertility rates and major increases in life expectancies. As the labor pools in these nations shrink, and spending on the elderly increases, their economies are likely to slow down. By 2050, the workforce in Japan and Russia is predicted to decrease by more than 30 percent, while the German workforce is expected to decline by 25 percent by that year. The governments of these countries have made financial commitments to the elderly in their populations which will consume huge percentages of their national GDP. For example, based on current numbers, more than 25% of the national GDPs of Japan, France and Germany will be consumed by these commitments by 2040. Political demography and evolution Differential reproductive success is the mechanism through which evolution takes place. For much of human history this occurred through migrations and wars of conquest, with disease and mortality through famine and war affecting the power of empires, tribes and city-states. Differential fertility also played a part, though typically reflected resource availability rather than cultural factors. Though culture has largely usurped this role, some claim that differential demography continues to affect cultural and political evolution. Some scholars believe there exists a form of "cultural selection" that will significantly affect future demographics due to significant differences in fertility rates between cultures that cannot be explained by factors such as income, such as within certain religious groups. In the book Shall the Religious Inherit the Earth?, Eric Kaufmann argues that demographic trends point to religious fundamentalists greatly increasing as a share of the population over the next century. From the perspective of evolutionary psychology, it is expected that selection pressure should occur for whatever psychological or cultural traits maximize fertility. Uneven transition, democratization and globalization The demographic transition from the late eighteenth century onwards opened up the possibility that significant change could occur within and between political units. Though the writings of Polybius and Cicero in classical times bemoaned the low fertility of the patrician elite as against their more fecund barbarian competitors, differential fertility has probably only recently emerged as a central aspect of political demography. This has come about due to medical advances which have lowered infant mortality while conquest migrations have faded as a factor in world history. Differences in immunity levels to infectious diseases between populations also play no major role in our age of modern medicine and widespread exposure to a common disease pool. It is not so much the trajectory of demographic transition that counts as the fact that it has become more intense and uneven in the late twentieth century as it has spread into the developing world. Uneven transitions lend themselves to differential growth rates between contending groups. These changes are in turn, magnified by democratization, which entrenches majority rule and privileges the power of numbers in politics as never before. Indeed, in many new democracies riven by ethnic and religious conflicts, elections are akin to censuses while groups seek to 'win the census'. Ethnic parties struggle to increase their constituencies through pronatalism ('wombfare'), oppose family planning, and contest census and election results. Ethnic, national and civilizational conflict One branch of political demography examines how differences in population growth between nation-states, religions, ethnic groups and civilizations affects the balance of power between these political actors. For instance, Ethiopia was projected to have a larger population than Russia in 2020, and while there were 3.5 Europeans per African in 1900, there will be four Africans for each European in 2050. Population has always counted for national power to some degree and it is unlikely that these changes will leave the world system unaffected. The same dynamic can be witnessed within countries due to differential ethnic population growth. Irish Catholics in Northern Ireland increased their share of the population through higher birthrates and the momentum of a youthful age structure from 35 to nearly 50 percent of the total between 1965 and 2011. Similar changes, also affected by in- and out-migration, have taken place in, amongst others, the United States (Hispanics), Israel-Palestine (Jews and Arabs), Kosovo (Albanians), Lebanon (Shia, with decline of Christians) and Nagorno-Karabakh (Armenians). In the US, the growth of Hispanics and Asians, and Hispanics' youthful age profile as against whites, has the potential to tilt more states away from the Republican Party. On the other hand, the fertility advantage of conservative over liberal white voters is significant and rising, thus the Republicans are poised to win a larger share of the white vote - especially over the very long run of 50 to 100 years. According to London-based scholar Eric Kaufmann, the high birth rates of religious fundamentalists as against seculars and moderates has contributed to an increase in religious fundamentalism and decrease of moderate religion within religious groups, as in Israel, the US and the Muslim Middle East. Kaufmann, armed with empirical from a number of countries, also posits that this will be further bolstered by the higher retention rates of religious fundamentalists, with individuals in religiously fundamentalist households less likely to become religiously non-observant than others. See also Religious demography § Religious demographics. Age structure and politics Youth bulges A second avenue of inquiry considers age structures: be these 'youth bulges' or aging populations. Young populations are associated with a ratio of dependents to producers: a high proportion of the population under age 16 puts pressure on resources. A 'youth bulge' of those in the 16-30 bracket creates a different set of problems. A large population of adolescents entering the labor force and electorate strains at the seams of the economy and polity, which were designed for smaller populations. This creates unemployment and alienation unless new opportunities are created quickly enough - in which case a 'demographic dividend' accrues because productive workers outweigh young and elderly dependents. Yet the 16-30 age range is associated with risk-taking, especially among males. In general, youth bulges in developing countries are associated with higher unemployment and, as a result, a heightened risk of violence and political instability. For some, the transition to more mature age structures is almost a sine qua non for democratization. Population aging Population aging presents the obverse effect: older populations are less risk-taking and less prone to violence and instability. However, like those under-16, they place great strain on the social safety net, especially in countries committed to old-age provision and high-quality medical care. Some observers believe that the advent of a much older planet, courtesy of below-replacement fertility in Europe, North America, China and much of the rest of Asia and Latin America, will produce a 'geriatric peace'. Others are concerned that population aging will bankrupt the welfare state and handicap western liberal democracies' ability to project power abroad to defend their interests. A more cautious climate could also herald slower economic growth, less entrepreneurship and reduced productivity in mature democracies. However, some argue that older people in the developed world have much higher productivity, human capital and better health than their counterparts in developing countries, so the economic effects of population aging will be largely mitigated. Other branches of political demography Other areas in political demography address the political impact of skewed sex ratios (typically caused by female infanticide or neglect), urbanization, global migration, and the links between population, environment and conflict Emerging discipline The study of political demography is in its early stages and can be traced back to the works of figures such as Jack Goldstone, whom is often considered to be the father of Political Demography. Since 2000 the subject has drawn the attention of policymakers and journalists and is now emerging as an academic subfield. Panels on political demography appear at demography conferences such as the Population Association of America (PAA) and European Association for Population Studies (EAPS). There is now a political demography section at the International Studies Association. A number of important international conferences have also taken place since 2006 on the subject. See also Natalism Religious demography Quiverfull Jack Goldstone Philip Longman Myron Weiner Ben Wattenberg World population Demographic engineering References External links The Political Demography of Ethnicity, Nationalism and Religion Eric Kaufmann's website Webcast of book launch of Political Demography, at Woodrow Wilson Center, Jan. 10, 2012 - featuring Jack Goldstone, Eric Kaufmann, Mark Haas, Elizabeth Leahy, and chaired by Geoff Dabelko Demography and Security: The Politics of Population Change, conference at Weatherhead Center, Harvard University, May 7-8, 2009 International Studies Association, Political Demography Section Shall the Religious Inherit the Earth?: Religiosity, Fertility and Politics Ruy Teixeira US political demographics website William Frey US political demographics site In sociology, symbolic ethnicity is a nostalgic allegiance to, love for, and pride in a cultural tradition that can be felt and lived without having to be incorporated to the person's everyday behavior; as such, a symbolic ethnic identity usually is composed of images from mass communications media. Etymology The term was introduced in the article "Symbolic Ethnicity: The Future of Ethnic Groups and Cultures in America" (1979), by Herbert J. Gans, in the journal Ethnic and Racial Studies. Development The development of symbolic ethnicity, as a sociological phenomenon, is attributed to mainly to ethnic European immigrants of second and subsequent generations, because "Black, Hispanic, Asian and Indian Americans do not have the option of a symbolic ethnicity, at present, in the United States"; a socio-economic circumstance "in which ethnicity does not matter for white Americans, [yet] it does matter for non-whites". This view, however, ignores the complicated history of actual race relations in the United States, including persons of black ancestry who appeared phenotypically close enough to perceived norms of "whiteness" to allow them to pass as white. It also ignores the reality of many Americans of Cuban, Argentine, and other Latino descent who have fair complexions and who are often subsumed into the general "white" population, including on historical Census Bureau returns, which did not have a separate category for "Hispanic". That term did not refer to a race in the traditional conception of the term, as it was understood during the 19th century. Many Latinos were recorded on official US government records as simply "white". This is doubly true for fair-skinned Latinos who only speak English, and remains true to this day, as the primary marker of ethnicity for Latino group membership is not physical appearance but rather language spoken. Lastly, there are a number of "thin-blooded" Native American tribes where many members appear phenotypically white, such as the Seminole Tribe of Florida, which often require only 25% blood quotient to be a member of the tribe (ie, 3 white ancestors and 1 Native American ancestor). Many of these people can easily pass themselves as white, if they so choose, thus rendering their ethnicity "optional", as well. Overview In the U.S., symbolic ethnicity is an important component of American cultural identity, assumed as "a voluntary, personally chosen identity marker, rather than the totally ascribed characteristic" determined by physical appearance. As a sociological phenomenon, symbolic ethnicity is attributed to Americans of European ancestry, most of whom either are influenced by or assimilated to the White Anglo-Saxon Protestant (WASP) community. As such, symbolic ethnicity is the process of social identity whereby the person's "ethnic identity is solely associated with iconic elements of the culture" from which he or she originated. Gans's investigations concentrated on the later generations of Roman Catholic and Jewish Americans who had "begun to re-associate themselves with their ethnic culture", noting that "the ethnic associations were mainly symbolic, and that the traditional community interactions were lost". Those Catholic and Jewish Americans identified "their ethnic race in a personal perspective, as opposed to a communal" perspective, which actions produced an "outward ethnic identity that uses superficial symbols and icons to label and categorizes a certain race". That is to say, people identify their ethnicity by way of images from the mass communications media, as accepted through past associations derived from social and historical judgments. In (E)race: Symbolic Ethnicity and the Asian Image (1993), Stephen Lee describes symbolic ethnicity: From unrelenting integration of outside influences, self-definition becomes less associated with the community as a collective and becomes more associated with personal ethnicity as self. As the definition of ethnicity becomes increasingly personal, the need to reassert the community associations decreases. Ethnicity then becomes a symbolic identity more than a lifestyle. The definition of ethnicity, as formed by cinema, follows this symbolic pattern. In fact, in most cinemas that deals with ethnic integration, ethnic lifestyle is inseparable from its symbolic codes. Ethnic lifestyle is not an associative or collective means of existence, but a symbolic code—an icon. In the book Identity and Belonging: Rethinking Race and Ethnicity in Canadian Society (2006), by B. Singh Bolaria and Sean P. Hier, symbolic ethnicity is defined by, with, and in the actions of "individuals who identify as Irish, for example, on occasions such as Saint Patrick's Day, on family holidays, or for vacations. They do not usually belong to Irish-American organizations, live in Irish neighborhoods, work in Irish jobs, or marry other Irish people." Therefore, the symbolic identity of "being Irish" is: . . . [an] ethnicity that is individualistic in nature and without real social cost for the individual. These symbolic identifications are essentially leisure-time activities, rooted in nuclear family traditions and reinforced by the voluntary enjoyable aspects of being ethnic. In terms of the derogatory term Plastic Paddy used to describe symbolic ethnicity in the Irish diaspora, Hickman (2002) states that the use of this term was "a part of the process by which the second-generation Irish are positioned as inauthentic within the two identities, of Englishness and Irishness... The message from each is that second-generation Irish are 'really English' and many of the second-generation resist this." This perspective suggests that symbolic ethnicity is a result of assimilation and some assimilated individuals may prefer to explore a culture that they may not have been raised with to a significant extent. Many displays of what could be argued to be symbolic ethnicity, such as the study of Scottish Gaelic by the Scottish diaspora in North America, do not necessarily conform to the stereotype of individuals feeling entitled to a cultural ethnicity due to ancestry. Most Gaelic learners in one study, even those with Scottish ancestry, stated that Gaelic ethnic identity was not related to ancestry. See also Plastic Paddy Symbolic religiosity Tartanry References Further reading Gans, Herbert J. (1 January 1979). "Symbolic ethnicity: The future of ethnic groups and cultures in America*". Ethnic and Racial Studies. 2 (1): 1–20. doi:10.1080/01419870.1979.9993248. Waters, Mary C. (1990). Ethnic options : choosing identities in America. Berkeley: University of California Press. ISBN 9780520070837. The social impact of thong underwear has been covered extensively in the media, ranging from bans on wearing thongs to thongs for children. Overview Monica Lewinsky gave evidence during the Lewinsky scandal that she was flirting with Bill Clinton in Leon Panetta's office, and that she lifted her jacket to show him the straps of her thong underwear above her pants. Some of the news media in America used thong underwear as a metonym for smut in the Starr Report. According to feminist commentator Carrie Lukas, Lewinsky "with her thong-snapping seduction, forever changed the image of the D.C. junior staffer from aspiring policy wonk to sexual temptress." Marketing analysts Marian L. Salzman, Ira Matathia and Ann O'Reilly observed in the book Buzz: Harness the Power of Influence and Create Demand that thong brands are riding on the wide media coverage of thongs to create buzz. Photographer Lauren Greenfield wrote in her book Girl Culture, "Understanding the dialectic between the extreme and the mainstream – the anorexic and the dieter, the stripper and the teenager who bares her midriff or wears a thong – is essential to understanding contemporary feminine identity." In 2004, political commentator Cedric Muhammad wrote in essay The Thong versus the Veil, "We wondered at the end of the day, of the two groups of women most prominently featured on American TV these days, who gains more respect for their intellect and spirit – the Afghan woman who is so totally veiled that you can't even see her eyes or the Black woman in the R&B and Hip-Hop video who dances while wearing a bikini and thong?" Christian commentary The rise of thong usage has been asserted by Christian minister Oneil McQuick to be linked to a rise of sexualization in society, and by Christian writer Philo Thelos to be linked to a rise in the desire to go unclothed. When discussing the trend of wearing thongs, Christian writer Sharon Daugherty commented in her book What Guys See That Girls Don't: Or Do They? that the fashion industry "may have changed the mindset of our society". This was followed by her observation that "the whole idea of wearing so that no panty line or bumps can show isn't substantiated" and that "the thong was created by fashion designers to arouse sexual thoughts". School restrictions In 1999 a Miami University male professor was banned from using the school's recreation center because he refused to stop wearing thong swimwear. The professor challenged the school in court. In 2000 a high school principal in Salinas, California was in the center of a variety of controversies including bans on certain types of clothing to the extent that "thong panties were unofficially banned." One student alleged that she was given a dress-code violation note for wearing a thong. This story made national headlines in the United States. In 2002, a female high school vice principal in San Diego physically checked up to 100 female students' underwear as they entered the school for a dance, with or without student permission, causing an uproar among students and some parents and eliciting an investigation by the school into the vice principal's conduct. In her defense, the vice principal said the checks were for student safety and not specifically because of the wearing of thongs. In 2003, the head teacher of a British primary school voiced her concern after learning that female students as young as 10 were wearing thong underwear to school. This incident led to a media debate about the appropriateness of thong underwear marketed to young girls. In 2003 the University of Victoria Law School in Canada briefly put the school logo on thongs, but quickly pulled them from sale after controversy sprang up. In the mid-2000s the dress code for St. Ambrose Academy, a Roman Catholic middle school and high school in Madison, Wisconsin, specifically described swimsuits with "thong-cut legs" as inappropriate. In the same decade Dixon High School in Dixon, California had a dress code which specified that all undergarments – specifically listing thongs, along with bras and briefs – must be covered. Sports In 2012 the 4-H program at the University of California specifically forbade "string, thong or crochet" swimsuits for women. For men, the dress code specified "swim trunks only (no shorts, cut-off pants, or Speedos)." A similar policy by Virginia FCCLA bans "skimpy bikini or thong type suits" for women and specifies "swim trunks" for men ("no speedos"). In 2001, Vicky Botwright, then 16th seeded in women's squash circuit and dubbed the "Lancashire Hot Bot", was prohibited by the Women's International Squash Players Association (WISPA) from wearing a thong and a sports bra in the British Open Championships. Initially, WISPA was "suggesting a thong was inappropriate", but in the end decided no formal ruling was needed against thongs. Botwright stated that "we should be able to wear skimpy clothing if we want to, as some of the more conventional outfits we wear can be quite restrictive..." In 2004, Alexander Putnam competed in the London Marathon in a green thong and painted as a tropical tree to protest against logging in Congo. Female bodybuilders in America are prohibited from wearing thong or T-back swimsuits if contests are being filmed for television, otherwise they are allowed to do so. For younger girls It is difficult to point out exactly when younger girls, or those under eighteen years old, started to wear thongs. However, in 1997, an owner of a Fort Worth, Texas boutique noted that many high school girls wore thongs under their dresses when attending prom. By 1999, there is at least one documented case of middle-school aged girls wearing them. The popularity of thongs among young girls was to an extent that thong sales for tweens, or those between 7 and 12 years old, totaled about $400,000 in 2000 but increased to about $1.6 million for 2003. The 2000s saw a rise in the popularity of thongs among younger girls, who have been dubbed "thong feminists" by comedian Janeane Garofalo. According to child therapist Ron Taffel, when 12-year-old girls wear a thong, "it's not about rebellion against adults"; rather, he says that the thong is a "statement to other kids that they are part of this very, very intense, powerful second family of peer group and pop culture that is shaping kids' wants, needs and feelings." Developmental psychologist Deborah Tolman does not agree that all young girl thong-wearing is sexualized; she states that "[k]ids are engaged with their sexuality at younger ages, but they're not necessarily sexually active", and she says tween thong wearers may be facing "...social pressure to look sexy – without crossing over the murky line into seeming slutty". Understanding the risqué nature of the underwear, in order to buy thongs, girls adopted a number of strategies. At times, after a bit of prodding, some would receive permission from their parents to buy and wear them. While others did not receive parental permission to purchase thong underwear, they would do so anyway by using their allowance. Knowing that some parents who discovered that their daughters bought thongs without permission would throw the underwear in the garbage, many girls would try their best to hide the underwear during laundry days. The growing popularity of thongs among young girls strongly affected the environment of many middle and high schools across the Western world. By 2004, in at least one American high school, thong exposure was common enough that one student stated that it happened "all the time. Several times a day." Whether revealed accidentally or purposefully, the underwear became a ubiquitous part of many middle and high schools with girls exposing their thongs walking to school, sitting down in class or in the cafeteria, bending over at a locker, and even while participating at school-sanctioned functions such as dances. The look became so ubiquitous that when describing teenage actress Keira Knightley, who was described as wearing "baggy trousers hanging off her hips to expose a flash of pink thong," the Evening Standard stated that Knightley "look[ed] like any teenager." With thongs entrenched as a must-have item by the mid-2000s, some girls experienced strong pressure to wear them. In 2005, at one American high school, a high school first-year student, who did not like thongs, noted that she might have been the only student in her physical education class who was not wearing them. As one of the few who did not, while changing in the locker, another student teased her and called her "Granny" due to her decision not to wear thongs. A thirteen-year-old living in Canada, writing in 2006, expressed similar frustrations; some of her peers made fun of her because she did not wear thongs. Adding insult to injury, she noted that her friends could wear thongs but she could not. While sometimes the pressures to wear thongs were explicit, at other times they may have been purely in one's head. A high school first-year student living in the United States, writing in 2006, noted the mental difficulties of changing in a locker room where most of the girls wore thongs. While this particular student did not note being explicitly teased by her peers, she imagined that other girls were doing so behind her back. This feeling played a role in her desire to ask her mother for thongs. Entertainment media would pick up on the trend. In one particularly infamous episode from the show Degrassi: Next Generation, the teenaged character Manny Santos dropped her innocent look by adopting a more risqué fashion sense that showcased an exposed rhinestone-studded blue thong over low-rise jeans. The episode may have been inspired by a real-life decision made by her real-life actress Cassie Steele. In an effort "to be taken seriously and be mature and more sexy", the then-teenager purposefully revealed a whale tail during rehearsal. Afterwards, the aforementioned episode would be written and produced. Degrassi was not the only form of media to showcase the trend. Kaley Cuoco, portraying 17-year-old high school student Bridget, would reveal her thong in multiple scenes for the premiere episode of the 2002 show 8 Simple Rules. When teased, Bridget, perhaps echoing the sentiments of many girls her age, defiantly noted that she is part of the "thong generation." In the 2003 film Thirteen, Evie Zamora, played by actress Nikki Reed, is portrayed as a free-spirited but troubled middle school student. One key aspect of Evie's behavior is her tendency to expose her thong underwear. Evan Rachel Wood, who portrayed fellow middle school student Tracy Freeland, befriended Evie and quickly adopted her exposed thong habit. Tracy's appropriation of this behavior is an important part of her character's transformation from being a mild-mannered honor student to a troubled teenager who steals and does drugs. The trend has been attributed to pop idols like Britney Spears and Jennifer Lopez. In 2002 Abercrombie & Fitch released a line of thong underwear targeted for girls ages 10–16, though critics pointed out that children as young as seven could fit one of the thongs. A spokesman for A&F, Hampton Carney, stated that he could list "at least 100 reasons why a young girl would want thong underwear." This controversy spawned a great deal of free publicity for Abercrombie, including a chain letter that received wide circulation. In 2007 British retailer Argos removed from sale its G-string panties and padded bras for nine-year-old girls, following negative response from the public. In Japan, photobooks and DVDs of underaged girls in T-back thongs have become popular as "T-back Junior Idols", a phenomenon which has been criticised as a disguised form of child pornography. Other incidents In October 2002, Florida officials banned thongs from Daytona Beach. In 2007 the Tennessee Department of Correction banned prison visitors from wearing thong or g-string underwear. In the words of Correction Commissioner George Little, prisoners "don't need any help getting turned on." In 2019, an online retailer's high-cut 'front thong' bodysuit garnered a negative response from women, who felt the garment was too revealing and that it could cause discomfort to the genital area. Popular culture In 1999, R&B singer Sisqó recorded the "Thong Song" on his Unleash the Dragon album. Writer-director Glen Weiss made three movies titled the Thong Girl, based on the comic book of the same name. The story of the films revolved around the Thong Girl, an independent superhero. Parts of the film were shot in Nashville's mayor's office in 2007. In 2007, Beermaker Rolling Rock aired a commercial poking fun of male thong wearers during Super Bowl XLI. In 2002, Shefali Zariwala, a model in India, became known as the "thong girl" for her performance in which her thong is visible, in the music video of Kaanta Laga. The album sold two million copies and the music video became an overnight success in India. In 2008, a diamond studded thong worth US$122,000 was featured in a Singapore lingerie fashion show. It had 518 brilliant-cut diamonds, totaling 30 carats (6.0 g), studded into the front of a black lace thong in a floral pattern, as well as 27 white gold tassels hanging off it. Actor Tom Holland revealed his experience wearing a thong for his role as Spider-Man in 2021, citing the garment's unique role in his costume design. This garnered widespread media coverage and sparked discussions about the practicality and impact of thong underwear in modern fashion and film. See also Sexual objectification Sexual revolution Underwear as outerwear Whale tail == References == Science Theatre is an undergraduate student-run science outreach organization at Michigan State University's East Lansing campus, performing science theatre. Science Theatre visits schools and events throughout Michigan performing interactive science demonstrations for K-12 students on-stage or up-close. Science Theatre performers are undergraduate and graduate student volunteers and all performances are made free of charge. The group's performances consist of arrangements from its catalog of more than seventy demonstrations in biology, chemistry, and physics. Additionally, Science Theatre performs comprehensive shows in Astronomy, Environmental science, Pressure, the Periodic Table, Quantum Mechanics, and FRIB-related science. Science Theatre was founded in April 1991 under a grant from the National Science Foundation and received the 1993 AAAS Award for Public Understanding of Science and Technology. Science Theatre is a four-time winner of the Outreach Award from the Michigan State University Department of Physics and Astronomy. References MSU Science Theatre Official Webpage 1993 AAAS Award for Public Understanding of Science and Technology 2 February 2009 State News coverage 10 March 2009 Fox TV coverage 11 March 2009 Daily Press (Michigan) coverage 10 June 2009 Capital Gains Coverage Rural delivery service refers to services for the delivery of mail to rural areas. In many countries, rural mail delivery follows different rules and practices from that in urban areas. For example, in some areas rural delivery may require homeowners to travel to a centralized mail delivery depot or a community mailbox rather than being directly served by a door-to-door mail carrier; and even if direct door-to-door delivery is offered, houses still may even not have their own unique mailing addresses at all, but an entire road instead may be assigned a single common address, such as a rural route number. Examples include Rural Free Delivery in the United States, the rural route system in Canada, and the Rural Mail Box addressing system in Australia. Because of the differences in the handling and delivery of mail in rural areas, rural letter carriers often follow different regulatory standards from urban postal workers; for example, rural postal delivery workers may not be required to wear a uniform and may be allowed to use their own vehicles rather than driving a postal truck. In Canada, rural letter carriers were for many years not considered employees of Canada Post but private contractors. See also List of postal entities Timeline of postal history Rural Internet == References == Monoculturalism is the policy or process of supporting, advocating, or allowing the expression of the culture of a single social or ethnic group. It generally stems from beliefs within the dominant group that their cultural practices are superior to those of minority groups and is often related to the concept of ethnocentrism, which involves judging another culture based on the values and standards of one's own culture, though this is usually untrue if cultural nationalism is dominant, as opposed to ethno-nationalism. It may also involve the process of assimilation whereby other ethnic groups are expected to adopt the culture and practices of the dominant ethnic group. Monoculturalism, in the context of cultural diversity, is the opposite of multiculturalism. Rather than the suppression of different ethnic groups within a given society, sometimes monoculturalism manifests as the active preservation of a country's national culture via the exclusion of external influences. Japan, South Korea, and North Korea are examples of this form of monoculturalism. However it may also be the result of less intentional factors such as geographic isolation, historical racial homogeneity, or political isolation. Ethnocentric monoculturalism Monoculturalism is often closely associated with ethnocentrism. Ethnocentrism is the practice of framing one's way of life as natural and valid, and applying that belief system to interpret the characteristics of other cultures. Instances In genocide Many of the genocides practiced throughout history were based on ethnic supremacy. Ethnic supremacy is assumed by one group within a culture, following some distinct action by an external group or from one of the ethnic groups. With European intervention in places like Rwanda, social institutions worked to socially construct an ethnic inferiority, distinguishing the Hutus and Tutsis from one another and causing what would be one of the most horrific demonstrations of genocide in modern history. A similar example to that of the Rwandan genocide was the ongoing civil war in Burma. The civil war spanned from a constitution that granted Burma their independence from the British Empire in which a group of leaders created conditions that did not involve many of Burma's ethnic minorities, and instigated a fight from them. Many of these ethnic minorities in Burma, including the Karen, have been significantly displaced by the military junta and placed into refugee camps in bordering nations. The remaining ethnic minorities have been living in poor conditions, and have been met by a variety of human rights abuses. Globalization Globalization involves the free movement of goods, capital, services, people, technology and information throughout the world. It also involves the international integration of potentially very different countries through the adoption of the same or similar worldviews, ideologies, and other aspects of culture. American academic Anthony J. Marsella argues that this is monoculturalism on a grand scale. Potentially it could lead to the suppression and loss of different ethnic cultures on a global scale. See also Criticism of multiculturalism Cultural diversity Cultural homogenization Monoethnicity References Further reading Tambini, Damian (1996). "Explaining monoculturalism: Beyond Gellner's theory of nationalism". Critical Review: A Journal of Politics and Society. 10 (2): 251–270. doi:10.1080/08913819608443420. Conversi, Daniele (2008). "Democracy, Nationalism and Culture: A Social Critique of Liberal Monoculturalism". Sociology Compass. 2 (1): 156–182. doi:10.1111/j.1751-9020.2007.00063.x. Civilian victimization is the intentional use of violence against noncombatants in a conflict. It includes both lethal forms of violence (such as killings), as well as non-lethal forms of violence such as torture, forced expulsion, and rape. According to this definition, civilian victimization is only a subset of harm that occurs to civilians during conflict, excluding that considered collateral damage of military activity. However, "the distinction between intentional and unintentional violence is highly ambivalent" and difficult to determine in many cases. Scholars have identified various factors that may either provide incentives for the use of violence against civilians, or create incentives for restraint. Violence against civilians occurs in many types of civil conflict, and can include any acts in which force is used to harm or damage civilians or civilian targets. It can be lethal or nonlethal. During periods of armed conflict, there are structures, actors, and processes at a number of levels that affect the likelihood of violence against civilians. Violence towards civilians is not “irrational, random, or the result of ancient hatreds between ethnic groups.”: 91 Rather, violence against civilians may be used strategically in a variety of ways, including attempts to increase civilian cooperation and support; increase costs to an opponent by targeting their civilian supporters; and physically separate an opponent from its civilian supporters by removing civilians from an area. Patterns of violence towards civilians can be described at a variety of levels and a number of determinants of violence against civilians have been identified. Describing patterns of violence Francisco Gutiérrez-Sanín and Elisabeth Jean Wood have proposed a conceptualization of political violence that describes an actor in terms of its pattern of violence, based on the "repertoire, targeting, frequency, and technique in which it regularly engages." Actors can include any organized group fighting for political objectives. Repertoire covers the forms of violence used; targeting identifies those attacked in terms of social group; frequency is the measurable occurrence of violence; and techniques are the types of weapons or technology used. This framework can be applied to observed patterns of violence without considering the intentionality of the actor. Other frameworks focus on motivation of the actor. Repertoires may include both lethal forms of violence against civilians such as killings, massacres, bombings, and terrorist attacks, and nonlethal forms of violence, such as forced displacement and sexual violence. In indirect violence heavy weapons such as tanks or fighter planes are used remotely and unilaterally. In direct violence perpetrators act face-to-face with the victims using small weapons such as machetes or rifles. Targets may be chosen collectively, as members of a particular ethnic, religious, or political group. This is sometimes referred to as categorical violence. Targets may also be chosen selectively, identifying specific individuals who are seen as opposing a political group or aiding its opponents. Techniques can vary greatly depending on the level of technology and amount of resources available to combatants. There are considerable impacts of technology over time, including the introduction of new technologies of rebellion. For example, changes in communication infrastructure may affect violence against civilians. If such technology facilitates organization by armed groups and increases contests over territory, violence against civilians in those areas is also likely to increase. As government surveillance of digital information increases, the use of targeted, selective violence against civilians by governments has been shown to increase. Analysis of levels of violence Theoretical explanations at various levels of analysis can co-exist and interact with one another. The following levels of analysis can be useful in understanding such dynamics: International At the international level, institutions, ideologies and the distribution of power and resources shape technologies of rebellion and political interactions, including both international and domestic wars. During the Cold War, the United States and the Soviet Union provided military and financial backing to both governments and rebellious groups, who engaged in irregular civil wars. Such conflicts frequently involve the use of violence to control civilians and territory. The decade following the dissolution of the Soviet Union was marked by a decline in worldwide battle deaths and the number of armed conflicts in the world. International norms and ideas also influence conflict and the use of violence against civilians. The period following World War II, from 1946 to 2013, has also been regarded as showing a decline in conflict. The United Nations General Assembly adopted the Universal Declaration of Human Rights in 1946. International actors signed the Genocide Convention in 1948 and the Geneva Conventions in 1949, formalizing protections for noncombatants and international norms for human rights and humanitarian standards. Transnational non-governmental organizations such as Human Rights Watch and Amnesty International have become active in surfacing information, advocating for human rights, mobilizing international public opinion, and influencing both social norms and international law. Interactions between foreign governments and rebel groups who receive their support can affect violence against civilians. Groups receiving external support become less dependent on local civilian populations and have less incentive to limit violence against civilians. Foreign aid to rebels is associated with higher levels of both combat-related death and civilian targeting. However, foreign actors that are democracies or have strong human rights lobbies are less likely to support groups that engage in violence against civilians. The international strategic environment also shapes government perceptions of threat. Perceptions of threat due to external military intervention may lead to increases in governmental mass killing of civilians and violence against domestic out-groups. The scrutiny and criticism of international and domestic actors can affect government use of violence, by increasing the perceived costs of violence against civilians. Governments and rebel groups that are vulnerable domestically and that seek international legitimacy are more likely to comply with international humanitarian law and exercise restraint toward civilians. Domestic and subnational Political organization occurs not just at a national level, but at many levels, including provinces, states, legislative districts, and cities. In many countries, national and local politics differ in scale and in the extent to which subnational governments afford and support their citizen's political rights and civil liberties. Relationships between government (at various levels), armed groups and domestic populations affect violence against civilians. Governments that rely on a broad base of domestic and institutional support are more likely to exercise restraint toward civilians. These may include democratic governments, inclusive governments, and governments in which institutions have not consolidated power. Similarly, rebel groups that need the support of a broad domestic constituency or of local civilians are less likely to target civilians and to engage in terrorism. Rebel groups whose political constituents live in the area that they control are more likely to use governance structures like elections to obtain cooperation and less likely to use political violence. Rebel groups that control areas inhabited by nonconstituents are more likely to use violence to obtain resources and cooperation. Ideology may strongly influence the ways in which governments and rebels define their constituencies, affecting patterns of violence. Where national, subnational or local institutions follow exclusionary ideologies, ethnic or other out-groups may become identified as nonconstituents and targeted, sometimes to the point of displacement, ethnic cleansing or genocide. Violence against civilians may vary over space and time with the extent to which military forces are contesting a territory. Stathis Kalyvas has theorized that selective violence is more likely to occur where control is asymmetric, with one group exercising dominant but not complete control of an area. Indiscriminate violence may be more likely to occur where one side controls an area. It has also been shown that indiscriminate violence is more likely to occur at a distance from a country's center of power. Opinions differ widely on whether there is a relationship between the relative military capacity of a government or rebel group and the likelihood that it will engage in patterns of violence against civilians. This may also vary depending on the type of violence involved. However, there is evidence that cutting off access to external sources of support may cause a group to become more dependent on the support of its local population and less likely to engage in violence against civilians. Organizational At the organizational level, researchers have examined the dynamics and ideology of armed groups: how they recruit and train their members, how organizational norms about the use of violence against civilians are established and maintained, and the role of group leaders and political ideology in shaping organizations and behavior. While some studies argue that violence against civilians reflects a lack of control over an organization's members and the absence of norms that inhibit violence, other researchers emphasize the social dynamics of armed groups and ways in which they may actively break down social norms that inhibit violence. Jeremy Weinstein has argued that armed groups develop certain organizational structures and characteristics as a result of their available resources. According to this view, organizations that depend on external resources are predicted to attract low-commitment members, and have trouble controlling their use of violence against civilians. Organizations that are dependent on local resources will tend to attract higher-commitment, ideologically motivated members from local communities, which will help to control their use of violence against civilians. Other researchers focus on organizational structure and its effects on behavior, without assuming that they are driven by resource endowment. They suggest that processes of education, training, and organizational control are important both in producing strategic violence and in establishing restraints against the use of violence against civilians. The ideology of armed groups is a key factor influencing both their organizational structure and member behavior. Some Marxist groups, which emphasize political education, have been less likely to use violence against civilians. The ideology of other armed groups, including governments, can actively promote violence and direct it at particular targets. Such groups often use "exclusionary ethnic or national ideologies or narratives" which have resulted in mass killings and genocide. Accounts from multiple countries have documented the "practice, norms, and other socialization processes" which armed groups have used to gain recruits, socialize group members, establish new norms of behavior and build group cohesion. Methods can include forced recruitment, systematic brutalization, and gang rape. Such groups create a “culture of violence” in which "horrifying acts of cruelty" are directed at both group members and civilians and become routine. The risk to civilians from such organizations is high. Individual On an individual level, people may be influenced to participate in armed conflicts due to economic motivations or incentive structures. Research in this area often views violence against civilians as a by-product of economic processes such as competition for resources. Researchers have also studied emotional and psychological factors relating to the use of violence, which are generally related to other factors such as strategy, opportunity, socialization, and other group-level processes. The emotions of shame, disgust, resentment, and anger have been linked to violence against civilians. While research suggests that emotions such as fear affect the polarization of attitudes, material and structural opportunities are important mediators of the expression of violence. At the individual level, researchers are examining the category of “civilian" in greater detail, to better understand the use of violence against different types of noncombatants. Such research also emphasizes the agency of civilians who are themselves actors during wartime and the ways in which they may respond to armed groups. There is evidence to suggest that local civilian institutions can sometimes mitigate violence by governments and rebel groups. Research also examines concerns such as the use of violence against humanitarian aid workers, and the targeting of women. Consequences of violence against civilians A relatively new area of research asks how individuals, groups, communities and domestic and international audiences respond to violence against civilians. Legacies of violence can last for many years and across generations, long after the violence occurred. Evidence on the effects of wartime violence on ethnic polarization is mixed. Research from various countries suggests that civilian responses to violence are not uniform. However, civilians do blame actors who have acted violently against their communities, and may withdraw their support, provide support to opposing forces, or vote for an opposing political party in elections. Such outcomes are more likely to occur in the area where the violence was experienced, and when the perpetrators of violence are considered outsiders. Individuals are likely to respond to violence by rejecting the ideology of the perpetrating group, particularly if the violence was severe. Those exposed to violence are likely to engage in prosocial behavior and to increase their political engagement. Research on the effectiveness of groups using violence against civilians in gaining political ends is mixed. Macro-level evidence suggests that rebel groups are likely to gain support from Western international actors in situations where governments are employing violence against civilians and rebel groups are showing restraint towards civilians. The United Nations is more likely to deploy peacekeepers when conflicts involve high levels of violence towards civilians. However, peacekeeping missions are more likely to be effective at protecting civilians from rebel groups than from governments. See also Child murder Civilian casualty ratio Collective punishment Dehumanization Gaza Strip famine Genocide Incitement to genocide Indiscriminate attack State crime == References == Women-led uprisings are mass protests that are initiated by women as an act of resistance or rebellion in defiance of an established government. A protest is a statement or action taken part to express disapproval of or object an authority, most commonly led in order to influence public opinion or government policy. They range from village food riots against imposed taxes to protests that initiated the Russian Revolution. Some women-led mass protests deliberately set out to emphasise the gender (or gender role) of the organisers and participants: for example, the Mothers of the Plaza de Mayo emphasised their common role as mothers by marching in white headscarves to symbolise the diapers of their lost children. In other settings, women may strip naked in order to draw attention to their cause, or to shame or intimidate those whom they are protesting.: 21 Early history The creation of the first human societies Studies of contemporary hunter-gatherers show that their strong sense of moral community is maintained by autonomous individuals who constantly resist any form of personal domination. In fact, many hunter-gatherers are so egalitarian and communistic that even a non-Marxist anthropologist like Christopher Boehm argues that hunter-gatherer societies – the first human societies – must have originated in uprisings against dominant males. Chris Knight, and other anthropologists influenced by Karl Marx and Friedrich Engels, have theorised that these uprisings were led by women looking for collective support to ease their childcare burdens. They have used a wide range of evidence from anthropology, primatology, mythic narratives, evolutionary biology and archaeology. Some Marxists have dismissed these ideas. However, although the idea of women-led uprisings creating the first societies is controversial, a number of highly respected anthropologists have taken the thesis seriously. (Mary Douglas, Robin Dunbar, David Lewis-Williams, Caroline Humphrey, Marilyn Strathern, Clive Gamble, Keith Hart and Chris Stringer have all made favourable comments about Knight's work.) Boudica Boudica was a queen of the British Celtic Iceni tribe who led an uprising against the conquering forces of the Roman Empire in AD 60 or 61. She died shortly after its failure and was said to have poisoned herself. She is considered a British folk hero. 17th and 18th centuries Food riots E. P. Thompson's classic article "The Moral Economy of the English Crowd in the 18th Century" emphasised women's role in many food riots. He argued that the rioters insisted on the idea of a moral community that was obliged to feed them and their families. As one contemporary commentator wrote: "Women are more disposed to be mutinous ... [and] in all public tumults they are foremost in violence and ferocity." John Bohstedt later argued that Thompson had exaggerated women's role in food riots. Thompson responded by forcefully rejecting Bohstedt's criticism. While it is not possible to know the exact level of women's involvement in 18th century food riots, it appears that, at the very least, women led or initiated a significant minority of such riots and they participated in many more. Women participated more fully in food riots than they had in earlier anti-impressment riots of 1747, in which they defended community interest and enforced community morality. These riots of revolution and resistance opened up opportunities for women to take political action as social and economic influencers, and not just as a republican's wife or a mother. He indicates that women's new assertiveness had something to do with the weakening of the patriarchal control of women as feudalism declined and market relations expanded. Men and women participated in food riots in Ireland, Belgium, the Netherlands and Germany (where contemporary reports claimed that women initiated many riots). Dutch tax riots in the seventeenth and eighteenth centuries were more numerous and often more violent, with participants of both lower and lower-middle classes, whereas food riots drew only lower-class participants. In the seventeenth and eighteenth centuries, at least 26 riots and 50 demonstrations involved women, with 10 being chiefly under their control. Women did the cooking and purchased foodstuffs for their families; therefore, they were the first parties to be confronted with scarcities of food and high prices. In doing so, women generally controlled much of the household finances. Another reason for their participation is due to the fact that food riots typically started in market places near shops and mills, which is where women gathered the most. One of the most prominent tax riots in 1616 has even gone on record as "the Women's Revolt of Delft". Women also conducted nearly a third of food riots during the American Revolution despite the fact that they were excluded from the vote, unqualified to serve as jurors at courts and law, and were essentially politically disabled by their dependent status. It made a difference that Americans knew that women figured prominently in food riots in England and Europe, and it made a difference that ideas of equity, neighborly dealing, and charity informed American women's daily lives in the colonial period. Roughly 100 women marched a "Female Riot" and took to the streets in July 1777 insisting on their right to enforce equitable exchange. 19th century French Revolution Women were especially prominent in food riots in French marketplaces (although men dominated those in the countryside). The most momentous French food riot was the Women's March on Versailles. This occurred in October 1789, when the market women of Paris began calling the men 'cowards' and declaring: 'We will take over!' The women proceeded to march to Versailles with soldiers following them. The crowd then forced the King to return to Paris where, three years later, women were again major participants in the demonstrations that led to the abolition of the monarchy. A police inspector said in 1793: "It is mainly the women who are stirred up, women who in turn communicate all their frenzy to the men, heating them up with their seditious propositions and stimulating the most violent effervescence." Meanwhile, women in the countryside initiated 'counter-revolutionary' protests against the new government's policies of the repression of the Church and the conscription of male peasants into the army. During the French Revolution, women led the fight for religion. Their fight would lead the way for the feminization of religions. Women felt that they were responsible for maintaining a spiritual balance within their family. They fought harder than their male counterparts, sometimes invoking violent and illegal actions to get their voices heard. If women were arrested, the men in their lives would downplay the damage they could do, and women were seen as more hysterical and vulnerable as a whole, so society generally thought little of their violent and illegal actions. But if a woman refused or avoided taking part in petitions or marches, she would be shamed until guilted into taking part. Women during the French Revolution also fought for their own rights. Aristocratic women were not as likely to partake in the activities that could ruin their family and/or their chance of inheriting the family fortune (or what she would receive), so they were reluctant to participate. Working class women also faced this dilemma, but because they were already suppressed, the good of what they could achieve outweighed the loss of family pride and/or fortune. Louis XVI had allowed all people who paid taxes to vote, but since women could not pay taxes, they could not vote. While the Third Estate made rules, women would present their opinions through pamphlets and petitions to let the Third Estate know what they wanted. Pétition des femmes du Tiers-Etat au Roi stated that women wanted education to go beyond French and Latin for church, more jobs to be available to women, and to raise the maximum pay of 5–6 sous. Motion en Faveur du Sexe and Discours préliminaire de la pauvre Javotte focused on dowries and marriage. In the working class, finding a job was hard enough for men, harder for women, and saving enough money to get married was almost impossible. Women did not want to have to pay a dowry to get married; this applied only in the Third Estate. The Rights of Woman by Olympe de Gouges was a complete pamphlet that stated all the rights that women should have. It was copied almost word for word from the Declaration of the Rights of Man, but applied to women. Another booklet, Griefs et plaintes des femmes mal mariées, criticised marriage laws that entailed women submitting to men, and demanded the legalization of divorce. Within the bourgeoisie, Madame Etta Palm van Aelder was a leading figure in fighting for women's rights. She demanded the equal right to education, political freedom, divorce and the legal freedom of women of age 21 and over. While political freedom would not be gained until after the Revolution, all of van Aelder's other demands would be met in some way. In August 1792, women aged 21 and above were given legal freedom from their parents. In September 1792, women were granted the right to divorce and the Law of 1794 eased the divorce process. Educational programs were advanced and allowed women to be trained for careers, but they still did not obtain equality. Female teachers were paid less than males and primary school classes were divided by gender. After the advancements and improvements in the educational system, women were not much better off than before. R. B. Rose argues that despite the efforts of women during the Revolution, little changed. The Revolution was a revolution for the men, and a place of chaos for the women. The French Revolutionary Constitution of 1791 allowed women to be labeled as citizens, but nothing else. They did not have voting rights or the ability to run for office. The Napoleonic Code of 1804 suppressed wives into submission to their husbands, reversing all equality demands made during the Revolution. Women still could not own land because they could not legally sign any contract, putting the land into the hands of her closest male relative. Franco-Prussian War During the Franco-Prussian War of 1871, women were prominent in preventing the army from moving their cannons from Paris, an event which helped spark the Paris Commune. 20th century British women's suffrage movement During the early twentieth century, women's protests for the right to vote became particularly militant in Britain. Many of the core organisers of mass protests were women. Strategies used by protestors included mass demonstrations, arson, widespread window breaking and attempts to storm both Parliament and Buckingham Palace. After World War I broke out in 1914, the mainstream suffragette movement suspended its protests in order to focus on the war effort. World War I During World War I, women led large numbers of food riots in Germany, Russia, Italy and elsewhere. Women workers also led the way in strike-waves in Berlin and Paris. The German authorities reported that union leaders were doing 'everything possible to prevent such disturbances and strikes over food provisions, but ... it is the countless female workers who constantly agitate and stir things up.' Women's prominence in these struggles helped delegitimize the war, and the regimes that were fighting it, paving the way for the huge strike-waves and revolutions at the end of the war. Women participated in and organised several food riots that broke out in North America during the early twentieth century. Women also led food riots in Japan and non-belligerent Spain. Women's protests against high food prices spread across Spain in both 1913 and 1918. In Barcelona, in 1918, women used the slogan: 'In the name of humanity, all women take to the streets!' They organised repeated demonstrations and attacked shops, warehouses, government offices and music halls. Women also staged food riots during the Spanish Civil War. Russian Revolution Karl Marx had recognized that "great social revolutions are impossible without the feminine ferment" and, in 1917, it was Petrograd's (Saint Petersburg) female workers who spread the idea of a general strike on 8 March, International Women's Day. On that day, hundreds of women threw stones and snowballs at factory windows demanding for bread. Economic depression in 1917 particularly devastated the working-class women because prices of daily necessities increased tremendously, but their low wages did not compensate for the increase in prices of goods. After a long and tiring day of labor, women had to line up for hours just to get a loaf of bread. Sometimes after wasting hours waiting in line, the bread would run out. The protests were led by thousand of female workers and inspired male factory workers to join them and demand changes. Women participated in the riots by attacking police stations and bakeries. However, many troops refused to shoot protesting women, who were often walking with their children. As Leon Trotsky later wrote, the women took hold of the soldiers' rifles and 'beseeched almost commanded: "put down your bayonets and join us"', and, within five days, the centuries-old Tsarist regime had collapsed. As Bolsheviks took over Petrograd and Moscow in October 1917, tense relationship were discern between rulers and working-class women. Scarcity and hunger made it very difficult for Russian workers to transform society themselves and women's participation did not continue at the same level as in February–March 1917. However, it was women's food protests, in May 1918, that sparked the first major wave of workers' unrest against the new Bolshevik authorities. Later, during Joseph Stalin's program of breakneck industrialization and forced collectivization, women were again at the forefront of the workers' strikes and peasant protests that resisted this brutal policy. Stalin's regime was, however, able to contain all resistance through starvation and repression. Colonial Nigeria There were several uprisings led by women in Colonial Nigeria. In November 1929, the Women's War began after thousands of Igbo women started rioting in protest over the actions of native warrant chiefs and introduction of new taxes on palm products. During the riots, Igbo women utilised sociocultural and sociopolitical networks which had existed prior to British colonisation, sending messages through market and kinship networks to other villages calling for a mikiri meeting. They also took advantage of strikes, boycotts, and force to project their opinions and retaliate against authority. Women of wealth and generosity who could speak well typically took leading roles in village wide mikiri gatherings, which had the largest influence on the rise of the Women's War. During mikiri, decisions were made on how to respond against being wronged by the Warrant Chief's corruption and by the taxes that they anticipated to be enforced upon them. During the late 1940s, the Abeokuta Women's Revolt protested the colonial government's imposition of new taxes upon women. Funmilayo Ransome-Kuti led mass protests of women outside the palace of Ladapo Ademola. United States civil rights movement It was a boycott of segregated buses by African-American women that sparked the civil rights movement in 1955. This case inspired activists across the world to make a change and fight oppression. American women increasingly rejected commonplace patriarchal family structures and sexual repression in the 1960s, influencing the sexual revolution, protests for equal pay, and a greater visibility of women in American culture. The revived feminist movement then helped transform gender roles in the following decades. Stormé DeLarverie, a biracial butch lesbian activist, is credited with inciting the Stonewall uprising in New York City in 1969, a major turning point in the 1960s–1970s gay liberation movement. United Kingdom Women were also at the forefront of many working class struggles in the 1970s and 1980s. In the British Isles, women's protests and leadership were significant during The Troubles in Northern Ireland, during the Grunwick dispute and during the miners' strike. Iran For decades, Iranian women struggled with basic human rights and oppression due to traditional religious affiliations and political attributes. Their Islamic beliefs regarding gender equality concealed by higher power authorities and the domination of man towards Iranian women. In 1979, in the course of the so-called Islamic Revolution to oust the Shah of Iran, women gathered and protested on the streets but the changes they had called for never came into being, instead a totalitarian state came into being called the Islamic Republic of Iran. It wasn't until 1990s where the impact of the Islamic Revolution became clearer as intolerant of women. This is when young women and activists started pushing against the Islamic ideologies, for instance, the processes of getting a divorce or wearing clothes that were considered "revealing" by the authoritarian rule. Significant changes in basic human rights and the oppression of Iranian women has been continuing since 1990s and it has been progressing notably until today with Gen Z Iranians having access to the world online. In 2022 the "first feminist revolution" began with the murder by state forces of 22 year old Kurdish-Iranian Mahsa Amini, sometimes now called the Mahsa Amini protests. The call for "Women, Life, Freedom" as become their signature chant and continues today, with heavy costs in lives of children, youths, adults and pensioners, in the cities and in the more mountainous, especially the Kurdish regions of Iran. The September 2022 protests have seen some shifts in sanctions from the United Kingdom, Canada and USA. However the struggle in Iran call for much further steps in Sanctions against Iran against a violent and oppressive regime that will neither allow women to enter football stadiums, nor walk without a headscarf and loose clothing in public. 21st century Women continue to play a prominent role in many food riots – for example, in 2008 over 1,000 women protested the Peruvian government's response to rising food prices. On January 20, 2017, the day after Donald Trump was inaugurated as the 45th US president, women, men, and children marched in protest of Trump and to promote solidarity with other women in order to resist women's oppression and mistreatment. Over 680 marches throughout the US and in more than 68 countries around the world were held as part of the Women's March. More than 1,000,000 people participated in the "flagship march" in Washington, D.C. Rojava Revolution Northern Syria is what is referred to as the Rojava. The Rojava revolution, or Rojava conflict, refers to the armed struggle that has taken place since 2012. The Rojava Revolution has been characterized by the prominent role women have had during these times of strife. The novel Revolution in Rojava: Democratic Autonomy and Women's Liberation in Syrian Kurdistan documents the Kurdish women and how they were and still are oppressed. As Kurds, they were denied basic rights, in many cases even citizenship; and as women they were trapped in patriarchal domination. The Kurdish women's movement seeks to overcome the alienation of Kurdish women. The fight for women's rights has always been a part of Kurdish history. One of the first signs of revolution in Rojava was the election of Hêvî Îbrahîm to the post of the prime minister in February 2014. Indeed, many women were assuming leadership positions. Asya Abdullah is regarded to be one of the most radical and effective revolutionaries in the world today. She has been the driving force in the battle for Kurdish freedom. She wants women around the world to become more aware of their own fight. With this transformation, women also began getting involved with security and military roles. In 2012, women from the PYD, the People's Defense Units, created a unit dedicated to the fight for women. The Women's fighting units, also known as YPJ, have played a role in the liberations of towns like Kobanî and Manbij. Since September 2014, Kurdish women have been playing a leading role in the fight against ISIS. The creation of the YPJ is a fascinating development in a region where women's rights are often repressed. But with the formation of these groups, has come with sexist media coverage. The media is more concerned with the fighters' looks rather than what they are fighting for. Women in the PKK guerrilla army follow Jineology which simply put, means women's science. The goal of jineology is to give women and society access to science and knowledge and to strengthen the connections of science and knowledge to society. The Rojava Revolution has been characterized by a high level of women participation in politics, safe houses for women dealing with sexual assault or violence, and women-run academies dedicated to the study of jineology. Out of the conflict of the popular uprising in Syria, Kurdish women took up arms alongside men and took control of Northern Syria, also known as Rojava. The women fight alongside men both in mixed units of the People's Defense Units (YPG) as well as in their own Women's Protection Units (YPJ). Women constitute an estimated 35–40% of the entire Kurdish forces. Mahsa Amini protests Civil unrest and protests against the government of Iran associated with the death in police custody of Mahsa Amini began on 16 September 2022 and are ongoing as of December 2022. Amini, a Kurdish women had been arrested by the Guidance Patrol, Iran's religious morality police, for allegedly violating Iran's mandatory hijab law, which requires all women to wear the hijab (Islamic veil) in public. The Guidance Patrol alleged that Amini was wearing her hijab improperly, and according to eyewitnesses, she had been severely beaten by officers, an assertion denied by Iranian authorities. As the protests spread from Amini's hometown of Saqqez to other cities in the province of Kurdistan and throughout the country, the government responded with widespread Internet blackouts, nationwide restrictions on social media usage, tear gas and gunfire. Although the protests have not been as deadly as those in 2019 (when more than 1,500 were killed), they have been "nationwide, spread across social classes, universities, the streets [and] schools", and called the "biggest challenge" to the government of Iran since the Islamic Revolution in 1979. as of 27 December 2022 at least 476 people, including 64 minors, had been killed as a result of the government's intervention in the protests; an estimated 18,480 have been arrested throughout at least 134 cities and towns, and at 132 universities. Supreme Leader Ayatollah Ali Khamenei dismissed the widespread unrest not only as "riots" but also as a "hybrid war" caused by foreign states and dissidents abroad. Women, including schoolchildren, have played a key role in the demonstrations, with many removing their hijab in solidarity with Amini. In addition to demands for increased rights for women, the protests have demanded the overthrow of the Islamic Republic, setting them apart from previous major protest movements in Iran, which have focused on election results or economic woes. See also International Women's Day Women's March on Versailles The Bread and Roses strike by immigrant textile workers in Lawrence, Massachusetts in 1912 was led to a large extent by women. Women's Social and Political Union February Revolution Huda Sha'arawi Rosa Parks Women Against Pit Closures Origins of society List of food riots List of women who led a revolt or rebellion Mud March (suffragists) Abolition Riot of 1836 Jenny Geddes Women in the decolonisation of Africa Notes References Further reading Thompson, E. P. Customs in Common. Smith, Barbara Clark (1994). "Food Rioters and the American Revolution". The William and Mary Quarterly. 51 (1): 3–38. doi:10.2307/2947003. ISSN 0043-5597. JSTOR 2947003. Hufton, Olwen H. (1992). Women and the Limits of Citizenship in the French Revolution. University of Toronto Press. ISBN 978-0-8020-6837-8. Applewhite, Harriet; Levy, Darlene. Women and Politics in the Age of Democratic Revolution. Kaplan, Temma (1987). "Women and Communal Strikes in the Crisis of 1917–1922". In Bridenthal, Renate (ed.). Becoming Visible, Women in European History. pp. 429–50. Applewhite, Harriet; Levy, Darlene (1987). "Women and Political Revolution in Paris". In Bridenthal, Renate (ed.). Becoming Visible, Women in European History. Houghton Mifflin. pp. 279–308. ISBN 978-0-395-41950-2. OCLC 15714486. Retrieved 23 August 2021. Daniel, Ute (1997). The War from Within: German Women in the First World War. Berg Publishers. ISBN 978-1-85973-147-5. Allen, Keith (2003). "Food and the German Home-Front". In Braybon, Gail (ed.). Evidence, History and the Great War. Berghahn Books. pp. 172–97. ISBN 978-1-57181-801-0. Ortaggi, Simonetta (2003). "Italian Women During the Great War". In Braybon, Gail (ed.). Evidence, History and the Great War. Berghahn Books. pp. 216–38. ISBN 978-1-57181-801-0. JSTOR j.ctt9qd8db.15. Retrieved 23 August 2021. Engel, Barbara Alpern (December 1997). "Subsistence Riots in Russia during World War One". Journal of Modern History. 69 (4): 696–721. doi:10.1086/245591. ISSN 0022-2801. JSTOR 10.1086/245591. S2CID 54573745. Kaplan, Temma (1 April 1982). "Female Consciousness and Collective Action: The Case of Barcelona, 1910–1918". Signs. 7 (3): 545–66. doi:10.1086/493899. ISSN 0097-9740. JSTOR 3173854. S2CID 144602956. Chatterjee, Choi (2002). Celebrating Women; Gender, Festival, Culture and Bolshevik Ideology. University of Pittsburgh Press. ISBN 978-0-8229-4178-1. Viola, Lynne (1986). "Babi Bunty and Peasant Women's Protests during Collectivisation". Russian Review. 45 (1): 23–42. doi:10.2307/129400. ISSN 0036-0341. JSTOR 129400. Kuumba, M. Bahati (2001). Gender and Social Movements. Rowman Altamira. ISBN 978-0-7591-0188-3. Knight, Chris. Solidarity and Sex. Knight, Chris; Power, Camilla; Watts, Ian (April 1995). "The Human Symbolic Revolution" (PDF). Cambridge Archaeological Journal. 5 (1): 75–114. doi:10.1017/S0959774300001190. ISSN 1474-0540. S2CID 54701302. Archived from the original (PDF) on 2013-09-15. In sociolinguistics, a sociolect is a form of language (non-standard dialect, restricted register) or a set of lexical items used by a socioeconomic class, profession, age group, or other social group. Sociolects involve both passive acquisition of particular communicative practices through association with a local community, as well as active learning and choice among speech or writing forms to demonstrate identification with particular groups. The term sociolect might refer to socially restricted dialects, but it is sometimes also treated as equivalent with the concept of register, or used as a synonym for jargon and slang. Sociolinguists—people who study sociolects and language variation—define a sociolect by examining the social distribution of specific linguistic terms. For example, a sociolinguist would examine the use of the second person pronoun you within a given population. If one distinct social group used yous as the plural form of the pronoun, then this could indicate the existence of a sociolect. A sociolect is distinct from a regional dialect (regiolect) because social class, rather than geographical subdivision, substantiates the unique linguistic features. Overview A sociolect, defined by leading sociolinguist and philosopher Peter Trudgill, is "a variety or lect which is thought of as being related to its speakers' social background rather than geographical background.": 122 This idea of a sociolect began with the commencement of dialectology, the study of different dialects in relation to society, which has been established in countries such as England for many years, but only recently has the field garnered more attention.: 26 However, as opposed to a dialect, the basic concept of a sociolect is that a person speaks in accordance with their social group whether it is with regard to one's ethnicity, age, gender, etc. As William Labov once said, "the sociolinguistic view ... is that we are programmed to learn to speak in ways that fit the general pattern of our communities.": 6 Therefore, what we are surrounded with in our environment determines how we speak; hence, our actions and associations. Distinguished from dialect The main distinction between sociolects (social dialects) and dialects proper (geographical dialects), which are often confused, is the settings in which they are created. A dialect's main identifier is geography: a certain region uses specific phonological, morphosyntactic or lexical rules.: 35 Asif Agha expands the concept by stating that "the case where the demographic dimension marked by speech are matters of geographic provenance alone, such as speaker's birth locale, extended residence and the like".: 135 However, a sociolect's main identifier is a socioeconomic class, age, gender, and/or ethnicity in a certain speech community. An example of a dialectal difference, based on region, is the use of the words soda or pop and coke in different parts of the United States. As Thomas E. Murray states, "coke is used generically by thousands of people, especially in the southern half of the country." On the other hand, pop is known to be a term that is used by many citizens in the northern half of the country. An example of a sociolect difference, based on social grouping, is the zero copula in African American Vernacular English. It occurs in a specific ethnic group but in all areas of the United States.: 48 William Labov gives an example: "he here" instead of "he's here.": 38 Definitions Code switching is "the process whereby bilingual or bidialectal speakers switch back and forth between one language or dialect and another within the same conversation".: 23 Diglossia, associated with the American linguist Charles A. Ferguson, which describes a sociolinguistic situation such as those that obtain in Arabic-speaking countries and in German-speaking Switzerland. In such a diglossic community, the prestigious standard of 'High' (or H) variety, which is linguistically related to but significantly different from the vernacular or 'Low' (or L) varieties, has no native speakers.: 389 Domain is "different language, dialects, or styles are used in different social contexts".: 41 Language attitudes are "social in origin, but that they may have important effects on language behavior, being involved in acts of identity, and on linguistic change.": 73 Linguistic variable is "a linguistic unit...initially developed...in order to be able to handle linguistics variation. Variables may be lexical and grammatical, but are most often phonological". Example of British English (h) which is sometimes present and sometimes not.: 83 Pragmatics is the meaning of a word in social context, while semantics has "purely linguistic meaning".: 107 Register is "a language variety that is associated with a particular topic, subject, or activity...." Usually, it is defined by vocabulary, but has grammatical features as well.: 110 Examples Tamil caste system The following is an example of the lexical distinction between the Mudaliyar and the Iyengar groups of the Tamil-speaking people in India. The Iyengar group is part of the Brahmin caste which is scholarly and higher in the caste hierarchy than the non-Brahmin or Mudaliyar, caste.: 136 The Mudaliyars use many of the same words for things that are differentiated within the Iyengars' speech. For example, as seen below, the difference between drinking water, water in general, and non-potable water is used by one word in the non-Brahmin caste and three separate words in the Brahmin caste. Furthermore, Agha references how the use of different speech reflects a "departure from a group-internal norm".: 139 For example, if the non-Brahmin caste uses Brahmin terms in their mode of speech it is seen as self-raising, whereas if people within the Brahmin caste use non-Brahmin speech it is seen as pejoratives.: 138 Therefore, depending on which castes use certain words the pragmatics change. Hence, this speech system is determined by socioeconomic class and social context. Norwegian dialect-based sociolect Norwegian does not have a spoken standard and is heavily dependent on dialect variants. The following example shows the difference between the national written standard and a spoken variant, where the phonology and pronunciation differ. These are not sociolectic differences per se. As Agha states, "Some lexical contrasts are due to the phonological difference (e.g., R makes more consonantal and vocalic distinctions than B), while others are due to the morphological difference (e.g., difference in plural suffixes and certain verb inflections) between two varieties.: 140 Diglossia The chart below gives an example of diglossia in Arabic-speaking nations and where it is used. Diglossia is defined by Mesthrie as "[a] situation where two varieties of a language exist side by side". Classical Arabic is known as al-fuṣḥā (الفصحى), while the colloquial dialect depends on the country. For example, šāmi (شامي) is spoken in Lebanon and parts of Syria. In many situations, there is a major lexical difference among words in the classical and colloquial speech, as well as pronunciation differences, such as a difference in short vowels, when the words are the same. Although a specific example of diglossia was not given, its social context is almost if not more important. For example, Halliday states that "in areas with Diglossia, the link between language and success is apparent as the higher, classical register is learned through formal education".: 175 African American Vernacular English (AAVE) Below is an example of African American Vernacular English, showing the addition of the verbal -s not just on third-person singular verbs in the present tense such as in Standard American English, but added onto infinitives, first-person present verbs, and third-person past perfect verbs.: 49 He can goes out. I don't know how to gets no girls. He'd knows that. Further examples of the phenomenon in AAVE are provided below. Below are examples of the lack of the possessive ending; -s is usually absent in AAVE but contains a rule As Labov states, "[the] use -s to indicate possession by a single noun or pronoun, but never between the possessor and the possessed.": 49 "This is hers, This is mines, This is John's, but not in her book, my book, John book": 49 "Interview with Bryan A., seven years old, a struggling reader in a West Philadelphia elementary school: If I don't get out my mom room, I get in trouble and when I don't get out my sister room she hit me. Bernicia penpal gave me one. That's what he did to my cousin Raymond dog at my cousin house. I was acting like I stole my sister food. At the museum, it was fun, we went in somebody heart.": 49 Effects Code-switching Many times within communities that contain sociolects that separate groups linguistically it is necessary to have a process where the independent speech communities can communicate in the same register; even if the change is as simple as different pronunciation. Therefore, the act of code-switching becomes essential. Code-switching is defined as "the process whereby bilingual or bidialectal speakers switch back and forth between one language or dialect and another within the same conversation".: 23 At times code-switching can be situational, depending on the situation or topical, depending on the topic. Halliday terms this the best when he defines the role of discourse, stating that "it is this that determines, or rather correlates with, the role played by the language activity in the situation".: 20 Therefore, meaning that which register is used depends on the situation and lays out the social context of the situation, because if the wrong register is used, then the wrong context is placed on the words. Furthermore, referring back to the diglossia expressed in the Arab-speaking world and the Tamil caste system in India, which words are used must be appropriate to not only the social class of the speaker, but the situation, the topic, and the need for courtesy. A more comprehensive definition is stated, "Code-switching is not only a definition of the situation but an expression of social hierarchy.": 137 See also == References == Due to the successful passage of a law in November 2008, the lawful practice of polygamy in the autonomous region of Iraqi Kurdistan is almost impossible. When Kurdistan was divided (from 1994 to 2005), polygamy was abolished under areas controlled by the Patriotic Union of Kurdistan while it remained legal under regions controlled by the Kurdistan Democratic Party. The severe restrictions on polygamy provoked a fierce outcry within the Islamic community, many of whose members feel polygamy to be their right as Muslims as it is ordained by the Qur'an. Feminist groups, on the other hand, viewed the legislation as a semi-victory; as they will not stop until they have fully abolished polygamy in the region. == References == Peter Franzevich Lesgaft (Russian: Пётр Францевич Лесгафт; 21 September 1837 – 1909) was a Russian teacher, anatomist, physician and social reformer. He was the founder of the modern system of physical education and medical-pedagogical control in physical training, one of founders of theoretical anatomy. Lesgaft National State University of Physical Education, Sport and Health in St. Petersburg is named after him. Unity and integrity of all organs in human body was the basis of Peter Lesgaft system of the pointed exercises for both physical development and intellectual, moral and aesthetic education. Outdoor games were his favorite means in both physical development and formation of character of a child. Biography Peter Lesgaft was born on 21 September 1837 in Saint Petersburg, the third son of a jeweler of German descent. In 1861 he graduated from Imperial Medical-Surgical Academy in St. Petersburg and remained there as a teacher of anatomy. In 1869 he became a professor at the University of Kazan, but soon was barred from teaching for his outspoken criticism of the unscientific methods used. In 1872 he became consultant on therapeutic gymnastics in the private practice of Dr. Berglindt. In 1872–1874 he supervised a group of Russian women for the first time allowed to employment in the Imperial Medical-Surgical Academy. He also became known for publication of a descriptive history of sport in Europe and ancient Greece and an article on naturalistic gymnastics. As a result, he was put in charge of the physical training of military cadets. In 1875, he was sponsored by the Russian Military Ministry to spend two summers in Western Europe, studying the systems of physical education. During that time he visited 26 cities in 13 European countries. He carefully studied British system, visiting English public schools, the Central Army Gymnastics School at Aldershot, the Royal Military Academy at Woolwich and Oxford University. In 1877, he published "Relationship of Anatomy to Physical Education" and "The Major Purpose of Physical Education in Schools". He was able to organizing courses for physical education instructors for the military academies — until then non-existent. In 1893 Peter Lesgaft organized Biological laboratory, which in 1918 was transformed into P. F. Lesgaft Institute of Natural Science. External links Peter Lesgaft short biography and photo The Founder of Russian Physical Education. By James Riordan University of Bradford, England Cultural deprivation is a theory in sociology where a person has inferior norms, values, skills and knowledge. The theory states that people of lower social classes experience cultural deprivation compared with those above and that this disadvantages them, as a result of which the gap between classes increases. For example, in education, lower-class students can suffer from cultural deprivation as their parents do not know the best school for their child, but middle-class parents "know the system", and so can send their children to the best school for them. This puts the lower-class students at a disadvantage, thus increasing inequality and the gap between middle-class and lower-class students. Proponents of this theory argue that working class culture (regardless of race, gender, ethnicity or other factors) inherently differs from that of people in the middle class. This difference in culture means that while middle-class children can easily acquire cultural capital by observing their parents, working-class children cannot, and this deprivation is self-perpetuating. The theory claims that the middle class gains cultural capital as the result of primary socialization, while the working class does not. Cultural capital helps the middle class succeed in society because their norms and values facilitate educational achievement and subsequent employability. Working-class members of society that lack cultural capital do not pass it on to their children, perpetuating the class system. Middle-class children's cultural capital allows them to communicate with their middle-class teachers more effectively than working-class children and this contributes to social inequality. Bourdieu claimed that state schools are set up to make everybody middle-class, although only the middle class and some high-achieving working class have the cultural capital to achieve this. From a Marxist perspective, cultural deprivation observes that the resources available to the working class are limited and that working-class children enter school less well-prepared than others. Sources Willis, Paul E. (1977). Learning to Labor: How Working Class Kids Get Working Class Jobs. Columbia University Press. ISBN 978-0-231-05357-0. Webb, Jen; Schirato, Tony; Danaher, Geoff (29 March 2002). Understanding Bourdieu. SAGE Publications. ISBN 978-0-7619-7463-5. Morais, Ana; Neves, I.; Davies, B.; Daniels, H. (1 January 2001). Towards a Sociology of Pedagogy: The Contributions of Basil Bernstein to Research. Peter Lang Publishing, Incorporated. ISBN 978-0-8204-5585-3. Further reading Bernstein, B. (2002) "Educational Codes and Social Control", British Journal of Sociology of Education, 23: 4. (The whole of this edition is useful for understanding Basil Bernstein). Chitty, C. (2002) "Education and Social Class". The Political Quarterly, 73 (2), pp. 208–210. Legewie, J. and DiPrete, T. A. (2012) "School Context and the Gender Gap in Educational Achievement". American Sociological Review, 77, (3), pp. 463–485. Leathwood, C. and Archer, L. (2004) "Social class and educational inequalities: the local and the global", in Pedagogy, Culture and Society, 12, (1). Leicester, M. (1991). Equal Opportunities in School: Social Class, Sexuality, Race, Gender and Special Needs, Harlow: Longman. Mac an Ghaill, M. (1996) "Sociology of Education, state schooling and social class: beyond critiques of the New Right hegemony", British Journal of Sociology of Education, 17: 163-176. Marks, G. N. (2011) "Issues in the Conceptualization and Measurement of Socioeconomic Background: Do Different Measures Generate Different Conclusions?", Social Indicators Research, 104, (2), pp. 225–251. Reay, D. (2001) "'Finding or losing yourself?': working-class relationships to education", Journal of Education Policy 16(4): 333-346. == References == Youth culture refers to the societal norms of children, adolescents, and young adults. Specifically, it comprises the processes and symbolic systems that are shared by the youth and are distinct from those of adults in the community. An emphasis on clothes, popular music, sports, vocabulary, and dating typically sets youth apart from other age groups. Within youth culture, there are many constantly changing youth subcultures, which may be divided based on race, ethnicity, economic status, public appearance, or a variety of other factors. Existence There is a debate surrounding the presence, existence, and origins of youth culture. Some researchers argue that youth culture is not a separate culture, as their values and morals are not distinct from those of their parents. Additionally, peer influence varies greatly among contexts, gender, age, and social status, making a single "youth culture" difficult to define. which differ from those of their parent's culture. Janssen et al. used the terror management theory (TMT) to argue for the existence of youth culture. They tested the following hypothesis: "If youth culture serves to help adolescents deal with problems of vulnerability and finiteness, then reminders of mortality should lead to increased allegiance to cultural practices and beliefs of the youth." The results supported the hypothesis and the outcome of previous studies, and suggest that youth culture is a culture. Schwartz and Merten used adolescent language to argue that youth culture is distinct from the rest of society. Schwartz argued that high school students used their vocabulary to create meanings that are distinct to adolescents. Specifically, the adolescent status terminology (the words that adolescents use to describe hierarchical social statuses) contains qualities and attributes that are not present in adult status judgments. According to Schwartz, this reflects a difference in social structures and the ways that adults and teens experience social reality. This difference indicates cultural differences between adolescents and adults, which supports the presence of separate youth culture. Movements Throughout the twentieth century, youth have had a strong influence on both lifestyle and culture. The flappers and the Mods are two examples of the impact of youth culture on society. The flappers were young women that were confident about a prosperous future after World War I. This liveliness showed in their new attitudes in life in which they openly drank, smoked, and, in some cases, socialized with gangster-type men. The fashionable dress at the time also reflected the flapper's new lifestyle. Mods emerged during a time of war and political and social troubles, and stemmed from a group called the modernists. They were young men and women who came from all classes who believed that their fashion choices "gave them entrée everywhere" and empowered them. The Mods' style and embrace of modern technology spread from the UK overseas to North America and other countries. Theories The presence of youth culture is a relatively recent historical phenomenon. There are several dominant theories about the emergence of youth culture in the 20th century, which include hypotheses about the historical, economic, and psychological influences on the presence of youth culture. One historical theory credits the emergence of youth culture to the beginning of compulsory schooling. James Coleman argues that age segregation is the root of separate youth culture. Before mandatory education, many children and adolescents interacted primarily with adults. In contrast, modern children associate extensively with others their age. These interactions allow adolescents to develop shared experiences and meanings, which are the root of youth culture. Another theory posits that some cultures facilitate the development of youth culture, while others do not. The basis of this distinction is the presence of universalistic or particularistic norms. Particularistic norms are guidelines for behavior that vary from one individual to another. In contrast, universalistic norms apply to all members of society. Universalistic norms are more likely to be found in industrialized societies. Modernization in the last century has encouraged universalistic norms since interaction in modern societies makes it necessary for everyone to learn the same set of norms. Modernization and universalistic norms have encouraged the growth of youth culture. The need for universalistic norms has made it impractical for young people's socialization to come primarily from immediate family members, which would lead to significant variation in the communicated norms. Therefore, many societies use age grouping, such as in schools, to educate their children on societies' norms and prepare them for adulthood; youth culture is a byproduct of this tactic. Because children spend so much time together and learn the same things as the rest of their age group, they develop their own culture. Psychological theorists have noted the role of youth culture in identity development. Youth culture may be a means of finding identity when one's path in life is not always clear. Erik Erikson theorized that the vital psychological conflict of adolescence is identity versus role confusion. The goal of this stage of life is to answer the question, "Who am I?" In many societies, adolescents are expected to behave like children and take on adult roles. Some psychologists have theorized that forming youth culture is a step to adopt an identity that reconciles these two conflicting expectations. For example, Talcott Parsons posited that adolescence is when young people transition from reliance on parents to autonomy. In this transitory state, dependence on the peer group serves as a stand-in for parents. Burlingame restated this hypothesis in 1970. He wrote that adolescents replace parents with the peer group and that this reliance on the peer group diminishes as youth enter adulthood and take on adult roles. Fasick relates youth culture as a method of identity development to the simultaneous elongation of childhood and the need for independence in adolescence. According to Fasick, adolescents face contradictory pulls from society. Compulsory schooling keeps them socially and economically dependent on their parents, while young people need to achieve some sort of independence to participate in the market economy of modern society. As a means of coping with these contrasting aspects of adolescence, youth create freedom through behavior—specifically, through leisure-oriented activities done with peers. Impact on adolescents For decades, adults have worried that youth subcultures were the root of moral degradation and changing values in younger generations. Researchers have characterized youth culture as embodying values that are "in conflict with those of the adult world". Common concerns about youth culture include a perceived lack of interest in education, involvement in risky behaviors like substance use and sexual activity, and engaging extensively in leisure activities. These perceptions have led many adults to believe that adolescents hold different values than older generations and to perceive youth culture as an attack on the morals of current society. These worries have prompted the creation of parenting websites such as The Youth Culture Report and the Center for Parent Youth Understanding, whose goal is to preserve the values of older generations in young people. There is no consensus among researchers about whether youth subcultures hold different beliefs than adults do. Some researchers have noted the simultaneous rise in age segregation and adolescent adjustment problems such as suicide, delinquency, and premarital pregnancy. However, most evidence suggests that these youth problems are not a reflection of different morals held by younger generations. Multiple studies have found that most adolescents hold views that are similar to their parents. One study challenged the theory that adolescent cohorts had distanced themselves from their parents by finding that between 1976 and 1982, their problems increased, and they became less peer-oriented. A second study's findings that adolescents' values were more similar to their parents in the 1980s than in the 1960s and '70s echoes Sebald's finding. Another study did find differences between adolescents' and parents' attitudes but found that the differences were in the degree of belief, not in the behavior itself. There may also be pluralistic ignorance on the part of youth when comparing their attitudes to peers and parents. A study by Lerner et al. asked college students to compare their attitudes on several issues to their peers and parents. Most students rated their attitudes as falling somewhere between their parents' more conservative attitudes and their peers' more liberal attitudes. The authors suggested that the reason for this is that the students perceived their friends as more liberal than they were. Sports, language, music, clothing, and dating tend to be superficial ways of expressing autonomy—they can be adopted without compromising one's beliefs or values. Some areas in which adolescents assert autonomy can cause long-term consequences, such as substance use and sexual activity. The impact of youth culture on deviance and sexual behavior is debatable. More than 70 percent of American high school students report having drunk alcohol. Similarly, about two-thirds of teenagers have engaged in sexual intercourse by the time they leave high school. As drinking and having sex may be common in adolescence, many researchers include them as aspects of youth culture. While engaging in these activities can have harmful consequences, the majority of adolescents who engage in these risky behaviors do not suffer long-term consequences. The possibilities of addiction, pregnancy, incarceration, and other negative outcomes are some potentially negative effects of participation in youth culture. Research demonstrates that many factors may influence youth to engage in high-risk behaviors, including "a lack of stable role models, heightened family stresses, lowered levels of family investment, weakened emotional bonds between parents and their children, lowered levels of social capital and social control, and a lack of hope in ones [sic] future". Teen culture may also have benefits for adolescents. Peer influence can have a positive effect on adolescents' well-being; for example, most teens report that peer pressure stops them from using drugs or engaging in sexual activity. Impact on society in general Young people can make changes in society, such as through youth-led revolutions. Organizations of young people, which were often based on student identity, were crucial to the American civil rights movement, which included organizations like the Southern Student Organizing Committee, Students for a Democratic Society, and the Student Nonviolent Coordinating Committee. The Freedom Summer campaign relied heavily on college students; hundreds of students engaged in registering African Americans to vote, teaching in "Freedom Schools", and organizing the Mississippi Freedom Democratic Party. The American protests in the Vietnam War were also student-driven. Many college campuses opposed the war with sit-ins and demonstrations. Organizations such as the Young Americans for Freedom, the Student Libertarian Movement, and the Student Peace Union were based on youth status and contributed to anti-war activities. Some scholars have claimed that the activism during the Vietnam War was symbolic of a youth culture whose values were against mainstream American culture. In the early 2010s, the Arab Spring illustrated how young people played roles in demonstrations and protests. The movement was initiated primarily by young people, mostly college students dissatisfied with the opportunities afforded to them. The participation of young people prompted Time magazine to include several youth members of the movement in its 2011 list of 100 most influential people. Additionally, this movement utilized social media (which is considered an aspect of youth culture) to schedule, coordinate, and publicize events. See also == References == Surveillance is the monitoring of behavior, many activities, or information for the purpose of information gathering, influencing, managing, or directing. This can include observation from a distance by means of electronic equipment, such as closed-circuit television (CCTV), or interception of electronically transmitted information like Internet traffic. Increasingly, governments may also obtain consumer data through the purchase of online information, effectively expanding surveillance capabilities through commercially available digital records. It can also include simple technical methods, such as human intelligence gathering and postal interception. Surveillance is used by citizens, for instance for protecting their neighborhoods. It is widely used by governments for intelligence gathering, including espionage, prevention of crime, the protection of a process, person, group or object, or the investigation of crime. It is also used by criminal organizations to plan and commit crimes, and by businesses to gather intelligence on criminals, their competitors, suppliers or customers. Religious organizations charged with detecting heresy and heterodoxy may also carry out surveillance. Auditors carry out a form of surveillance. Surveillance can unjustifiably violate people's privacy and is often criticized by civil liberties activists. Democracies may have laws that seek to restrict governmental and private use of surveillance, whereas authoritarian governments seldom have any domestic restrictions. Espionage is by definition covert and typically illegal according to the rules of the observed party, whereas most types of surveillance are overt and are considered legal or legitimate by state authorities. International espionage seems to be common among all types of countries. Methods Computer The vast majority of computer surveillance involves the monitoring of data and traffic on the Internet. In the United States for example, under the Communications Assistance For Law Enforcement Act, all phone calls and broadband Internet traffic (emails, web traffic, instant messaging, etc.) are required to be available for unimpeded real-time monitoring by federal law enforcement agencies. There is far too much data on the Internet for human investigators to manually search through all of it. Therefore, automated Internet surveillance computers sift through the vast amount of intercepted Internet traffic to identify and report to human investigators the traffic that is considered interesting or suspicious. This process is regulated by targeting certain "trigger" words or phrases, visiting certain types of web sites, or communicating via email or online chat with suspicious individuals or groups. Billions of dollars per year are spent by agencies, such as the NSA, the FBI and the now-defunct Information Awareness Office, to develop, purchase, implement, and operate systems such as Carnivore, NarusInsight, and ECHELON to intercept and analyze all of this data to extract only the information which is useful to law enforcement and intelligence agencies. Computers can be a surveillance target because of the personal data stored on them. If someone is able to install software, such as the FBI's Magic Lantern and CIPAV, on a computer system, they can easily gain unauthorized access to this data. Such software could be installed physically or remotely. Another form of computer surveillance, known as van Eck phreaking, involves reading electromagnetic emanations from computing devices in order to extract data from them at distances of hundreds of meters. The NSA runs a database known as "Pinwale", which stores and indexes large numbers of emails of both American citizens and foreigners. Additionally, the NSA runs a program known as PRISM, which is a data mining system that gives the United States government direct access to information from technology companies. Through accessing this information, the government is able to obtain search history, emails, stored information, live chats, file transfers, and more. This program generated huge controversies in regards to surveillance and privacy, especially from U.S. citizens. Telephones The official and unofficial tapping of telephone lines is widespread. In the United States for instance, the Communications Assistance for Law Enforcement Act (CALEA) requires that all telephone and VoIP communications be available for real-time wiretapping by Federal law enforcement and intelligence agencies. Two major telecommunications companies in the U.S.—AT&T Inc. and Verizon—have contracts with the FBI, requiring them to keep their phone call records easily searchable and accessible for Federal agencies, in return for $1.8 million per year. Between 2003 and 2005, the FBI sent out more than 140,000 "National Security Letters" ordering phone companies to hand over information about their customers' calling and Internet histories. About half of these letters requested information on U.S. citizens. Human agents are not required to monitor most calls. Speech-to-text software creates machine-readable text from intercepted audio, which is then processed by automated call-analysis programs, such as those developed by agencies such as the Information Awareness Office, or companies such as Verint, and Narus, which search for certain words or phrases, to decide whether to dedicate a human agent to the call. Law enforcement and intelligence services in the United Kingdom and the United States possess technology to activate the microphones in cell phones remotely, by accessing phones' diagnostic or maintenance features in order to listen to conversations that take place near the person who holds the phone. The StingRay tracker is an example of one of these tools used to monitor cell phone usage in the United States and the United Kingdom. Originally developed for counterterrorism purposes by the military, they work by broadcasting powerful signals that cause nearby cell phones to transmit their IMSI number, just as they would to normal cell phone towers. Once the phone is connected to the device, there is no way for the user to know that they are being tracked. The operator of the stingray is able to extract information such as location, phone calls, and text messages, but it is widely believed that the capabilities of the StingRay extend much further. A lot of controversy surrounds the StingRay because of its powerful capabilities and the secrecy that surrounds it. Mobile phones are also commonly used to collect location data. The geographical location of a mobile phone (and thus the person carrying it) can be determined easily even when the phone is not being used, using a technique known as multilateration to calculate the differences in time for a signal to travel from the cell phone to each of several cell towers near the owner of the phone. The legality of such techniques has been questioned in the United States, in particular whether a court warrant is required. Records for one carrier alone (Sprint), showed that in a given year federal law enforcement agencies requested customer location data 8 million times. In response to customers' privacy concerns in the post Edward Snowden era, Apple's iPhone 6 has been designed to disrupt investigative wiretapping efforts. The phone encrypts e-mails, contacts, and photos with a code generated by a complex mathematical algorithm that is unique to an individual phone, and is inaccessible to Apple. The encryption feature on the iPhone 6 has drawn criticism from FBI director James B. Comey and other law enforcement officials since even lawful requests to access user content on the iPhone 6 will result in Apple supplying "gibberish" data that requires law enforcement personnel to either break the code themselves or to get the code from the phone's owner. Because the Snowden leaks demonstrated that American agencies can access phones anywhere in the world, privacy concerns in countries with growing markets for smart phones have intensified, providing a strong incentive for companies like Apple to address those concerns in order to secure their position in the global market. Apple has made several moves to emphasize their concern for privacy, in order to appeal to more consumers. In 2011, Apple stopped the use of permanent device identifiers, and in 2019, they banned the ability of third parties to track on children's apps. Although the CALEA requires telecommunications companies to build into their systems the ability to carry out a lawful wiretap, the law has not been updated to address the issue of smart phones and requests for access to e-mails and metadata. The Snowden leaks show that the NSA has been taking advantage of this ambiguity in the law by collecting metadata on "at least hundreds of millions" of "incidental" targets from around the world. The NSA uses an analytic tool known as CO-TRAVELER in order to track people whose movements intersect and to find any hidden connections with persons of interest. The Snowden leaks have also revealed that the British Government Communications Headquarters (GCHQ) can access information collected by the NSA on American citizens. Once the data has been collected, the GCHQ can hold on to it for up to two years. The deadline can be extended with the permission of a "senior UK official". Cameras Surveillance cameras, or security cameras, are video cameras used for the purpose of observing an area. They are often connected to a recording device or IP network, and may be watched by a security guard or law enforcement officer. Cameras and recording equipment used to be relatively expensive and required human personnel to monitor camera footage, but analysis of footage has been made easier by automated software that organizes digital video footage into a searchable database, and by video analysis software (such as VIRAT and HumanID). The amount of footage is also drastically reduced by motion sensors which record only when motion is detected. With cheaper production techniques, surveillance cameras are simple and inexpensive enough to be used in home security systems, and for everyday surveillance. Video cameras are one of the most common methods of surveillance. As of 2016, there are about 350 million surveillance cameras worldwide. About 65% of these cameras are installed in Asia. The growth of CCTV has been slowing in recent years. In 2018, China was reported to have a huge surveillance network of over 170 million CCTV cameras with 400 million new cameras expected to be installed in the next three years, many of which use facial recognition technology. In the United States, the Department of Homeland Security awards billions of dollars per year in Homeland Security grants for local, state, and federal agencies to install modern video surveillance equipment. For example, the city of Chicago, Illinois, recently used a $5.1 million Homeland Security grant to install an additional 250 surveillance cameras, and connect them to a centralized monitoring center, along with its preexisting network of over 2000 cameras, in a program known as Operation Virtual Shield. Speaking in 2009, Chicago Mayor Richard Daley announced that Chicago would have a surveillance camera on every street corner by 2016. New York City received a $350 million grant towards the development of the Domain Awareness System, which is an interconnected system of sensors including 18,000 CCTV cameras used for continual surveillance of the city by both police officers and artificial intelligence systems. In the United Kingdom, the vast majority of video surveillance cameras are not operated by government bodies, but by private individuals or companies, especially to monitor the interiors of shops and businesses. According to 2011 Freedom of Information Act requests, the total number of local government operated CCTV cameras was around 52,000 over the entirety of the UK. The prevalence of video surveillance in the UK is often overstated due to unreliable estimates being requoted; for example one report in 2002 extrapolated from a very small sample to estimate the number of cameras in the UK at 4.2 million (of which 500,000 were in Greater London). More reliable estimates put the number of private and local government operated cameras in the United Kingdom at around 1.85 million in 2011. In the Netherlands, one example city where there are cameras is The Hague. There, cameras are placed in city districts in which the most illegal activity is concentrated. Examples are the red-light districts and the train stations. As part of China's Golden Shield Project, several U.S. corporations, including IBM, General Electric, and Honeywell, have been working closely with the Chinese government to install millions of surveillance cameras throughout China, along with advanced video analytics and facial recognition software, which will identify and track individuals everywhere they go. They will be connected to a centralized database and monitoring station, which will, upon completion of the project, contain a picture of the face of every person in China: over 1.3 billion people. Lin Jiang Huai, the head of China's "Information Security Technology" office (which is in charge of the project), credits the surveillance systems in the United States and the U.K. as the inspiration for what he is doing with the Golden Shield Project. The Defense Advanced Research Projects Agency (DARPA) is funding a research project called Combat Zones That See that will link up cameras across a city to a centralized monitoring station, identify and track individuals and vehicles as they move through the city, and report "suspicious" activity (such as waving arms, looking side-to-side, standing in a group, etc.). At Super Bowl XXXV in January 2001, police in Tampa, Florida, used Identix's facial recognition software, FaceIt, to scan the crowd for potential criminals and terrorists in attendance at the event (it found 19 people with pending arrest warrants). Governments often initially claim that cameras are meant to be used for traffic control, but many of them end up using them for general surveillance. For example, Washington, D.C. had 5,000 "traffic" cameras installed under this premise, and then after they were all in place, networked them all together and then granted access to the Metropolitan Police Department, so they could perform "day-to-day monitoring". The development of centralized networks of CCTV cameras watching public areas—linked to computer databases of people's pictures and identity (biometric data), able to track people's movements throughout the city, and identify whom they have been with—has been argued by some to present a risk to civil liberties. Trapwire is an example of such a network. Social network analysis One common form of surveillance is to create maps of social networks based on data from social networking sites such as Facebook, MySpace, Twitter as well as from traffic analysis information from phone call records such as those in the NSA call database, and others. These social network "maps" are then data mined to extract useful information such as personal interests, friendships & affiliations, wants, beliefs, thoughts, and activities. Many U.S. government agencies such as the Defense Advanced Research Projects Agency (DARPA), the National Security Agency (NSA), and the Department of Homeland Security (DHS) are investing heavily in research involving social network analysis. The intelligence community believes that the biggest threat to U.S. power comes from decentralized, leaderless, geographically dispersed groups of terrorists, subversives, extremists, and dissidents. These types of threats are most easily countered by finding important nodes in the network, and removing them. To do this requires a detailed map of the network. Jason Ethier of Northeastern University, in his study of modern social network analysis, said the following of the Scalable Social Network Analysis Program developed by the Information Awareness Office: The purpose of the SSNA algorithms program is to extend techniques of social network analysis to assist with distinguishing potential terrorist cells from legitimate groups of people.... In order to be successful SSNA will require information on the social interactions of the majority of people around the globe. Since the Defense Department cannot easily distinguish between peaceful citizens and terrorists, it will be necessary for them to gather data on innocent civilians as well as on potential terrorists. AT&T developed a programming language called "Hancock", which is able to sift through enormous databases of phone call and Internet traffic records, such as the NSA call database, and extract "communities of interest"—groups of people who call each other regularly, or groups that regularly visit certain sites on the Internet. AT&T originally built the system to develop "marketing leads", but the FBI has regularly requested such information from phone companies such as AT&T without a warrant, and, after using the data, stores all information received in its own databases, regardless of whether or not the information was ever useful in an investigation. Some people believe that the use of social networking sites is a form of "participatory surveillance", where users of these sites are essentially performing surveillance on themselves, putting detailed personal information on public websites where it can be viewed by corporations and governments. In 2008, about 20% of employers reported using social networking sites to collect personal data on prospective or current employees. Biometric Biometric surveillance is a technology that measures and analyzes human physical and/or behavioral characteristics for authentication, identification, or screening purposes. Examples of physical characteristics include fingerprints, DNA, and facial patterns. Examples of mostly behavioral characteristics include gait (a person's manner of walking) or voice. Facial recognition is the use of the unique configuration of a person's facial features to accurately identify them, usually from surveillance video. Both the Department of Homeland Security and DARPA are heavily funding research into facial recognition systems. The Information Processing Technology Office ran a program known as Human Identification at a Distance which developed technologies that are capable of identifying a person at up to 500 ft (150 m) by their facial features. Another form of behavioral biometrics, based on affective computing, involves computers recognizing a person's emotional state based on an analysis of their facial expressions, how fast they are talking, the tone and pitch of their voice, their posture, and other behavioral traits. This might be used for instance to see if a person's behavior is suspect (looking around furtively, "tense" or "angry" facial expressions, waving arms, etc.). A more recent development is DNA profiling, which looks at some of the major markers in the body's DNA to produce a match. The FBI is spending $1 billion to build a new biometric database, which will store DNA, facial recognition data, iris/retina (eye) data, fingerprints, palm prints, and other biometric data of people living in the United States. The computers running the database are contained in an underground facility about the size of two American football fields. The Los Angeles Police Department is installing automated facial recognition and license plate recognition devices in its squad cars, and providing handheld face scanners, which officers will use to identify people while on patrol. Facial thermographs are in development, which allow machines to identify certain emotions in people such as fear or stress, by measuring the temperature generated by blood flow to different parts of the face. Law enforcement officers believe that this has potential for them to identify when a suspect is nervous, which might indicate that they are hiding something, lying, or worried about something. In his paper in Ethics and Information Technology, Avi Marciano maps the harms caused by biometric surveillance, traces their theoretical origins, and brings these harms together in one integrative framework to elucidate their cumulative power. Marciano proposes four types of harms: Unauthorized use of bodily information, denial or limitation of access to physical spaces, bodily social sorting, and symbolic ineligibility through construction of marginality and otherness. Biometrics' social power, according to Marciano, derives from three main features: their complexity as "enigmatic technologies", their objective-scientific image, and their increasing agency, particularly in the context of automatic decision-making. Aerial Aerial surveillance is the gathering of surveillance, usually visual imagery or video, from an airborne vehicle—such as an unmanned aerial vehicle, helicopter, or spy plane. Military surveillance aircraft use a range of sensors (e.g. radar) to monitor the battlefield. Digital imaging technology, miniaturized computers, and numerous other technological advances over the past decade have contributed to rapid advances in aerial surveillance hardware such as micro-aerial vehicles, forward-looking infrared, and high-resolution imagery capable of identifying objects at extremely long distances. For instance, the MQ-9 Reaper, a U.S. drone plane used for domestic operations by the Department of Homeland Security, carries cameras that are capable of identifying an object the size of a milk carton from altitudes of 30,000 feet (9.1 km), and has forward-looking infrared devices that can detect the heat from a human body at distances of up to 60 kilometers (37 mi). In an earlier instance of commercial aerial surveillance, the Killington Mountain ski resort hired 'eye in the sky' aerial photography of its competitors' parking lots to judge the success of its marketing initiatives as it developed starting in the 1950s. The United States Department of Homeland Security is in the process of testing UAVs to patrol the skies over the United States for the purposes of critical infrastructure protection, border patrol, "transit monitoring", and general surveillance of the U.S. population. Miami-Dade police department ran tests with a vertical take-off and landing UAV from Honeywell, which is planned to be used in SWAT operations. Houston's police department has been testing fixed-wing UAVs for use in "traffic control". The United Kingdom, as well, is working on plans to build up a fleet of surveillance UAVs ranging from micro-aerial vehicles to full-size drones, to be used by police forces throughout the U.K. In addition to their surveillance capabilities, MAVs are capable of carrying tasers for "crowd control", or weapons for killing enemy combatants. Programs such as the Heterogeneous Aerial Reconnaissance Team program developed by DARPA have automated much of the aerial surveillance process. They have developed systems consisting of large teams drone planes that pilot themselves, automatically decide who is "suspicious" and how to go about monitoring them, coordinate their activities with other drones nearby, and notify human operators if something suspicious is occurring. This greatly increases the amount of area that can be continuously monitored, while reducing the number of human operators required. Thus a swarm of automated, self-directing drones can automatically patrol a city and track suspicious individuals, reporting their activities back to a centralized monitoring station. In addition, researchers also investigate possibilities of autonomous surveillance by large groups of micro aerial vehicles stabilized by decentralized bio-inspired swarming rules. Corporate Corporate surveillance is the monitoring of a person or group's behavior by a corporation. The data collected is most often used for marketing purposes or sold to other corporations, but is also regularly shared with government agencies. It can be used as a form of business intelligence, which enables the corporation to better tailor their products and/or services to be desirable by their customers. Although there is a common belief that monitoring can increase productivity, it can also create consequences such as increasing chances of deviant behavior and creating punishments that are not equitable to their actions. Additionally, monitoring can cause resistance and backlash because it insinuates an employer's suspicion and lack of trust. Data mining and profiling Data mining is the application of statistical techniques and programmatic algorithms to discover previously unnoticed relationships within the data. Data profiling in this context is the process of assembling information about a particular individual or group in order to generate a profile — that is, a picture of their patterns and behavior. Data profiling can be an extremely powerful tool for psychological and social network analysis. A skilled analyst can discover facts about a person that they might not even be consciously aware of themselves. Economic (such as credit card purchases) and social (such as telephone calls and emails) transactions in modern society create large amounts of stored data and records. In the past, this data was documented in paper records, leaving a "paper trail", or was simply not documented at all. Correlation of paper-based records was a laborious process—it required human intelligence operators to manually dig through documents, which was time-consuming and incomplete, at best. But today many of these records are electronic, resulting in an "electronic trail". Every use of a bank machine, payment by credit card, use of a phone card, call from home, checked out library book, rented video, or otherwise complete recorded transaction generates an electronic record. Public records—such as birth, court, tax and other records—are increasingly being digitized and made available online. In addition, due to laws like CALEA, web traffic and online purchases are also available for profiling. Electronic record-keeping makes data easily collectable, storable, and accessible—so that high-volume, efficient aggregation and analysis is possible at significantly lower costs. Information relating to many of these individual transactions is often easily available because it is generally not guarded in isolation, since the information, such as the title of a movie a person has rented, might not seem sensitive. However, when many such transactions are aggregated they can be used to assemble a detailed profile revealing the actions, habits, beliefs, locations frequented, social connections, and preferences of the individual. This profile is then used, by programs such as ADVISE and TALON, to determine whether the person is a military, criminal, or political threat. In addition to its own aggregation and profiling tools, the government is able to access information from third parties—for example, banks, credit companies or employers, etc.—by requesting access informally, by compelling access through the use of subpoenas or other procedures, or by purchasing data from commercial data aggregators or data brokers. The United States has spent $370 million on its 43 planned fusion centers, which are national network of surveillance centers that are located in over 30 states. The centers will collect and analyze vast amounts of data on U.S. citizens. It will get this data by consolidating personal information from sources such as state driver's licensing agencies, hospital records, criminal records, school records, credit bureaus, banks, etc.—and placing this information in a centralized database that can be accessed from all of the centers, as well as other federal law enforcement and intelligence agencies. Under United States v. Miller (1976), data held by third parties is generally not subject to Fourth Amendment warrant requirements. Human operatives A tail may surreptitiously track and report on the movements and contacts of a person of interest. Such following by one or more people may provide useful in formation in relatively densely populated urban environments. Organizations that have enemies who wish to gather information about the groups' members or activities face the issue of potential infiltration. In addition to operatives' infiltrating an organization, the surveilling party may exert pressure on certain members of the target organization to act as informants (i.e., to disclose the information they hold on the organization and its members). Fielding operatives is very expensive, and governments with wide-reaching electronic surveillance tools at their disposal, rather than gathering the sort of information which operatives can provide, may use less problematic forms of surveillance—such as those mentioned above. Nevertheless, the use of human infiltrators remains common. For instance, in 2007 documents surfaced showing that the FBI planned to field a total of 15,000 undercover agents and informants in response to an anti-terrorism directive (issued by President George W. Bush in 2004) that ordered intelligence and law-enforcement agencies to increase their HUMINT capabilities. In some home invasion cases, thieves may use "casing surveillance” to determine if a victim's property such as a collection of firearms are worth stealing. Satellite imagery On May 25, 2007, the U.S. Director of National Intelligence Michael McConnell authorized the National Applications Office (NAO) of the Department of Homeland Security to allow local, state, and domestic Federal agencies to access imagery from military intelligence Reconnaissance satellites and Reconnaissance aircraft sensors which can now be used to observe the activities of U.S. citizens. The satellites and aircraft sensors will be able to penetrate cloud cover, detect chemical traces, and identify objects in buildings and "underground bunkers", and will provide real-time video at much higher resolutions than the still-images produced by programs such as Google Earth. Identification and credentials One of the simplest forms of identification is the carrying of credentials. Some nations have an identity card system to aid identification, whilst others are considering it but face public opposition. Other documents, such as passports, driver's licenses, library cards, banking or credit cards are also used to verify identity. If the form of the identity card is "machine-readable", usually using an encoded magnetic stripe or identification number (such as a Social Security number), it corroborates the subject's identifying data. In this case it may create an electronic trail when it is checked and scanned, which can be used in profiling, as mentioned above. Wireless tracking This section refers to methods that involve the monitoring of tracking devices through the aid of wireless signals. Mobile phones Mobile carrier antennas are also commonly used to collect geolocation data on mobile phones. The geographical location of a powered mobile phone (and thus the person carrying it) can be determined easily (whether it is being used or not), using a technique known as multilateration to calculate the differences in time for a signal to travel from the cell phone to each of several cell towers near the owner of the phone. Dr. Victor Kappeler of Eastern Kentucky University indicates that police surveillance is a strong concern, stating the following statistics from 2013: Of the 321,545 law enforcement requests made to Verizon, 54,200 of these requests were for "content" or "location" information—not just cell phone numbers or IP addresses. Content information included the actual text of messages, emails and the wiretapping of voice or messaging content in real-time. A comparatively new off-the-shelf surveillance device is an IMSI-catcher, a telephone eavesdropping device used to intercept mobile phone traffic and track the movement of mobile phone users. Essentially a "fake" mobile tower acting between the target mobile phone and the service provider's real towers, it is considered a man-in-the-middle (MITM) attack. IMSI-catchers are used in some countries by law enforcement and intelligence agencies, but their use has raised significant civil liberty and privacy concerns and is strictly regulated in some countries. In March 2020, British daily The Guardian, based on the claims of a whistleblower, accused the government of Saudi Arabia of exploiting global mobile telecom network weaknesses to spy on its citizens traveling around the United States. The data shared by the whistleblower in support of the claims, showed that a systematic spying campaign was being run by the kingdom exploiting the flaws of SS7, a global messaging system. The data showed that millions of secret tracking commands originated from Saudi in a duration of four-months, starting from November 2019. RFID tagging Radio-frequency identification (RFID) tagging is the use of very small electronic devices (called "RFID tags") which are applied to or incorporated into a product, animal, or person for the purpose of identification and tracking using radio waves. The tags can be read from several meters away. They are extremely inexpensive, costing a few cents per piece, so they can be inserted into many types of everyday products without significantly increasing the price, and can be used to track and identify these objects for a variety of purposes. Some companies appear to be "tagging" their workers by incorporating RFID tags in employee ID badges. Workers in U.K. considered strike action in protest of having themselves tagged; they felt that it was dehumanizing to have all of their movements tracked with RFID chips. Some critics have expressed fears that people will soon be tracked and scanned everywhere they go. On the other hand, RFID tags in newborn baby ID bracelets put on by hospitals have foiled kidnappings. In a 2003 editorial, CNET News.com's chief political correspondent, Declan McCullagh, speculated that, soon, every object that is purchased, and perhaps ID cards, will have RFID devices in them, which would respond with information about people as they walk past scanners (what type of phone they have, what type of shoes they have on, which books they are carrying, what credit cards or membership cards they have, etc.). This information could be used for identification, tracking, or targeted marketing. As of 2021, this has largely not come to pass. RFID tagging on humans A human microchip implant is an identifying integrated circuit device or RFID transponder encased in silicate glass and implanted in the body of a human being. A subdermal implant typically contains a unique ID number that can be linked to information contained in an external database, such as personal identification, medical history, medications, allergies, and contact information. Several types of microchips have been developed in order to control and monitor certain types of people, such as criminals, political figures and spies, a "killer" tracking chip patent was filed at the German Patent and Trademark Office (DPMA) around May 2009. Verichip is an RFID device produced by a company called Applied Digital Solutions (ADS). Verichip is slightly larger than a grain of rice, and is injected under the skin. The injection reportedly feels similar to receiving a shot. The chip is encased in glass, and stores a "VeriChip Subscriber Number" which the scanner uses to access their personal information, via the Internet, from Verichip Inc.'s database, the "Global VeriChip Subscriber Registry". Thousands of people have already had them inserted. In Mexico, for example, 160 workers at the Attorney General's office were required to have the chip injected for identity verification and access control purposes. Implantable microchips have also been used in healthcare settings, but ethnographic researchers have identified a number of ethical problems with such uses; these problems include unequal treatment, diminished trust, and possible endangerment of patients. Radar Geolocation devices Global Positioning System In the U.S., police have planted hidden GPS tracking devices in people's vehicles to monitor their movements, without a warrant. In early 2009, they were arguing in court that they have the right to do this. Several cities are running pilot projects to require parolees to wear GPS devices to track their movements when they get out of prison. Devices Covert listening devices and video devices, or "bugs", are hidden electronic devices which are used to capture, record, and/or transmit data to a receiving party such as a law enforcement agency. The U.S. has run numerous domestic intelligence operations, such as COINTELPRO, which have bugged the homes, offices, and vehicles of thousands of U.S. citizens, usually political activists, subversives, and criminals. Law enforcement and intelligence services in the U.K. and the United States possess technology to remotely activate the microphones in cell phones, by accessing the phone's diagnostic/maintenance features, in order to listen to conversations that take place nearby the person who holds the phone. Postal services As more people use faxes and e-mail the significance of surveilling the postal system is decreasing, in favor of Internet and telephone surveillance. But interception of post is still an available option for law enforcement and intelligence agencies, in certain circumstances. This is not a common practice, however, and entities like the US Army require high levels of approval to conduct. The U.S. Central Intelligence Agency and Federal Bureau of Investigation have performed twelve separate mail-opening campaigns targeted towards U.S. citizens. In one of these programs, more than 215,000 communications were intercepted, opened, and photographed. Stakeout A stakeout is the coordinated surveillance of a location or person. Stakeouts are generally performed covertly and for the purpose of gathering evidence related to criminal activity. The term derives from the practice by land surveyors of using survey stakes to measure out an area before the main building project begins. Internet of things The Internet of Things (IoT), is a network of physical devices. These devices can collect data to one another without human intervention. IoTs can be used for identification, monitoring, location tracking, and health tracking. While IoTs can be used as time-saving tools that make activities simpler, they raise the concern of government surveillance and privacy regarding how data will be used. Controversy Support Supporters of surveillance systems believe that these tools can help protect society from terrorists and criminals. They argue that surveillance can reduce crime by three means: by deterrence, by observation, and by reconstruction. Surveillance can deter by increasing the chance of being caught, and by revealing the modus operandi. This requires a minimal level of invasiveness. Another method on how surveillance can be used to fight criminal activity is by linking the information stream obtained from them to a recognition system (for instance, a camera system that has its feed run through a facial recognition system). This can for instance auto-recognize fugitives and direct police to their location. A distinction here has to be made however on the type of surveillance employed. Some people that support video surveillance in city streets may not support indiscriminate telephone taps and vice versa. Besides the types, the way in which this surveillance is done also matters a lot; i.e. indiscriminate telephone taps are supported by much fewer people than say telephone taps done only to people suspected of engaging in illegal activities. Surveillance can also be used to give human operatives a tactical advantage through improved situational awareness, or through the use of automated processes, i.e. video analytics. Surveillance can help reconstruct an incident and prove guilt through the availability of footage for forensics experts. Surveillance can also influence subjective security if surveillance resources are visible or if the consequences of surveillance can be felt. Some of the surveillance systems (such as the camera system that has its feed run through a facial recognition system mentioned above) can also have other uses besides countering criminal activity. For instance, it can help in retrieving runaway children, abducted or missing adults and mentally disabled people. Other supporters simply believe that there is nothing that can be done about the loss of privacy, and that people must become accustomed to having no privacy. As Sun Microsystems CEO Scott McNealy said: "You have zero privacy anyway. Get over it." Another common argument is: "If you aren't doing something wrong then you don't have anything to fear." That is, one does not have a right to privacy regarding illegal activities, while those following the law suffer no harm from surveillance and so have no standing to object to it. Beyond the heroically self-serving identification of what is wrong with what is illegal, the ethical fly in this ointment is the tacit premise that the individual has no duty to preserve the health of the state—the antithesis of the principle that only the consent of the governed can adequately serve as the moral foundation of a (just) state and warrant the vast gulf between its power (and agency) and that of the individual. Opposition With the advent of programs such as the Total Information Awareness program and ADVISE, technologies such as high speed surveillance computers and biometrics software, and laws such as the Communications Assistance for Law Enforcement Act, governments now possess an unprecedented ability to monitor the activities of their subjects. Many civil rights and privacy groups, such as the Electronic Frontier Foundation and American Civil Liberties Union, have expressed concern that by allowing continual increases in government surveillance of citizens we will end up in a mass surveillance society, with extremely limited, or non-existent political and/or personal freedoms. Fears such as this have led to numerous lawsuits such as Hepting v. AT&T. Some critics state that the claim made by supporters should be modified to read: "As long as we do what we're told, we have nothing to fear." For instance, a person who is part of a political group which opposes the policies of the national government, might not want the government to know their names and what they have been reading, so that the government cannot easily subvert their organization, arrest, or kill them. Other critics state that while a person might not have anything to hide right now, the government might later implement policies that they do wish to oppose, and that opposition might then be impossible due to mass surveillance enabling the government to identify and remove political threats. Further, other critics point to the fact that most people do have things to hide. For example, if a person is looking for a new job, they might not want their current employer to know this. Also if an employer wishes total privacy to watch over their own employee and secure their financial information it may become impossible, and they may not wish to hire those under surveillance. In December 2017, the Government of China took steps to oppose widespread surveillance by security-company cameras, webcams, and IP cameras after tens-of-thousands were made accessible for internet viewing by IT company Qihoo Totalitarianism Programs such as the Total Information Awareness program, and laws such as the Communications Assistance For Law Enforcement Act have led many groups to fear that society is moving towards a state of mass surveillance with severely limited personal, social, political freedoms, where dissenting individuals or groups will be strategically removed in COINTELPRO-like purges. Kate Martin, of the Center For National Security Studies said of the use of military spy satellites being used to monitor the activities of U.S. citizens: "They are laying the bricks one at a time for a police state." Some point to the blurring of lines between public and private places, and the privatization of places traditionally seen as public (such as shopping malls and industrial parks) as illustrating the increasing legality of collecting personal information. Traveling through many public places such as government offices is hardly optional for most people, yet consumers have little choice but to submit to companies' surveillance practices. Surveillance techniques are not created equal; among the many biometric identification technologies, for instance, face recognition requires the least cooperation. Unlike automatic fingerprint reading, which requires an individual to press a finger against a machine, this technique is subtle and requires little to no consent. Psychological/social effects Some critics, such as Michel Foucault, believe that in addition to its obvious function of identifying and capturing individuals who are committing undesirable acts, surveillance also functions to create in everyone a feeling of always being watched, so that they become self-policing. This allows the State to control the populace without having to resort to physical force, which is expensive and otherwise problematic. With the development of digital technology, individuals have become increasingly perceptible to one another, as surveillance becomes virtual. Online surveillance is the utilization of the internet to observe one's activity. Corporations, citizens, and governments participate in tracking others' behaviours for motivations that arise out of business relations, to curiosity, to legality. In her book Superconnected, Mary Chayko differentiates between two types of surveillance: vertical and horizontal. Vertical surveillance occurs when there is a dominant force, such as the government that is attempting to control or regulate the actions of a given society. Such powerful authorities often justify their incursions as a means to protect society from threats of violence or terrorism. Some individuals question when this becomes an infringement on civil rights. Horizontal diverges from vertical surveillance as the tracking shifts from an authoritative source to an everyday figure, such as a friend, coworker, or stranger that is interested in one's mundane activities. Individuals leave traces of information when they are online that reveal their interests and desires of which others observe. While this can allow people to become interconnected and develop social connections online, it can also increase potential risk to harm, such as cyberbullying or censoring/stalking by strangers, reducing privacy. In addition, Simone Browne argues that surveillance wields an immense racializing quality such that it operates as "racializing surveillance." Browne uses racializing surveillance to refer to moments when enactments of surveillance are used to reify boundaries, borders, and bodies along racial lines and where the outcome is discriminatory treatment of those who are negatively racialized by such surveillance. Browne argues racializing surveillance pertains to policing what is "in or out of place." Privacy Numerous civil rights groups and privacy groups oppose surveillance as a violation of people's right to privacy. Such groups include: Electronic Privacy Information Center, Electronic Frontier Foundation, American Civil Liberties Union and Privacy International. There have been several lawsuits such as Hepting v. AT&T and EPIC v. Department of Justice by groups or individuals, opposing certain surveillance activities. Legislative proceedings such as those that took place during the Church Committee, which investigated domestic intelligence programs such as COINTELPRO, have also weighed the pros and cons of surveillance. Court cases People vs. Diaz (2011) was a court case in the realm of cell phone privacy, even though the decision was later overturned. In this case, Gregory Diaz was arrested during a sting operation for attempting to sell ecstasy. During his arrest, police searched Diaz's phone and found more incriminating evidence including SMS text messages and photographs depicting illicit activities. During his trial, Diaz attempted to have the information from his cell phone removed from evidence, but the courts deemed it as lawful and Diaz's appeal was denied on the California State Court level and, later, the Supreme Court level. Just three short years after, this decision was overturned in the case Riley vs. California (2014). Riley vs. California (2014) was a U.S. Supreme Court case in which a man was arrested for his involvement in a drive-by shooting. A few days after the shooting the police made an arrest of the suspect (Riley), and, during the arrest, the police searched him. However, this search was not only of Riley's person, but also the police opened and searched his cell phone, finding pictures of other weapons, drugs, and of Riley showing gang signs. In court, the question arose whether searching the phone was lawful or if the search was protected by the 4th amendment of the constitution. The decision held that the search of Riley's cell phone during the arrest was illegal, and that it was protected by the 4th Amendment. Countersurveillance, inverse surveillance, sousveillance Countersurveillance is the practice of avoiding surveillance or making surveillance difficult. Developments in the late twentieth century have caused counter surveillance to dramatically grow in both scope and complexity, such as the Internet, increasing prevalence of electronic security systems, high-altitude (and possibly armed) UAVs, and large corporate and government computer databases. Other examples include encrypted messenger apps such as Signal and privacy cryptocurrencies such as Monero and ZCash. Inverse surveillance is the practice of the reversal of surveillance on other individuals or groups (e.g., citizens photographing police). Well-known examples include George Holliday's recording of the Rodney King beating and the organization Copwatch, which attempts to monitor police officers to prevent police brutality. Counter-surveillance can be also used in applications to prevent corporate spying, or to track other criminals by certain criminal entities. It can also be used to deter stalking methods used by various entities and organizations. Sousveillance is inverse surveillance, involving the recording by private individuals, rather than government or corporate entities. Popular culture In literature George Orwell's novel Nineteen Eighty-Four portrays a fictional totalitarian surveillance society with a very simple mass surveillance system consisting of human operatives, informants, and two-way "telescreens" in people's homes. Because of the impact of this book, mass-surveillance technologies are commonly called "Orwellian" when considered problematic. The book The Handmaid's Tale, as well as a film and TV series based on it, portray a totalitarian Christian theocracy where all citizens are kept under constant surveillance. In the book The Girl with the Dragon Tattoo, Lisbeth Salander uses computers to get information on people, as well as other standard surveillance methods, as a freelancer. V for Vendetta, a British graphic novel written by Alan Moore David Egger's novel The Circle exhibits a world where a single company called "The Circle" produces all of the latest and highest quality technologies from computers and smartphones, to surveillance cameras known as "See-Change cameras". This company became associated with politics when it started a movement where politicians went "transparent" by wearing See-Change cameras to prevent the public from keeping secrets about their daily work activity. In this society, sharing personal information and experiences becomes mandatory because The Circle believes everyone should have access to all information freely. However, as Eggers illustrates, this takes a toll on the individuals and disrupts power between governments and private companies. The Circle presents extreme ideologies surrounding mandatory surveillance. Eamon Bailey, one of the Wise Men, or founders of The Circle, believes that possessing the tools to access information about anything or anyone, should be a human right given to all of the world's citizens. By eliminating all secrets, any behaviour that has been deemed shameful will either become normalized or no longer considered shocking. Negative actions will eventually be eradicated from society altogether, through the fear of being exposed to other citizens This would be achieved partly by everyone going transparent, which Bailey highly supports. However, none of the Wise Men ever became transparent themselves. One primary goal of The Circle is to have all of the world's information filtered through The Circle, a process they call "Completion". A single, private company would then have full access and control over all information and privacy of individuals and governments. Ty Gospodinov, the first founder of The Circle, has significant concerns about the completion of the circle. He warns that this step would give The Circle too much power and control, quickly leading to totalitarianism. In music The Dead Kennedys' song "I Am The Owl" is about government surveillance and social engineering of political groups. The Vienna Teng song "Hymn of Acxiom" is about corporate data collection and surveillance. Onscreen The film Gattaca portrays a society that uses biometric surveillance to distinguish between people who are genetically engineered "superior" humans and genetically natural "inferior" humans. In the movie Minority Report, the police and government intelligence agencies use micro aerial vehicles in SWAT operations and for surveillance purposes. HBO's crime-drama series The Sopranos regularly portrays the FBI's surveillance of the DiMeo Crime Family. Audio devices they use include "bugs" placed in strategic locations (e.g., in "I Dream of Jeannie Cusamano" and "Mr. Ruggerio's Neighborhood") and hidden microphones worn by operatives (e.g., in "Rat Pack") and informants (e.g., in "Funhouse", "Proshai, Livushka" and "Members Only"). Visual devices include hidden still cameras (e.g., in "Pax Soprana") and video cameras (e.g., in "Long Term Parking"). The movie THX-1138 portrays a society wherein people are drugged with sedatives and antidepressants, and have surveillance cameras watching them everywhere they go. The movie The Lives of Others portrays the monitoring of East Berlin by agents of the Stasi, the GDR's secret police. The movie The Conversation portrays many methods of audio surveillance. The movie V for Vendetta, a 2005 dystopian political thriller film directed by James McTeigue and written by the Wachowskis, is about British government trying to brainwash people by media, obtain their support by fearmongering, monitor them by mass surveillance devices, and suppress or kill any political or social objection. The movie Enemy of the State a 1998 American action-thriller film directed by Tony Scott is about using U.S. citizens' data to search their background and surveillance devices to capture everyone that is identified as "enemy". The British TV series The Capture explores the potential for video surveillance to be manipulated in order to support a conviction to pursue a political agenda. See also Computer and network surveillance Mass surveillance Sousveillance Surveillance aircraft Surveillance art Surveillance capitalism Surveillance system monitor Trapwire Participatory surveillance PRISM (surveillance program) The Age of Surveillance Capitalism Vulkan files leak Surveillance in New Zealand Surveillance in the Ottoman Empire References Further reading Allmer, Thomas. (2012). Towards a Critical Theory of Surveillance in Informational Capitalism. Frankfurt am Main: Peter Lang. ISBN 978-3-631-63220-8 Andrejevic, Mark. 2007. iSpy: Surveillance and Power in the Interactive Era. Lawrence, KS: University Press of Kansas. ISBN 0700616861 Ball, Kirstie, Kevin D. Haggerty, and David Lyon, eds. (2012). Routledge Handbook of Surveillance Studies. New York: Routledge. ISBN 1138026026 Brayne, Sarah. (2020). Predict and Surveil: Data, Discretion, and the Future of Policing. New York: Oxford University Press. ISBN 0190684097 Browne, Simone. (2015). Dark Matters: On the Surveillance of Blackness. Durham: Duke University Press. ISBN 978-0822359197 Coleman, Roy, and Michael McCahill. 2011. Surveillance & Crime. Thousand Oaks, Calif.: Sage. ISBN 1847873537 Feldman, Jay. (2011). Manufacturing Hysteria: A History of Scapegoating, Surveillance, and Secrecy in Modern America. New York, NY: Pantheon Books. ISBN 0-375-42534-9 Fuchs, Christian, Kees Boersma, Anders Albrechtslund, and Marisol Sandoval, eds. (2012). "Internet and Surveillance: The Challenges of Web 2.0 and Social Media". New York: Routledge. ISBN 978-0-415-89160-8 Garfinkel, Simson, Database Nation; The Death of Privacy in the 21st Century. O'Reilly & Associates, Inc. ISBN 0-596-00105-3 Gilliom, John. (2001). Overseers of the Poor: Surveillance, Resistance, and the Limits of Privacy, University Of Chicago Press, ISBN 978-0-226-29361-5 Haque, Akhlaque. (2015). Surveillance, Transparency and Democracy: Public Administration in the Information Age. University of Alabama Press, Tuscaloosa, AL. ISBN 978-0-8173-1877-2 Harris, Shane. (2011). The Watchers: The Rise of America's Surveillance State. London, UK: Penguin Books Ltd. ISBN 0-14-311890-0 Hier, Sean P., & Greenberg, Joshua (Eds.). (2009). Surveillance: Power, Problems, and Politics. Vancouver, CA: UBC Press. ISBN 0-7748-1611-2 Jensen, Derrick and Draffan, George (2004) Welcome to the Machine: Science, Surveillance, and the Culture of Control Chelsea Green Publishing Company. ISBN 978-1-931498-52-4 Lewis, Randolph. (2017). Under Surveillance: Being Watched in Modern America. Austin: University of Texas Press. ISBN 1477312439 Lyon, David (1994). The Electronic Eye: The Rise of Surveillance Society. Minneapolis: University of Minnesota Press. ISBN 978-0816625154 Lyon, David (2001). Surveillance Society: Monitoring in Everyday Life. Philadelphia: Open University Press. ISBN 978-0-335-20546-2 Lyon, David (Ed.). (2006). Theorizing Surveillance: The Panopticon and Beyond. Cullompton, UK: Willan Publishing. ISBN 978-1-84392-191-2 Lyon, David (2007). Surveillance Studies: An Overview. Cambridge: Polity Press. ISBN 978-0-7456-3591-0 Lyon, David (2018). The Culture of Surveillance: Watching as a Way of Life. Cambridge: Polity Press. ISBN 978-0745671734 Marx, Gary T. (2016) Windows into the Soul: Surveillance and Society in an Age of High Technology. Chicago: University of Chicago Press. ISBN 978-0-226-28607-5 Matteralt, Armand. (2010). The Globalization of Surveillance. Cambridge, UK: Polity Press. ISBN 0-7456-4511-9 Monahan, Torin, ed. (2006). Surveillance and Security: Technological Politics and Power in Everyday Life. New York: Routledge. ISBN 9780415953931 Monahan, Torin. (2010). Surveillance in the Time of Insecurity. New Brunswick: Rutgers University Press. ISBN 0813547652 Monahan, Torin, and David Murakami Wood, eds. (2018). Surveillance Studies: A Reader. New York: Oxford University Press. ISBN 978-0-190-29782-4 Parenti, Christian The Soft Cage: Surveillance in America From Slavery to the War on Terror, Basic Books, ISBN 978-0-465-05485-5 Petersen, J.K. (2012) Handbook of Surveillance Technologies, Third Edition, Taylor & Francis: CRC Press, 1020 pp., ISBN 978-1-439873-15-1 Staples, William G. (2000). Everyday Surveillance: Vigilance and Visibility in Post-Modern Life. Lanham, MD: Rowman & Littlefield Publishers. ISBN 0-7425-0077-2 Yan, W. (2019). Introduction to Intelligent Surveillance: Surveillance Data Capture, Transmission, and Analytics . Springer Publishers. ISBN 3030107124 General information "Special Issue on Surveillance Capitalism – nine articles analyzing financial, social, political, legal, historical, security and other aspects of US and international surveillance and spying programs and their relation to capitalism". Monthly Review. 2014. (Volume 66, Number 3, July–August) ACLU, "The Surveillance-Industrial Complex: How the American Government Is Conscripting Businesses and Individuals in the Construction of a Surveillance Society" Balkin, Jack M. (2008). "The Constitution in the National Surveillance State", Yale Law School Bibo, Didier and Delmas-Marty, "The State and Surveillance: Fear and Control" EFF Privacy Resources EPIC Privacy Resources ICO. (September 2006). "A Report on the Surveillance Society for the Information Commissioner by the Surveillance Studies Network". Privacy Information Center Archived February 21, 2009, at the Wayback Machine "The NSA Files (Dozens of articles about the U.S. National Security Agency and its spying and surveillance programs)". The Guardian. London. June 8, 2013. Historical information COINTELPRO—FBI counterintelligence programs designed to neutralize political dissidents Reversing the Whispering Gallery of Dionysius – A Short History of Electronic Surveillance in the United States Legal resources EFF Legal Cases Guide to lawful intercept legislation around the world External links Media related to Surveillance at Wikimedia Commons In research design, especially in psychology, social sciences, life sciences and physics, operationalization or operationalisation is a process of defining the measurement of a phenomenon which is not directly measurable, though its existence is inferred from other phenomena. Operationalization thus defines a fuzzy concept so as to make it clearly distinguishable, measurable, and understandable by empirical observation. In a broader sense, it defines the extension of a concept—describing what is and is not an instance of that concept. For example, in medicine, the phenomenon of health might be operationalized by one or more indicators like body mass index or tobacco smoking. As another example, in visual processing the presence of a certain object in the environment could be inferred by measuring specific features of the light it reflects. In these examples, the phenomena are difficult to directly observe and measure because they are general/abstract (as in the example of health) or they are latent (as in the example of the object). Operationalization helps infer the existence, and some elements of the extension, of the phenomena of interest by means of some observable and measurable effects they have. Sometimes multiple or competing alternative operationalizations for the same phenomenon are available. Repeating the analysis with one operationalization after the other can determine whether the results are affected by different operationalizations. This is called checking robustness. If the results are (substantially) unchanged, the results are said to be robust against certain alternative operationalizations of the checked variables. The concept of operationalization was first presented by the British physicist N. R. Campbell in his 'Physics: The Elements' (Cambridge, 1920). This concept spread to humanities and social sciences. It remains in use in physics. Theory History Operationalization is the scientific practice of operational definition, where even the most basic concepts are defined through the operations by which we measure them. The practice originated in the field of physics with the philosophy of science book The Logic of Modern Physics (1927), by Percy Williams Bridgman, whose methodological position is called "operationalism". Bridgman wrote that in the theory of relativity a concept like "duration" can split into multiple different concepts. In refining a physical theory, it may be discovered that what was thought to be one concept is actually two or more distinct concepts. Bridgman proposed that if only operationally defined concepts are used, this will never happen. Bridgman's theory was criticized because "length" is measured in various ways (e.g. it is impossible to use a measuring rod to measure the distance to the Moon), so "length" logically is not one concept but many, with some concepts requiring knowledge of geometry. Each concept is to be defined by the measuring operation used. So the criticism is that there are potentially infinite concepts, each defined by the methods that measured it, such as angle of sighting, day of the solar year, angular subtense of the moon, etc. which were gathered together, some astronomical observations taken over a period of thousands of years. In the 1930s, Harvard experimental psychologist Edwin Boring and students Stanley Smith Stevens and Douglas McGregor, struggling with the methodological and epistemological problems of defining measurement of psychological phenomena, found a solution in reformulating psychological concepts operationally, as it had been proposed in the field of physics by Bridgman, their Harvard colleague. This resulted in a series of articles that were published by Stevens and McGregor from 1935, that were widely discussed in the field of psychology and led to the Symposium on operationism in 1945, to which Bridgman also contributed. Operationalization The practical 'operational definition' is generally understood as relating to the theoretical definitions that describe reality through the use of theory. The importance of careful operationalization can perhaps be more clearly seen in the development of general relativity. Einstein discovered that there were two operational definitions of "mass" being used by scientists: inertial, defined by applying a force and observing the acceleration, from Newton's second law of motion; and gravitational, defined by putting the object on a scale or balance. Previously, no one had paid any attention to the different operations used because they always produced the same results, but the key insight of Einstein was to posit the principle of equivalence that the two operations would always produce the same result because they were equivalent at a deep level, and work out the implications of that assumption, which is the general theory of relativity. Thus, a breakthrough in science was achieved by disregarding different operational definitions of scientific measurements and realizing that they both described a single theoretical concept. Einstein's disagreement with the operationalist approach was criticized by Bridgman as follows: "Einstein did not carry over into his general relativity theory the lessons and insights he himself has taught us in his special theory." (p. 335). In the social sciences Operationalization is often used in the social sciences as part of the scientific method and psychometrics. Particular concerns about operationalization arise in cases that deal with complex concepts and complex stimuli (e.g., business research, software engineering) where unique threats to validity of operationalization are believed to exist. Anger example For example, a researcher may wish to measure the concept "anger." Its presence, and the depth of the emotion, cannot be directly measured by an outside observer because anger is intangible. Rather, other measures are used by outside observers, such as facial expression, choice of vocabulary, loudness and tone of voice. If a researcher wants to measure the depth of "anger" in various persons, the most direct operation would be to ask them a question, such as "are you angry", or "how angry are you?". This operation is problematic, however, because it depends upon the definition of the individual. Some people might be subjected to a mild annoyance, and become slightly angry, but describe themselves as "extremely angry," whereas others might be subjected to a severe provocation, and become very angry, but describe themselves as "slightly angry." In addition, in many circumstances it is impractical to ask subjects whether they are angry. Since one of the measures of anger is loudness, the researcher can operationalize the concept of anger by measuring how loudly the subject speaks compared to his normal tone. However, this must assume that loudness is a uniform measure. Some might respond verbally while others might respond physically. Economics objections One of the main critics of operationalism in social science argues that "the original goal was to eliminate the subjective mentalistic concepts that had dominated earlier psychological theory and to replace them with a more operationally meaningful account of human behavior. But, as in economics, the supporters ultimately ended up "turning operationalism inside out". "Instead of replacing 'metaphysical' terms such as 'desire' and 'purpose'" they "used it to legitimize them by giving them operational definitions." Thus in psychology, as in economics, the initial, quite radical operationalist ideas eventually came to serve as little more than a "reassurance fetish" for mainstream methodological practice." Tying to conceptual frameworks The above discussion links operationalization to measurement of concepts. Many scholars have worked to operationalize concepts like job satisfaction, prejudice, anger etc. Scale and index construction are forms of operationalization. There is not one perfect way to operationalize. For example, in the United States the concept distance driven would be operationalized as miles, whereas kilometers would be used in Europe. Operationalization is part of the empirical research process. An example is the empirical research question of if job satisfaction influences job turnover. Both job satisfaction and job turnover need to be measured. The concepts and their relationship are important — operationalization occurs within a larger framework of concepts. When there is a large empirical research question or purpose the conceptual framework that organizes the response to the question must be operationalized before the data collection can begin. If a scholar constructs a questionnaire based on a conceptual framework, they have operationalized the framework. Most serious empirical research should involve operationalization that is transparent and linked to a conceptual framework. Another example, the hypothesis Job satisfaction reduces job turnover is one way to connect (or frame) two concepts – job satisfaction and job turnover. The process of moving from the idea job satisfaction to the set of questionnaire items that form a job satisfaction scale is operationalization. For example, it is possible to measure job satisfaction using only two simple questions: "All in all, I am satisfied with my job", and, "In general, I like my job." Operationalization uses a different logic when testing a formal (quantitative) hypothesis and testing working hypothesis (qualitative). For formal hypotheses the concepts are represented empirically (or operationalized) as numeric variables and tested using inferential statistics. Working hypotheses (particularly in the social and administrative sciences), however, are tested through evidence collection and the assessment of the evidence. The evidence is generally collected within the context of a case study. The researcher asks if the evidence is sufficient to "support" the working hypothesis. Formal operationalization would specify the kinds of evidence needed to support the hypothesis as well as evidence which would "fail" to support it. Robert Yin recommends developing a case study protocol as a way to specify the kinds of evidence needed during the data collection phases. He identifies six sources of evidence: documentation; archival records; interviews; direct observations; participant observation and physical or cultural artifacts. In the field of public administration, Shields and Tajalli (2006) have identified five kinds of conceptual frameworks (working hypothesis, descriptive categories, practical ideal type, operations research, and formal hypothesis). They explain and illustrate how each of these conceptual frameworks can be operationalized. They also show how to make conceptualization and operationalization more concrete by demonstrating how to form conceptual framework tables that are tied to the literature and operationalization tables that lay out the specifics of how to operationalize the conceptual framework (measure the concepts). See also Proxy (statistics) Notes Further reading Bridgman, P.W. (1927). "The Logic of Modern Physics". {{cite journal}}: Cite journal requires |journal= (help) A. Cornelius Benjamin (1955) Operationism via HathiTrust Teknonymy (from Ancient Greek: τέκνον 'child' and ὄνομα 'name') is the practice of referring to parents by the names of their children. This practice can be found in many different cultures around the world. The term was coined by anthropologist Edward Burnett Tylor in an 1889 paper. Such names are called teknonyms, teknonymics, or paedonymics. Examples Teknonymy can be found in: Various Austronesian peoples: The Cocos Malays of Cocos (Keeling) Islands, where parents are known by the name of their first-born child. For instance, a man named Hashim and his wife, Anisa, have a daughter named Sheila. Hashim is now known as Pak Sheila (literally 'Sheila's father') and Anisa as Mak Sheila ('Sheila's mother'). Toba Batak people of Indonesia. The case is very similar to the Cocos Malays. Balinese people of Indonesia. Dayak and related indigenous peoples of Borneo, like the Penan Betsileo people of Madagascar, in particular the Zafimaniry subgroup language of the Madurese people of Indonesia Mentawai people of Indonesia Tao people of Taiwan the Korean language; for example, if a Korean woman has a child named Su-min, she might be called Su-min Eomma (meaning 'mother of Su-min') the Chinese language has a similar but also very flexible phenomenon. Suppose a boy's nickname at home is 二儿 (Er'er), then the father of the child can call the child's mother 他妈/孩儿他妈/二儿他妈, meaning 'his mom'/'child's mom'/'Er'er's mom', respectively. Similar applies to the boy's mother calling her husband (i.e., the boy's father) by changing 妈 ('mom') to 爸/爹 ('dad'). This usage occurs mostly between parents, but can also be found in other limited scenarios, e.g. a teacher calling a child's parents. Bangladeshi people the Arabic-speaking world; for example, if a Saudi man named Hasan has a male child named Zayn, Hasan will now be known as Abu Zayn (literally 'father of Zayn'). Similarly, Umm Malik (Malik is a name used for males) is 'mother of Malik'. This is known as a kunya in Arabic and is used as a sign of respect for others. areas of Amazonia the Zuni language, indigenous to New Mexico various African peoples, particularly in West Africa the Nupe people of Nigeria; for example, if a man has a son named Isyaku, he will be known as Baba Isyaku, whereas his wife would be called Nna Isyaku. the Yoruba language of West Africa; for example, if a woman has a son named Femi, she will now be known as iya Femi (meaning 'mother of Femi') and her husband baba Femi (meaning 'father of Femi'). the Hausa language of West Africa; for example, if a man has a son named Adam, the man will be known as Baban Adam, while his wife would be called Maman Adam. Swahili, as spoken in Tanzania and Kenya; for example, if a woman has a son named Musa, the woman would be known as Mama Musa. Musa's father would be known as Baba Musa. to some extent, among Habesha people in the Horn of Africa See also Michitsuna no Haha Korean name Patronymy "Stacy's Mom" References External links The dictionary definition of teknonym at Wiktionary Emotional labor is the work of trying to feel the right feeling for a job, either by evoking or suppressing feelings. It requires the capacity to manage and produce a feeling to fulfill the emotional requirements of a job. More specifically, workers are expected to regulate their personas during interactions with customers, co-workers, clients, and managers. This includes analysis and decision-making in terms of the expression of emotion, whether actually felt or not, as well as its opposite: the suppression of emotions that are felt but not expressed. This is done so as to produce a certain feeling in the customer or client that will allow the company or organization to succeed. Roles that have been identified as requiring emotional labor include those involved in education, public administration, law, childcare, health care, social work, hospitality, media, advocacy, aviation and espionage. As particular economies move from a manufacturing to a service-based economy, more workers in a variety of occupational fields are expected to manage their emotions according to employer demands when compared to sixty years ago. Definition The sociologist Arlie Hochschild provided the first definition of emotional labor, which is displaying certain emotions to meet the requirements of a job. The related term emotion work refers to displaying emotions you don't feel within the private sphere of one's home or interactions with family and friends. Hochschild identified three emotion regulation strategies: cognitive, bodily, and expressive. Within cognitive emotion work, one attempts to change images, ideas, or thoughts in hopes of changing the feelings associated with them. For example, one may associate a family picture with feeling happy and think about said picture whenever attempting to feel happy. Within bodily emotion work, one attempts to change physical symptoms in order to create a desired emotion. For example, one may attempt deep breathing in order to reduce anger. Within expressive emotion work, one attempts to change expressive gestures to change inner feelings, such as smiling when trying to feel happy. While emotion work happens within the private sphere, emotional labor is emotion management within the workplace according to employer expectations. Jobs involving emotional labor are defined as those that: require face-to-face or voice-to-voice contact with the public. require the worker to produce an emotional state in another person. allow the employer, through training and supervision, to exercise a degree of control over the emotional activities of employees. Hochschild (1983) argues that within this commodification process, service workers are estranged from their own feelings in the workplace. Alternate usage The term has been applied in modern contexts to refer to household tasks, specifically unpaid labor that is often expected of women, e.g. having to remind their partner of chores. The term can also refer to informal counseling, such as providing advice to a friend or helping someone through a breakup. When Hochschild was interviewed about this shifting usage, she described it having undergone concept creep, expressing that it made the concept blurrier and was sometimes being applied to things that were simply just labor, although how carrying out this labor made a person feel could make it emotional labor as well. Determinants Societal, occupational, and organizational norms. For example, empirical evidence indicates that in typically "busy" stores there is more legitimacy to express negative emotions than there is in typically "slow" stores, in which employees are expected to behave in accordance with the display rules. Hence, the emotional culture to which one belongs influences the employee's commitment to those rules. Dispositional traits and inner feeling on the job; such as employees' emotional expressiveness, which refers to the capability to use facial expressions, voice, gestures, and body movements to transmit emotions; or employees' level of career identity (the importance of the career role to self-identity), which allows them to express the organizationally-desired emotions more easily (because there is less discrepancy between expressed behavior and emotional experience when engaged in their work). Supervisory regulation of display rules; Supervisors are likely to be important definers of display rules at the job level, given their direct influence on workers' beliefs about high-performance expectations. Moreover, supervisors' impressions of the need to suppress negative emotions on the job influence the employees' impressions of that display rule. Surface and deep acting Arlie Hochschild's foundational text divided emotional labor into two components: surface acting and deep acting. Surface acting occurs when employees display the emotions required for a job without changing how they actually feel. Deep acting is an effortful process through which employees change their internal feelings to align with organizational expectations, producing more natural and genuine emotional displays. Although the underlying processes differ, the objective of both is typically to show positive emotions, which are presumed to impact the feelings of customers and bottom-line outcomes (e.g. sales, positive recommendations, and repeat business). However, research generally has shown surface acting is more harmful to employee health. Without a consideration of ethical values, the consequences of emotional work on employees can easily become negative. Business ethics can be used as a guide for employees on how to present feelings that are consistent with ethical values, and can show them how to regulate their feelings more easily and comfortably while working. Careers In the past, emotional labor demands and display rules were viewed as a characteristic of particular occupations, such as restaurant workers, cashiers, hospital workers, bill collectors, counselors, secretaries, and nurses. However, display rules have been conceptualized not only as role requirements of particular occupational groups, but also as interpersonal job demands, which are shared by many kinds of occupations. Teachers Zhang et al. (2019) looked at teachers in China, using questionnaires the researchers asked about their teaching experience and their interaction with the children and their families. According to numerous studies, early childhood education is important to a child's development, which can have an effect on the teachers emotional labor, along with their emotional labor having an effect on the children. A big focus in this study was the use of surface acting in early childhood teacher. Zhang et al. (2019) found that surface acting was used significantly less than deep and natural acting in kindergarten teachers, along with early childhood teacher are less likely to fake or suppress their feelings. They also found that more experienced teachers had higher levels of emotional labor, because they either have more skills to suppress their emotions, or they are less driven to use surface acting. Bill collectors In 1991, Sutton did an in-depth qualitative study into bill collectors at a collection agency. He found that unlike the other jobs described here where employees need to act cheerful and concerned, bill collectors are selected and socialized to show irritation to most debtors. Specifically, the collection agency hired agents who seemed to be easily aroused. The newly hired agents were then trained on when and how to show varying emotions to different types of debtors. As they worked at the collection agency, they were closely monitored by their supervisors to make sure that they frequently conveyed urgency to debtors. Bill collectors' emotional labor consists of not letting angry and hostile debtors make them angry and to not feel guilty about pressuring friendly debtors for money. They coped with angry debtors by publicly showing their anger or making jokes when they got off the phone. They minimized the guilt they felt by staying emotionally detached from the debtors. Childcare workers The skills involved in childcare are often viewed as innate to women, making the components of childcare invisible. However, a number of scholars have not only studied the difficulty and skill required for childcare, but also suggested that the emotional labor of childcare is unique and needs to be studied differently. Performing emotional labor requires the development of emotional capital, and that can only be developed through experience and reflection. Through semi-structured interviews, Edwards (2016) found that there were two components of emotional labor in childcare in addition to Hochschild's original two: emotional consonance and suppression. Edwards (2016) defined suppression as hiding emotion and emotional consonance as naturally experiencing the same emotion that one is expected to feel for the job. Caring for those with special needs Caring for those with special needs may be paid or unpaid work. Food-industry workers Wait staff In her 1991 study of waitresses in Philadelphia, Paules examines how these workers assert control and protect their self identity during interactions with customers. In restaurant work, Paules argues, workers' subordination to customers is reinforced through "cultural symbols that originate from deeply rooted assumptions about service work." Because the waitresses were not strictly regulated by their employers, waitresses' interactions with customers were controlled by the waitresses themselves. Although they are stigmatized by the stereotypes and assumptions of servitude surrounding restaurant work, the waitresses studied were not negatively affected by their interactions with customers. To the contrary, they viewed their ability to manage their emotions as a valuable skill that could be used to gain control over customers. Thus, the Philadelphia waitresses took advantage of the lack of employer-regulated emotional labor in order to avoid the potentially negative consequences of emotional labor. Though Paules highlights the positive consequences of emotional labor for a specific population of waitresses, other scholars have also found negative consequences of emotional labor within the waitressing industry. Through eighteen months of participant observation research, Bayard De Volo (2003) found that casino waitresses are highly monitored and monetarily bribed to perform emotional labor in the workplace. Specifically, Bayard De Volo (2003) argues that through a sexualized environment and a generous tipping system, both casino owners and customers control waitresses' behavior and appearance for their own benefit and pleasure. Even though the waitresses have their own forms of individual and collective resistance mechanisms, intense and consistent monitoring of their actions by casino management makes it difficult to change the power dynamics of the casino workplace. Fast-food employees By using participant observation and interviews, Leidner (1993) examines how employers in fast food restaurants regulate workers' interactions with customers. According to Leidner (1993), employers attempt to regulate workers' interactions with customers only under certain conditions. Specifically, when employers attempt to regulate worker–customer interactions, employers believe that "the quality of the interaction is important to the success of the enterprise", that workers are "unable or unwilling to conduct the interactions appropriately on their own", and that the "tasks themselves are not too complex or context-dependent." According to Leidner (1993), regulating employee interactions with customers involves standardizing workers' personal interactions with customers. At the McDonald's fast food restaurants in Leidner's (1993) study, these interactions are strictly scripted, and workers' compliance with the scripts and regulations are closely monitored. Along with examining employers' attempts to regulate employee–customer interactions, Leidner (1993) examines how fast-food workers' respond to these regulations. According to Leidner (1993), meeting employers' expectations requires workers to engage in some form of emotional labor. For example, McDonald's workers are expected to greet customers with a smile and friendly attitude independent of their own mood or temperament at the time. Leidner (1993) suggests that rigid compliance with these expectations is at least potentially damaging to workers' sense of self and identity. However, Leidner (1993) did not see the negative consequences of emotional labor in the workers she studied. Instead, McDonald's workers attempted to individualize their responses to customers in small ways. Specifically, they used humor or exaggeration to demonstrate their rebellion against the strict regulation of their employee–customer interactions. Physicians According to Larson and Yao (2005), empathy should characterize physicians' interactions with their patients because, despite advancement in medical technology, the interpersonal relationship between physicians and patients remains essential to quality healthcare. Larson and Yao (2005) argue that physicians consider empathy a form of emotional labor. Specifically, according to Larson and Yao (2005), physicians engage in emotional labor through deep acting by feeling sincere empathy before, during, and after interactions with patients. On the other hand, Larson and Yao (2005) argue that physicians engage in surface acting when they fake empathic behaviors toward the patient. Although Larson and Yao (2005) argue that deep acting is preferred, physicians may rely on surface acting when sincere empathy for patients is impossible. Overall, Larson and Yao (2005) argue that physicians are more effective and enjoy more professional satisfaction when they engage in empathy through deep acting due to emotional labor. Police work According to Martin (1999), police work involves substantial amounts of emotional labor by officers, who must control their own facial and bodily displays of emotion in the presence of other officers and citizens. Although policing is often viewed as stereotypically masculine work that focuses on fighting crime, policing also requires officers to maintain order and provide a variety of interpersonal services. For example, police must have a commanding presence that allows them to act decisively and maintain control in unpredictable situations while having the ability to actively listen and talk to citizens. According to Martin (1999), a police officer who displays too much anger, sympathy, or other emotion while dealing with danger on the job will be viewed by other officers as someone unable to withstand the pressures of police work, due to the sexist views of many police officers. While being able to balance this self-management of emotions in front of other officers, police must also assertively restore order and use effective interpersonal skills to gain citizen trust and compliance. Ultimately, the ability of police officers to effectively engage in emotional labor affects how other officers and citizens view them. Public administration Many scholars argue that the amount of emotional work required between all levels of government is greatest on the local level. It is at the level of cities and counties that the responsibility lies for day to day emergency preparedness, firefighters, law enforcement, public education, public health, and family and children's services. Citizens in a community expect the same level of satisfaction from their government, as they receive in a customer service-oriented job. This takes a considerate amount of work for both employees and employers in the field of public administration. Mastracci and Adams (2017) looks at public servants and how they may be at risk of being alienated because of their unsupported emotional labor demands from their jobs. This can cause surface acting and distrust in management. There are two comparisons that represent emotional labor within public administration, "Rational Work versus Emotion Work", and "Emotional Labor versus Emotional Intelligence." Performance Many scholars argue that when public administrators perform emotional labor, they are dealing with significantly more sensitive situations than employees in the service industry. The reason for this is because they are on the front lines of the government, and are expected by citizens to serve them quickly and efficiently. When confronted by a citizen or a co-worker, public administrators use emotional sensing to size up the emotional state of the citizen in need. Workers then take stock of their own emotional state in order to make sure that the emotion they are expressing is appropriate to their roles. Simultaneously, they have to determine how to act in order to elicit the desired response from the citizen as well as from co-workers. Public Administrators perform emotional labor through five different strategies: Psychological First Aid, Compartments and Closets, Crazy Calm, Humor, and Common Sense. Definition: rational work vs. emotion work According to Mary Guy, Public administration does not only focus on the business side of administration but on the personal side as well. It is not just about collecting the water bill or land ordinances to construct a new property, it is also about the quality of life and sense of community that is allotted to individuals by their city officials. Rational work is the ability to think cognitively and analytically, while emotional work means to think more practically and with more reason. Definition: intelligence vs. emotional intelligence Knowing how to suppress and manage one's own feelings is known as emotional intelligence. The ability to control one's emotions and to be able to do this at a high level guarantees one's own ability to serve those in need. Emotional intelligence is performed while performing emotional labor, and without one the other can not be there. Sex work Gender Macdonald and Sirianna (1996) use the term "emotional proletariat" to describe service jobs in which "workers exercise emotional labor wherein they are required to display friendliness and deference to customers." Because of deference, these occupations tend to be stereotyped as female jobs, independent of the actual number of women working the job. According to Macdonald and Sirianna (1996), because deference is a characteristic demanded of all those in disadvantaged structural positions, especially women, when deference is made a job requirement, women are likely to be overrepresented in these jobs. Macdonald and Sirianna (1996) claim that "[i]n no other area of wage labor are the personal characteristics of the workers so strongly associated with the nature of the work." Thus, according to Macdonald and Sirianna (1996), although all workers employed within the service economy may have a difficult time maintaining their dignity and self-identity due to the demands of emotional labor, such an issue may be especially problematic for women workers. Emotional labor also affects women by perpetuating occupational segregation and the gender wage gap. Job segregation, which is the systematic tendency for men and women to work in different occupations, is often cited as the reason why women lack equal pay when compared to men. According to Guy and Newman (2004), occupational segregation and ultimately the gender wage gap can at least be partially attributed to emotional labor. Specifically, work-related tasks that require emotional work thought to be natural for women, such as caring and empathizing are requirements of many female-dominated occupations. However, according to Guy and Newman (2004), these feminized work tasks are not a part of formal job descriptions and performance evaluations: "Excluded from job descriptions and performance evaluations, the work is invisible and uncompensated. Public service relies heavily on such skills, yet civil service systems, which are designed on the assumptions of a bygone era, fail to acknowledge and compensate emotional labor." According to Guy and Newman (2004), women working in positions that require emotional labour in addition to regular work are not compensated for this additional labour because of the sexist notion that the additional labour is to be expected of them by the fact of being a woman. Guy and Azhar (2018) found that emotive expressions between sexes is affected by culture. This study found that there is variability to how women and men interpret emotive words, and specifically results showed that culture played a huge role in these gender differences. Disability People with disabilities are becoming increasingly part of the labor force due to societal attitudes about inclusion and neoliberal pressures around reducing welfare. Roles that require emotional labor may be more difficult for people with certain kinds of disabilities to perform. People with disabilities may also have to put more time and energy to perform a task than non-disabled people, for instance, when they routinely encounter prejudice and stigma (as would be the case for many groups experiencing prejudice) including disability-unfriendly structures (accessibility, administrative or social). On the other hand, due to the routine experience of navigating unhelpful structures and prejudice, people with disabilities can have the dual advantage of better skills in finding ways round problems without expending emotional energy, like feeling surprised, and a more empathetic understanding of the experiences of other people with similar problems. Inclusive or unfriendly organizational culture also has an impact, and workplaces may require workers with disabilities to downplay their impairments in order to "fit in" creating an extra burden of emotional labor. Most individuals will experience complex effects of their disabilities on their emotional labor in a given job role at a specified organisation. Implications Positive affective display in service interactions, such as smiling and conveying friendliness, are positively associated with customer positive feelings, and important outcomes, such as intention to return, intention to recommend a store to others, and perception of overall service quality. There is evidence that emotional labor may lead to employees' emotional exhaustion and burnout over time, and may also reduce employees' job satisfaction. That is, higher degree of using emotion regulation on the job is related to higher levels of employees' emotional exhaustion, and lower levels of employees' job satisfaction. There is empirical evidence that higher levels of emotional labor demands are not uniformly rewarded with higher wages. Rather, the reward is dependent on the level of general cognitive demands required by the job. That is, occupations with high cognitive demands evidence wage returns with increasing emotional labor demands; whereas occupations low in cognitive demands evidence a wage "penalty" with increasing emotional labor demands. Additionally, innovations that increase employee empowerment — such as conversion into worker cooperatives, co-managing schemes, or flattened workplace structures — have been found to increase workers' levels of emotional labor as they take on more workplace responsibilities. Coping skills Coping occurs in response to psychological stress—usually triggered by changes—in an effort to maintain mental health and emotional well-being. Life stressors are often described as negative events (loss of a job). However, positive changes in life (a new job) can also constitute life stressors, thus requiring the use of coping skills to adapt. Coping strategies are the behaviors, thoughts, and emotions that you use to adjust to the changes that occur in your life. The use of coping skills will help a person better themselves in the work place and perform to the best of their ability to achieve success. There are many ways to cope and adapt to changes. Some ways include: sharing emotions with peers, having a healthy social life outside of work, being humorous, and adjusting expectations of self and work. These coping skills will help turn negative emotion to positive and allow for more focus on the public in contrast to oneself. See also References == Further reading == The cost of raising a child varies widely from country to country. It is usually determined according to a formula that accounts for major areas of expenditure, such as food, housing, and clothing. However, any given family's actual expenses may differ from the estimates. For example, the rent on a home does not usually change when the tenants have another child, so the family's housing costs may remain the same. In other cases, the home may be too small, in which case the family might move to a larger home at a higher cost. The formula may also account for inflation, as prices are constantly changing, and it will indirectly affect how much it costs to raise a child. Developing countries According to Globalissues.org, "Almost half the world—over three billion people—live on less than US$2.50 a day." This statistic includes children. The calculation of the cost to raise a child in developing countries is difficult, since families often do not operate with currency, but barter or trade to provide for their children. It is argued that in developing areas the balance between earnings and costs of having children is changing, because the mean number of children per couple in many developing areas has decreased dramatically, especially in Asia, North Africa and the Near East. According to a 2020 report, 356 million children – 17.5 per cent – live in extreme poverty (less than US$1.90 a day). Argentina Argentina's INDEC provides a breakdown of minimum costs per person in household, known as the "canasta básica total", this metric doesn't measure the average cost, but the minimum cost (poverty line) and is published monthly. All values are per "equivalent adult". Data for June 2022. Exchange rate 1 US dollar is 135,7500 ARG (July 2022). This aggregates to around $ 41,501.40 from birth to 18 years old. India Based on an estimate by Economic Times in April 2011 and adjusted to inflation for August 2022, the cost of raising a child from birth to age of majority (21 Years) for a middle to upper-middle income family comes to about ₹1.17 crore (equivalent to ₹1.2 crore or US$150,000 in 2023) in total. Cost break up is as follows: Note: Estimate assumes cost of birth, but doesn't consider any major illness in child. Developed countries United Kingdom Child Poverty Action Group’s annual cost of a child report looks at how much it costs families to provide a minimum socially acceptable standard of living for their children. The 2022 report shows the cost of raising a child from birth to 18 years old as £157,562 for a couple family or £208,735 for a single parent/guardian. The Times estimates that it costs £202,660 to raise a child from birth to 18 in the UK. This includes the cost of housing and childcare. This works out to an average approximate of £11,250 per year, or £938 per month. United States Based on a survey by the U.S. Department of Agriculture, the table below shows the estimated Average Spending on Children by Families. The data comes from the Consumer Expenditure Survey by the U.S. Department of Labor, conducted from 2005-06. The figures have been updated to 2011 dollars using the Consumer Price Index. However, some dispute the numbers as being biased high for political reasons (e.g., Texas A&M University Finance Professor H. Swint Friday: "The numbers, reported by the U.S. Department of Agriculture, are outrageously misleading. Often government statistics are produced for political objectives that cause the research methodology to be biased toward finding the highest dollar amount to support the objective."). These figures from the USDA go up to age 18, and do not include any college or university education. Nor does it offer any spending estimates if the child remains in the home as a dependent after the age of 18. Both tables are for the United States overall, not based on any specific region in the country. All numbers are in US dollars. All numbers are in US dollars. See also References Further reading Wolf DA, Lee RD, Miller T, Donehower G, Genest A. (2011). Fiscal externalities of becoming a parent. Popul Dev Rev, 37(2): 241-66. https://doi.org/10.1111/j.1728-4457.2011.00410.x. External links "Raising that '07 baby will cost $204,060 in U.S." - from Reuters, retrieved August 5, 2012. "Cost of raising children not as high as government would have you believe" - from caller.com, retrieved March 31, 2013. "Cost of Raising Children in the US" - from Globe Life Insurance, retrieved September 11, 2013 Logosphere (Greek from logos / nous) (coined by Mikhail Bakhtin) is an adaptation of the concepts biosphere and noosphere: logosphere is derived from the interpretation of words' meanings, conceptualized through an abstract sphere. Overview The logosphere is not active like Vernadsky’s noosphere, but still occupies a type of four-dimensional space. The chronotope is the conduit through which meaning enters the logosphere. Mikhail Bakhtin's chronotope, or time-space (deterministic) makes outside-the-logosphere (unintelligible) information relevant to the logosphere through narrative structure. Time takes on a protagonist's 'flesh'. The adventure chronotope is thus characterized by a technical, abstract connection between space and time, by the reversibility of moments in a temporal sequence, and by their interchangeability in space. ...Every concretization, of even the most simple and everyday variety, would introduce its own rule-generating force, its own order, its inevitable ties to human life and to the time specific to that life. ...Biographical time is not reversible vis-à-vis the events of life itself, which are inseparable from historical events. But with regard to character, such time is reversible[.] Logosphere applications Technological conceptualizations The term was later taken up by virtual reality enthusiasts to describe the logical universe. Telecommunications The logosphere, in decades past, has been used in reference to the new world of communication created by the invention of the radio. French philosopher Gaston Bachelard proclaimed, "Everyone can hear everyone else and we can all listen in peace." This "domain of world speech" should be called the logosphere, he reasoned. == References == An extended family is a family that extends beyond the nuclear family of parents and their children to include aunts, uncles, grandparents, cousins or other relatives, all living nearby or in the same household. Particular forms include the stem and joint families. Description In some circumstances, the extended family comes to live either with or in place of a member of the immediate family. These families include, in one household or close proximity, relatives in addition to an immediate family. An example would be an elderly parent who moves in with his or her children due to old age. In modern Western cultures dominated by immediate family constructs, the term has come to be used generically to refer to grandparents, uncles, aunts, and cousins, whether they live together within the same household or not. However, it may also refer to a family unit in which several generations live together within a single household. In some cultures, the term is used synonymously with consanguineous family. A stem family is a kind of extended family, first discussed by Frédéric Le Play. Parents will live with one child and his/her spouse, as well as the children of both, while other children will leave the house or remain in it, unmarried. The stem family is sometimes associated with inegalitarian inheritance practices, as in Japan and Korea, but the term has also been used in some contexts to describe a family type where parents live with a married child and his or her spouse and children, but the transfer of land and moveable property is more or less egalitarian, as in the case of traditional Romania, northeastern Thailand or Mesoamerican indigenous peoples. In these cases, the child who cares for the parents usually receives the house in addition to his or her own share of land and moveable property. Sociology Often, it has been presumed that extended family groups sharing a single household enjoy specific advantages, such as a greater sense of security and belonging due to sharing a wider pool of members to serve as resources during a crisis, and more role models to help perpetuate desired behavior and cultural values. However, even in cultures in which adults are expected to leave home after marriage to begin their own nuclear-based households, the extended family often forms an important support network offering similar advantages. Particularly in working-class communities, grown children tend to establish their own households within the same general area as their parents, aunts, uncles, and grandparents. These extended family members tend to gather often for family events and to feel responsible for helping and supporting one another, both emotionally and financially. While contemporary families may be considered more mobile in general than in the past, sociologists find that this has not necessarily resulted in the disintegration of extended family networks. Rather, technological aids such as the Internet and social networking sites such as Facebook are now commonly used to retain contact and maintain these family ties. Particularly in the case of single-parent households, it can be helpful for extended family members to share a single household in order to share the burden of meeting expenses. On the other hand, sharing a household can present a disadvantage depending on the sizes and number of families involved, particularly when only a few members shoulder most of the responsibility to meet expenses for the family's basic needs. An estimated 49 million Americans (16.1% of the total population) live in homes comprising three or more generations, up from 42 million in 2000. This situation is similar in Western Europe. Another 34 percent live within a kilometer of their children. Around the world In many cultures, such as in those of Asians, Middle Easterners, Africans, Indigenous peoples like Native Americans and Pacific Islanders, and Latin Americans and Caribbeans, even for Eastern Europeans and Southern Europeans (Orthodox/Catholic countries), extended families are the basic family unit. That is to say the modern western nuclear family is not the norm. Even in Western Europe, extended families (mostly of the stem type) were also clearly prevalent, England being a rare exception. In Britain and the United States, during the Industrial Revolution (approximately 1750 to 1900), more people lived in extended families than at any time before or since. It is common for today's world to have older children in nuclear families to reach walking up to driving age ranges before meeting extended family members. Geographical isolation is common for middle-class families who move based on occupational opportunities while family branches "retain [their] basic independence". Some extended families hold family reunions or opportunities for gathering regularly, normally around holiday time frames, to reestablish and integrate a stronger family connection. This allows individual nuclear families to connect with extended family members. Where families consist of multiple generations living together, the family is usually headed by the elders. More often than not, it consists of grandparents, their sons, and their sons' families in patriarchal and especially patrilineal societies. Extended families make discussions together and solve the problem. Indian subcontinent Historically, for generations South Asia had a prevailing tradition of the joint family system or undivided family. The joint family system is an extended family arrangement prevalent throughout the Indian subcontinent, particularly in India, consisting of many generations living in the same home, all bound by the common relationship. A patrilineal joint family consists of an older man and his wife, his sons and unmarried daughters, his sons' wives and children. The family is headed by a patriarch, usually the oldest male, who makes decisions on economic and social matters on behalf of the entire family. The patriarch's wife generally exerts control over the household, minor religious practices and often wields considerable influence in domestic matters. Family income flows into a common pool, from which resources are drawn to meet the needs of all members, which are regulated by the heads of the family. Recent trend in the United States In the early stages of the twentieth century, it was not very common to find many families with extended kin in their household, which may have been due to the idea that the young people in these times typically waited to establish themselves and start a household before they married and filled a home. As life expectancy becomes older and programs such as Social Security benefit the elderly, the old are now beginning to live longer than prior generations, which then may lead to generations mixing together. According to results of a study by Pew Research Center in 2010, approximately 50 million (nearly one in six) Americans, including rising numbers of seniors, live in households with at least two adult generations, and often three. It has become an ongoing trend for elderly generations to move in and live with their children, as they can give them support and help with everyday living. The main reasons cited for this shift are an increase in unemployment and slumped housing prices and arrival of new immigrants from Asian and South American countries. According to the U.S. Census Bureau, there were 2.7 million grandparents raising their grandchildren in 2009. The dramatic increase in grandparent-headed households has been attributed to many factors including parental substance abuse. In 2003, the number of U.S. "family groups" where one or more subfamilies live in a household (e.g. a householder's daughter has a child. The mother-child is a subfamily) was 79 million. Two-point-six million of U.S. multigenerational family households in 2000 had a householder, the householder's children, and the householder's grandchildren. That is 65 percent of multigenerational family households in the U.S. So it is twice as common for a grandparent to be the householder than for adult children to bring parents into their home. The increase in the number of multigenerational households has created complex legal issues, such as who in the household has authority to consent to police searches of the family home or private bedrooms. Besides the legal issues that multigenerational households could create, there are issues that may arise from households where the grandparents are the sole guardians. The Supporting Grandparents Raising Grandchildren Act was signed into law on July 7, 2018 after unanimously passing the U.S. House and Senate. It was first introduced in the Senate on May 10, 2017 by Senators Susan Collins (R-ME) and Bob Casey Jr. (D-PA). Out of this came The Supporting Grandparents Raising Grandchildren Advisory Council which will identify, promote, coordinate, and disseminate to the public information, resources, and the best practices available to help grandparents and other older relatives both meet the needs of the children in their care and maintain their own physical and mental health and emotional well-being. Mexico Mexican society is composed of three-generational units consisting of grandparents, children, and grandchildren. Further close relationships are maintained with the progenitors of these families and are known as kin or "cousins". When one is born, they are born into two extended families, a kinship group of sometimes 70 people. The group traditionally acts as a cohesive unit, pooling resources and influence. The extended family also consists of spouses and siblings. This is in contrast to the two generational American nuclear family. Some scholars have used the term "grand-family" to describe the close relationship between grandparents, children, and grandchildren in Mexican society. Larissa A. Lomnitz and Marisol Perez-Lizaur, for example, describe the grand-family as "the basic unit of family solidarity in Mexico", where basic family obligations between grandparents, children, and grandchildren include "economic support, participation in family rituals, and social recognition". Economic background The relative economic deprivation of racial and ethnic minorities leads to higher levels of extended family involvement; primarily because blacks and Latinos have less money and education than whites, they are more likely to give and receive help from kin. Having family on which one can rely is very important in times of economic hardship especially if there are children involved. Living in an extended family provides constant care for children and support for other members of the family as well. Analysis of the National Survey of Families and Households suggests there are differences between whites and other ethnic groups because of economic differences among racial groups: blacks and Latinos less often have the economic resources that allow the kind of privatization that the nuclear family entails. Extended kinship, then, is a survival strategy in the face of economic difficulties. Being able to rely on not only two parents but grandparents, aunts, uncles, brothers, and sisters helps to create a support system which in turn brings families closer together. Living in an extended family provides many things that a nuclear family does not. The number of multigenerational households has been steadily rising because of the economic hardships people are experiencing today. According to the AARP, multigenerational households have increased from 5 million in 2000 to 6.2 million in 2008. "There's no question that with some ethnicities that are growing in America, it is more mainstream and traditional to have multigenerational households. We're going to see that increasing in the general population as well," says AARP's Ginzler. While high unemployment and housing foreclosures of the recession have played a key role in the trend, Pew Research Center exec VP and co-author of its multigenerational household study Paul Taylor said it has been growing over several decades, fueled by demographic and cultural shifts such as the rising number of immigrants and the rising average age of young-adult marriages. The importance of an extended family is one that many people may not realize, but having a support system and many forms of income may help people today because of the difficulties in finding a job and bringing in enough money. See also Cluster genealogy == References == Ontopoetics is a philosophical concept that involves the communicative engagement of self with the world and the world with the self. It is also described as a "poetic order" that unfolds alongside the "causal order" in the process of the communicative engagement with reality and participating in it. It includes the perception of cues or signals, or the expression of actors, as well as "the construction of impressions on re-actors by the deliberate choice of attractive signifiers that communicate factual or illusory realities". Ontopoetics is not considered a theory but a view of reality and an understanding of the world as a communicative presence. Concept Ontopoetics is derived from the Greek words ontos ("that which is" - "I am" or "being") and poiesis ("coming into being" - creation" or "bringing forth"). It is also noted that the poetic element to the concept connotes a complexity that embraces diversity of experiences so that those that do not lie within the bounds of one's tradition are not rejected or denied. The concept also includes the manner by which humans respond to the symmetries around them. It is distinguished from panpsychism in the sense that it does not merely claim that the world is psychoactive but that it is responsive to us so that it can be called forth if engaged on an expressive plane, one of meaning and not merely of causation. According to Freya Mathews, the occurrence of meaningful communicative exchanges between self and the world and world and self allows a glimpse of the inner, psychoactive dimension that is inherent in materiality but occluded by materialism. As a concept, ontopoetics looks into the creative relationship between things and focuses on the poetic infrastructure of creation (e.g. order of an insect, structure of a seed, or the composition of a bird song). Aside from the cues, expressions, or signifiers made to communicate realities, ontopoetics also covers the "construction of imaginary situations by certain species" such as animal cheating, mimicking, and playing. Ontopoetics holds that the world is not only object-domain as represented by physics but is also "a field of meaning". According to Mathews, this understanding of the world allows for unmasking of realities and experience that are not familiar or known to science. This is attributed to the manner by which the paradigm produces a more dynamic and responsive self and poetic voice as experience and knowledge are directed by receptiveness, playfulness, and openness across human-nature divisions. The idea is that conceptual intelligence cannot access a depth of reality because it tends to trivialize it. This is also the case for possibilities of experience that are routinely open but are taken for granted. In ontopoetics, a painting or a poem can capture reality better than common language or common perception because these apprehend it in its irreducible essence. In addition, these artworks are said to also coincide with metaphysical intuition. Ontopoetics argues for a model of ontological plurality. It suggests that all truths and realities are potentially but not exclusively true and real. In the Will to Power, where Nietzsche argued for the impossibility of truth, it was maintained that "there are no facts, only interpretations." Ontopoetics stands in opposition to global perspectives (e.g. atomism and economism) due to its focus on field-concordance between psyche, meaning, and cosmos. It has been described as a fresh conception of the Cartesian split of external appearance and reality, problematizing it through a dialogic consciousness or poetics as ontology. Another conceptualization frames ontopoetics as a critique founded on the idea that the expression of being beyond what is representationally constructed as reality leads to the philosophical thought on art. Friedrich Nietzsche Friedrich Nietzsche proposed that the world and existence are aesthetic phenomena in his deconstruction of truth. Ontopoetics is part of this aesthetic metaphysics of the world and the metaphysical aesthetics of art where the work of God as the artist-creator and its representation in the work of art are distinguished as first and second levels, respectively. These perspectives would later influence aspects of the philosophy of thinkers such as Martin Heidegger and Gilles Deleuze. The application of the Nietzschean ontopoetics to art is based on a conceptualization of reality that includes elements hidden behind a "veil" created by the interplay of institutions and false needs. Art in this case articulates what the veil hides. Ontopoetics has also influenced a group of artists identified as concretists, who emphasize ontopoetic issues in their art, particularly when relating reality in itself. It is also evident in the works of Marcel Duchamp, which featured the notion of "event" and the application of "analogy" in his sculptures. His art took into consideration the onto-epistemic dynamics that lurk behind manmade structure-events, which invite the spectator to make the next move based on an analogy, assumption, or deduction. Ontopoetics in this approach is considered a constitutive and creative step. Martin Heidegger Heidegger maintained that truth can never be extracted from 'the sheer "not" of beings'. He subscribed to the Nietzschean idea that the world is a phenomenon that informs the ontopoetic conception of history. He maintained that the poetic is a type of unfolding of historical existence instead of cultural achievement. In his ontopoetics, there is also the conceptualization that art does not prioritize a preference for aesthetics but focuses on the happening of being where being takes place in the midst of beings. The poetic phenomenon called the Heideggerian Mit-da ("with-there) is said to illustrate the thinker's perspective concerning the anxiety of knowledge. Here, ontopoetics, as the rhythmic dimension of reality in knowledge, has an emotional opening of the word that finds the company of the world "on the edge of existence". == References == Public speaking is the practice of delivering speeches to a live audience. Throughout history, public speaking has held significant cultural, religious, and political importance, emphasizing the necessity of effective rhetorical skills. It allows individuals to connect with a group of people to discuss any topic. The goal as a public speaker may be to educate, teach, or influence an audience. Public speakers often utilize visual aids like a slideshow, pictures, and short videos to get their point across. The ancient Chinese philosopher Confucius, a key figure in the study of public speaking, advocated for speeches that could profoundly affect individuals, including those not present in the audience. He believed that words possess the power to inspire actions capable of changing the world. In the Western tradition, public speaking was extensively studied in Ancient Greece and Ancient Rome, where it was a fundamental component of rhetoric, analyzed by prominent thinkers. Aristotle, the ancient Greek philosopher, identified three types of speeches: deliberative (political), forensic (judicial), and epideictic (ceremonial or demonstrative). Similarly, the Roman philosopher and orator Cicero categorized public speaking into three purposes: judicial (courtroom), deliberative (political), and demonstrative (ceremonial), closely aligning with Aristotle's classifications. In modern times, public speaking remains a highly valued skill in various sectors, including government, industry, and advocacy. It has also evolved with the advent of digital technologies, incorporating video conferencing, multimedia presentations, and other innovative forms of communication. Purposes The main objective of public speaking is to inform or change the audience's thoughts and actions. The function of public speaking is determined by the speaker's intent, but it is possible for the same speaker, with the same intent, to deliver substantially different speeches to different audiences. Public speaking is frequently directed at a select and sometimes restricted audience, consisting of individuals who may hold different perspectives. This audience can encompass enthusiastic supporters of the speaker, reluctant attendees with opposing views, or strangers with varying levels of interest in the speaker's topic. Proficient speakers recognize that even a modest-sized audience is not a uniform entity but rather a diverse assembly of individuals. Public speaking aims to either reassure an anxious audience or to alert a complacent audience of something important. Once the speaker has determined which of these approaches is required, they will use a combination of storytelling and informational approaches to achieve their goals. The purposes of speech can vary depending on the targeted audience. Speeches during ceremonies may incorporate humor or stories shared from moments in the life of the person celebrated. Speeches focusing on politics will use persuasion that listeners take a course of action, and forensic speeches are debates in which participants take sides, defend certain beliefs, and are judged on how well they can support their argument. Persuasion Persuasion is a term that is derived from the Latin word "persuadere." Persuasive speaking aims to change the audience's beliefs and is commonly used in political debates. Leaders use such public forums in an attempt to persuade their audience, whether they be the general public or government officials. Persuasive speaking involves four essential elements: (i) the speaker or persuader; (ii) the audience; (iii) the speaking method; and (iv) the message the speaker is trying to convey. When attempting to persuade an audience to change their opinions, a speaker appeals to their emotions and beliefs. Various techniques exist for speakers to gain audience support. Speakers can demand action from the audience, use inclusive language like 'we' and 'us' to create unity between the speaker and the audience, and choose words with strong connotations to intensify a message's impact. Rhetorical questions, anecdotes, generalizations, exaggerations, metaphors, and irony may be employed to increase the likelihood of persuading an audience. Though historically uncommon, speakers today are enabled to utilise statistics, data as well as other sources of information, such as the internet, in order to strengthen their argument, stance or proposal; This has only evolved during the modern era, having been generally unavailable at the current rate in the years beforehand with the exception of media via newspapers, television, although claims given by speakers have often been subject to inaccurate information provided by the aforementioned, often in direct correlation with the big lie means of oratory. This has been further intensified through the recent evolution of mass media in most nations. Education Public speaking can often take an educational form, where the speaker transfers knowledge to an audience. TED Talks are an example of educational public speaking. The speakers inform their audience about different topics, such as science, technology, religion, economics, human society, and psychology. TED speakers can use the platform to share personal experiences with traumatic events, such as abuse, bullying, grief, assault, suicidal ideation, near-death encounters, and mental illness. They may attempt to raise awareness and acceptance of stigmatizing issues, such as disabilities, racial differences, LGBTQ rights, children's rights, and women's rights. TED Conferences, LLC, is a media organization that posts talks online for free distribution under the slogan: "ideas worth spreading". TED was originally built by Richard Saul Furman in February 1984 as a conference and has been held annually since 1990. Talks delivered in these conferences are usually posted online. The videos of these recorded speeches and talks inspire native and non-native speakers of English to learn the language and presentation style that is used. As such, TED Talk videos can help improve speaking skills and vocabulary retention. There have been many studies that have proven the benefits of teaching public speaking strategies to students in an academic setting, including a higher level of self-confidence and helping to render community well-being with access to a variety of information. Harvard University offers a range of courses in public speaking, including persuasive communication and personal narratives. With the continued popularity of academic conferences and TED talks taking place worldwide, public speaking has become an essential subject in academia for scholarly and professional advancement. Additionally, work meetings and presentations require proficiency in public speaking to actively formulate ideas and solutions, and modern technology helps companies release information to a wider audience. Intervention The intervention style of speaking is a relatively new method proposed by rhetorical theorist William R. Brown. This style revolves around the theory of idealism, which holds that humans create a symbolic meaning for life and the things around them. Due to this, the symbolic meaning of everything changes based on the way one communicates. When approaching communication with an intervention style, communication is understood to be responsible for the constant changes in society, behaviors, and how one considers the meaning behind objects, ideologies, and everyday life. From an interventional perspective, when individuals communicate, they are intervening with what is already a reality and might "shift symbolic reality." This approach to communication encompasses the possibility or idea that one may be responsible for unexpected outcomes due to what and how one communicates. This perspective widens the scope of focus from a single speaker who is intervening to a multitude of speakers all communicating and intervening, simultaneously affecting the world around us. History India The literature of Ancient India is richly endowed with contributions to the development of a sui generis theory of rhetoric. In ancient India, around 700 BCE, public debates by Indian rhetors on the topic of religion were a popular form of entertainment. The Vedic hymns, composed over three millennia ago, demonstrate a refined sense of rhetoric possessed by the intellectual stratum of the society, as seen in their effective employment of similes. The Ramayana and the Mahabharata, India's iconic epics, provide valuable insights into the country's ancient rhetorical traditions, featuring numerous speeches and debates that employ sophisticated systems of categorization. The Upanishads, a seminal work of Vedic philosophical dialogues, exhibit a thoughtful approach to categorizing technical terms, underscoring the value of clear classification. The famed Hindu discourse known as the Bhagavad Gita (in the Mahabharata) serves as a classic example of deliberative rhetoric. The Buddhist tradition of India places emphasis on the value of engaging in calm and humorous discourse. China In Ancient China, the use of rhetoric was delayed, largely because the country then lacked rhetoricians who could train students. It was understood that Chinese rhetoric was part of Chinese philosophy, which schools taught focusing on two concepts: "Wen" (rhetoric); and "Zhi" (thoughtful content). Ancient Chinese rhetoric shows strong connections with modern public speaking, as Chinese rhetoric placed a high value on ethics. Ancient Chinese rhetoric had three objectives: (i) using language to reflect people's feelings; (ii) using language to be more pointed, effective, and impactful; and (iii) using rhetoric as an "aesthetic tool." Chinese rhetoric traditionally focused more on the written than the spoken word, but both share similar characteristics of construction. A unique and key difference between Chinese and Western rhetoric is the audience targeted for persuasion. In Chinese rhetoric, state rulers were the audience, whereas Western rhetoric targets the public. Another difference between Chinese and Western rhetoric practices is how a speaker establishes credibility or Ethos. In Chinese rhetoric, the speaker does not focus on individual credibility, like Western rhetoric. Instead, the speaker focuses on collectivism by sharing personal experiences and establishing a connection between the speaker's concern and the audience's interest. Chinese employs three standards in assessing public rhetoric: Tracing: This standard evaluates how well the speaker is doing compared to traditional speaking practices. Examination: This standard evaluates how well the speaker considers the audience's daily lives. Practice: This standard evaluates how relevant the topic or argument is to the "state, society, and people." Greece Although evidence of public speaking training exists in ancient Egypt, the first known writing on oratory is 2,000 years old from ancient Greece. This work elaborates on principles drawn from the practices and experiences of ancient Greek orators. Aristotle, one of the first oratory teachers to use definitive rules and models, believed that successful speakers combined, to varying degrees, three qualities in their speech: reasoning, which he called Logos; credentials, which he called Ethos; and emotion, which he called Pathos. Aristotle's work became an essential part of a liberal arts education during the Middle Ages and the Renaissance. The classical antiquity works by the ancient Greeks capture how they taught and developed the art of public speaking thousands of years ago. In classical Greece and Rome, rhetoric was the main component of composition and speech delivery, both critical skills for use in public and private life. In ancient Greece, citizens spoke for themselves rather than having professionals, such as modern lawyers, speak for them. Any citizen who wished to succeed in court, politics, or social life had to learn public speaking techniques. Rhetorical tools were first taught by a group of teachers called Sophists, who taught paying students how to speak effectively using their methods. Separately from the Sophists, Socrates, Plato, and Aristotle developed their theories of public speaking, teaching these principles to students interested in learning rhetorical skills. Plato founded The Academy and Aristotle founded The Lyceum to teach these skills. Demosthenes was a well-known orator from Athens. After his father died when he was 7, he had three legal guardians: Aphobus, Demophon, and Theryppides. His inspiration for public speaking came from learning that his guardians had robbed him of the money his father left for his education. His first public speech was in the court proceeding he brought against his three guardians. After that, Demosthenes continued to practice public speaking. He is known for sticking pebbles into his mouth to improve his pronunciation, talking while running so that he would not lose his breath, and practicing speaking in front of a mirror to improve his delivery. When Philip II, the ruler of Macedon, tried to conquer the Greeks, Demosthenes made a speech called Kata Philippou A. In this speech, he spoke about why he opposed Philip II as a threat to all of Greece. This was the first of several speeches known as the Philippics. He made other speeches known as the Olynthiacs. Both series of speeches favored independence and rallied Athenians against Philip II. Rome During the political rise of the Roman Republic, Roman orators copied and modified the ancient Greek techniques of public speaking. Instruction in rhetoric developed into a full curriculum, including instruction in grammar (study of the poets), preliminary exercises (progymnasmata), and preparation of public speeches (declamation) in both forensic and deliberative genres. In Latin, rhetoric was heavily influenced by Cicero, an orator during the Roman Empire, and emphasized a broad education in all areas of the humanities. Other areas of rhetorical study included the use of wit and humor, the appeal to the listener's emotions, and the use of digressions. Oratory in the Roman Empire, though less central to political life than during the Republic, remained important in law and entertainment. Famous orators were celebrities in ancient Rome, becoming wealthy and prominent in society. The ornate Latin style was the primary form of oration through the mid-20th century. After World War II and the increased use of film and television, the Latin oration style began to fall out of favor. This cultural change likely had to do with the rise of the scientific method and the emphasis on a "plain" style of speaking and writing. Even today's formal oratory is much less ornate than in the Classical Era. Theorists Aristotle's "Rhetoric" In one of his most famed writings, "Rhetoric", written in 350 BCE, Aristotle described mastering the art of public speaking. In this and other works by Aristotle, rhetoric is the act of publicly persuading an audience. Rhetoric is similar to dialect: he defines both as being acts of persuasion. However, dialect is the act of persuading someone in private, whereas rhetoric is about persuading people in a public setting. Aristotle defines someone who practices rhetoric or a "rhetorician" as an individual who can comprehend persuasion and how it is applied. Aristotle divides rhetoric into three elements: (i) the speaker; (ii) the topic or point of the speech; and (iii) the audience. Aristotle also classifies oration into three types: (i) political, used to convince people to take or not take action; (ii) forensic, usually used in law related to accusing or defending someone; and (iii) ceremonial, which recognizes someone positively or negatively. Aristotle breaks down the political category into five focuses or themes: "ways and means, war and peace, national defense, imports and exports, and legislation." These focuses are broken down into detail so that the speaker can effectively influence an audience to agree and support the speaker's ideas. The focus of "ways and means" deals with economic aspects of how the country is spending money. "Peace and War" focuses on what the country has to offer in terms of military power, how war has been conducted, how war has affected the country in the past, and how other countries have conducted war. "National defense" deals with considering a country's position and strength in the event of an invasion. Fortifying structures and points with a strategic advantage should all be considered. "Food supply" is concerned with the ability to support a country in regards to food, importing and exporting food, and carefully making decisions to arrange agreements with other countries. "Legislation" is the most important to Aristotle. The legislation of a country is the most crucial aspect because everything is affected by the policies and laws set by the people in power. In Aristotle's "Rhetoric" writing, he mentions three strategies someone can use to try to persuade an audience: Establishing the character of a speaker (Ethos), influencing the emotional element of the audience (Pathos), and focusing on the argument specifically (Logos). Aristotle believes establishing the character of a speaker is effective in persuasion because the audience will believe what the speaker is saying to be true if the speaker is credible and trustworthy. With the audience's emotional state, Aristotle believes that individuals do not make the same decisions when in different moods. Because of this, one needs to try to influence the audience by being in control of one's emotions, making persuasion effective. The argument itself can affect the attempt to persuade by making the argument of the case so clear and valid that the audience will understand and believe that the speaker's point is real. In the last part of "Rhetoric", Aristotle mentions that the most critical piece of persuasion is to know in detail what makes up government and to attack what makes it unique: "customs, institutions, and interest". Aristotle also states that everyone is persuaded by considering people's interests and how the society in which they live influences their interests. Cicero's Five Canons of Rhetoric In his writing De Inventione, Cicero explained the five canons or tenets of rhetoric. The five canons apply to rhetoric and public speaking. The five canons are invention, arrangement, style, memory, and delivery. Invention is the process of coming up with what to say to persuade the audience of the key points. Individuals will need to understand their topic, brainstorm their ideas, and discover effective research strategies that they can use to get their point across. Arrangement is the process of structuring ideas together. Cicero and the Roman rhetorician Quintilian identified the structure of a text as Exordium, Narrative, Partition, Confirmation, Refutation, and Peroration (or conclusion). In today's text, the structure has been reduced to introduction, body, and conclusion. Style is the process of choosing language and constructing your presentation to create an emotional response from the audience. Individuals can achieve this by using language and rhetoric devices like analogy, allusion and alliteration. Memory is remembering enough so that individuals are able to fully and fluently present without reading off a paper or note cards. This includes figures of speech, which can be used to improve memory. Roman rhetoricians made a distinction between natural memory (an innate ability) and artificial memory (particular techniques that enhanced natural abilities). Delivery is the last of the five canons of rhetoric. It Involves using all the tools available to effectively communicate. Methods and tools like tone of voice, change of pace, pauses, volume, body language, positioning and props are all effective in delivering the point. Glossophobia The fear of speaking in public, known as glossophobia or public speaking anxiety, is often mentioned as one of the most common phobias. The reason is uncertain, but it has been speculated that this fear is primal, similar to how animals fear being seen by predators. The apprehension experienced when speaking in public can have several causes, such as social anxiety disorder, or a prior experience of public humiliation. This can be related to stage fright. Training Effective public speaking can be developed by joining a club such as Rostrum, Toastmasters International, Association of Speakers Clubs (ASC), or Speaking Circles, in which members are assigned exercises to improve their speaking skills. Members learn by observation and practice and hone their skills by listening to constructive suggestions, followed by new public speaking exercises. Tips for improving public speaking: Rehearse Craft speech that targets audience. Organize it in a way that attracts audience attention. Adapt to audiences' reaction. Make your speech interesting through use of language. Use tone and body language. Refrain from script and make an outline Refrain from making gestures that distract audience. Make your intro interesting and leave audience with something to think about at ending. Use audiovisual aids that enhance or clarify your speech. Toastmasters International Toastmasters International is a public speaking organization with over 15,000 clubs worldwide and more than 300,000 members. This organization helps individuals with their public speaking skills, as well as leadership skills necessary to become effective public speakers such as content development, club development, and speech contests. Members of the club meet and work together on their skills; each member practices giving speeches, while the other members evaluate and provide feedback. A typical meeting also includes Table Topics, which refers to impromptu speaking, that is, talking about different topics without having anything planned. Members can volunteer to serve as a meeting functionary to help facilitate the meeting using their public speaking and leadership skills. The functionary roles enable each member the opportunity to speak at least one time at the meetings. Members can participate in a variety of speech contests, in which the winners can compete in the annual World Championship of Public Speaking. Australian Rostrum Rostrum is another public speaking organization, founded in Australia, with more than 100 clubs all over the country. This organization aims at helping people become better communicators, no matter the occasion. At the meetings, speakers can gain skills by presenting speeches, while members provide feedback to those presenting. Qualified speaking trainers attend these meetings as well, and provide professional feedback at the end of the meetings. There are competitions that are held for members to participate in. An online club is also available for members, no matter where they live. Self-training solutions The new millennium has seen a notable increase in the number of training solutions, offered in the form of video and online courses. Videos can provide simulated examples of behaviors to emulate. Professional public speakers often engage in ongoing training and education to refine their craft. This may include seeking guidance to improve their speaking skills, such as learning better storytelling techniques, learning how to use humor as a communication tool effectively, and continuously researching their topic area of focus. They also recognize that content is king and advocate writing as a self-training exercise because it requires a speaker to focus on developing the content, not just speaking techniques. Professional speakers Public speaking for business and commercial events is often done by professionals, whose expertise is well established. These speakers can be contracted independently, through representation by a speakers bureau, or by other means. Public speaking plays a large role in the professional world. It is believed that 70 percent of all jobs involve some form of public speaking. Most professional roles require some sort of public speaking skills. Individuals will often be expected to perform tasks like training staff, leading meetings, and pitching proposals. Modern Technology New technology has opened different forms of public speaking that are non-traditional such as TED Talks, which are conferences that are broadcast globally. This form of public speaking has created a wider audience base because public speaking can now reach both physical and virtual audiences. These audiences can be watching from all around the world. YouTube is another platform that allows public speaking to reach a larger audience. On YouTube, people can post videos of themselves. Audiences can watch these videos for all types of purposes. Multimedia presentations can contain different video clips, sound effects, animation, laser pointers, remote control clickers, and endless bullet points. All adding to the presentation and evolving our traditional views of public speaking. Public speakers may use audience response systems. For large assemblies, the speaker will usually speak with the aid of a public address system or microphone and loudspeaker. Telecommunication Telecommunication and videoconferencing are also forms of public speaking. David M. Fetterman of Stanford University wrote in his 1997 article Videoconferencing over the Internet: "Videoconferencing technology allows geographically disparate parties to hear and see each other usually through satellite or telephone communication systems." This technology is helpful for large conference meetings and face-to-face communication between parties without demanding the inconvenience of travel. Notable modern theorists Harold Lasswell developed Lasswell's model of communication. Five basic elements of public speaking are described in this theory: the communicator, message, medium, audience, and effect. In short, the speaker should be answering the question "who says what in which channel to whom with what effect?" Several other other models and theories were created in the 1950s, 60s and 70s. These tend to include emphasis on feedback from listeners, as well as understandings of context, shared knowledge and shared experience between people, and communication noise. Women and public speaking Australia An organization called the Penguin Club of Australia was founded in Sydney in 1937 and aimed at developing women's communication skills. Led by Jean Ellis, the organization spread to other territories of Australia and current-day Papua New Guinea over time. A main premise of the organization was that it was created "for women by women." They renamed to "Speaking Made Easy" in 2020. Great Britain The British political activist, Emmeline Pankhurst, founded the Women's Social and Political Union (WSPU) on October 10, 1903. The organization was aimed towards fighting for women's right to a parliamentary vote, which only men were granted at the time. Emmeline was known for being a powerful orator, who led many women to rebel through militant forms until the outbreak of World War I in 1914. Japan Kishida Toshiko (1861–1901) was a female speaker during the Meiji era in Japan. In October 1883, she publicly delivered a speech entitled 'Hakoiri Musume' (Daughters Kept in Boxes) in front of approximately 600 people. Presented in Yotsu no Miya Theater in Kyoto, she criticized the action of parents that shelter their daughters from the outside world. Despite her prompt arrest, Kishida demonstrated the ability of Japanese women to evoke women's issues, experiences, and liberation in public spaces, through the use of public speaking. Pakistan Malala Yousafzai, a public speaker born in the Swat Valley in Pakistan, is an educational activist for women and girls. After the Taliban restricted the educational rights of women in the Swat Valley, Yousafzai presented her first speech, How Dare the Taliban Take Away My Basic Right to Education?, in which she protested the shutdowns of the schools. She presented this speech to the press in Peshawar, bringing more awareness to the situation in Pakistan. She is known for her "inspiring and passionate speech" about educational rights given at the United Nations. She is the youngest person ever to receive the Nobel Peace Prize, at the age of 17, which was awarded to her in 2014. Her public speaking has brought worldwide attention to the difficulties of young girls in Pakistan. She continues to advocate for educational rights for women and girls worldwide through the Malala Fund, to help girls around the world receive 12 years of education. United States During the 18th and 19th centuries in the United States, a prohibition was instituted whereby women were precluded from engaging in public discourse within the confines of the courtroom, the Senate floor, and the pulpit. It was deemed improper for a woman to be heard in a public setting. Exceptions existed for women from the Quaker religion, allowing them to speak publicly in meetings of the church. Frances Wright was one of the first female public speakers in the United States, advocating equal education for both women and men through large audiences and the press. Maria Stewart, a woman of African American descent, was also one of the first female speakers of the United States, lecturing in Boston in front of both men and women just four years after Wright, in 1832 and 1833, on educational opportunities and abolition for young girls. The first female agents and sisters of the American Anti-Slavery Society Angelina Grimké and Sarah Grimké created a platform for public lectures to women and conducted tours between 1837 and 1839. The sisters advocated that slavery relates to women's rights and that women need equality. They came to a disagreement with churches that did not want the two speaking publicly due to them being women. See also References Further reading Anderson, Chris. The Official TED Guide to Public Speaking. Houghton Mifflin Harcourt, Boston, 2016. Carnegie, Dale· Arthur R. Pell. Public Speaking for Success. 2006 Carnegie, Dale. Public Speaking and Influencing Men in Business. 2003 Carnegie, Dale. How to Develop Self-Confidence & Influence People by Public Speaking. New York: Pocket Books, 1926 Collins, Philip. "The Art of Speeches and Presentations" (John Wiley & Sons, 2012). Esenwein, J. Berg and Carnegie, Dale, The Art of Public Speaking (1915) Fairlie, Henry. "Oratory in Political Life," History Today (Jan 1960) 10#1 pp. 3–13. A survey of political oratory in Great Britain from 1730 to 1960. Flintoff, John-Paul. "A Modest Book About How To Make An Adequate Speech" (Short Books, 2021). excerpt Gold, David, and Catherine L. Hobbs, eds. Rhetoric, History, and Women's Oratorical Education: American Women Learn to Speak (Routledge, 2013). Heinrichs, Jay. "Thank You For Arguing" (Penguin, 2008). Lucas, Stephen E. The Art of Public Speaking (13th ed. McGraw Hill, 2019). Noonan, Peggy. "Simply Speaking" (Regan Books, 1998). Parry-Giles, Shawn J., and J. Michael Hogan, eds. The Handbook of Rhetoric and Public Address (2010) excerpt Sproule, J. Michael. "Inventing public speaking: Rhetoric and the speech book, 1730–1930." Rhetoric & Public Affairs 15.4 (2012): 563–608. excerpt Turner, Kathleen J., Randall Osborn, et al. Public speaking (11th ed. Houghton Mifflin, 2017). excerpt External links "How to speak so that people want to listen", a June 2013 TED Talk Information diving is the practice of recovering technical data, sometimes confidential or secret, from discarded material. In recent times, this has chiefly been from data storage elements in discarded computers, most notably recoverable data remaining on hard drives. Those in charge of discarding computers usually neglect to erase the hard drive. It is often in such circumstances for an information diver to copy installed software (e.g., word processors, operating systems, computer games, etc.). Other data may also be available, such as credit card information that was stored on the machine. Companies claim to be especially careful with customer data, but the number of data breaches by any type of entity (e.g., education, health care, insurance, government, ...) suggest otherwise. In the UK, information diving has been referred to as "binology". Today, files, letters, memos, photographs, IDs, passwords, credit cards, and more can be found in dumpsters. Many people do not consider that sensitive information on items they discarded may be recovered. Such information, when recovered, is sometimes usable for fraudulent purposes (see also "identity theft" and physical information security). This method of dumpster diving is also sometimes used by attorneys or their agents when seeking to enforce court-ordered money judgments: the judgment debtor's trash may contain information about assets that can then be more-readily located for levying. Supposedly, information diving was more common in the 1980s due to lax security; when businesses became aware of the need for increased security in the early 1990s, sensitive documents were shredded before being placed in dumpsters. There is still considerable Internet activity on the subject of dumpster diving, so it is unlikely to have stopped with the widespread introduction of document shredding. Security mythology has it that curious hackers or malicious crackers commonly use this technique. Cases Printed manuals In earlier times, the available discarded data included printed manuals and design records. In a famous case, a student, Jerry Schneider, discovered some discarded manuals for a telephone system ordering/shipping system and was able to build a business selling 'surplus' gear ordered from the telephone company as though it was for an internal company department. Discarded computers Two MIT students purchased a large number of obsolete computers at yard sales, and they were able to obtain information such as credit card information and tax return data. They published a paper, Remembrance of Things Past, documenting their discoveries. Dumpster diving Dumpster diving is commonly practiced by "watchdog" organizations seeking information on groups they are investigating. The Trinity Foundation successfully used this technique to report on the activities of televangelist Robert Tilton and was also able to obtain information on Benny Hinn. See also Dumpster diving E-waste Credit card fraud Copyright infringement of software Benjamin Pell == References == In United States law, public accommodations are generally defined as facilities, whether publicly or privately owned, that are used by the public at large. Examples include retail stores, rental establishments, and service establishments as well as educational institutions, recreational facilities, and service centers. Under U.S. federal law, public accommodations must be accessible to the disabled and may not discriminate on the basis of "race, color, religion, or national origin." Private clubs were specifically exempted under federal law as well as religious organizations. The definition of public accommodation within the Title II of the Civil Rights Act of 1964 is limited to "any inn, hotel, motel, or other establishment which provides lodging to transient guests" and so is inapplicable to churches, mosques, synagogues, et al. Section 12187 of the ADA also exempts religious organizations from public accommodation laws, but religious organizations are encouraged to comply. Most U.S. states have various laws (non-uniform) that provide for nondiscrimination in public accommodations, and some may be broader than federal law. Federal law Federal legislation dealing with public accommodations include these: Title II of the Civil Rights Act of 1964 Title III of the Americans with Disabilities Act of 1990 State laws Many states and their subdivisions prohibited discrimination in places of public accommodation prior to the enactment of Title II of the Civil Rights Act of 1964. By 1964, 31 states had such laws, many dating back to the late 19th century. As of 2015, 45 states have an anti-discrimination public accommodation law for nondisabled individuals. The laws all protect against discrimination based upon race, gender, ethnicity, and religion. There are 19 states that prohibit discrimination in public accommodation based upon age. Because a right to public accommodation for gay and transgender people does not exist in federal law, in more than half the states in the U.S., discrimination in public accommodation against LGBT people remains legal. Several states also have protections for breastfeeding in public. In addition several states provide for non-discrimination in public accommodation when based upon sexual orientation or gender identity. Private clubs were exempted under federal law but not in many states' laws. For example, in interpreting a Minnesota law in their 1984 ruling Roberts v. United States Jaycees, the United States Supreme Court declared the previously all-male United States Junior Chamber, a chamber of commerce organization for men between the ages of 18 and 36, to be a public accommodation, thus compelling it to admit women. See also Anti-discrimination law Civil Rights Act of 1875 List of cities and counties in the United States offering an LGBT non-discrimination ordinance Reasonable accommodation Masterpiece Cakeshop v. Colorado Civil Rights Commission References Further reading Cortner, Richard C. (2001). Civil Rights and Public Accommodations: The Heart of Atlanta Motel and McClung Cases. Lawrence, Kansas: University Press of Kansas. ISBN 978-0-7006-1077-8. Carothers, Leslie A. (1968). The Public Accommodations Law of 1964: Arguments, Issues and Attitudes in a Legal Debate. Northampton, Massachusetts: Smith College. OCLC 160269. Mook, Jonathan R. (2009). ADA Amendments Act of 2008 and its impact on public accommodations and commercial facilities. Newark, New Jersey: Matthew Bender (Lexis-Nexis). OCLC 428087829. Office on the Americans with Disabilities Act, United States Department of Justice (1992). The Americans with Disabilities Act Title III technical assistance manual. Washington, D.C.: United States Government. Gottry, James M. (2011). "Just Shoot Me: Public Accommodation Anti-Discrimination Laws Take Aim at First Amendment Freedom of Speech". Vanderbilt Law Review. 64 (3): 961–1003. Singer, Joseph William (2015). "We Don't Serve Your Kind Here: Public Accommodation and the Mark of Sodom" (PDF). Boston University Law Review. 95: 929–50. SSRN 2615153. Sepinwall, Amy J. (2015). "Conscience and Complicity: Assessing Pleas for Religious Exemptions in 'Hobby Lobby's' Wake" (PDF). The University of Chicago Law Review. 82 (4): 1897–980. JSTOR 43655477. Archived from the original (PDF) on 2016-10-13. Retrieved 2017-01-17. McClain, Linda C. (2011). "Religious and Political Virtues and Values in Congruence or Conflict?: On Smith, Bob Jones University, and Christian Legal Society" (PDF). Cardozo Law Review. 32 (5): 1959–2007. SSRN 1833518. Archived from the original (PDF) on January 18, 2017. Tevis, Britt P. 2021. "“Jews Not Admitted”: Anti-Semitism, Civil Rights, and Public Accommodation Laws." Journal of American History, Volume 107, Issue 4, Pages 847–870. The Middle East Treaty Organization (METO) is a non-governmental organization founded in 2017 by a coalition of civil-society activists and disarmament practitioners, with the aim to rid the Middle East of all weapons of mass destruction (WMD). This proposal is in line with the 1970s proposal for a Middle East nuclear weapon free zone, albeit with broader scope following the 1990 Mubarak Initiative to include chemical and biological as well as nuclear weapons. Working toward the broader vision of regional security and peace, METO defines its purpose as the establishment of a zone free of weapons of mass destruction (WMDFZ) in the Middle East. To achieve that end, the organization embraces a traditional treaty-based approach relying on diplomatic mechanisms and civil society campaigns. This strategy is supported through programming and events centered around policy debates, advocacy and education. Three strategic pillars underlie METO's treaty-based approach for achieving the Middle East WMDFZ: A WMDFZ Treaty, based on a text negotiated, agreed and adopted by regional governments and relevant stakeholders through an inclusive, multilateral track I and track II diplomatic process facilitated by METO and partner organizations (including formal United Nations negotiations). A regional organization, which must be established to oversee and carry out functions necessary to the treaty’s eventual implementation, verification and compliance. Engagement with civil society, in particular to foster a civil society movement that can formulate demands to regional and international governments to advance the goals of the proposed treaty. METO is an international partner of International Campaign to Abolish Nuclear Weapons, International Physicians for the Prevention of Nuclear War, Geneva Centre for Security Policy, British American Security Information Council, Abolition 2000, and Geneva Disarmament Platform. Draft Treaty and Annual UN Conferences METO began facilitating the creation of a draft treaty text that would form the basis for discussion on a WMDFZ Treaty in 2017 through track 1.5-2 diplomacy. Roundtable negotiations to discuss the treaty were organized among senior diplomatic and former diplomatic figures from regional governments and representatives from international organizations, as well as subject experts. The draft treaty text facilitated by METO's process was brought to the United Nations General Assembly by Egypt on 22 December 2018, alongside a proposal to launch an annual conference to discuss the zone. The UN General Assembly resolved to convene an annual meeting on the establishment of a Middle East WMDFZ. The first annual conference was held from 18 November to 22 November 2019 at UN Headquarters in New York, presided over by the UN Permanent Representative from Jordan. Almost all states of the region attended the conference, including the 22 members of the Arab League and Iran, as well as four nuclear-armed states China, France, Russia, and the United Kingdom, alongside other observer states and international organizations. The only regional country that did not participate was Israel. The conference adopted a Final Report and Political Declaration articulating the participating member states' commitment to pursue the elaboration of a consensus-based, legally binding treaty to establish a WMDFZ in the Middle East through an open and inclusive process involving all states in the region. They agreed to meet again from 16 to 20 November 2020. That meeting was postponed until 29 November 2021 - 03 December 2021 because of COVID. The third conference was held 14–18 November 2022. The fourth was held 13 to 17 November 2023, with the fifth scheduled for 18 to 22 November 2024. A description of all conferences held to date and planned is available from the website of the United Nations Office for Disarmament Affairs (UNODA). METO in Publications and Media As part of the organization's education and advocacy programs, METO staff frequently contribute articles to academic publications and mainstream media outlets, as well as through film, radio and podcast productions. Articles Emad Kiyaei, Tony Robinson, Sharon Dolev, “Non-proliferation and Regional Cooperation in the Middle East,” Brown Journal of World Affairs, January 2021. Tariq Rauf, "Achieving the Possible: “Weapons of Mass Destruction Free Zone in the Middle East”", Inter Press Service, November 2019. Sharon Dolev, Emad Kiyaei, and Dina Saadallah, "Achieving the Possible: a WMD-free zone in the Middle East", Reaching Critical Will, November 2019. Paul Ingram and Emad Kiyaei, "Middle East WMD-Free Zone: Thinking the Possible", The Cairo Review of Global Affairs, Fall 2019. UN Office for Disarmament Affairs, "A Draft Treaty for a WMD Free Zone in the Middle East: Time to Envisage the Practical", UNODA, October 2017. Reports METO and GCSP, Round Table report on the Abraham Accords and WMDFZ, January 2021. Sharon Dolev, “Israel”, in Assuring destruction forever: 2020 edition, Reaching Critical Will, June 2020. Book Emad Kiyaei and Seyed Hossein Mousavian, A Middle East Free of Weapons of Mass Destruction, Routledge, April 2020. Documentary film Tony Robinson and Álvaro Orús, documentary film: "The Beginning of the End of Nuclear Weapons", Pressenza IPA, May 2019. Podcast The organization produces a fortnightly podcast series, In The Zone, which explores constructive approaches to improve the chances of achieving a WMDFZ in the Middle East. In the series, Paul Ingram and Anahita Parsa conduct interviews with WMD disarmament experts on technical and political solutions to overcome obstacles, how to build trust between countries and more broadly how to improve peace and security for people in the region. The podcast series is published on Pressenza and available on Soundcloud. == References == Bessie Behan (after marriage, Lewis; 1872–1956) was an American social leader. Biography Elizabeth Antoinette Behan was born in New Orleans, Louisiana, on March 6, 1872. Her father was Gen. William J. Behan, a prominent southern merchant and sugar planter who served as Mayor of New Orleans. Bessie's mother, Kate Walker Behan, was the city's social leader and philanthropist. She was educated at home by skilled governesses and had all the advantages of much travel. Her associations in New Orleans made the acquisition of the French language easy and natural, and she was thus master of two languages. After completing her education, she debuted in New Orleans's society in 1891. The most coveted social honor in New Orleans in that era was to be chosen queen in the Mardi Gras Carnival. That honor fell to Behan in the carnival of 1891. She was the youngest woman yet selected for coronation in that festival. On October 18, 1892, she married Dr. Hampden Sidney Lewis in New Orleans. In 1913, she received a degree in French literature from the University of the Sorbonne. Lewis made a gift to the Howard Memorial Library in 1924: a collection of literature dealing with the Confederacy period of the Confederacy. The books and the complete set of the Southern Historical Society papers belonged to Lewis' parents. Her jewels from the Rex Ball, Carnival of 1891, were donated to the Louisiana State Museum in 1929. References External links Works related to Woman of the Century/Bessie Behan at Wikisource Since the Industrial Revolution, use of the family in advertising has become a prominent practice in marketing campaigns to increase profits. Some sociologists say that these advertisements can influence behavior and attitudes; advertisers tend to portray family members in an era's traditional, socially-acceptable roles. History After the Industrial Revolution, advertising increased and the use of family images became prevalent. Advertising changed, from information about the availability of goods in 17th- and 18th-century Europe for an audience who lived and worked near the vendors (and their wares) to multi-million-dollar campaigns which attempted to connect and persuade people around the world. Large companies emerged as mass producers, products were branded, and customers began exhibiting brand loyalty. Persuading consumers to purchase one brand instead of another became vital to advertising in competitive industrial markets. Advertising's size and scope changed as marketing strategies began to target specific audience, using symbols, representations, and stereotypes (including the family). Because family life stresses group benefits, preferences, and successes over those of the individual, collectivist societies tend to use more family symbols in advertising than individualist societies. Collectivist South Korea has more success with family advertisements than the United States, which views itself as individualistic. Post-industrial advertising reaffirmed widely-held social values such as heterosexuality and those of the middle class, neglecting alternative values or lifestyles. Countries such as Japan continue to present the family stereotypically, especially in television advertisements. Many advertising agencies, however, have begun to more accurately reflect consumer diversity in lifestyles and family types. Function The family, a popular symbol in commercial advertising, is used to increase profit and develop a positive reputation with consumers. It functions on three levels of persuasion: social, psychological and personal. Social persuasion appeals to one's role in a group and one's corresponding expectations; it appeals to reference groups, social class, culture and subculture. The family symbol is socially persuasive, appealing to one's role in the family and their corresponding expectations. There is emotional pressure, due to psychological attachments in family relationships. Psychological persuasion in advertising appeals to motivation, attitudes, and personality. A parent might want to purchase a product which limits harm for their child due to their emotional attachment to the child, and emotional and psychological persuasion are popular advertising strategies. Family affects audiences at the psychological level, the level at which advertising is most effective. Personal persuasion appeals to one's demographic identity or consumer behaviors. The family is persuasive because although a family may make a purchase decision as a unit, one family member may make most of its buying decisions. Targeting that person, noting their role in the family and the corresponding responsibility to make family purchase decisions, is more productive than targeting other family members. The McDonald's Corporation in India has successfully marketed itself as "McDonald's Family Restaurant". Sociological interpretations Advertising is used to attract customers to a business's products or services, making statements about race, social class, gender, values, and family. It describes these social categories and prescribes behavior according to social ideals or norms. According to Belk and Pollay (cited in Burke's master's thesis), advertisements show the ideal life and instruct in how to live. Targeting specific groups of people for products and services, advertisements reflect changes in social norms and acceptable behavior. Some argue that the image of a family only plays a symbolic role, reflecting contemporary cultural values. Sociologists have challenged the public to study ads with family images as marketing messages and vehicles for behavior and attitudes towards society. Advertisements depicting families note the transition from modernity to postmodernity. This transition is that of middle class nuclear families (where heterosexuality is the norm) to the recognition and acceptance of a variety of family types, embracing societal polysexuality and plurality, the transition from popular culture to sub-cultures and multiculturalism. According to literary critic Fredric Jameson, "Our advertising ... is fed by postmodernism in all the arts, and is inconceivable without it." Family members Parents Throughout history, mothers have been portrayed as the primary physical caregivers of children. Physical caregiving includes tasks such as breastfeeding and changing diapers. Some theorize that women have a natural instinct to provide care. Fathers have been more likely to be portrayed as playing with their children, more with sons than daughters. Similar to the decline in wives being portrayed solely as housekeepers, the portrayal of mothers as the primary physical caretakers of children has also declined; mothers are portrayed more in recreational activities with their offspring. Other family members Like fathers, other male family members (including sons and grandsons) are primarily portrayed in play activities with children. Young female family members are more likely depicted in activities related to household chores and child care. Grandparents are largely absent from advertising. Family images depend on their source and the audience the source intends to reach. In a women's magazine, such as Good Housekeeping, women are portrayed primarily as housewives. Husbands As wives in advertising have reflected the general view of their appropriate role, husbands also reflect the cultural values surrounding their role; images of the husband working outside the home and handling the family finances are common. These roles were especially prevalent in 1920, 1936, and 1970. Husbands have generally not been depicted in advertising doing household chores, except when they humorously perform them badly. The portrayal of husbands and wives in an intimate, romantic relationship has increased. Wives Advertising generally reflects popular contemporary attitudes toward gender roles. During the 1920s, when relatively few wives worked outside the home, working wives were rarely portrayed in advertising. This changed during the Great Depression, when more wives entered the workforce. Since then, as housekeeping becomes a less-important family role, the number of advertisements depicting women performing household tasks has declined. See also == References == The concept of demographic threat (or demographic bomb) is a term used in political conversation or demography to refer to population increases from within a minority ethnic or religious group in a given country that is perceived as threatening to the ethnic, racial or religious majority, stability of the country or to the identity of said countries in which it is present. The term is often used by the far-right as a racial dogwhistle. Examples in countries Australia In 1984, Geoffrey Blainey, an Australian historian and academic criticised a comment by a spokesman to Immigration Minister Stewart West of the Australian Labor Party that "the increasing Asianisation was inevitable". Blainey responded, "I do not accept the view, widely held in the Federal Cabinet, that some kind of slow Asian takeover of Australia is inevitable. I do not believe that we are powerless. I do believe that we can with good will and good sense control our destiny.... As a people, we seem to move from extreme to extreme. In the past 30 years the government of Australia has moved from the extreme of wanting a white Australia to the extreme of saying that we will have an Asian Australia and that the quicker we move towards it the better". In the 1996 Australian federal election, Pauline Hanson was elected to the Division of Oxley. In her controversial maiden speech to the Australian House of Representatives, she expressed her belief that Australia "was in danger of being swamped by Asians". Hanson went on to form the One Nation Party, which initially won nearly one quarter of the vote in Queensland state elections before it entering a period of decline because of internal disputes. The name "One Nation" was meant to signify national unity in contrast to what Hanson claimed as an increasing division in Australian society caused by government policies favouring migrants (multiculturalism) and indigenous Australians. Bahrain Thousands of Bahraini Shia Muslims protested in March 2011 against the Bahraini government's naturalisation policy of granting citizenship to Sunni Muslims from other countries serving in the military of Bahrain. Bhutan Bhutan has a long-standing concern with the demographic threat posed by the immigration of ethnically distinct Nepali immigrants. Canada During the 19th and 20th centuries (until the 1960s), the French-speaking Catholic minority of Canada managed to maintain its share of the population due to a high birth rate, dubbed the "revenge of the cradle." Estonia In Estonia, one of the causes of the Singing Revolution was the concern over the demographic threat to the national identity posed by the influx of individuals from foreign ethnic groups to work on such large Soviet development projects as phosphate mining. India Many Hindu Indians see Muslims as a "demographic threat" because of their large population growth due to high fertility rates and because of the high rate of illegal immigration from Bangladesh. Israel In the 1950s, Shoham Melamad found that the high fertility rate of Arabs was viewed as a demographic threat to the Jewish nation. Rabbi Menachem Mendel Schneerson, however, stated that Arabs in Israel should be treated equally to any other Israeli citizens and be allowed to have children just like any other citizen. A 1967 Maariv editorial suggested that Jews should be encouraged to have large families, while Palestinians in the West Bank, Gaza Strip and in Israel should be encouraged to adopt birth control measures. Schnitzer also advocated for the adoption of an open policy encouraging Arabs to emigrate from Israel. In 2003, Benjamin Netanyahu opined that if the percentage of Arab citizens of Israel rises above its current level of about 20 percent, Israel would not be able to retain a Jewish demographic majority, the basis of Israel's self-definition as a "Jewish democratic state". Netanyahu's comments were criticized as racist by Arab Knesset members and the Association for Civil Rights in Israel. In May 2009, Michael Oren wrote an article in Commentary in which he discussed the "Arab Demographic Threat" as one of "Seven Existential Threats" facing Israel. In 2005, Shimon Peres told US officials that Israel had "lost" land in the Negev "to the Bedouin" and would need to take steps to "relieve" the "demographic threat". In 2010, Netanyahu warned in a government meeting that a Negev "without a Jewish majority" would pose "a palpable threat". In February 2014, then Israeli finance minister Yair Lapid said failure to establish a Palestinian state would leave Israel facing a demographic threat that could undermine its Jewish and democratic nature. Malaysia The Malaysian government has been accused of masterminding Project IC to alter the demographic pattern of the East Malaysian state of Sabah. New Zealand Fears of changing demographics caused by high immigration from the Asia-Pacific region and birth rates of the native Māori people, particularly among older Pākehā (European-descended New Zealanders), have found their way into parliamentary politics. Extremist groups in New Zealand, such as Action Zealandia, have been blacklisted by social media networks for inciting fears about "demographic replacement" at the "expense of the European community". The 2019 Christchurch mosque shootings also drew closer attention to such groups forming links with like-minded ethno-nationalists overseas, in a ‘networked white rage’. Northern Ireland In Northern Ireland, Protestants are more likely to favour continued political union with the United Kingdom, and Catholics are more likely to favour political union with the Republic of Ireland. When Ireland was partitioned in the 1920s and Northern Ireland came into existence, Protestants were roughly 60% of the population, but as a result of higher fertility rates among Catholics, their share of the population has dropped to less than 50% in the 2011 census, while Catholics numbered only slightly fewer than Protestants. There is debate over whether and to what extent the trend will continue and its possible impact on the political situation. Russia Russia fears the "demographic threat" posed by the potential for "large-scale Chinese immigration" to its thinly populated far east. Illegal immigration of Chinese nationals is a special concern. There were also fears of a Muslim-majority Russia eventually coming into fruition (for instance, by Paul A. Goble), though such fears have also been criticized as being unrealistic, irrational, and/or unfounded. Sweden Sweden's main statistics bureau, Statistics Sweden (SCB), does not keep any record of ethnicity, but about 20% of Sweden's population is of foreign background. Some immigrants in Sweden feel that they experience "betweenship" that arises when others ascribe them an identity that they do not hold. The growing numbers of immigrants has coincided with the rise of and anti-immigration political party, the Sweden Democrats, which believe in a demographic threat, especially by the rise of Islam in Sweden. Since the 1990s, polls show that people in Sweden have gradually become more positive to asylum-seekers. United States Some in the United States have expressed concern about the "demographic threat" posed by migrants from Latin America, particularly Mexico, and their descendants. In a similar vein, in 2000, Peter Brimelow of the immigration restrictionist website VDARE set forth a conspiracy theory that the Democratic Party, with the illogically imagined support of the Republican Party, is importing a new, less white, electorate that is more favorable to the former. See also Demographic trap Fifth column Great Replacement Majority-minority Natalism Political demography Revenge of the cradle References Bibliography Masalha, Nur (2000). Imperial Israel And The Palestinians: The Politics of Expansion. Pluto Press. ISBN 0-7453-1615-8 Shenhav, Yehouda (2006). The Arab Jews: A Postcolonial Reading of Nationalism, Religion, and Ethnicity. Stanford University Press. ISBN 0-8047-5296-6 Pat Buchanan, The Death of the West (2001) External links Ynet: Demographic threat a myth Haaretz: Netanyahu's use of the term "demographic bomb" 'What Counts is the Counting: Statistical Manipulation as a Solution to Israel’s “Demographic Problem”, Middle East Journal, Volume 67, No. 2, Spring 2013 pp. 185–205 Satire is a genre of the visual, literary, and performing arts, usually in the form of fiction and less frequently non-fiction, in which vices, follies, abuses, and shortcomings are held up to ridicule, often with the intent of exposing or shaming the perceived flaws of individuals, corporations, government, or society itself into improvement. Although satire is usually meant to be humorous, its greater purpose is often constructive social criticism, using wit to draw attention to both particular and wider issues in society. Satire may also poke fun at popular themes in art and film. A prominent feature of satire is strong irony or sarcasm—"in satire, irony is militant", according to literary critic Northrop Frye— but parody, burlesque, exaggeration, juxtaposition, comparison, analogy, and double entendre are all frequently used in satirical speech and writing. This "militant" irony or sarcasm often professes to approve of (or at least accept as natural) the very things the satirist wishes to question. Satire is found in many artistic forms of expression, including internet memes, literature, plays, commentary, music, film and television shows, and media such as lyrics. Etymology and roots The word satire comes from the Latin word satur and the subsequent phrase lanx satura. Satur meant "full", but the juxtaposition with lanx shifted the meaning to "miscellany or medley": the expression lanx satura literally means "a full dish of various kinds of fruits". The use of the word lanx in this phrase, however, is disputed by B.L. Ullman. The word satura as used by Quintilian, however, was used to denote only Roman verse satire, a strict genre that imposed hexameter form, a narrower genre than what would be later intended as satire. Quintilian famously said that satura, that is a satire in hexameter verses, was a literary genre of wholly Roman origin (satura tota nostra est). He was aware of and commented on Greek satire, but at the time did not label it as such, although today the origin of satire is considered to be Aristophanes' Old Comedy. The first critic to use the term satire in the modern broader sense was Apuleius. To Quintilian, the satire was a strict literary form, but the term soon escaped from the original narrow definition. Robert Elliott writes: As soon as a noun enters the domain of metaphor, as one modern scholar has pointed out, it clamours for extension; and satura (which had had no verbal, adverbial, or adjectival forms) was immediately broadened by appropriation from the Greek word for "satyr" (satyros) and its derivatives. The odd result is that the English "satire" comes from the Latin satura; but "satirize", "satiric", etc., are of Greek origin. By about the 4th century AD the writer of satires came to be known as satyricus; St. Jerome, for example, was called by one of his enemies 'a satirist in prose' ('satyricus scriptor in prosa'). Subsequent orthographic modifications obscured the Latin origin of the word satire: satura becomes satyra, and in England, by the 16th century, it was written 'satyre.' The word satire derives from satura, and its origin was not influenced by the Greek mythological figure of the satyr. In the 17th century, philologist Isaac Casaubon was the first to dispute the etymology of satire from satyr, contrary to the belief up to that time. Humour The rules of satire are such that it must do more than make you laugh. No matter how amusing it is, it doesn't count unless you find yourself wincing a little even as you chuckle. Laughter is not an essential component of satire; in fact, there are types of satire that are not meant to be "funny" at all. Conversely, not all humour, even on such topics as politics, religion or art is necessarily "satirical", even when it uses the satirical tools of irony, parody, and burlesque. Even light-hearted satire has a serious "after-taste": the organizers of the Ig Nobel Prize describe this as "first make people laugh, and then make them think". Social and psychological functions Satire and irony in some cases have been regarded as the most effective source to understand a society, the oldest form of social study. They provide the keenest insights into a group's collective psyche, reveal its deepest values and tastes, and the society's structures of power. Some authors have regarded satire as superior to non-comic and non-artistic disciplines like history or anthropology. In a prominent example from ancient Greece, philosopher Plato, when asked by a friend for a book to understand Athenian society, referred him to the plays of Aristophanes. Historically, satire has satisfied the popular need to debunk and ridicule the leading figures in politics, economy, religion and other prominent realms of power. Satire confronts public discourse and the collective imaginary, playing as a public opinion counterweight to power (be it political, economic, religious, symbolic, or otherwise), by challenging leaders and authorities. For instance, it forces administrations to clarify, amend or establish their policies. Satire's job is to expose problems and contradictions, and it is not obligated to solve them. Karl Kraus set in the history of satire a prominent example of a satirist role as confronting public discourse. For its nature and social role, satire has enjoyed in many societies a special freedom license to mock prominent individuals and institutions. The satiric impulse, and its ritualized expressions, carry out the function of resolving social tension. Institutions like the ritual clowns, by giving expression to the antisocial tendencies, represent a safety valve which re-establishes equilibrium and health in the collective imaginary, which are jeopardized by the repressive aspects of society. The state of political satire in a given society reflects the tolerance or intolerance that characterizes it, and the state of civil liberties and human rights. Under totalitarian regimes any criticism of a political system, and especially satire, is suppressed. A typical example is the Soviet Union where the dissidents, such as Aleksandr Solzhenitsyn and Andrei Sakharov were under strong pressure from the government. While satire of everyday life in the USSR was allowed, the most prominent satirist being Arkady Raikin, political satire existed in the form of anecdotes that made fun of Soviet political leaders, especially Brezhnev, famous for his narrow-mindedness and love for awards and decorations. Classifications Satire is a diverse genre which is complex to classify and define, with a wide range of satiric "modes". Horatian, Juvenalian, Menippean Satirical literature can commonly be categorized as either Horatian, Juvenalian, or Menippean. Horatian Horatian satire, named for the Roman satirist Horace (65–8 BCE), playfully criticizes some social vice through gentle, mild, and light-hearted humour. Horace (Quintus Horatius Flaccus) wrote Satires to gently ridicule the dominant opinions and "philosophical beliefs of ancient Rome and Greece". Rather than writing in harsh or accusing tones, he addressed issues with humor and clever mockery. Horatian satire follows this same pattern of "gently [ridiculing] the absurdities and follies of human beings". It directs wit, exaggeration, and self-deprecating humour toward what it identifies as folly, rather than evil. Horatian satire's sympathetic tone is common in modern society. A Horatian satirist's goal is to heal the situation with smiles, rather than by anger. Horatian satire is a gentle reminder to take life less seriously and evokes a wry smile. Juvenalian Juvenalian satire, named for the writings of the Roman satirist Juvenal (late first century – early second century AD), is more contemptuous and abrasive than the Horatian. Juvenal disagreed with the opinions of the public figures and institutions of the Empire and actively attacked them through his literature. "He utilized the satirical tools of exaggeration and parody to make his targets appear monstrous and incompetent". Juvenal's satire follows this same pattern of abrasively ridiculing societal structures. Juvenal also, unlike Horace, attacked public officials and governmental organizations through his satires, regarding their opinions as not just wrong, but evil. Following in this tradition, Juvenalian satire addresses perceived social evil through scorn, outrage, and savage ridicule. This form is often pessimistic, characterized by the use of irony, sarcasm, moral indignation and personal invective, with less emphasis on humor. Strongly polarized political satire can often be classified as Juvenalian. A Juvenal satirist's goal is generally to provoke some sort of political or societal change because he sees his opponent or object as evil or harmful. A Juvenal satirist mocks "societal structure, power, and civilization" by exaggerating the words or position of his opponent in order to jeopardize their opponent's reputation and/or power. Jonathan Swift has been established as an author who "borrowed heavily from Juvenal's techniques in [his critique] of contemporary English society". Menippean Satire vis-à-vis teasing In the history of theatre there has always been a conflict between engagement and disengagement on politics and relevant issue, between satire and grotesque on one side, and jest with teasing on the other. Max Eastman defined the spectrum of satire in terms of "degrees of biting", as ranging from satire proper at the hot-end, and "kidding" at the violet-end; Eastman adopted the term kidding to denote what is just satirical in form, but is not really firing at the target. Nobel laureate satirical playwright Dario Fo pointed out the difference between satire and teasing (sfottò). Teasing is the reactionary side of the comic; it limits itself to a shallow parody of physical appearance. The side-effect of teasing is that it humanizes and draws sympathy for the powerful individual towards which it is directed. Satire instead uses the comic to go against power and its oppressions, has a subversive character, and a moral dimension which draws judgement against its targets. Fo formulated an operational criterion to tell real satire from sfottò, saying that real satire arouses an outraged and violent reaction, and that the more they try to stop you, the better is the job you are doing. Fo contends that, historically, people in positions of power have welcomed and encouraged good-humoured buffoonery, while modern day people in positions of power have tried to censor, ostracize and repress satire. Teasing (sfottò) is an ancient form of simple buffoonery, a form of comedy without satire's subversive edge. Teasing includes light and affectionate parody, good-humoured mockery, simple one-dimensional poking fun, and benign spoofs. Teasing typically consists of an impersonation of someone monkeying around with his exterior attributes, tics, physical blemishes, voice and mannerisms, quirks, way of dressing and walking, and/or the phrases he typically repeats. By contrast, teasing never touches on the core issue, never makes a serious criticism judging the target with irony; it never harms the target's conduct, ideology and position of power; it never undermines the perception of his morality and cultural dimension. Sfottò directed towards a powerful individual makes him appear more human and draws sympathy towards him. Hermann Göring propagated jests and jokes against himself, with the aim of humanizing his image. Classifications by topics Types of satire can also be classified according to the topics it deals with. From the earliest times, at least since the plays of Aristophanes, the primary topics of literary satire have been politics, religion and sex. This is partly because these are the most pressing problems that affect anybody living in a society, and partly because these topics are usually taboo. Among these, politics in the broader sense is considered the pre-eminent topic of satire. Satire which targets the clergy is a type of political satire, while religious satire is that which targets religious beliefs. Satire on sex may overlap with blue comedy, off-color humor and dick jokes. Scatology has a long literary association with satire, as it is a classical mode of the grotesque, the grotesque body and the satiric grotesque. Shit plays a fundamental role in satire because it symbolizes death, the turd being "the ultimate dead object". The satirical comparison of individuals or institutions with human excrement, exposes their "inherent inertness, corruption and dead-likeness". The ritual clowns of clown societies, like among the Pueblo Indians, have ceremonies with filth-eating. In other cultures, sin-eating is an apotropaic rite in which the sin-eater (also called filth-eater), by ingesting the food provided, takes "upon himself the sins of the departed". Satire about death overlaps with black humor and gallows humor. Another classification by topics is the distinction between political satire, religious satire and satire of manners. Political satire is sometimes called topical satire, satire of manners is sometimes called satire of everyday life, and religious satire is sometimes called philosophical satire. Comedy of manners, sometimes also called satire of manners, criticizes mode of life of common people; political satire aims at behavior, manners of politicians, and vices of political systems. Historically, comedy of manners, which first appeared in British theater in 1620, has uncritically accepted the social code of the upper classes. Comedy in general accepts the rules of the social game, while satire subverts them. Another analysis of satire is the spectrum of his possible tones: wit, ridicule, irony, sarcasm, cynicism, the sardonic and invective. The type of humour that deals with creating laughter at the expense of the person telling the joke is called reflexive humour. Reflexive humour can take place at dual levels of directing humour at self or at the larger community the self identifies with. The audience's understanding of the context of reflexive humour is important for its receptivity and success. Satire is found not only in written literary forms. In preliterate cultures it manifests itself in ritual and folk forms, as well as in trickster tales and oral poetry. It appears also in graphic arts, music, sculpture, dance, cartoon strips, and graffiti. Examples are Dada sculptures, Pop Art works, music of Gilbert and Sullivan and Erik Satie, punk and rock music. In modern media culture, stand-up comedy is an enclave in which satire can be introduced into mass media, challenging mainstream discourse. Comedy roasts, mock festivals, and stand-up comedians in nightclubs and concerts are the modern forms of ancient satiric rituals. Development Ancient Egypt One of the earliest examples of what might be called satire, The Satire of the Trades, is in Egyptian writing from the beginning of the 2nd millennium BC. The text's apparent readers are students, tired of studying. It argues that their lot as scribes is not only useful, but far superior to that of the ordinary man. Scholars such as Helck think that the context was meant to be serious. The Papyrus Anastasi I (late 2nd millennium BC) contains a satirical letter which first praises the virtues of its recipient, but then mocks the reader's meagre knowledge and achievements. Ancient Greece The Greeks had no word for what later would be called "satire", although the terms cynicism and parody were used. Modern critics call the Greek playwright Aristophanes one of the best known early satirists: his plays are known for their critical political and societal commentary, particularly for the political satire by which he criticized the powerful Cleon (as in The Knights). He is also notable for the persecution he underwent. Aristophanes' plays turned upon images of filth and disease. His bawdy style was adopted by Greek dramatist-comedian Menander. His early play Drunkenness contains an attack on the politician Callimedon. The oldest form of satire still in use is the Menippean satire by Menippus of Gadara. His own writings are lost. Examples from his admirers and imitators mix seriousness and mockery in dialogues and present parodies before a background of diatribe. As in the case of Aristophanes plays, menippean satire turned upon images of filth and disease. Ancient China Satire, or fengci (諷刺) the way it is called in Chinese, goes back at least to Confucius, being mentioned in the Book of Odes (Shijing 詩經). It meant "to criticize by means of an ode". In the pre-Qin era it was also common for schools of thought to clarify their views through the use of short explanatory anecdotes, also called yuyan (寓言), translated as "entrusted words". These yuyan usually were brimming with satirical content. The Daoist text Zhuangzi is the first to define this concept of Yuyan. During the Qin and Han dynasty, however, the concept of yuyan mostly died out through their heavy persecution of dissent and literary circles, especially by Qin Shi Huang and Han Wudi. Roman world The first Roman to discuss satire critically was Quintilian, who invented the term to describe the writings of Gaius Lucilius. The two most prominent and influential ancient Roman satirists are Horace and Juvenal, who wrote during the early days of the Roman Empire. Other important satirists in ancient Latin are Gaius Lucilius and Persius. Satire in their work is much wider than in the modern sense of the word, including fantastic and highly coloured humorous writing with little or no real mocking intent. When Horace criticized Augustus, he used veiled ironic terms. In contrast, Pliny reports that the 6th-century-BC poet Hipponax wrote satirae that were so cruel that the offended hanged themselves. In the 2nd century AD, Lucian wrote True History, a book satirizing the clearly unrealistic travelogues/adventures written by Ctesias, Iambulus, and Homer. He states that he was surprised they expected people to believe their lies, and stating that he, like them, has no actual knowledge or experience, but shall now tell lies as if he did. He goes on to describe a far more obviously extreme and unrealistic tale, involving interplanetary exploration, war among alien life forms, and life inside a 200 mile long whale back in the terrestrial ocean, all intended to make obvious the fallacies of books like Indica and The Odyssey. Medieval Islamic world Medieval Arabic poetry included the satiric genre hija. Satire was introduced into Arabic prose literature by the author Al-Jahiz in the 9th century. While dealing with serious topics in what are now known as anthropology, sociology and psychology, he introduced a satirical approach, "based on the premise that, however serious the subject under review, it could be made more interesting and thus achieve greater effect, if only one leavened the lump of solemnity by the insertion of a few amusing anecdotes or by the throwing out of some witty or paradoxical observations. He was well aware that, in treating of new themes in his prose works, he would have to employ a vocabulary of a nature more familiar in hija, satirical poetry." For example, in one of his zoological works, he satirized the preference for longer human penis size, writing: "If the length of the penis were a sign of honor, then the mule would belong to the (honorable tribe of) Quraysh". Another satirical story based on this preference was an Arabian Nights tale called "Ali with the Large Member". In the 10th century, the writer Tha'alibi recorded satirical poetry written by the Arabic poets As-Salami and Abu Dulaf, with As-Salami praising Abu Dulaf's wide breadth of knowledge and then mocking his ability in all these subjects, and with Abu Dulaf responding back and satirizing As-Salami in return. An example of Arabic political satire included another 10th-century poet Jarir satirizing Farazdaq as "a transgressor of the Sharia" and later Arabic poets in turn using the term "Farazdaq-like" as a form of political satire. The terms "comedy" and "satire" became synonymous after Aristotle's Poetics was translated into Arabic in the medieval Islamic world, where it was elaborated upon by Islamic philosophers and writers, such as Abu Bischr, his pupil Al-Farabi, Avicenna, and Averroes. Due to cultural differences, they disassociated comedy from Greek dramatic representation and instead identified it with Arabic poetic themes and forms, such as hija (satirical poetry). They viewed comedy as simply the "art of reprehension", and made no reference to light and cheerful events, or troubled beginnings and happy endings, associated with classical Greek comedy. After the Latin translations of the 12th century, the term "comedy" thus gained a new semantic meaning in Medieval literature. Ubayd Zakani introduced satire in Persian literature during the 14th century. His work is noted for its satire and obscene verses, often political or bawdy, and often cited in debates involving homosexual practices. He wrote the Resaleh-ye Delgosha, as well as Akhlaq al-Ashraf ("Ethics of the Aristocracy") and the famous humorous fable Masnavi Mush-O-Gorbeh (Mouse and Cat), which was a political satire. His non-satirical serious classical verses have also been regarded as very well written, in league with the other great works of Persian literature. Between 1905 and 1911, Bibi Khatoon Astarabadi and other Iranian writers wrote notable satires. Medieval Europe In the Early Middle Ages, examples of satire were the songs by Goliards or vagants now best known as an anthology called Carmina Burana and made famous as texts of a composition by the 20th-century composer Carl Orff. Satirical poetry is believed to have been popular, although little has survived. With the advent of the High Middle Ages and the birth of modern vernacular literature in the 12th century, it began to be used again, most notably by Chaucer. The disrespectful manner was considered "unchristian" and ignored, except for the moral satire, which mocked misbehaviour in Christian terms. Examples are Livre des Manières by Étienne de Fougères (~1178), and some of Chaucer's Canterbury Tales. Sometimes epic poetry (epos) was mocked, and even feudal society, but there was hardly a general interest in the genre. In the High Middle Ages the work Reynard the Fox, written by Willem die Madoc maecte, and its translations were a popular work that satirized the class system at the time. Representing the various classes as certain anthropomorphic animals. As example, the lion in the story represents the nobility, which is portrayed as being weak and without character, but very greedy. Versions of Reynard the Fox were also popular well into the early modern period. The Dutch translation Van den vos Reynaerde is considered a major medieval Dutch literary work. In the Dutch version De Vries argues that the animal characters represent barons who conspired against the Count of Flanders. Early modern western satire Direct social commentary via satire returned in the 16th century, when texts such as the works of François Rabelais tackled more serious issues. Two major satirists of Europe in the Renaissance were Giovanni Boccaccio and François Rabelais. Other examples of Renaissance satire include Till Eulenspiegel, Reynard the Fox, Sebastian Brant's Narrenschiff (1494), Erasmus's Moriae Encomium (1509), Thomas More's Utopia (1516), and Carajicomedia (1519). The Elizabethan (i.e. 16th-century English) writers thought of satire as related to the notoriously rude, coarse and sharp satyr play. Elizabethan "satire" (typically in pamphlet form) therefore contains more straightforward abuse than subtle irony. The French Huguenot Isaac Casaubon pointed out in 1605 that satire in the Roman fashion was something altogether more civilised. Casaubon discovered and published Quintilian's writing and presented the original meaning of the term (satira, not satyr), and the sense of wittiness (reflecting the "dishfull of fruits") became more important again. Seventeenth-century English satire once again aimed at the "amendment of vices" (Dryden). In the 1590s a new wave of verse satire broke with the publication of Hall's Virgidemiarum, six books of verse satires targeting everything from literary fads to corrupt noblemen. Although Donne had already circulated satires in manuscript, Hall's was the first real attempt in English at verse satire on the Juvenalian model. The success of his work combined with a national mood of disillusion in the last years of Elizabeth's reign triggered an avalanche of satire—much of it less conscious of classical models than Hall's — until the fashion was brought to an abrupt stop by censorship. Another satiric genre to emerge around this time was the satirical almanac, with François Rabelais's work Pantagrueline Prognostication (1532), which mocked astrological predictions. The strategies François utilized within this work were employed by later satirical almanacs, such as the Poor Robin series that spanned the 17th to 19th centuries. Ancient and modern India Satire (Kataksh or Vyang) has played a prominent role in Indian and Hindi literature, and is counted as one of the "ras" of literature in ancient books. With the commencement of printing of books in local language in the nineteenth century and especially after India's freedom, this grew. Many of the works of Tulsi Das, Kabir, Munshi Premchand, village minstrels, Hari katha singers, poets, Dalit singers and current day stand up Indian comedians incorporate satire, usually ridiculing authoritarians, fundamentalists and incompetent people in power. In India, it has usually been used as a means of expression and an outlet for common people to express their anger against authoritarian entities. Age of Enlightenment The Age of Enlightenment, an intellectual movement in the 17th and 18th centuries advocating rationality, produced a great revival of satire in Britain. This was fuelled by the rise of partisan politics, with the formalisation of the Tory and Whig parties—and also, in 1714, by the formation of the Scriblerus Club, which included Alexander Pope, Jonathan Swift, John Gay, John Arbuthnot, Robert Harley, Thomas Parnell, and Henry St John, 1st Viscount Bolingbroke. This club included several of the notable satirists of early-18th-century Britain. They focused their attention on Martinus Scriblerus, "an invented learned fool... whose work they attributed all that was tedious, narrow-minded, and pedantic in contemporary scholarship". In their hands astute and biting satire of institutions and individuals became a popular weapon. The turn to the 18th century was characterized by a switch from Horatian, soft, pseudo-satire, to biting "juvenal" satire. Jonathan Swift was one of the greatest of Anglo-Irish satirists, and one of the first to practise modern journalistic satire. For instance, In his A Modest Proposal Swift suggests that Irish peasants be encouraged to sell their own children as food for the rich, as a solution to the "problem" of poverty. His purpose is of course to attack indifference to the plight of the desperately poor. In his book Gulliver's Travels he writes about the flaws in human society in general and English society in particular. John Dryden wrote an influential essay entitled "A Discourse Concerning the Original and Progress of Satire" that helped fix the definition of satire in the literary world. His satirical Mac Flecknoe was written in response to a rivalry with Thomas Shadwell and eventually inspired Alexander Pope to write his satirical Dunciad. Alexander Pope (b. May 21, 1688) was a satirist known for his Horatian satirist style and translation of the Iliad. Famous throughout and after the long 18th century, Pope died in 1744. Pope, in his The Rape of the Lock, is delicately chiding society in a sly but polished voice by holding up a mirror to the follies and vanities of the upper class. Pope does not actively attack the self-important pomp of the British aristocracy, but rather presents it in such a way that gives the reader a new perspective from which to easily view the actions in the story as foolish and ridiculous. A mockery of the upper class, more delicate and lyrical than brutal, Pope nonetheless is able to effectively illuminate the moral degradation of society to the public. The Rape of the Lock assimilates the masterful qualities of a heroic epic, such as the Iliad, which Pope was translating at the time of writing The Rape of the Lock. However, Pope applied these qualities satirically to a seemingly petty egotistical elitist quarrel to prove his point wryly. Other satirical works by Pope include the Epistle to Dr Arbuthnot. Daniel Defoe pursued a more journalistic type of satire, being famous for his The True-Born Englishman which mocks xenophobic patriotism, and The Shortest-Way with the Dissenters—advocating religious toleration by means of an ironical exaggeration of the highly intolerant attitudes of his time. The pictorial satire of William Hogarth is a precursor to the development of political cartoons in 18th-century England. The medium developed under the direction of its greatest exponent, James Gillray from London. With his satirical works calling the king (George III), prime ministers and generals (especially Napoleon) to account, Gillray's wit and keen sense of the ridiculous made him the pre-eminent cartoonist of the era. Ebenezer Cooke (1665–1732), author of "The Sot-Weed Factor" (1708), was among the first writers of literary satire in Colonial America. Benjamin Franklin (1706–1790) and others followed, using satire to shape an emerging nation's culture through its sense of the ridiculous. Satire in Victorian England Several satiric papers competed for the public's attention in the Victorian era (1837–1901) and Edwardian period, such as Punch (1841) and Fun (1861). Perhaps the most enduring examples of Victorian satire, however, are to be found in the Savoy Operas of Gilbert and Sullivan. In fact, in The Yeomen of the Guard, a jester is given lines that paint a very neat picture of the method and purpose of the satirist, and might almost be taken as a statement of Gilbert's own intent: "I can set a braggart quailing with a quip, The upstart I can wither with a whim; He may wear a merry laugh upon his lip, But his laughter has an echo that is grim!" Novelists such as Charles Dickens (1812–1870) often used passages of satiric writing in their treatment of social issues. Continuing the tradition of Swiftian journalistic satire, Sidney Godolphin Osborne (1808–1889) was the most prominent writer of scathing "Letters to the Editor" of the London Times. Famous in his day, he is now all but forgotten. His maternal grandfather William Eden, 1st Baron Auckland was considered to be a possible candidate for the authorship of the Junius letters. Osborne's satire was so bitter and biting that at one point he received a public censure from Parliament's then Home Secretary Sir James Graham. Osborne wrote mostly in the Juvenalian mode over a wide range of topics mostly centered on British government's and landlords' mistreatment of poor farm workers and field laborers. He bitterly opposed the New Poor Laws and was passionate on the subject of the British government's botched response to the Great Irish Famine and the mistreatment of British soldiers during the Crimean War. A number of works of fiction during this time, influenced by Egyptomania, used the backdrop of Ancient Egypt as a device for satire. Some works, like Edgar Allan Poe's Some Words with a Mummy (1845) and Grant Allen's My New Year's Eve Among the Mummies (1878), portrayed Egyptian civilization as having already achieved many of the Victorian era's advancements (like the steam engine and gaslamps) in an effort to satire the notion of progress. Other works, like Jane Loudon's The Mummy!: Or a Tale of the Twenty-Second Century, satirized Victorian curiosities with the afterlife. Later in the nineteenth century, in the United States, Mark Twain (1835–1910) grew to become American's greatest satirist: his novel Huckleberry Finn (1884) is set in the antebellum South, where the moral values Twain wishes to promote are completely turned on their heads. His hero, Huck, is a rather simple but goodhearted lad who is ashamed of the "sinful temptation" that leads him to help a fugitive slave. In fact his conscience, warped by the distorted moral world he has grown up in, often bothers him most when he is at his best. He is prepared to do good, believing it to be wrong. Twain's younger contemporary Ambrose Bierce (1842–1913) gained notoriety as a cynic, pessimist and black humorist with his dark, bitterly ironic stories, many set during the American Civil War, which satirized the limitations of human perception and reason. Bierce's most famous work of satire is probably The Devil's Dictionary (1906), in which the definitions mock cant, hypocrisy and received wisdom. 20th-century satire Karl Kraus is considered the first major European satirist since Jonathan Swift. In 20th-century literature, satire was used by English authors such as Aldous Huxley (1930s) and George Orwell (1940s), which under the inspiration of Zamyatin's Russian 1921 novel We, made serious and even frightening commentaries on the dangers of the sweeping social changes taking place throughout Europe. Anatoly Lunacharsky wrote 'Satire attains its greatest significance when a newly evolving class creates an ideology considerably more advanced than that of the ruling class, but has not yet developed to the point where it can conquer it. Herein lies its truly great ability to triumph, its scorn for its adversary and its hidden fear of it. Herein lies its venom, its amazing energy of hate, and quite frequently, its grief, like a black frame around glittering images. Herein lie its contradictions, and its power.' Many social critics of this same time in the United States, such as Dorothy Parker and H. L. Mencken, used satire as their main weapon, and Mencken in particular is noted for having said that "one horse-laugh is worth ten thousand syllogisms" in the persuasion of the public to accept a criticism. Novelist Sinclair Lewis was known for his satirical stories such as Main Street (1920), Babbitt (1922), Elmer Gantry (1927; dedicated by Lewis to H. L. Mencken), and It Can't Happen Here (1935), and his books often explored and satirized contemporary American values. The film The Great Dictator (1940) by Charlie Chaplin is itself a parody of Adolf Hitler; Chaplin later declared that he would have not made the film if he had known about the concentration camps. Modern Soviet satire was very popular in the 1920s and 1930s. This form of satire is recognized by its level of sophistication and intelligence used, along with its own level of parody. Since there is no longer the need of survival or revolution to write about, modern Soviet satire focused on the quality of life. In the United States 1950s, satire was introduced into American stand-up comedy most prominently by Lenny Bruce and Mort Sahl. As they challenged the taboos and conventional wisdom of the time, were ostracized by the mass media establishment as sick comedians. In the same period, Paul Krassner's magazine The Realist began publication, to become immensely popular during the 1960s and early 1970s among people in the counterculture; it had articles and cartoons that were savage, biting satires of politicians such as Lyndon Johnson and Richard Nixon, the Vietnam War, the Cold War and the War on Drugs. This baton was also carried by the original National Lampoon magazine, edited by Doug Kenney and Henry Beard and featuring blistering satire written by Michael O'Donoghue, P.J. O'Rourke, and Tony Hendra, among others. Prominent satiric stand-up comedian George Carlin acknowledged the influence The Realist had in his 1970s conversion to a satiric comedian. A more humorous brand of satire enjoyed a renaissance in the UK in the early 1960s with the satire boom, led by comedians including Peter Cook, Alan Bennett, Jonathan Miller, and Dudley Moore, whose stage show Beyond the Fringe was a hit not only in Britain, but also in the United States. Other significant influences in 1960s British satire include David Frost, Eleanor Bron and the television program That Was The Week That Was. Joseph Heller's most famous work, Catch-22 (1961), satirizes bureaucracy and the military, and is frequently cited as one of the greatest literary works of the twentieth century. Departing from traditional Hollywood farce and screwball, director and comedian Jerry Lewis used satire in his self-directed films The Bellboy (1960), The Errand Boy (1961) and The Patsy (1964) to comment on celebrity and the star-making machinery of Hollywood. The film Dr. Strangelove (1964) starring Peter Sellers was a popular satire on the Cold War. Sellers and the British satire boom had a direct influence on the comedy troupe Monty Python. Empire magazine called Monty Python's Life of Brian (1979) "an unrivalled satire on religion". Severino "Nonoy" Marcelo's 1978 Philippine adult animated comedy film, Tadhana, presents a satirical, humorous and poignant view of the Philippines' history of Spanish colonization. Contemporary satire Contemporary popular usage of the term "satire" is often very imprecise. While satire often uses caricature and parody, by no means are all uses of these or other humorous devices satiric. Refer to the careful definition of satire that heads this article. The Cambridge Companion to Roman Satire also warns of the ambiguous nature of satire: [W]hile "satire," or perhaps rather "satiric(al)," are words we run up against constantly in analyses of contemporary culture [...], the search for any defining formal charcteristic (sic) [of satire] that will link past to present may turn out to be more frustrating than enlightening. Satire is used on many UK television programmes, particularly popular panel shows and quiz shows such as Mock the Week (2005–2022) and Have I Got News for You (1990–ongoing). It is found on radio quiz shows such as The News Quiz (1977–ongoing) and The Now Show (1998–2024). One of the most watched UK television shows of the 1980s and early 1990s, the puppet show Spitting Image was a satire of the royal family, politics, entertainment, sport and British culture of the era. Court Flunkey from Spitting Image is a caricature of James Gillray, intended as a homage to the father of political cartooning. Created by DMA Design in 1997, satire features prominently in the British video game series Grand Theft Auto. Another example is the Fallout series, namely Interplay-developed Fallout: A Post Nuclear Role Playing Game (1995). Other games utilizing satire include Postal (1997), State of Emergency (2002), Phone Story (2011), and 7 Billion Humans (2018). Trey Parker and Matt Stone's South Park (1997–ongoing) relies almost exclusively on satire to address issues in American culture, with episodes addressing racism, anti-Semitism, militant atheism, homophobia, sexism, environmentalism, corporate culture, political correctness and anti-Catholicism, among many other issues. Satirical web series and sites include Emmy-nominated Honest Trailers (2012–), Internet phenomena-themed Encyclopedia Dramatica (2004–), Uncyclopedia (2005–), self-proclaimed "America's Finest News Source" The Onion (1988–). and The Onion's Christian conservative counterpart The Babylon Bee (2016–). In the United States, Stephen Colbert's television program, The Colbert Report (2005–14) is instructive in the methods of contemporary American satire; sketch comedy television show Saturday Night Live is also known for its satirical impressions and parodies of prominent persons and politicians, among some of the most notable, their parodies of U.S. political figures Hillary Clinton and of Sarah Palin. Colbert's character is an opinionated and self-righteous commentator who, in his TV interviews, interrupts people, points and wags his finger at them, and "unwittingly" uses a number of logical fallacies. In doing so, he demonstrates the principle of modern American political satire: the ridicule of the actions of politicians and other public figures by taking all their statements and purported beliefs to their furthest (supposedly) logical conclusion, thus revealing their perceived hypocrisy or absurdity. In the United Kingdom, a popular modern satirist was the late Sir Terry Pratchett, author of the internationally best-selling Discworld book series. One of the most well-known and controversial British satirists is Chris Morris, co-writer and director of Four Lions. In Canada, satire has become an important part of the comedy scene. Stephen Leacock was one of the best known early Canadian satirists, and in the early 20th century, he achieved fame by targeting the attitudes of small-town life. In more recent years, Canada has had several prominent satirical television series and radio shows. Some, including CODCO, The Royal Canadian Air Farce, This Is That, and This Hour Has 22 Minutes deal directly with current news stories and political figures, while others, like History Bites present contemporary social satire in the context of events and figures in history. The Beaverton is a Canadian news satire site similar to The Onion. Canadian songwriter Nancy White uses music as the vehicle for her satire, and her comic folk songs are regularly played on CBC Radio. In Hong Kong, there was a well-known Australian Kim Jong-un impersonator Howard X whom often utilised satire to show his support for Hong Kong city's pro-democracy movements and liberation of North Korea. He believed that humour is a very powerful weapon and he often made it clear that he imitates the dictator to satirize him, not to glorify him. Throughout his career as a professional impersonator, he had also worked with multiple organisations and celebrities to create parodies and to stir up conversations of politics and human rights. Cartoonists often use satire as well as straight humour. Al Capp's satirical comic strip Li'l Abner was censored in September 1947. The controversy, as reported in Time, centred on Capp's portrayal of the US Senate. Said Edward Leech of Scripps-Howard, "We don't think it is good editing or sound citizenship to picture the Senate as an assemblage of freaks and crooks... boobs and undesirables." Walt Kelly's Pogo was likewise censored in 1952 over his overt satire of Senator Joe McCarthy, caricatured in his comic strip as "Simple J. Malarky". Garry Trudeau, whose comic strip Doonesbury focuses on satire of the political system, and provides a trademark cynical view on national events. Trudeau exemplifies humour mixed with criticism. For example, the character Mark Slackmeyer lamented that because he was not legally married to his partner, he was deprived of the "exquisite agony" of experiencing a nasty and painful divorce like heterosexuals. This, of course, satirized the claim that gay unions would denigrate the sanctity of heterosexual marriage. Like some literary predecessors, many recent television satires contain strong elements of parody and caricature; for instance, the popular animated series The Simpsons and South Park both parody modern family and social life by taking their assumptions to the extreme; both have led to the creation of similar series. As well as the purely humorous effect of this sort of thing, they often strongly criticise various phenomena in politics, economic life, religion and many other aspects of society, and thus qualify as satirical. Due to their animated nature, these shows can easily use images of public figures and generally have greater freedom to do so than conventional shows using live actors. News satire is also a very popular form of contemporary satire, appearing in as wide an array of formats as the news media itself: print (e.g. The Onion, Waterford Whispers News, Private Eye), radio (e.g. On the Hour), television (e.g. The Day Today, The Daily Show, Brass Eye) and the web (e.g. Faking News, El Koshary Today, Babylon Bee, The Beaverton, The Daily Bonnet and The Onion). Other satires are on the list of satirists and satires. In an interview with Wikinews, Sean Mills, President of The Onion, said angry letters about their news parody always carried the same message. "It's whatever affects that person", said Mills. "So it's like, 'I love it when you make a joke about murder or rape, but if you talk about cancer, well my brother has cancer and that's not funny to me.' Or someone else can say, 'Cancer's hilarious, but don't talk about rape because my cousin got raped.' Those are rather extreme examples, but if it affects somebody personally, they tend to be more sensitive about it." Satire is also gaining recognition for its value in social science research, particularly when authors are seeking to unpack complex social issues like gendered racism. Satire is regularly used by social movements covering a range of issues to achieve strategic goals. US community organizer and author of Rules for Radicals, Saul Alinsky, stated, 'Humour is essential to a successful tactician, for the most potent weapons known to [people] are satire and ridicule. Techniques Literary satire is usually written out of earlier satiric works, reprising previous conventions, commonplaces, stance, situations and tones of voice. Exaggeration is one of the most common satirical techniques. Contrarily, diminution is also a satirical technique. Legal status For its nature and social role, satire has enjoyed in many societies a special freedom license to mock prominent individuals and institutions. In Germany, Japan, and Italy satire is protected by the constitution. Since satire belongs to the realm of art and artistic expression, it benefits from broader lawfulness limits than mere freedom of information of journalistic kind. In some countries a specific "right to satire" is recognized and its limits go beyond the "right to report" of journalism and even the "right to criticize". Satire benefits not only of the protection to freedom of speech, but also to that to culture, and that to scientific and artistic production. Australia In September 2017 The Juice Media received an e-mail from the Australian National Symbols Officer requesting that the use of a satirical logo, called the "Coat of Harms" based on the Australian Coat of Arms, no longer be used as they had received complaints from the members of the public. Coincidentally 5 days later a Bill was proposed to Australian parliament to amend the Criminal Code Act 1995. If passed, those found to be in breach of the new amendment can face 2–5 years imprisonment. As of June 2018, the Criminal Code Amendment (Impersonating a Commonwealth Body) Bill 2017 was before the Australian Senate with the third reading moved May 10, 2018. Censorship and criticism Descriptions of satire's biting effect on its target include 'venomous', 'cutting', 'stinging', vitriol. Because satire often combines anger and humor, as well as the fact that it addresses and calls into question many controversial issues, it can be profoundly disturbing. Typical arguments Because it is essentially ironic or sarcastic, satire is often misunderstood. A typical misunderstanding is to confuse the satirist with their persona. Bad taste Common uncomprehending responses to satire include revulsion (accusations of poor taste, or that "it's just not funny" for instance) and the idea that the satirist actually does support the ideas, policies, or people being ridiculed. For instance, at the time of its publication, many people misunderstood Swift's purpose in A Modest Proposal, assuming it to be a serious recommendation of economically motivated cannibalism. Much later in history, in the weeks following 9/11 the American public at large found works of satire to be in bad taste and not appropriate for the social climate at the time. Some media outlets at the time, like essayist Roger Rosenblatt in an editorial for Time magazine's September 24 issue, would go so far as to claim that irony was dead. Targeting the victim Some critics of Mark Twain see Huckleberry Finn as racist and offensive, missing the point that its author clearly intended it to be satire (racism being in fact only one of a number of Mark Twain's known concerns attacked in Huckleberry Finn). This same misconception was suffered by the main character of the 1960s British television comedy satire Till Death Us Do Part. The character of Alf Garnett (played by Warren Mitchell) was created to poke fun at the kind of narrow-minded, racist, little Englander that Garnett represented. Instead, his character became a sort of anti-hero to people who actually agreed with his views. (The same situation occurred with Archie Bunker in American TV show All in the Family, a character derived directly from Garnett.) The Australian satirical television comedy show The Chaser's War on Everything has suffered repeated attacks based on various perceived interpretations of the "target" of its attacks. The "Make a Realistic Wish Foundation" sketch (June 2009), which attacked in classical satiric fashion the heartlessness of people who are reluctant to donate to charities, was widely interpreted as an attack on the Make a Wish Foundation, or even the terminally ill children helped by that organisation. Prime Minister of the time Kevin Rudd stated that The Chaser team "should hang their heads in shame". He went on to say that "I didn't see that but it's been described to me. ...But having a go at kids with a terminal illness is really beyond the pale, absolutely beyond the pale." Television station management suspended the show for two weeks and reduced the third season to eight episodes. Romantic prejudice The romantic prejudice against satire is the belief spread by the romantic movement that satire is something unworthy of serious attention; this prejudice has held considerable influence to this day. Such prejudice extends to humour and everything that arouses laughter, which are often underestimated as frivolous and unworthy of serious study. For instance, humor is generally neglected as a topic of anthropological research and teaching. History of opposition toward notable satires Because satire criticises in an ironic, essentially indirect way, it frequently escapes censorship in a way more direct criticism might not. Periodically, however, it runs into serious opposition, and people in power who perceive themselves as attacked attempt to censor it or prosecute its practitioners. In a classic example, Aristophanes was persecuted by the demagogue Cleon. 1599 book ban In 1599, the Archbishop of Canterbury John Whitgift and the Bishop of London Richard Bancroft, whose offices had the function of licensing books for publication in England, issued a decree banning verse satire. The decree, now known as the Bishops' Ban of 1599, ordered the burning of certain volumes of satire by John Marston, Thomas Middleton, Joseph Hall, and others; it also required histories and plays to be specially approved by a member of the Queen's Privy Council, and it prohibited the future printing of satire in verse. The motives for the ban are obscure, particularly since some of the books banned had been licensed by the same authorities less than a year earlier. Various scholars have argued that the target was obscenity, libel, or sedition. It seems likely that lingering anxiety about the Martin Marprelate controversy, in which the bishops themselves had employed satirists, played a role; both Thomas Nashe and Gabriel Harvey, two of the key figures in that controversy, suffered a complete ban on all their works. In the event, though, the ban was little enforced, even by the licensing authority itself. 21st-century polemics In 2005, the Jyllands-Posten Muhammad cartoons controversy caused global protests by offended Muslims and violent attacks with many fatalities in the Near East. It was not the first case of Muslim protests against criticism in the form of satire, but the Western world was surprised by the hostility of the reaction: Any country's flag in which a newspaper chose to publish the parodies was being burnt in a Near East country, then embassies were attacked, killing 139 people in mainly four countries; politicians throughout Europe agreed that satire was an aspect of the freedom of speech, and therefore to be a protected means of dialogue. Iran threatened to start an International Holocaust Cartoon Competition, which was immediately responded to by Jews with an Israeli Anti-Semitic Cartoons Contest. In 2006 British comedian Sacha Baron Cohen released Borat: Cultural Learnings of America for Make Benefit Glorious Nation of Kazakhstan, a "mockumentary" that satirized everyone, from high society to frat boys. The film was criticized by many. Although Baron Cohen is Jewish, some complained that it was antisemitic, and the government of Kazakhstan boycotted the film. The film itself had been a reaction to a longer quarrel between the government and the comedian. In 2008, popular South African cartoonist and satirist Jonathan Shapiro (who is published under the pen name Zapiro) came under fire for depicting then-president of the ANC Jacob Zuma in the act of undressing in preparation for the implied rape of 'Lady Justice' which is held down by Zuma loyalists. The cartoon was drawn in response to Zuma's efforts to duck corruption charges, and the controversy was heightened by the fact that Zuma was himself acquitted of rape in May 2006. In February 2009, the South African Broadcasting Corporation, viewed by some opposition parties as the mouthpiece of the governing ANC, shelved a satirical TV show created by Shapiro, and in May 2009 the broadcaster pulled a documentary about political satire (featuring Shapiro among others) for the second time, hours before scheduled broadcast. On December 29, 2009, Samsung sued Mike Breen, and the Korea Times for $1 million, claiming criminal defamation over a satirical column published on Christmas Day, 2009. On April 29, 2015, the UK Independence Party (UKIP) requested Kent Police investigate the BBC, claiming that comments made about Party leader Nigel Farage by a panelist on the comedy show Have I Got News For You might hinder his chances of success in the general election (which would take place a week later), and claimed the BBC breached the Representation of the People Act. Kent Police rebuffed the request to open an investigation, and the BBC released a statement, "Britain has a proud tradition of satire, and everyone knows that the contributors on Have I Got News for You regularly make jokes at the expense of politicians of all parties." Satirical prophecy Satire is occasionally prophetic: the jokes precede actual events. Among the eminent examples are: The 1784 presaging of modern daylight saving time, later actually proposed in 1907. While an American envoy to France, Benjamin Franklin anonymously published a letter in 1784 suggesting that Parisians economise on candles by arising earlier to use morning sunlight. In the 1920s, an English cartoonist imagined a laughable thing for the time: a hotel for cars. He drew a multi-story car park. The second episode of Monty Python's Flying Circus, which debuted in 1969, featured a sketch entitled "The Mouse Problem" (meant to satirize contemporary media exposés on homosexuality), which depicted a cultural phenomenon similar to some aspects of the modern furry fandom (which did not become widespread until the 1980s, over a decade after the sketch was first aired). The comedy film Americathon, released in 1979 and set in the United States of 1998, predicted a number of trends and events that would eventually unfold in the near future, including an American debt crisis, Chinese capitalism, the fall of the Soviet Union, a presidential sex scandal, and the popularity of reality shows. In January 2001, a satirical news article in The Onion, entitled "Our Long National Nightmare of Peace and Prosperity Is Finally Over" had newly elected President George Bush vowing to "develop new and expensive weapons technologies" and to "engage in at least one Gulf War-level armed conflict in the next four years". Furthermore, he would "bring back economic stagnation by implementing substantial tax cuts, which would lead to a recession". This prophesied the Iraq War, the Bush tax cuts, and the Great Recession. In 1975, the first episode of Saturday Night Live included an ad for a triple blade razor called the Triple-Trac; in 2001, Gillette introduced the Mach3. In 2004, The Onion satirized Schick and Gillette's marketing of ever-increasingly multi-blade razors with a mock article proclaiming Gillette will now introduce a five-blade razor. In 2006, Gillette released the Gillette Fusion, a five-blade razor. After the Iran nuclear deal in 2015, The Onion ran an article with the headline "U.S. Soothes Upset Netanyahu With Shipment Of Ballistic Missiles". Sure enough, reports broke the next day of the Obama administration offering military upgrades to Israel in the wake of the deal. In July 2016, The Simpsons released the most recent in a string of satirical references to a potential Donald Trump presidency (although the first was made back in a 2000 episode). Other media sources, including the popular film Back to the Future Part II have also made similar satirical references. Infinite Jest, published in 1996, described an alternate America following the presidency of Johnny Gentle, a celebrity who had not held prior political office. Gentle's signature policy was the erection of a wall between the United States and Canada for use as a hazardous waste dump. The US territory behind the wall was "given" to Canada, and the Canadian government was forced to pay for the wall. This appeared to parody the signature campaign promise and background of Donald Trump. See also Culture jamming Freedom of the press Onomasti komodein Parody religion Satiric misspellings Sage writing L'Équarrissage pour tous Notes References Citations Sources Bibliography Bosworth, Clifford Edmund (1976), The Mediaeval Islamic Underworld: The Banu Sasan in Arabic Society and Literature, Brill Publishers, ISBN 90-04-04392-6. Branham, R Bracht; Kinney, Daniel (1997). Introduction. Satyrica. By Petronius. University of California Press. ISBN 9780520211186. Clark, John R (1991), The Modern Satiric Grotesque and its traditions, Lexington: U of Kentucky P, ISBN 9780813130323. Corum, Robert T. (2002), "The rhetoric of disgust and contempt in Boileau", in Birberick, Anne Lynn; Ganim, Russell (eds.), The Shape of Change: Essays in Early Modern Literature and La Fontaine in Honor of David Lee Rubin, Rodopi, ISBN 9042014490. Davenport, A, ed. (1969), The Poems, Liverpool University Press. Elliott, Robert C (2004), "The nature of satire", Encyclopædia Britannica. Fo, Dario (1990), "Satira e sfottò", in Allegri, Luigi (ed.), Dialogo provocatorio sul comico, il tragico, la follia e la ragione (interview) (in Italian), pp. 2, 9. Fo, Dario (1993), Provocative Dialogue on the Comic, the Tragic, Folly and Reason, London: Methuen Publishing (transl.). Frye, Northrop (1957), Anatomy of Criticism (in particular the discussion of the 4 "myths"). Hall, Joseph. "Virgidemiae". In Davenport (1969). Hodgart, Matthew; Connery, Brian (2009) [1969], Satire: Origins and Principles, Transaction Publishers, ISBN 9781412833646. Pietrasik, Vanessa (2011), La satire en jeu. Critique et scepticisme en Allemagne à la fin du XVIIIe siècle (in French), Tusson: Du Lérot éditeur, Charente. Test, George Austin (1991), Elliott's Bind; or, What Is Satire, Anyway? in Satire: Spirit & Art, University of South Florida Press, ISBN 9780813010878 Wilson, R Rawdon (2002), The hydra's tale: imagining disgust, University of Alberta, ISBN 9780888643681. Massimo Colella, Seicento satirico: Il Viaggio di Antonio Abati (con edizione critica in appendice), in «La parola del testo», XXVI, 1-2, 2022, pp. 77–100. Further reading Bloom, Edward A (1972), "Sacramentum Militiae: The Dynamics of Religious Satire", Studies in the Literary Imagination, 5: 119–42. Bronowski, Jacob; Mazlish, Bruce (1993) [1960], The Western Intellectual Tradition From Leonardo to Hegel, Barnes & Noble, p. 252. Connery, Brian A, Theorizing Satire: A Bibliography, Oakland University. Dooley, David Joseph (1972), Contemporary satire, Holt, Rinehart and Winston of Canada, ISBN 9780039233853. Feinberg, Leonard, The satirist. Lee, Jae Num (1971), Scatology in Continental Satirical Writings from Aristophanes to Rabelais and English Scatological Writings from Skelton to Pope, 1,2,3 maldita madre. Swift and Scatological Satire, Albuquerque: U of New Mexico P, pp. 7–22, 23–53. Theories/critical approaches to satire as a genre Connery, Brian; Combe, Kirk, eds. (1995). Theorizing Satire: Essays in Literary Criticism. New York: St. Martin's Press. p. 212. ISBN 0-312-12302-7. Draitser, Emil (1994), Techniques of Satire: The Case of Saltykov-Shchedrin, Berlin-New York: Mouton de Gruyter, ISBN 3-11-012624-9. Hammer, Stephanie, Satirizing the Satirist. Highet, Gilbert, Satire. Kernan, Alvin, The Cankered Muse. Kindermann, Udo (1978), Satyra. Die Theorie der Satire im Mittellateinischen, Vorstudie zu einer Gattungsgeschichte (in German), Nürnberg. Κωστίου, Αικατερίνη (2005), Εισαγωγή στην Ποιητική της Ανατροπής: σάτιρα, ειρωνεία, παρωδία, χιούμορ (in Greek), Αθήνα: Νεφέλη{{citation}}: CS1 maint: publisher location (link) The plot of satire Seidel, Michael, Satiric Inheritance. Zdero, Rad (2008), Entopia: Revolution of the Ants. External links Garnett, Richard (1911). "Satire" . In Chisholm, Hugh (ed.). Encyclopædia Britannica. Vol. 24 (11th ed.). Cambridge University Press. pp. 228–229. Harry Furniss Parliamentary Satire Book – 1890s – UK Parliament Living Heritage Critical juncture theory focuses on critical junctures, i.e., large, rapid, discontinuous changes, and the long-term causal effect or historical legacy of these changes. Critical junctures are turning points that alter the course of evolution of some entity (e.g., a species, a society). Critical juncture theory seeks to explain both (1) the historical origin and maintenance of social order, and (2) the occurrence of social change through sudden, big leaps. Critical juncture theory is not a general theory of social order and change. It emphasizes one kind of cause (involving a big, discontinuous change) and kind of effect (a persistent effect). Yet, it challenges some common assumptions in many approaches and theories in the social sciences. The idea that some changes are discontinuous sets it up as an alternative to (1) "continuist" or "synechist" theories that assume that change is always gradual or that natura non facit saltus – Latin for "nature does not make jumps." The idea that such discontinuous changes have a long-term impact stands in counterposition to (2) "presentist" explanations that only consider the possible causal effect of temporally proximate factors. Theorizing about critical junctures began in the social sciences in the 1960s. Since then, it has been central to a body of research in the social sciences that is historically informed. Research on critical junctures in the social sciences is part of the broader tradition of comparative historical analysis and historical institutionalism. It is a tradition that spans political science, sociology and economics. Within economics, it shares an interest in historically oriented research with the new economic history or cliometrics. Research on critical junctures is also part of the broader "historical turn" in the social sciences. Origins in the 1960s and early 1970s The idea of episodes of discontinuous change, followed by periods of relative stability, was introduced in various fields of knowledge in the 1960s and early 1970s. Kuhn's paradigm shifts Philosopher of science Thomas Kuhn's landmark work The Structure of Scientific Revolutions (1962) introduced and popularized the idea of discontinuous change and the long-term effects of discontinuous change. Kuhn argued that progress in knowledge occurs at times through sudden jumps, which he called paradigm shifts. After paradigm shifts, scholars do normal science within paradigms, which endure until a new revolution came about. Kuhn challenged the conventional view in the philosophy of science at the time that knowledge growth could be understood entirely as a process of gradual, cumulative growth. Stephen Jay Gould writes that "Thomas Kuhn’s theory of scientific revolutions" was "the most overt and influential" scholarly work to make a "general critique of gradualism" in the twentieth century. Gellner's neo-episodic model of change Anthropologist Ernest Gellner proposed a neo-episodic model of change in 1964 that highlights the "step-like nature of history" and the "remarkable discontinuity" between different historical periods. Gellner contrasts the neo-episodic model of change to an evolutionary model that portrays "the pattern of Western history" as a process of "continuous and sustained and mainly endogenous upward growth." Sociologist Michael Mann adapted Gellner's idea of "'episodes' of major structural transformation" and called such episodes "power jumps." Lipset and Rokkan's critical junctures Sociologist Seymour Lipset and political scientist Stein Rokkan introduced the idea of critical junctures and their long-term impact in the social sciences in 1967. The ideas presented in the coauthored 1967 work were elaborated by Rokkan in Citizens, Elections, and Parties (1970). Gellner had introduced a similar idea in the social sciences. However, Lipset and Rokkan offered a more elaborate model and an extensive application of their model to Europe (see below). Although Gellner influenced some sociologists, the impact of Lipset and Rokkan on the social sciences was greater. Gould's punctuated equilibrium model Kuhn's ideas influenced paleontologist Stephen Jay Gould, who introduced the idea of punctuated equilibrium in the field of evolutionary biology in 1972. Gould's initial work on punctuated equilibrium was coauthored with Niles Eldredge. Gould's model of punctuated equilibrium drew attention to episodic bursts of evolutionary change followed by periods of morphological stability. He challenged the conventional model of gradual, continuous change - called phyletic gradualism. The critical juncture theoretical framework in the social sciences Since its launching in 1967, research on critical junctures has focused in part on developing a theoretical framework, which has evolved over time. In studies of society, some scholars use the term "punctuated equilibrium" model, and others the term "neo-episodic" model. Studies of knowledge continue to use the term "paradigm shift". However, these terms can be treated as synonyms for critical juncture. Developments in the late 1960s–early 1970s Key ideas in critical junctures research were initially introduced in the 1960s and early 1970s by Seymour Lipset, Stein Rokkan, and Arthur Stinchcombe. Critical junctures and legacies Seymour Lipset and Stein Rokkan (1967) and Rokkan (1970) introduced the idea that big discontinuous changes, such as the reformation, the building of nations, and the Industrial Revolution, reflected conflicts organized around social cleavages, such as the center-periphery, state-church, land-industry, and owner-worker cleavages. In turn, these big discontinuous changes could be seen as critical junctures because they generated social outcomes that subsequently remained "frozen" for extensive periods of time. In more general terms, Lipset and Rokkan's model has three components: (1) Cleavage. Strong and enduring conflicts that polarize a political system. Four such cleavages were identified: The center–periphery cleavage, a conflict between a central nation-building culture and ethnically linguistically distinct subject populations in the peripheries. The state–church cleavage, a conflict between the aspirations of a nation-state and the church. The land–industry cleavage, a conflict between landed interests and commercial/industrial entrepreneurs. The worker–employer cleavage, a conflict between owners and workers. (2) Critical juncture. Radical changes regarding these cleavages happen at certain moments. (3) Legacy. Once these changes occur, their effect endures for some time afterwards. Rokkan (1970) added two points to these ideas. Critical junctures could set countries on divergent or convergent paths. Critical junctures could be "sequential," such that a new critical junctures does not totally erase the legacies of a previous critical juncture but rather modifies that previous legacy. The reproduction of legacies through self-replicating causal loops Arthur Stinchcombe (1968) filled a key gap in Lipset and Rokkan's model. Lipset and Rokkan argued that critical junctures produced legacies, but did not explain how the effect of a critical juncture could endure over a long period. Stinchcombe elaborated the idea of historical causes (such as critical junctures) as a distinct kind of cause that generates a "self-replicating causal loop." Stinchcombe explained that the distinctive feature of such a loop is that "an effect created by causes at some previous period becomes a cause of that same effect in succeeding periods." This loop was represented graphically by Stinchcombe as follows: X t1 ––> Y t2 ––> D t3 ––> Y t4 ––> D t5 ––> Y t6 Stinchcombe argued that the cause (X) that explains the initial adoption of some social feature (Y) was not the same one that explains the persistence of this feature. Persistence is explained by the repeated effect of Y on D and of D on Y. Developments in the early 1980s–early 1990s Additional contributions were made in the 1980s and early 1990s by various political scientists and economists. Punctuated equilibrium, path dependence, and institutions Paul A. David and W. Brian Arthur, two economists, introduced and elaborated the concept of path dependence, the idea that past events and decisions affect present options and that some outcomes can persist due to the operation of a self-reinforcing feedback loop. This idea of a self-reinforcing feedback loop resembles that of a self-replicating causal loop introduced earlier by Stinchcombe. However, it resonated with economists and led to a growing recognition in economics that "history matters." The work by Stephen Krasner in political science incorporated the idea of punctuated equilibrium into the social sciences. Krasner also drew on the work by Arthur and connected the idea of path dependence to the study of political institutions. Douglass North, an economist and Nobel laureate, applied the idea of path dependence to institutions, which he defined as "the rules of the game in a society," and drew attention to the persistence of institutions. A synthesis Political scientists Ruth Berins Collier and David Collier, in Shaping the Political Arena (1991), provided a synthesis of many ideas introduced from the 1960s to 1990, in the form of the following "five-step template": Antecedent Conditions ––> Cleavage or Shock ––> Critical Juncture ––> Aftermath ––> Legacy These key concepts have been defined as follows: (1) "Antecedent conditions are diverse socioeconomic and political conditions prior to the onset of the critical juncture that constitute the baseline for subsequent change." (2) "Cleavages, shocks, or crises are triggers of critical junctures." (3) "Critical junctures are major episodes of institutional change or innovation." (4) "The aftermath is the period during which the legacy takes shape." (5) "The legacy is an enduring, self-reinforcing institutional inheritance of the critical juncture that stays in place and is stable for a considerable period." Debates in the 2000s–2010s Following a period of consolidation of critical junctures framework, few new developments occurred in the 1990s. However, since around 2000, several new ideas were proposed and many aspects of the critical junctures framework are the subject of debate. Critical junctures and incremental change An important new issue in the study of change is the relative role of critical junctures and incremental change. On the one hand, the two kinds of change are sometimes starkly counterposed. Kathleen Thelen emphasizes more gradual, cumulative patterns of institutional evolution and holds that "the conceptual apparatus of path dependence may not always offer a realistic image of development." On the other hand, path dependence, as conceptualized by Paul David is not deterministic and leaves room for policy shifts and institutional innovation. Critical junctures and contingency Einar Berntzen notes another debate: "Some scholars emphasize the historical contingency of the choices made by political actors during the critical juncture." For example, Michael Bernhard writes that critical junctures "are periods in which the constraints of structure have weakened and political actors have enhanced autonomy to restructure, overturn, and replace critical systems or sub-systems." However, Berntzen holds that "other scholars have criticized the focus on agency and contingency as key causal factors of institutional path selection during critical junctures" and "argue that a focus on antecedent conditions of critical junctures is analytically more useful." For example, Dan Slater and Erica Simmons place a heavy emphasis on antecedent conditions. Legacies and path dependence The use of the concept of path dependence in the study of critical junctures has been a source of some debate. On the one hand, James Mahoney argues that "path dependence characterizes specifically those historical sequences in which contingent events set into motion institutional patterns or event chains that have deterministic properties" and that there are two types of path dependence: "self-reinforcing sequences" and "reactive sequences." On the other hand, Kathleen Thelen and other criticize the idea of path dependence determinism, and Jörg Sydow, Georg Schreyögg, and Jochen Koch question the idea of reactive sequences as a kind of path dependence. Institutional and behavioral path dependence The study of critical junctures has commonly been seen as involving a change in institutions. However, many works extend the scope of research of critical junctures by focusing on changes in culture. Avidit Acharya, Matthew Blackwell, and Maya Sen state that the persistence of a legacy can be "reinforced both by formal institutions, such as Jim Crow laws (a process known as institutional path dependence), and also by informal institutions, such as family socialization and community norms (a process we call behavioral path dependence)." Substantive applications in the social sciences Topics and processes A critical juncture approach has been used in the study of many fields of research: state formation, political regimes, regime change and democracy, party system, public policy, government performance, and economic development. In addition, many processes and events have been identified as critical junctures. Pre-1760 power jumps Michael Mann, in The Sources of Social Power (1986), relies on Gellner's neo-episodic model of change and identifies a series of "power jumps" in world history prior to 1760 - the idea of power jumps is similar to that of a critical juncture. Some of the examples of power jumps identified by Mann are: The domestication of animals and the development of agriculture Law codes in written form The military revolution The use of Hoplites and phalanxes in war. The creation of the polis The diffusion of literacy The formation of modern states Modern era critical junctures Some of the processes in the modern era that are commonly seen as critical junctures in the social sciences are: State formation. The Industrial Revolution. Political and social revolutions, such as the Glorious Revolution of 1688, the French Revolution of 1789, and the Russian Revolution of 1917. Wars, such as World War I and World War II Colonialism and decolonization. The end of slavery. Transitions to mass politics. Transitions to democracy. The Trente Glorieuses - the 30 years from 1945 to 1975 in Europe. The transition to neoliberalism in the 1980s and 1990s. The end of the Cold War in 1989. Considerable discussion has focused on the possibility that the COVID-19 pandemic will be a critical juncture. Examples of research Barrington Moore Jr.'s Social Origins of Dictatorship and Democracy: Lord and Peasant in the Making of the Modern World (1966) argues that revolutions (the critical junctures) occurred in different ways (bourgeois revolutions, revolutions from above, and revolutions from below) and this difference led to contrasting political regimes in the long term (the legacy)—democracy, fascism, and communism, respectively. In contrast to the unilinear view of evolution common in the 1960s, Moore showed that countries followed multiple paths to modernity. Collier and Collier's Shaping the Political Arena: Critical Junctures, the Labor Movement, and the Regime Dynamics in Latin America (1991) compares "eight Latin American countries to argue that labor-incorporation periods were critical junctures that set the countries on distinct paths of development that had major consequences for the crystallization of certain parties and party systems in the electoral arena. The way in which state actors incorporated labor movements was conditioned by the political strength of the oligarchy, the antecedent condition in their analysis. Different policies towards labor led to four specific types of labor incorporation: state incorporation (Brazil and Chile), radical populism (Mexico and Venezuela), labor populism (Peru and Argentina), and electoral mobilization by a traditional party (Uruguay and Colombia). These different patterns triggered contrasting reactions and counter reactions in the aftermath of labor incorporation. Eventually, through a complex set of intermediate steps, relatively enduring party system regimes were established in all eight countries: multiparty polarizing systems (Brazil and Chile), integrative party systems (Mexico and Venezuela), stalemated party systems (Peru and Argentina), and systems marked by electoral stability and social conflict (Uruguay and Colombia)." John Ikenberry's After Victory: Institutions, Strategic Restraint, and the Rebuilding of Order After Major Wars (2001) compares post-war settlements after major wars – following the Napoleonic Wars in 1815, the world wars in 1919 and 1945, and the end of the Cold War in 1989. It argues that "international order has come and gone, risen and fallen across historical eras" and that the "great moments of order building come after major wars – 1648, 1713, 1815, 1919, 1945, and 1989." In essence, peace conferences and settlement agreements put in place "institutions and arrangements for postwar order." Ikenberry also shows that "the actual character of international order has varied across eras and order building moments" and that "variations have been manifest along multiple dimensions: geographic scope, organizational logic, rules and institutions, hierarchy and leadership, and the manner in and degree to which coercion and consent undergird the resulting order." Seymour Martin Lipset, in The Democratic Century (2004), addresses the question why North America developed stable democracies and Latin America did not. He holds that the reason is that the initial patterns of colonization, the subsequent process of economic incorporation of the new colonies, and the wars of independence varies. The divergent histories of Britain and Iberia are seen as creating different cultural legacies that affected the prospects of democracy. Daron Acemoglu and James A. Robinson’s Why Nations Fail: The Origins of Power, Prosperity, and Poverty (2012) draws on the idea of critical junctures. A key thesis of this book is that, at critical junctures (such as the Glorious Revolution in 1688 in England), countries start to evolve along different paths. Countries that adopt inclusive political and economic institutions become prosperous democracies. Countries that adopt extractive political and economic institutions fail to develop political and economically. Debates in research Critical juncture research typically contrasts an argument about the historical origins of some outcome to an explanation based in temporally proximate factors. However, researchers have engaged in debates about what historical event should be considered a critical juncture. The rise of the West A key debate in research on critical junctures concerns the turning point that led to the rise of the West. Jared Diamond, in Guns, Germs and Steel (1997) argues that the development reaching back to around 11,000 BCE explain why key breakthroughs were made in the West rather than in some other region of the world. Michael Mitterauer, in Why Europe? The Medieval Origins of its Special Path (2010) traces the rise of the West to developments in the Middle Ages. Daron Acemoglu and James A. Robinson, in Why Nations Fail: The Origins of Power, Prosperity, and Poverty (2012) and The Narrow Corridor. States, Societies, and the Fate of Liberty (2019) argue that a critical juncture during the early modern age is what set the West on its distinctive path. Historical sources of economic development (with a focus on Latin America) Another key debate concerns the historical roots of economic development, a debate that has address Latin America in particular. Jerry F. Hough and Robin Grier (2015) claim that "key events in England and Spain in the 1260s explain why Mexico lagged behind the United States economically in the 20th century." Works by Daron Acemoglu, Simon H. Johnson, and James A. Robinson (2001); James Mahoney (2010); and Stanley Engerman and Kenneth Sokoloff (2012) focus on colonialism as the key turning point explaining long-term economic trajectories. Sebastián Mazzuca attributes Latin America's poor economic performance in the twentieth century to the distinctive state weakness resulting from the process of state formation in the nineteenth century, and the way in which national territories were formed, combining dynamic areas and backward peripheries. This claim complements and refines the usual ideas that attribute all forms of economic and social backwardness in Latin America to colonial institutions. Rudiger Dornbusch and Sebastián Edwards (1991) see the emergence of mass politics in the mid-20th century as the key turning point that explains the economic performance of Latin America. Historical origins of the Asian developmental state Research on Asia includes a debate about the historical roots of developmental states. Atul Kohli (2004) argues that developmental states originate in the colonial period. Tuong Vu (2010) maintains that developmental states originate in the post-colonial period. Reception and impact Research on critical junctures is generally seen as an important contribution to the social sciences. Within political science, Berntzen argues that research on critical junctures "has played an important role in comparative historical and other macro-comparative scholarship." Some of the most notable works in the field of comparative politics since the 1960s rely on the concept of a critical juncture. Barrington Moore Jr.'s Social Origins of Dictatorship and Democracy: Lord and Peasant in the Making of the Modern World (1966) is broadly recognized as a foundational study in the study of democratization. Ruth Berins Collier and David Collier's Shaping the Political Arena: Critical Junctures, the Labor Movement, and the Regime Dynamics in Latin America (1991) has been characterized by Giovanni Capoccia and R. Daniel Kelemen as a "landmark work" and by Kathleen Thelen as a "landmark study ... of regime transformation in Latin America." Robert D. Putnam's Making Democracy Work: Civic Traditions in Modern Italy (1993) provides an analysis of the historical origins of social capital in Italy that is widely credited with launching a strand of research on social capital and its consequences in various fields within political science. Johannes Gerschewski describes John Ikenberry After Victory (2001) as a "masterful analysis." Frank Baumgartner and Bryan D. Jones's Agendas and Instability in American Politics (2009) is credited with having "a massive impact in the study of public policy." Within economics, the historically informed work of Douglass North, and Daron Acemoglu and James A. Robinson, is seen as partly responsible for the disciple's renewed interest in political institutions and the historical origins of institutions and hence for the revival of the tradition of institutional economics. See also Notes and references Further reading Theoretical framework Arthur, W. Brian, "Competing Technologies, Increasing Returns, and Lock-In by Historical Events." Economic Journal 99(394)(1989): 116–31. [1] Berntzen, Einar, "Historical and Longitudinal Analyses," pp. 390–405, in Dirk Berg-Schlosser, Bertrand Badie, and Leonardo Morlino (eds.), The SAGE Handbook of Political Science. Thousand Oaks, CA: SAGE, 2020. Capoccia, Giovanni, and R. Daniel Kelemen, "The Study of Critical Junctures: Theory, Narrative, and Counterfactuals in Historical Institutionalism." World Politics 59(3)(2007): 341–69. [2] Collier, David, and Gerardo L. Munck, "Building Blocks and Methodological Challenges: A Framework for Studying Critical Junctures." Qualitative and Multi-Method Research 15(1)(2017): 2–9. [3] Collier, David, and Gerardo L. Munck (eds.), Critical Junctures and Historical Legacies: Insights and Methods for Comparative Social Science (2022). Collier, Ruth Berins, and David Collier, Shaping the Political Arena: Critical Junctures, the Labor Movement, and the Regime Dynamics in Latin America. Princeton, NJ: Princeton University Press, 1991; Ch. 1: "Framework: Critical Junctures and Historical Legacies." [4] David, Paul A., "Clio and the Economics of QWERTY." American Economic Review 75(2)(1985): 332–37. [5] Gerschewski, Johannes, "Explanations of Institutional Change. Reflecting on a ‘Missing Diagonal’." American Political Science Review 115(1)(2021): 218–33. Krasner, Stephen D., "Approaches to the State: Alternative Conceptions and Historical Dynamics." Comparative Politics 16(2)(1984): 223–46. [6] Krasner, Stephen D., "Sovereignty: An Institutional Perspective." Comparative Political Studies 21(1)(1988): 66–94. [7] Mahoney, James, "Path Dependence in Historical Sociology." Theory and Society 29(4)(2000): 507–48. [8] Pierson, Paul, "Increasing Returns, Path Dependence, and the Study of Politics." American Political Science Review 94(2)(2000): 251–67. [9] Slater, Dan, and Erica Simmons, "Informative Regress: Critical Antecedents in Comparative Politics." Comparative Political Studies 43(7)(2010): 886–917. [10] Soifer, Hillel David, "The Causal Logic of Critical Junctures." Comparative Political Studies 45(12)(2012): 1572–1597. [11] Substantive applications Acemoglu, Daron, and James A. Robinson, Why Nations Fail: Origins of Power, Poverty and Prosperity (2012). Acemoglu, Daron, and James A. Robinson, The Narrow Corridor. States, Societies, and the Fate of Liberty (2019). Acharya, Avidit, Matthew Blackwell, and Maya Sen, Deep Roots: How Slavery Still Shapes Southern Politics (2018). Bartolini, Stefano, The Political Mobilization of the European Left, 1860–1980: The Class Cleavage (2000). Bartolini, Stefano, Restructuring Europe. Centre Formation, System Building, and Political Structuring between the Nation State and the European Union (2007). Baumgartner, Frank R., and Bryan D. Jones, Agendas and Instability in American Politics, 2nd ed. (2009). Calder, Kent, and Min Ye, The Making of Northeast Asia (2010). Caramani, Daniele, The Europeanization of Politics: The Formation of a European Electorate and Party System in Historical Perspective (2015). della Porta, Donatella et al., Discursive Turns and Critical Junctures: Debating Citizenship after the Charlie Hebdo Attacks (2020). Chibber, Vivek, Locked in Place: State-building and Late Industrialization in India (2003). Engerman, Stanley L., and Kenneth L. Sokoloff, Economic Development in the Americas since 1500: Endowments and Institutions (2012). Ertman, Thomas, Birth of the Leviathan: Building States and Regimes in Medieval and Early Modern Europe (1997). Fishman, Robert M., Democratic Practice: Origins of the Iberian Divide in Political Inclusion (2019). Gould, Andrew C., Origins of Liberal Dominance: State, Church, and Party in Nineteenth-Century Europe (1999). Grzymała-Busse, Anna M., Redeeming the Communist Past: The Regeneration of Communist Parties in East Central Europe (2002). Ikenberry, G. John, After Victory: Institutions, Strategic Restraint, and the Rebuilding of Order After Major Wars (2001). Karvonen, Lauri, and Stein Kuhnle (eds.), Party Systems and Voter Alignments Revisited (2000). Kurtz, Marcus, Latin American State Building in Comparative Perspective: Social Foundations of Institutional Order (2013). Lange, Matthew, Lineages of Despotism and Development. British Colonialism and State Power (2009). Lieberman, Evan S., Race and Regionalism in the Politics of Taxation in Brazil and South Africa (2003). Lipset, Seymour M., and Stein Rokkan (eds.), Party Systems and Voter Alignments: Cross-National Perspectives (1967). López-Alves, Fernando, State Formation and Democracy in Latin America, 1810–1900 (2000). Gregory M. Luebbert, Liberalism, Fascism, or Social Democracy: Social Classes and the Political Origins of Regimes in Interwar Europe (1991). Mahoney, James, The Legacies of Liberalism: Path Dependence and Political Regimes in Central America (2001). Møller, Jørgen, "Medieval Origins of the Rule of Law: The Gregorian Reforms as Critical Juncture?" Hague Journal on the Rule of Law 9(2)(2017): 265–82. Moore Jr., Barrington, Social Origins of Dictatorship and Democracy. Lord and Peasant in the Making of the Modern World (1966). Putnam, Robert D., with Robert Leonardi and Raffaella Nanetti, Making Democracy Work: Civic Traditions in Modern Italy (1993). Riedl, Rachel Beatty, Authoritarian Origins of Democratic Party Systems in Africa (2014). Roberts, Kenneth M., Changing Course in Latin America: Party Systems in the Neoliberal Era (2014). Rokkan, Stein, with Angus Campbell, Per Torsvik, and Henry Valen, Citizens, Elections, and Parties: Approaches to the Comparative Study of the Processes of Development (1970). Scully, Timothy R., Rethinking the Center: Party Politics in Nineteenth- and Twentieth-Century Chile (1992). Silva, Eduardo, and Federico M. Rossi (eds.), Reshaping the Political Arena in Latin America (2018). Tudor, Maya, The Promise of Power: The Origins of Democracy in India and Autocracy in Pakistan (2013). Yashar, Deborah, Demanding Democracy: Reform and Reaction in Costa Rica and Guatemala, 1870s-1950s (1997). External links The Critical Juncture Project, coordinated by David Collier and Gerardo L. Munck Primary, alternate, contingency and emergency (PACE) is a methodology used to build a communication plan. The method requires the author to determine the different stakeholders or parties that need to communicate and then determine, if possible, the best four, different, redundant forms of communication between each of those parties. Ideally, each method will be completely separate and independent of the other systems of communication; failure of any component/process in one should not affect any other means to communicate. PACE also defines the priority of communications systems. According to the United States Army, a PACE communication plan "designates the order in which an element will move through available communications systems until contact can be established with the desired distant element.". Agreed upon triggers inform parties when to move to another form. For each method, the receiver must first sense which one the sender is using and then respond, thus monitoring of more than one means is required. A PACE-based communication plan exists for a specific mission or task, not a specific unit, because the plan must consider both intra- and inter-unit sharing of information. An organization may have multiple plans for different situations, activities, and/or partners. Order and scope The PACE plan system is expressed as an order of communication precedence list; primary, alternate, contingency, and emergency. The first is usually the best, more reliable, most used, fastest, and highest bandwidth of the options. The last is usually the most time-consuming, costly, and inconvenient. For example, the plan does not designate such things as the exact radio channel or talk group to be used if you are using a radio, but the order in which you would plan to use the radio, then telephone, then email, then courier, and/or whatever agreed upon method of communications between groups. A PACE plan is not a frequency plan (which details frequency allocation and radio spectrum characteristics) or band plan (to avoid interference) or channel plan (which details which channels users listen/talk upon) or deployment plan (which details the users' radios types and locations). Development of PACE plans Developing a PACE plan is a collaborative effort balancing operational needs and communications options. It must consider users, technology, security/privacy risks, time, quality, training/practice, and cost. Emergency management and communications managers should coordinate the development of PACE plans for the many different functions and departments within your organization to ensure that Incident Command can maintain critical communication links. Departmental PACE plans should be coordinated with emergency management and the communications team. It is critical that individual departments nest their plan within the larger Emergency Plan to ensure that the organization has the resources to execute the plan and reduce unnecessary duplication of assets. Developing comprehensive PACE plans will not ensure perfect communications in a disaster, but may help to clear one more layer in the fog and friction found in every emergency situation. == References == Graphic violence refers to the depiction of especially explicit or detailed acts of violence in mass media. It may be real, simulated live action, or animated. Intended for viewing by mature audiences, graphic in this context is a synonym for explicit, referring to the clear and unabashed nature of the violence portrayed. Subterms Below are terms that categorized as or related to graphic violence. Gore The definition of gore is imagery depicting blood or gruesome injury. On the internet, the term is used as a catch-all for footage capturing real incidents of extreme body destruction, such as mutilation, work accidents, and zoosadism. The term "medical gore" is sometimes used to refer to particularly graphic real-life medical imagery, such as intense surgical procedures. The term is often considered a synonym for “graphic violence”, but some people or organizations distinguish between the terms “gore” and “graphic violence”. One example is Adobe Inc., which separates the terms “gore” and “graphic violence” for its publication service. Another example is the news site The Verge. It separates the term “gore” and “violence” when reporting the closure of LiveLeak, a website that was often used to host gore videos before its closure. The distribution of gore content has resulted in legal prosecution and sentencing. Hurtcore Hurtcore, a portmanteau of the words “hardcore” and “hurt”, is a name given to a particularly extreme form of child pornography, usually involving degrading violence, bodily harm, and murder relating to child sexual abuse. Graphic footage and documentary Some documentary films or photos contain graphic violence. Examples of graphic documentaries and footages are war and crime. Unlike gore contents, sharing graphic documentary and footage is legal, although the publication of graphic footage and documentary caused debates and complaints. Media Graphic violence generally consists of any clear and uncensored depiction of various violent acts. Commonly included depictions include murder, assault with a deadly weapon, dismemberment, accidents which result in death or severe injury, suicide, and torture. In all cases, it is the explicitness of the violence and the injury inflicted which results in it being labeled "graphic". In fictional depictions, appropriately realistic plot elements are usually included to heighten the sense of realism (i.e. blood effects, prop weapons, CGI). In order to qualify for the "graphic" designation, the violence depicted must generally be of a particularly unmitigated and unshielded nature; an example would be a video of a man being shot, bleeding from the wound, and crumpling to the ground. Graphic violence arouses strong emotions, ranging from titillation and excitement to utter revulsion and even terror, depending on the mindset of the viewer and the method in which it is presented. A certain degree of graphic violence has become de rigueur in adult "action" genre, and it is presented in an amount and manner carefully deliberated to excite the emotions of the target demographic without inducing disgust or revulsion. Even more extreme and grotesque acts of graphic violence (generally revolving around mutilation) are often used in the horror genre in order to inspire even stronger emotions of fear and shock (which the viewing demographic would presumably be seeking). It is a highly controversial topic. Many believe that exposure to graphic violence leads to desensitization to committing acts of violence in person. It has led to censorship in extreme cases, and regulation in others. One notable case was the creation of the US Entertainment Software Rating Board in 1994. Many nations now require varying degrees of approval from television, movie, and software rating boards before a work can be released to the public. On the other hand, some critics claim that watching violent media content can be cathartic, providing "acceptable outlets for anti-social impulses". Film Graphic violence is used frequently in horror, action, and crime films. Several of these films have been banned from certain countries for their violent content. Though violence in films is not an old topic, a recent study presented in an annual American Academy of Pediatrics conference showed that the "good guys" in superhero movies were on average more violent than the villains, potentially sending a strongly negative message to young viewers. News media News media on television and online video frequently cover violent acts. The coverage may be preceded with a warning, stating that the footage may be disturbing to some viewers. Sometimes graphic images are censored, by blurring or blocking a portion of the image, cutting the violent portions out of an image sequence or by removing certain portions of film footage from viewing. Music videos Graphic and gory violence has started appearing in music videos in recent times, an example being the controversial music video for the song "Rock DJ" by British rock vocalist Robbie Williams, which features self-mutilation. Another example of a music video containing strong violence is the music video for the song "Hurricane" by American rock band Thirty Seconds to Mars and "Happiness in Slavery" by American industrial rock group Nine Inch Nails. The music video for "Forced Gender Reassignment" by American deathgrind band Cattle Decapitation displays such intense graphic violence that it is not hosted by many popular video hosting sites like YouTube and Dailymotion and is only hosted by Vimeo. Video games Violent content has been a central part of video game controversy. Because violence in video games is interactive and not passive, critics such as Dave Grossman and Jack Thompson argue that violence in games hardens children to unethical acts, calling first-person shooter games "murder simulators", although no conclusive evidence has supported this belief. An example is the display of "gibs" (short for giblets), little bits or giant chunks of internal organs, flesh, and bone, when a character is killed. Internet On the internet, several sites dedicated to recordings of real graphic violence, referred to as "gore", exist, such as Bestgore.com and Goregrish.com. Furthermore, many content-aggregator sites such as Reddit or imageboards and 4chan have their own subsites which are dedicated to or allow that kind of content. Some of those sites also require that gore material to be marked as it, often by the internet slang "NSFL" (shorthand for "not safe for life"). This kind of media might depict reality footage of war, car crashes and other accidents, decapitations, suicide, terrorism, murder, or executions. A flash-animated web series titled Happy Tree Friends, the pioneer show of Mondo Media, is known for its graphic violence. See also Effects of violence in mass media Extreme cinema Influence of mass media Motion picture content rating system Pornographic film Snuff film Splatter film Television content rating system Video game content rating system Violence in art == References == Change and continuity is a classic dichotomy within the fields of history, historical sociology, and the social sciences more broadly. The question of change and continuity is considered a classic discussion in the study of historical developments. The dichotomy is used to discuss and evaluate the extent to which a historical development or event represents a decisive historical change or whether a situation remains largely unchanged. A good example of this discussion is the question of how much the Peace of Westphalia in 1648 represents an important change in European history. In a similar vein, historian Richard Kirkendall once questioned whether FDR's New Deal represented "a radical innovation or a continuation of earlier themes in American life?" and posed the question of whether "historical interpretations of the New Deal [should] stress change or emphasize continuity?" The issue here is if the New Deal marks something radically new (change) in US history or if the New Deal can be understood as a continuation (continuity) of tendencies in American history that were in place well before the 1930. The dichotomy is important in relation to constructing, discussing, and evaluating historical periodizations. In terms of creating and discussing periodization (e.g. the Enlightenment or the Victorian Era,) the dichotomy can be used to assess when a period can be said to start and end, thus making the dichotomy important in relation to understanding historical chronology. Economic historian Alexander Gerschenkron has taken issue with the dichotomy, arguing that continuity "appears to mean no more than absence of change, i.e. stability." German historian Reinhart Koselleck, however, has been said to challenge this dichotomy. == Notes == Women in musicology describes the role of women professors, scholars and researchers in postsecondary education musicology departments at postsecondary education institutions, including universities, colleges and music conservatories. Traditionally, the vast majority of major musicologists and music historians have been men. Nevertheless, some women musicologists have reached the top ranks of the profession. Carolyn Abbate (born 1956) is an American musicologist who did her PhD at Princeton University. She has been described by the Harvard Gazette as "one of the world's most accomplished and admired music historians". Susan McClary (born 1946) is a musicologist associated with the "New Musicology" who incorporates feminist music criticism in her work. McClary holds a PhD from Harvard University. One of her best known works is Feminine Endings (1991), which covers musical constructions of gender and sexuality, gendered aspects of traditional music theory, gendered sexuality in musical narrative, music as a gendered discourse and issues affecting women musicians. In the book, McClary suggests that the sonata form (used in symphonies and string quartets) may be a sexist or misogynistic procedure that constructs of gender and sexual identity. McClary's Conventional Wisdom (2000) argues that the traditional musicological assumption of the existence of "purely musical" elements, divorced from culture and meaning, the social and the body, is a conceit used to veil the social and political imperatives of the worldview that produces the classical canon most prized by supposedly objective musicologists. American musicologist Marcia Citron has asked "[w]hy is music composed by women so marginal to the standard 'classical' repertoire?" Citron "examines the practices and attitudes that have led to the exclusion of women composers from the received 'canon' of performed musical works." She argues that in the 1800s, women composers typically wrote art songs for performance in small recitals rather than symphonies intended for performance with an orchestra in a large hall, with the latter works being seen as the most important genre for composers; since women composers did not write many symphonies, they were deemed to be not notable as composers. Other notable women scholars include: Eva Badura-Skoda Ita Beausang Margaret Bent Suzanne Cusick Tina Frühauf Ursula Günther Maud Cuney Hare Barbara L. Kelly Liudmila Kovnatskaya Elizabeth Eva Leach Kendra Preston Leonard Carol Oja Nancy Reich Rosetta Reitz Joan Rimmer Elaine Sisman Hedi Stadlen Rose Rosengard Subotnik Judith Tick Anahit Tsitsikian Reba Wissner Eileen Southern Josephine Wright Ethnomusicologists Ethnomusicologists study the many musics around the world that emphasize their cultural, social, material, cognitive, biological, and other dimensions or contexts instead of or in addition to its isolated sound component or any particular repertoire. Ethnomusicology – a term coined by Jaap Kunst from the Greek words ἔθνος (ethnos, "nation") and μουσική (mousike, "music") – is often described as the anthropology or ethnography of music. Initially, ethnomusicology was almost exclusively oriented toward non-Western music, but now includes the study of Western music from anthropological, sociological and intercultural perspectives. Notable ethnomusicologists include: Judith Becker Frances Densmore Joanna Everharda La Rivière Fourie Ida Halpern Maud Karpeles Joan Rimmer Janet E Tobitt Ellen Koskoff == References == An Essay on the History of Civil Society is a book by Scottish Enlightenment philosopher Adam Ferguson, first published in 1767. The Essay established Ferguson's reputation in Britain and throughout Europe. In the second section of the third part of the Essay, while discussing the history of political establishments, Ferguson states "Every step and every movement of the multitude, even in what are termed enlightened ages, are made with equal blindness to the future; and nations stumble upon establishments, which are indeed the result of human action, but not the execution of any human design." Contents Part I. Of the General Characteristics of Human Nature. Part II. Of the History of Rude Nations. Part III. Of the History of Policy and Arts. Part IV. Of Consequences that result from the Advancement of Civil and Commercial arts. Part V. Of the Decline of Nations. Part VI. Of Corruption and Political Slavery. Reception The Essay was critically acclaimed upon publication with a wide readership for about thirty years after it was published. Voltaire praised Ferguson for "civilizing the Russians" as it was being taught in the University of Moscow. David Hume, a friend of Ferguson's and an admirer of his earlier Essay on Refinement (1759), disliked the book. Ferguson's writings on the division of labour in Part IV influenced Karl Marx. == Notes == The study of global communication is an interdisciplinary field focusing on global communication, or the ways that people connect, share, relate, and mobilize across geographic, political, economic, social, and cultural divides. Global communication implies a transfer of knowledge and ideas from centers of power to peripheries and the imposition of a new intercultural hegemony by means of the "soft power" of global news and entertainment... "International" Or "Global" With the end of the twentieth century and the turn of a new millennium, the global arena and the field of international communication were undergoing significant changes. Some authors started to use the term global communication because it goes beyond the bounds of individual states and emphasizes communication between and among peoples across borders and, importantly, the rise of transnational media corporations. International communication traditionally refers to communication between and among nation-states and connotes issues of national sovereignty, control of national information resources, and the supremacy of national governments. Nevertheless, earlier International communication theories have failed to develop models or research agendas that match the reality of the contemporary role of global communication . The old theories only explain part of the global picture and the theories of modernization, dependency, and cultural imperialism have failed to satisfactorily explain global communication. The term "global", implies a declining role of the state and state sovereignty. As a term, "international" has within it notions of bilateral or multilateral decisions. "Global" could be seen as an aspiration, also as a fear, of the weakening of the state. In addition, global may imply something more pervasive, more geographically inclusive than international. History The study of global communication increased dramatically after World War II due to military considerations coupled with their economic and political implications. Earlier attempts at theorizing have failed to develop models or research agendas that match the reality of the contemporary role of global communication. More global communication research was written in the decade from 1945 to 1955; most of the research of the 1950s dealt with propaganda and the cold war. By 1970, global communication research had grown to include a great variety of subjects, especially comparative mass communication systems, communication and national development and propaganda and public opinion. From the point of view of global communication scholars, previous theories of modernization, dependency, and cultural imperialism have failed to satisfactorily explain global communication. The old theories only explain part of the global picture. Technological development The emergence of global communication technologies may be considered the origin of the field of global communication in the nineteenth century. Numerous technical advances such as the creation of a new major global communication phenomenon, convergence, digital environments and the internet are some of the major engines driving the change from international communication to global communication. Global power shifts With the collapse of the Soviet Union, the shadow of Cold War has lifted to reveal shifting political, economic, and cultural alliances and conflicts. The increasing importance of these currents, especially in the cultural sphere, demands a reconsideration of the nature of the international communication field within the rubric of international relations. News agencies and propaganda Three key players are usually recognized as the founders of the international news agencies. In 1835, Charles-Louis Havas created the world's first news agency; In 1849, Bernhard Wolff started publishing stock market news and daily reports from Paris, London, Amsterdam, and Frankfurt; In 1849, Paul Julius Freiherr von Reuter established his own commercial service, the Reuter agency, and organized a worldwide exchange of news in 1870. In 1859, Reuter, Havas and the German Wolff agency reached an agreement to exchange news from all over the world, which was known as the League of Allied Agencies, or the " Ring Combination". In 1848, American News Agency Associated Press was founded and was formally admitted into the "Ring Combination" in 1887. There are some major factors that point to the growing importance of global communication in the world of the twenty-first century: world population explosion from geopolitics to gaiapolitics increased cross-cultural communication changing concept of community greater centralization of control information explosion changes in technologies greater dependence on global communication greater interdependence and democracy impact of communication on peace and war Theoretical approaches and perspectives Transcultural political economy Transcultural Political Economy is a concept that is presented in Global Communications by Paula Chakravartty and Yeuzhi Zhao. This concept looks at global communications and media studies in three major areas: global flows of information and culture, decentralizing the conceptual parameters of global information and media studies, and the normative debates in global communications in the context of neoliberalism. Transcultural Political Economy is a multidisciplinary study that focuses on the tensions between political economy and cultural studies. It "integrate[s] institutional and cultural analyzes and address urgent questions in global communications in the context of economic integration, empire formation, and the tensions associated with adapting new privatized technologies, neoliberalized and globalized institutional structures, and hybrid cultural forms and practices". Transcultural Political Economy addresses the issues surrounding the practice of neoliberalism and its creation of unequal power structures within the world system. Globalization theory Globalization theory was popularized in the 1990s as a model for understanding global communication. The concept of globalization inspired a number of theories from various schools of thought in communication studies that each emphasize different aspects of globalization. Many globalization theories highlight actors in the business sector as leaders in the processes of global integration. Transnationalizing business is often celebrated as progression toward a more interconnected world. Globalization theories are often associated with theories of modernity. Some scholars view globalization as the social, political, economic, and cultural integration of societies into a capitalist system; Others see globalization as a successor to modernity, while some see it as an iteration of imperialism. Some question the usefulness and legitimacy of globalization theory, arguing that it does not adequately conceptualize current international relations or function as a lens through which to examine everyday events. Many scholars criticize globalization theories as overzealous toward and unrealistic about the extent of global integration. Some scholars criticize social theorists for offering opinions and predictions based on theory with little practical evidence. In contrast, some scholars work to dispute the pessimistic views of globalization theory. World systems theory World-system theory is a macro-sociological perspective that seeks to explain the dynamics of the "capitalist world economy" as a "total social system". A world-system is what Wallerstein terms a "world-economy", integrated through the market rather than a political centre, in which two or more regions are interdependent with respect to necessities like food, fuel, and two or more polities compete for domination without the emergence of one single centre forever. World-system theory was first articulated by Immanuel Wallerstein. There are three major sources of the world-system theory which conceived by Wallerstein: the Annales school's general methodology, Marx's focus on accumulation process and competitive class struggles and so on, and dependence theory's neo-Marxist explanation of development processes. Referring to the transnational division of labor, world-system divides the world into core countries, peripheral countries, semi-peripheral countries and external areas. The core countries usually developed a strong central governments, extensive bureaucracies and large mercenary armies, which permit the local bourgeoisie to obtain control over international commerce and extract capital surpluses from the trade for benefits. The peripheral countries often lack strong central governments or been controlled by core countries, they export raw materials and rely on coercive labor practices. Semi-peripheries which served as buffers between the core and the peripheries. They retain limited but declining access to international banking and the production of high-cost high-quality manufactured goods.[3] External areas such as Russia maintain their own economic systems, they want to remain outside the modern world economy. Modernisation theory The theory of modernisation was developed by Daniel Lerner (1958) in The Passing of Traditional Society. According to Lerner, a “modernised” individual possesses the ability to empathise—that is, to imagine oneself in another person’s situation. This idea is rooted in the broader shift from traditional to modern societies. Modern societies are characterised by industrialisation, urbanisation, literacy, and participatory civic engagement. Modernisation theory views development as a linear process, asserting that nations must transform into modern societies in order to become sustainable and prosperous. Such development involves technological advancement and the growth of media systems, which play a key role in fostering a participatory culture. Post-colonialism Post-colonialism is a theoretical approach to looking at literature that examines the colonizer-colonized experience. It deals with the adaptation of formerly colonized nations and their development in cultural, political, economical aspects. Some Notable theoreticians include: Frantz Fanon, Edward Said, Gayatri Spivak, R Siva Kumar, Dipesh Chakrabarty, Derek Gregory. Cultural imperialism Cultural imperialism refers to the process by which a dominant civilisation exerts its cultural influence over another. Less economically prominent cultures often import elements of culture from Western countries, which possess the economic means to produce much of the world’s cultural media, primarily through global media transmission In this process, the less dominant civilisation adopts the customs, philosophies, worldviews and general ways of life of the dominant one. The theoretical foundations of the academic study of cultural imperialism are largely derived from Michel Foucault’s concepts of biopower and governmentality, as well as Edward Saïd’s notion of post-colonialism, with these theories interpreting cultural imperialism as a legacy of colonialism or as a form of Western hegemony. The core argument of cultural imperialism is based on media effects studies integrated with the traditional political economy approach. These studies suggest that there are two opposing effects: the negative effect, in which Western media imposes socio-political conflicts on developing countries and prompts resistance in order to preserve traditional cultural identities, and the positive effect, which involves exposure to Western media contributing to progressive issues such as women’s rights and racial equality. In contemporary usage, the term cultural imperialism most often refers to the global expansion of American culture, which includes brand name products, video media, fast food, and so on. Communication for development (C4D) Communication for Development (C4D) is a praxis oriented aspect of global communication studies that approaches global development with a focus on action and participation for social change enacted through communication systems. C4D underlines "voice, citizenship and collective action" as central values that promote citizen-led development where the visiting party provides guidance rather than direction within the host community. C4D often incorporates bottom-up theories of social change with the aim to create sustainable change which is believed to be more likely to occur if the efforts are planned, implemented, and sustained by community members themselves. Some development workers and academics suggest that a shared definition of communication for development should be clarified, because disagreement within the field can detract from the characteristics that most scholars view as central to current development, including participatory action research (PAR). Many C4D projects revolve around media systems as a central site for social change, which differentiates C4D from other approaches to development. Theories behind C4D highlight that development projects should be contextually situated and that communication technology will affect different types of social change accordingly. Global media studies Global media studies is a field of media study in a global scope. Media study deals with the content, history and effects of media. Media study often draws on theories and methods from the disciplines of cultural studies, rhetoric, philosophy, communication studies, feminist theory, political economy and sociology. Among these study approaches, political economic analysis is non-ignorable in understanding the current media and communication developments. But the political economic research has become more resilient because of stronger empirical studies, and the potential connections to policy-making and alternative praxis. Each country has its own distinct media ecosystem. The media of mainland China is state-run, so the political subjects are under the strict regulations set by the government while other areas such as sports, finance, and increasingly lucrative entertainment industry face less regulation from government. Canada has a well-developed media sector, but the mass media is threatened by the direct outcome of American economic and cultural imperialism which hinder the form of Canada's media identity. Many of the media in America are controlled by large for-profit corporations who reap revenues from advertisings, subscriptions and the sale of copyrighted materials. Currently, six corporations (Comcast, The Walt Disney Company, News Corporation, Time Warner, Viacom and CBS Corporation) have controlled roughly 90% of the America media. Such figures come from the policies of the federal government or the tendency to natural monopolies in the industry. Central debates Global power shifts Immanuel Wallerstein's world system theory develops a basic framework to understand global power shifts in the rise of the modern world. Wallerstein proposes four different categories: core, semi-periphery, periphery, and external, in terms of different region's relative position in the world system. The core regions are the ones that benefited the most from the capitalist world economy, such as England and France. The peripheral areas relied on and exported raw materials to the core, such as Eastern Europe and Latin America. The semi-peripheries are either core regions in decline or peripheries attempting to improve their relative position in the world system, such as Portugal and Spain. The external areas managed to remain outside the modern world economy, such as Russia. There are two basic types of global power shifts in the 21st century. One is traditional power transition amongst states, which follows Wallerstein's world system theory. For instance, the global power shifts from the West to the East since the rise of Asia. The other is power diffusion, the way that power move from states to non-states actors. For instance, "climate change, drug trade, financial flows, pandemics, all these things that cross borders outsider the control of governments." Global public sphere Public sphere theory, attributed to Jurgen Habermas, is a theory that in its basic premise conceives of democratic governments as those that can stand criticism that comes from public spheres. Public spheres are places, physical or imagined, where people discuss any kind of topic, particularly topics of a societal or political nature. Global public sphere is, therefore, a public that is made of people from across the globe, who come together to discuss and act on issues that concern them. The concept of global public sphere is linked to the shift of public sphere, from restricted to nation-state, to made of individuals and groups connected across as well as within borders. Since Plato, it can be argued that philosophers have been thinking about versions of a common space for all people to debate in; however, a global public sphere that can fit the description above began to appear much later. In the second half of the 20th century, the legacy of World War II and technological advancements created a new sense of the global and started the economic and political phenomena that we now call globalization. This includes the expansion of humankind into space, which gave individuals the sense of a global unity, the growth of satellite technology, which allowed for people across the globe to view the same television channels, and the internet, which can provide access to an unprecedented amount of information and spaces to connect with other people. Cultural industries The term "culture industry" appeared in the post-war period. At that time, culture and industry were argued to be opposites. "Cultural industries" are also referred to as the "Creative industries". Definitions and scope of cultural industries In the present day, there remain different interpretations of culture as an industry. For some, cultural industries are simply those industries that produce cultural goods and services. In the United Nations Educational, Scientific and Cultural Organization (UNESCO), the cultural industries are regarded as those industries that "combine the creation, production and commercialization of contents which are intangible and cultural in nature. These contents are typically protected by copyright and they can take the form of goods or services". According to UNESCO, an essential part of cultural industries is that they are "central in promoting and maintaining cultural diversity and in ensuring democratic access to culture". "Cultural industries" combines the cultural and economic, which gives the cultural industries a distinctive profile. In France, the "cultural industries" have recently been defined as a set of economic activities that combine the functions of conception, creation and production of culture with more industrial functions in the large-scale manufacture and commercialization cultural products. In Canada, economic activities involved in the preservation of heritage are also included in the definition of culture. Global cultural industries Since the rise of the cultural industries has occurred simultaneously with economic globalization, cultural industries have close connections with globalization and global communication. Herbert Schiller argued that the 'entertainment, communications and information (ECI) complex were having a direct impact on culture and human consciousness. As Schiller argued, the result of transnational corporate expansion is the perpetuation of cultural imperialism, defined as "the sum of the processes by which a society is brought into the modern world system and how its dominating stratum is attracted, pressured, forced, and sometimes bribed into shaping social institutions to correspond to, or even promote, the values and structures of the dominating centre of the system". The second wave of transnational corporate expansion, which began in the 1970s with the emergence of Export Processing Zones in developing countries, is focused on the development of global production networks. This process was described as a "new international division of labour" (NIDL) by the German political economists Frӧbel et al. (1980). Ernst and Kim have argued that GPNs are changing the nature of the multinational corporation itself, from "stand alone overseas investment projects, to "global network flagships" that integrate their dispersed supply, knowledge and customer bases into global and regional production networks", entailing a shift from top-down hierarchical models of corporate control to increasingly networked and collective forms of organization. Global media empires The largest firms in media and media-related industries have a very high international profile. Global media empires such as Disney, News Corporation, Time-Warner and Viacom-CBS now derive 25-45 per cent of their revenues outside of the United States. It is often argued that the global media are dominated by a small number of powerful media conglomerates. Edward S. Herman and Robert W. McChesney (1997) argued that the global media were "dominated by three or four dozen large transnational corporations (TNCs) with fewer than ten mostly US-based media conglomerates towering over the global market." Similarly, Manfred Steger has observed that " to a very large extent, the global cultural flows of our time are generated and directed by global media empires that rely on powerful communication technologies to spread their message." He also argued that during the last two decades, a few very large TNCs would come to dominate the global market for entertainment, news, television, and film. Diaspora Diaspora is often confused with exodus. Diasporas are minority groups that have a sense of connection with a larger community outside of the borders they currently inhabit, and through diasporic media create a sense of a larger identity and community, whether imagined or real. In scholarly work about diaspora in communication studies, the view of nation and culture as interchangeable terms is no longer prevalent. Stuart Hall theorized of hybridity, which he distinguished from "old style pluralism", "nomadic voyaging of the postmodern", and "global homogenization". Hybridity is the retention of an original identity and strong ties to an original country and tradition, but with the understanding that there is no unchanged, ideal nation of the past that they can return to. To be hybrid is to also adapt to a new culture and tradition without simply assimilating in it, but rather negotiating a place between the "original" and "new" cultures. In Communication studies, diaspora is discussed as the identity that unifies people across time and space, sometimes existing in physical spaces and other times existing in imagined 'non-spaces'. However, it has been argued that the concept of 'diaspora' implies ethnic homogeneity and essentializes identity to only ethnicity. One of the most cited and well-known works in the field of diasporic media is Hamid Naficy's work on exiled Iranian Americans' creation of cable television in the United States. Diasporic media refer to media that address the needs of particular ethnic, religious, and/or linguistic groups that live in multicultural settings . Diasporic media can be in the diaspora's traditional language or in another language, and they can include news or media from the "origin" country or they can contain the diaspora's local news or media. Diasporic media can be created in radio, television, film, music, in newspapers, magazines, and other publishing, as well as online. It can be argued that the development and spread of satellite television is an instrumental element of the growth of diasporic media today. Satellite television allowed migrants to access the news and popular culture from their homeland, as well as allowing people who speak the same language to access the same channels that might be produced outside of the "homeland" Contemporary studies of diaspora show that diasporic media are part of the change in the tendency Immanuel Wallerstein described in his world systems theory. The world systems theory postulates that much of the flow of people in the world has been from the 'periphery', or economically-developing states, towards the centre; which are often metropolitan, economically-wealthy states that grew their wealth in colonialist entrepreneurship. However, contrary to the movement of people, the flow of information (including media products), has tended to be from the centre to the periphery. Technology and media The advancement of media and technology have played the pivotal role in process of globalization and global communication. Cable television, ISDN, digitalization, direct broadcast satellites as well as the Internet have created a situation where vast amounts of information can be transferred around the globe in a matter of seconds. During the early 20th century, telegraph, telephony, and radio started the process of global communication. As media technologies developed intensely, they were thought to create, in Marshall McLuhan’ s famous words, a ‘‘global village.’’ The launch of Sputnik, the world’ s first artificial satellite, on October 4, 1957, marked the beginning of technologies that would further interconnect the world. The first live global television broadcast occurred when Neil Armstrong stepped onto the moon in July 1969. In November 1972, pay TV caused expansion of cable when Service Electric offered Home Box Office over its cable system. By 2000, over direct broadcast satellite, a household could receive channels from all over the world. Now with the World Wide Web, smart phones, tablet devices, smart televisions and other digital media devices, billions of people are now able to access media content that was once tied to particular communications media (print, broadcast) or platforms (newspapers, magazines, radio, television, cinema). Justice Justice in communication studies includes, but is not limited to, the concern with democratic process and fostering democratic publics . Jurgen Habermas theorized of public sphere (in The Structural Transformation of the Public Sphere) as the space that is created whenever matters of common concern are discussed between the state and civil society. Thus, public sphere includes not only the media, but also public protest in the form of marches, demonstrations, et cetera. There are, however, critiques of political economy in whose view it is impossible to work within the current system to produce democratic publics. Such a critique is that produced by Karl Marx, who saw institutions such as parliament, the state, the 'acceptable' public sphere, economic enterprises, and so on as structurally produced and perpetuated by a capitalist system, and thus they can not be mobilized to change it. In such a system, there can only be illusory justice, which is fair only within the logic of the system. This illusion of justice is produced through dominating ideology. Another issue of justice in communication studies is the question of decolonizing research methods and theoretical discourse . The idea of decolonizing research comes from a rejection of the functionalist approach, which assumed that research can be conducted in a vacuum, free of ideology or the researcher's biases. This approach assumed cultures to be unchanging, homogenous, and isolated from each other. The purpose of decolonizing research and discourse is to 'uncloak' research as an unbiased power structure, and produce research that is more self-aware. The approach in decolonizing research methods attempts to create methodologies that treat the people in the study as participants or partners, rather than subjects - which is a term that in itself carries strong connotations of colonialism. Decolonizing research also involves moving away from Eurocentric models that are assumed to work anywhere else, and instead to create work that is more useful in local contexts. Decolonial approaches specifically seek to produce knowledge about the mechanisms and effects of colonialism. These approaches allow former subjects to 'talk back', which is a reflection of independent agency, on the colonizer's own terms of research, rather than to be 'given' a voice, which is an unequal power structure. References External links Institutions, programs, and associations Simon Fraser University, Communication University of China – Global Communication MA Double Degree Program Annenberg School for Communication in University of Pennsylvania - Center for Global Communication Studies Oxford University – Global Media Chinese University of Hong Kong - Master of Arts in Global Communication LSE - MSc Double Degree in Global Media and Communications with Annenberg School, USC or Fudan University Saint Petersburg State University (SPbU), Freie Universität Berlin (FU) - Global Communication and International Journalism, Double Degree Master's Program The George Washington University - Global Communication M.A Program University of Washington - Global Communication Brown University - Brown +1: CUHK's Master of Arts in Global Communication Kennesaw State University - Master of Arts in Integrated Global Communication Utah State University - Global Communication: BA New York University Steinhardt School - Global and Transcultural Communication United Nations University(Tokyo) - Fukushima Global Communication Program University of Helsinki - Master's Degree Program in Media and Global Communication Tilburg University - Master of Global Communication University of Canberra Summer Program - Global Communications University of Erfurt - Global Communication M.A. University of Melbourne - Master of Global Media Communication In the field of physical security, security lighting is lighting that intended to deter or detect intrusions or other criminal activity occurring on a property or site. Planning considerations Security lighting to prevent intrusions may be counter-productive. Turning off lights halved the number of thefts and burglary in Övertorneå Sweden. A test in West Sussex UK showed that adding all-night lighting in some areas made people there feel safer, although crime rates increased 55% in those areas compared to control areas and to the county as a whole. In the early seventies, the public-school system in San Antonio, Texas, began leaving many of its school buildings, parking lots, and other property dark at night and found that the no-lights policy not only reduced energy costs but also dramatically cut vandalism. Bright, unshielded floodlights often prevent people from noticing criminal activity, and help criminals see what they are doing. While adequate lighting around a physical structure is deployed to reduce the risk of an intrusion, it is critical that the lighting be designed carefully as poorly arranged lighting can create glare which actually obstructs vision. Studies have shown that many criminals are aware of this effect and actively exploit it. The optimal design will also depend on whether the area will be watched directly by humans or by closed-circuit television, and on the location of the observers or cameras. Security lighting may be subject to vandalism, possibly to reduce its effectiveness for a subsequent intrusion attempt. Thus security lights should either be mounted very high, or else protected by wire mesh or tough polycarbonate shields. Other lamps may be completely recessed from view and access, with the light directed out through a light pipe or reflected from a polished aluminium or stainless steel mirror. For similar reasons high security installations may provide a stand-by power supply for their security lighting. Some typical considerations include: Reduce and prevent glare and situations mentioned above Shielded or full cut-off (FCO) lamp housings which conceal the bulb could be used, which should direct light onto the ground or target and away from observers. These lights should send no light above 80 degrees from the nadir. Lighting should be bright enough, and not "as bright as possible". In many cases a good rule of thumb is 0.5 watts per square metre (0.05 watts per square foot). This might need to be increased in complex environments, but conversely can be reduced in very open environments. Multiple lamps of moderate power instead of a few powerful lamps will reduce glare, provide more even illumination with reduced pools of shadow, and provide some redundancy if one lamp's bulb blows out or develops a bad ballast. Prevent malicious tampering or interference. This means that besides the lamp itself, the entire circuit from the source (electric company or generator), through the wires, to the lamp and back should be protected. Luminaires should be accessible so that the maintainer can replace blown bulbs as quickly as possible and clean the luminaires periodically. However they should be protected or somehow made inaccessible to tampering. Ensure the electric meter box is locked or inaccessible, or else power the lights from a different line. Control and power lines, where outside or vulnerable, should be either buried well underground (in conduits preferably) or at a height of at least 8 metres (about 24 feet). Ideally multiple circuits should be used to prevent an accidental or malicious short or cut causing all illumination to fail. Use Security lighting can be used in residential, commercial, industrial, institutional, and military settings. Some examples of security lighting include floodlights and low pressure sodium vapour lights. Most lights intended to be left on all night are high-intensity discharge lamps as these have good energy efficiency, thus reducing the cost of running a lamp for such long periods. A disadvantage of low pressure sodium lamps is that the colour is pure yellow, so the illuminated scene is seen without any colour differentiation. Consequently, high pressure sodium vapour lamps (which are still yellowish, but closer to golden white) are also used, at the cost of greater running expenses and increased light pollution. High pressure sodium lamps also take slightly longer to restrike after a power interruption. LED-based security lighting is becoming increasingly popular, due to its low electrical consumption (compared to non-LED lighting technologies), long lifespan, and options for different color spectrum ranges. Other lights may be activated by sensors such as passive infrared sensors (PIRs), turning on only when a person (or other mammal) approaches. PIR sensor activation can increase both the deterrent effect (since the intruder knows that he has been detected) and the detection effect (since a person will be attracted to the sudden increase in light). Some PIR units can be set up to sound a chime as well as turn on the light. Most modern units have a photocell so that they only turn on when it is dark. To reduce light pollution, the International Dark-Sky Association recommends the use of downward-facing security lights that preserve and protect the night time environment. During the South African energy crisis increased rates of metal theft, house breaking and robberies were reported in areas effected by the loss of security lighting due to a loss of electricity in some urban areas. U.S. NRC, 10 CFR 73.55(i)(6) Illumination For nuclear power plants in the United States (U.S.), per the U.S. Nuclear Regulatory Commission (NRC), 10 CFR Part 73, [security] lighting is mentioned four (4) times. The most notable mentioning contained in 10 CFR 73.55(i)(6) Illumination, which clearly identifies that licensees "-shall provide a minimum illumination level of 0.2 foot-candles, measured horizontally at ground level, in the isolation zones and appropriate exterior areas within the protected area-". [Ref] This is also the minimum illumination level specified in Table H–2 Minimum Night Firing Criteria of 10 CFR 73 Appendix H, for night firing. Per 10 CFR 73.46(b)(7) "-Tactical Response Team members, armed response personnel, and guards shall qualify and requalify, at least every 12 months, for day and night firing with assigned weapons in accordance with Appendix H-"; therefore on said respective shooting range [at night] per Appendix H, Table H-2, "-all courses [shall have] 0.2 foot-candles at center mass of target area-" applicable to handguns, shotguns, and rifles. [Ref] 1 foot-candle is approximately 10.76 lux, therefore the minimum illumination requirements for the above sections also reflect 2.152 lux. Limitations An important limitation to the usefulness of security lighting is the simple fact that it is only useful at night. This is particularly significant for home owners because, contrary to a widespread myth, most household burglaries occur during the day, when the occupants are away at work or shopping. As with any lighting, security lighting can reduce night vision, making it harder to see into areas that are unlit or are in shadow. Non-uniform illumination may also interfere with surveillance systems, as the wide dynamic range of security cameras may have difficulty adjusting to the changes in light intensity. See also Access control Environmental design Light pollution Physical Security Security Security engineering == References == The term cycle of violence refers to repeated and dangerous acts of violence as a cyclical pattern, associated with high emotions and doctrines of retribution or revenge. The pattern, or cycle, repeats and can happen many times during a relationship. Each phase may last a different length of time, and over time the level of violence may increase. The phrase has been increasingly widespread since first popularized in the 1970s. It often refers to violent behaviour learned as a child, and then repeated as an adult, therefore continuing on in a perceived cycle. Within a relationship A cycle of abuse generally follows the following pattern: Abuse – The abuser initiates aggressive, verbal or physical abuse, designed to control and oppress the victim. Guilt – The abuser feels guilty for inflicting abusive behavior, primarily out of a concern of being found guilty of abuse rather than feelings of sympathy for the victim. Excuses – Rationalization of the behavior, including blame and excuses. "Normal" behavior – The abuser regains personal control, creates a peaceful phase in an attempt to make the victim feel comfortable in the relationship. Fantasy and planning – thinking of what the victim has done wrong, how they will be punished, and developing a plan to realize the fantasy. Set-up – the plan is "put in motion." A cyclical nature of domestic violence is most prevalent in intimate terrorism (IT), which involve a pattern of ongoing control using emotional, physical and other forms of domestic violence and is what generally leads victims, who are most often women, to women's shelters. It is what was traditionally the definition of domestic violence and is generally illustrated with the "Power and Control Wheel" to illustrate the different and inter-related forms of abuse. Intimate terrorism is different from situational couple violence, which are isolated incidents of varying degrees of intensity. A general, intricate and complicated cycle of traumatic violence and healing map was developed by Olga Botcharova when she worked at the Center for International Studies. Intergenerational Intergenerational cycles of violence occur when violence is passed from parent to child, or sibling to sibling. Children exposed to domestic violence are likely to develop behavioral problems, such as regressing, exhibiting out of control behavior, and imitating behaviors. Children may think that violence is an acceptable behavior of intimate relationships and become either the abused or the abuser. Recent research has questioned whether certain effects of domestic violence exposure on children are moderated and/or mediated by maternal psychological response such as maternal post-traumatic stress disorder, dissociation, and related biological markers. An estimated 1/5 to 1/3 of teenagers subject to viewing domestic violence situations experience teen dating violence, regularly abusing or being abused by their partners verbally, mentally, emotionally, sexually and/or physically. Thirty to 50% of dating relationships can exhibit the same cycle of escalating violence in their marital relationships. Physical punishment of children has also been linked to later domestic violence. Family violence researcher Murray A. Straus believes that disciplinary spanking forms "the most prevalent and important form of violence in American families", whose effects contribute to several major societal problems, including later assaults on spouses. In politics In 1377, Arab philosopher Ibn Khaldun identified a cycle of violence in which successive dynasties take control of a state and establish asabiyyah or social cohesion, enabling them to expand to the limit. Excess 'pomp' causes the dynasty then to stagnate, become sedentary and collapse, giving way to conquest by a new, more ruthless dynasty. This cycle plays out over the course of three generations. According to John Mearsheimer, the cycle of violence between nations will continue indefinitely because the great powers fear each other, thus compete for power and dominance, in the belief that this will ensure safety. 'Cycle of violence' is also used more generally to describe any long-term factional dispute within a nation in which tit for tat acts of aggression occur frequently, as for example in Argentina in the 1970s, and Lebanon. See also References Further reading Books Engel, Beverly Breaking the Cycle of Abuse: How to Move Beyond Your Past to Create an Abuse-Free Future (2005) Biddix, Brenda FireEagle Inside the Pain: (a survivors guide to breaking the cycles of abuse and domestic violence) (2006) Hameen, Latifah Suffering In Silence: Breaking the Cycle of Abuse (2006) Hegstrom, Paul Angry Men and the Women Who Love Them: Breaking the Cycle of Physical and Emotional Abuse (2004) Herbruck, Christine Comstock Breaking the cycle of child abuse (1979) Marecek, Mary Breaking Free from Partner Abuse: Voices of Battered Women Caught in the Cycle of Domestic Violence (1999) Mills, Linda G. Violent Partners: A Breakthrough Plan for Ending the Cycle of Abuse (2008) Ney, Philip G. & Peters, Anna Ending the Cycle of Abuse: The Stories of Women Abused As Children & the Group Therapy Techniques That Helped Them Heal (1995) Pugh, Roxanne Deliverance from the Vicious Cycle of Abuse (2007) Quinn, Phil E. Spare the Rod: Breaking the Cycle of Child Abuse (Parenting/Social Concerns and Issues) (1988) Smullens, SaraKay Setting Yourself Free: Breaking the Cycle of Emtional Abuse in Family, Friendships, Work and Love (2002) Waldfogel, Jane The Future of Child Protection: How to Break the Cycle of Abuse and Neglect (2001) Wiehe, Vernon R. What Parents Need to Know About Sibling Abuse: Breaking the Cycle of Violence (2002) Academic journals Coxe, R & Holmes, W A study of the cycle of abuse among child molesters. Journal of Child Sexual Abuse, v10 n4 p111-18 2001 Dodge, K. A., Bates, J. E. and Pettit, G. S. (1990) Mechanisms in the cycle of violence. Science, 250: 1678-1681. Egeland, B., Jacobvitz, D., & Sroufe, L. A. (1988). Breaking the cycle of abuse: Relationship predictors. Child Development, 59(4), 1080-1088. Egeland, B & Erickson, M - Rising above the past: Strategies for helping new mothers break the cycle of abuse and neglect. Zero to Three 1990, 11(2):29-35. Egeland, B. (1993) A history of abuse is a major risk factor for abusing the next generation. In: R. J. Gelles and D. R. Loseke (eds) Current controversies on family violence. Newbury Park, Calif.; London: Sage. Furniss, Kathleen K. Ending the cycle of abuse: what behavioral health professionals need to know about domestic violence.: An article from: Behavioral Healthcare (2007) Glasser, M & Campbell, D & Glasser, A & Leitch I & Farrelly S Cycle of child sexual abuse: links between being a victim and becoming a perpetrator The British Journal of Psychiatry (2001) 179: 482-494 Kirn, Timothy F. Sexual abuse cycle can be broken, experts assert.(Psychiatry): An article from: Internal Medicine News (2008) Quayle, E Taylor, M - Child pornography and the Internet: Perpetuating a cycle of abuse Deviant Behavior, Volume 23, Issue 4 July 2002, pages 331 - 361 Stone, AE & Fialk, RJ Criminalizing the exposure of children to family violence: Breaking the cycle of abuse 20 Harv. Women's L.J. 205, Spring, 1997 Woods, J Breaking the cycle of abuse and abusing: Individual psychotherapy for juvenile sex Clinical Child Psychology and Psychiatry, Vol. 2, No. 3, 379-392 (1997) A political argument is an instance of a logical argument applied to politics. Political arguments are used by academics, media pundits, candidates for political office, and government officials. Political arguments are also used by citizens in ordinary interactions to comment on and understand political events. More often than not, political arguments tend to be circular, repeating the same facts as premises under perhaps slightly different guises. Much political argument concerns issues of taxation and government spending. The political argument should be distinguished from propaganda, in that propaganda has little or no structure or the rationale, if it exists, is egregiously fallacious. A classic example of political arguments is those contained in The Federalist Papers arguing in favor of ratification of the American constitution. There are several ways of classifying political argument: Based on the logical structure of the argument. Based on the purpose of the argument. Based on the subject matter dealt with in the argument. Purpose of political argument The purpose of an argument is usually to sway belief. A political argument can occur in the context of political theory; for instance Machiavelli's The Prince can be regarded as advice to rulers based on various kinds of arguments. Political argument though is not generally a purely intellectual activity, since it may also serve the strategic goal of promoting a political agenda. One usually thinks of political argument as exclusive to democracies, but in fact, some kinds of political argument may occur in undemocratic regimes as well, for example, to encourage greater sacrifice from the population, although it is more likely in such cases that propaganda will take the place of argument. In a democracy, though, political argument is particularly important, since there is a direct relationship between the beliefs of citizens and the structure of power. Moreover, the institutions of democracy in part define the relationships between beliefs and power. In this case, political argument is an important element of political strategy. It is also possible that in a democracy, propaganda may also replace argument; indeed, much political advertisement has no discernible logical structure, and in our definition falls under propaganda. This view of political argument in a democracy is closely related to the problem of social choice. Consider a social decision model of the kind used in the theory of social choice (such as used in stating Arrow's theorem). In this model society has a set of individuals X and it faces a set of (political) alternatives A which need to be ranked in some way. A ranking is a relation R between the elements of A which is transitive and reflexive. Two alternatives a, b can satisfy a R b and b R a. If this is the case, we say R is indifferent to a, b. Each individual x in X will have an idiosyncratic ranking Rx of the alternatives in A. A profile is any function P that associates with each individual x a ranking of alternatives Px; a profile is thus a function from X to rankings of A. An arbitration scheme (or constitution or voting scheme) is a way of producing a ranking Rsoc for the whole society from any profile P. Thus an arbitration scheme is itself a function P → Rsoc. Though any argument about politics is in a sense a political argument, an effective political argument is one that can actually change the social preference ranking. Effective political argument is a concept distinct from valid political argument. Example Consider an idealized system of tax policy; tax is based on a tax cutoff point T, that is individuals with income in excess of T pay taxes, and everyone else pays no taxes. In a simple majority rule social arbitration scheme, one might expect that a natural tax rate T can be determined: the median income (plus 1). However, a (possibly fallacious) political argument might attempt to change an individual's voting by their pocketbook by arguing that investment in capital and general welfare will increase by reducing taxes on higher income levels, which is raising the tax cutoff point. A political argument may be ineffective but may still have a purpose, for instance as a justification for an unpopular political action, or as part of a historical narrative. Structure of political argument Any argument claims to prove something. In the case of arguments used in politics, this something is an assertion about an element of the public sphere, such as economic policy, the environment, decisions about war and peace, abortion, etc. An argument cannot start from purely logical principles. An argument is based on premises and some methods for reasoning from premises to conclusions. The validity of an argument in politics can be evaluated in at least two ways: in purely semantic terms or in terms of adherence to certain rules of argument (which we can consider rules of fairness). Semantically, some of the premises used in an argument and the relationships between the assertions in the argument are associated with specific models of economic or political processes. Other premises are moral assumptions: whether a particular action is good or desirable. For example, arguments concerning war must consider questions about specific threats that the adversary poses, the likelihood of success, the cost of war, and so on. In practice, purely semantical evaluations of argument validity are extremely difficult to formulate in a politically neutral way, since political positions usually involve a commitment to some model of social and economic processes. References Hamilton, Alexander; Jay, John; Madison, James (2001). Scigliano, Robert (ed.). The Federalist: A Commentary on the Constitution of the United States. Modern Library. OCLC 1328372842. Arrow, Kenneth J. (1963). Social choice and individual values. Yale University Press. ISBN 0-300-01363-9. OCLC 186435742. {{cite book}}: ISBN / Date incompatibility (help) The Colloquium on Violence and Religion (COV&R) is an international organization dedicated to “exploring, critiquing, and developing” the mimetic theory proposed by the French historian, literary critic, and anthropological philosopher René Girard. Membership includes scholars of theology, religious studies, literary studies, philosophy, psychology, and other academic fields as well as clergy and other practitioners. Girard's work focused on the sources of human violence in mimetic (unconsciously imitative) desire and the centrality of religion in the formation of culture through the management of violence (the single-victim mechanism or scapegoat effect), but the scope of the Colloquium on Violence & Religion's interest has expanded beyond violence to mimetic desire's positive potential and beyond religion to other disciplines. The Colloquium on Violence & Religion is affiliated with regional organizations around the world devoted to Girard's work, mimetic theory, and peacemaking. History The Colloquium on Violence & Religion began with a meeting in 1990 at Stanford University with theologians James G. Williams, Robert Hamerton-Kelly, and Charles Mabee as its three co-founders. When constituted formally in 1991, it formed a board with Girard as honorary chair; Raymund Schwager, a theologian from the University of Innsbruck, as president; Williams as executive secretary; and Wolfgang Palaver (de), also a theologian from Innsbruck, as editor of the newsletter. Prominent board members have included James Alison, Eric Gans, and Walter Wink. Publications Michigan State University Press publishes the annual journal of Colloquium on Violence & Religion, Contagion: Journal of Violence, Mimesis, and Culture (ISSN 1930-1200) and two related series of books: Breakthroughs in Mimetic Theory and Studies in Violence, Mimesis, and Culture. Colloquium on Violence & Religion also publishes a quarterly online newsletter, The Bulletin of the Colloquium on Violence and Religion. A complete bibliography is included in the fully searchable Index Theologicus database. Annual meeting Colloquium on Violence & Religion holds an annual summer meeting, usually in July. The location has recently rotated in a three-year cycle between sites in North America, Europe, and the rest of the world. It also meets in conjunction with the annual meeting of the American Academy of Religion in November. Presidents 1991–1995: Raymund Schwager 1995–1999: Cesário Bandera 1999–2003: Diana Culbertson 2003–2007: Sandor Goodhart 2007–2011: Wolfgang Palaver (de) 2011–2015: Ann W. Astell 2015–2019: Jeremiah Alberg 2019–2023: Martha Reineke References External links Official website Index Theologicus bibliography of mimetic theory A cultural system is the interaction of different elements in culture. While a cultural system is very different from a social system, sometimes both systems together are referred to as the sociocultural system. Social theory A major concern in the social sciences is the problem of order. One way that social order has been theorized is according to the degree of integration of cultural and social factors. Action theory Talcott Parsons, a major figure in sociology and the main originator of action theory in the early 20th century, based his sociological theory of action system is built up around a general theory of society, which is codified within a cybernetic model featuring four functional imperatives: adaptation, goal-attainment, integration, and pattern maintenance. The hierarchy of systems is, from least to most encompassing system, respectively, behavioral organism, personality system, social system, and cultural system as well. Ritzer and Goodman (2004) summarize Parsons' view, "Parsons saw these action systems acting at different levels of analysis, starting with the behavioral organism and building to the cultural system. He saw these levels hierarchically, with each of the lower levels providing the impetus for the higher levels, with the higher levels controlling the lower levels." In an article, late in life, Parsons maintained that the term "functionalism" was an inappropriate characterization of his theory. System and social integration The British Sociologist David Lockwood argued for a contrast between social content and social transmission in his work on social structure and agency. Noting that social systems were distinct in structure and transmission. Lockwood's conceptual distinction influenced Jürgen Habermas' discussion in the classic Legitimation Crises, who made the now famous distinction between system integration and social integration of the lifeworld. Cultural and socio-cultural integration Margaret Archer (2004) in a revised edition of her classic work Culture and Agency, argues that the grand idea of a unified, integrated culture system, as advocated by early Anthropologists such as Bronisław Malinowski and later by Mary Douglas, is a myth. Archer reads this same myth through Pitirim Sorokin's influence and then Talcott Parsons' approach to cultural systems (2004:3). The myth of a unified, integrated cultural system was also advanced by Western Marxists such as by Antonio Gramsci through the theory of cultural hegemony through a dominant culture. Basic to these mistaken conceptions was the idea of culture as a community of meanings, which function independently in motivating social behavior. This combined two independent factors, community and meanings which can be investigated quasi-independently (2004:4) Archer, a proponent of critical realism, suggests that cultural factors can be objectively studied for the degree of compatibility (and that various aspects of cultural systems may be found to contradict each other in meaning and use). And, social or community factors in socialization may be studied in the context of the transmission of cultural factors by studying the social uniformity (or lack thereof) in the transmitted culture. Cultural systems are used (and inform society) both through idea systems and the structuring of social systems. To quote Archer in this regard: "logical consistency is a property of the world of ideas; causal consistency is a property of people. The main proposition here is the two are logically and empirically distinct and, hence can vary independently of one another. Thus it is perfectly conceivable that any social unit, from a community to a civilization, could be found the principle ideational elements (knowledge, belief, norms, language, mythology, etc.) of which do display considerable logical consistency – that is, the components are consistent, not contradictory – yet the same social unit may be low on causal consensus. " (2004:4) Archer notes that the opposite may be the case: low cultural logical consistency and high social consistency. Complex societies can include complex sociocultural systems that mix of cultural and social factors with various levels of contradiction and consistency. Research According to Burrowes (1996), in two recent approaches to the study of culture, in the 1980-1990s, the "cultural studies" and "cultural indicators" approaches, investigators explored the traditionally functionalist concern of "cultural systems integration." These two approaches could be synthesized in the investigating cultural systems. Burrowes (1996) writes, "If functionalism offers to this cross-fertilization a focus on the normative orders of society, the cultural indicators approach provides a rigorous methodology, and cultural studies caution a greater sensitivity to social hierarchies." Constrained by Merton's middle range theory [note: to be discussed here], the specification of cultural elements and social structures makes possible the investigation of specific cultural and social systems and their interaction. References Archer, Margaret S. 2004. Culture and Agency: The Place of Culture in Social Theory, Revised Edition. New York and Cambridge: Cambridge University Press. Burrowes, Carl Patrick. 1996. From Functionalism to Cultural Studies: Manifest Ruptures and Latent Continuities, Communication Theory, 6(1):88–103. Geertz, Clifford. 1966. "Religion as a Cultural System," in M. Banton (ed.), Anthropological Approaches to the Study of Religion. New York: Praeger, pp. 1–46. David Lockwood. 1964. “Social Integration and System Integration,” in G. Zollschan and W. Hirsch (eds.), Explorations in Social Change. Boston: Houghton Mifflin. Ritzer, George, and Douglas J. Goodman. 2004. "Structural Functionalism, Neofunctionalism, and Conflict Theory," in Sociological Theory, sixth edition. McGraw-Hill. A metropolis is a large city or conurbation which is a significant economic, political, and cultural area for a country or region, and an important hub for regional or international connections, commerce, and communications. A big city belonging to a larger urban agglomeration, but which is not the core of that agglomeration, is not generally considered a metropolis but a part of it. The plural of the word is metropolises, although the Latin plural is metropoles, from the Greek metropoleis (μητρoπόλεις). For urban areas outside metropolitan areas that generate a similar attraction on a smaller scale for their region, the concept of the regiopolis ("regio" for short) was introduced by urban and regional planning researchers in Germany in 2006. Etymology Metropolis (μητρόπολις) is a Greek word, (plural: metropoleis) coming from μήτηρ, mḗtēr meaning "mother" and πόλις, pólis meaning "city" or "town", which is how the Greek colonies of antiquity referred to their original cities, with whom they retained cultic and political-cultural connections. The word was used in post-classical Latin for the chief city of a province, the seat of the government and, in particular, ecclesiastically for the seat or see of a metropolitan bishop to whom suffragan bishops were responsible. This usage equates the province with the diocese or episcopal see. In a colonial context, it is the "mother city" of a colony, that is, the city which sent out settlers. The word has distant roots in the colonial past of Ancient Greece with first usage in Middle English around the 14th century. This was later generalized to a city regarded as a center of a specified activity, or any large, important city in a nation. Concept The concept of a "metropolis" as a "mother city" dates back to at least sixth-century Canterbury, where the term was used in a religious context, but the term began to be used to describe a large secular city starting with 16th-century London. London's cultural influence meant that until the 19th century, concepts of the "metropolis" were rarely used to describe other cities, though Edinburgh was also described as a "metropolis." While metropolis can often mean any large city, the metropolis is generally understood as a city which serves as a particular function as opposed to simply being large. Modern ideas of a metropolis have changed as modern city growth has created "polycentric" urban regions, where one city does not necessarily dominate its surroundings but instead is central to an economic region. Instead of a single "metropolis" fulfilling an economic role, large urban areas such as the Tokyo–Osaka corridor or the southern California built up area have been considered as a modern "metropolis" even though the region encompasses multiple cities. Usage as a mainland area In France, Portugal, Spain, and the Netherlands, the word metropolis (métropole (Fr.) / metrópole (Port.) / metrópoli (Spa.) / metropool (Dutch)) designates the mainland part of a country situated on or close to the European mainland; in the case of France, this means France without its overseas departments. For Portugal and Spain during the Spanish Empire and Portuguese Empire period, the term was used to designate Portugal or Spain minus its colonies (the Ultramar). In France métropole can also be used to refer to a large urban agglomeration; for example, "La Métropole de Lyon" (the Lyon Metropolis). By country The following countries either have a specific legal definition of "metropolis" or have a history of usage of the term (or a similar term). Americas Brazil The term used in Brazilian Portuguese for a metropolitan area is Região Metropolitana. In Brazil, the Greater São Paulo is the principal metropolis with over 21 million inhabitants. In the larger cities, such as São Paulo and Rio de Janeiro (population 12 million), favelas (slums) grew up over decades as people migrated from rural areas in order to find work. Other metropolises in Brazil with more than one million inhabitants include: Belém, Belo Horizonte, Brasília, Campinas, Curitiba, Fortaleza, Goiânia, Maceió, Manaus, Porto Alegre, Recife, Salvador and São Luís. Canada Statistics Canada defines a census metropolitan area as one or more adjacent municipalities situated around a major urban core where the urban core has a population of at least 100,000. Canada's six largest metropolises are Toronto, Montreal, Vancouver, Ottawa, Calgary, and Edmonton. United States In the United States, an incorporated area or group of areas having a population more than 50,000 is required to have a metropolitan planning organization in order to facilitate major infrastructure projects and to ensure financial solvency. Thus, a population of 50,000 or greater has been used as a de facto standard to define a metropolis in the United States. A similar definition is used by the United States Census Bureau. The bureau defines a Metropolitan Statistical Area as "at least one urbanized area of 50,000 or more inhabitants." The six largest metropolitan areas in the USA are New York, Los Angeles, Chicago, Dallas, Houston, and Washington, D.C., with New York being the largest. Mexico In Mexico, a metropolis is an urban area with a high concentration of population and activities, interacting with nearby municipalities. The Consejo Nacional de Población (CONAPO) last reviewed the criteria in 2018, and from that date, a metropolitan area in Mexico is defined as: a set of two or more municipalities where a city with a population of at least 100,000 is located, and whose urban area, functions and activities exceed the limits of the municipality, incorporating within its area of direct influence the predominantly urban neighboring municipalities, maintaining a high degree of socioeconomic integration. Also included are those municipalities that, due to their particular characteristics, are relevant to urban planning and policy for each metropolitan area. The six largest metropolitan areas are the Mexico City, Monterrey, Guadalajara, Puebla, Toluca and Tijuana. Asia India The 74th Amendment to the Indian Constitution defines a metropolitan area as an area having a population of 10 Lakh or 1 Million or more, comprised in one or more districts and consisting of two or more Municipalities or Panchayats or other contiguous areas, specified by the Governor by public notification to be a Metropolitan area. As of 2011 Census of India, India has 46 other cities with populations greater than one million. Delhi, Mumbai, Kolkata, Chennai, Bangalore, Hyderabad, Pune, Ahmedabad, Kochi, are among the largest of 23 metropolitan cities in India. Japan The Japanese legal term to (都) is by designation to be translated as "metropolis". However, existing translations predate the designation. Structured like a prefecture instead of a normal city, there is only one to in Japan, namely Tokyo. As of 2020, Japan has 12 other cities with populations greater than one million. The same Kanji character in Chinese, or in generic Japanese (traditional or non-specific), translates variously—city, municipality, special municipality—all qualify. Philippines The Philippines has three metropolises as defined by the National Economic and Development Authority. They are Manila, Cebu, and Davao. Greater Manila Area is the contiguous urbanization region or Extended Metropolitan Manila surrounding Metro Manila. This built-up zone includes Metro Manila and the neighboring provinces of Bulacan to the north, Cavite and Laguna to the south, and Rizal to the east. Though sprawl continues to absorb new zones, some urban zones are independent clusters of settlements surrounded by non-urban areas. South Korea In the South Korea, there are seven special and metropolitan cities at autonomous administrative levels. These are the most populous metropolitan areas in the country. In decreasing order of the population of 2015 census, they are Seoul, Busan, Incheon, Daegu, Daejeon, Gwangju and Ulsan. According to the census of 2015, cities of Changwon and Suwon also qualify for being elevated to the level of metropolitan cities (having population over 1 million), but any future plans to promote them into metropolitan city are unlikely to be accepted because of political concerns about the structure of administrative divisions. There are also some county-level cities with increasing population near 1 million, namely Goyang, Yongin, and Seongnam, but they are also unlikely to be promoted into metropolitan city because they are all satellite cities of Seoul. Europe France A 2014 law allowed any group of communes to cooperate in a larger administrative division called a métropole. One métropole, Lyon, also has status as a department. France's national statistics institute, Insee, designates 12 of the country's urban areas as metropolitan areas. Paris, Lyon and Marseille are the biggest, the other nine being Toulouse, Lille, Bordeaux, Nice, Nantes, Strasbourg, Rennes, Grenoble and Montpellier. Germany The largest German city by administrative borders is Berlin, while Rhine-Ruhr is the largest metropolitan area (with more than 10 million people). The importance of a city is measured with three groups of indicators, also called metropolitan functions: The decision making and control function, the innovation and competition function, and the gateway function. These functions are seen as key domains for metropolitan regions in developing their performance. In spatial planning, a metropolis is usually observed within its regional context, thus the focus is mainly set on the metropolitan regions. These regions can be mono central or multi central. Eleven metropolitan regions have been defined due to these indicators: Berlin-Brandenburg, Bremen-Oldenburg, Dresden-Halle-Leipzig, Frankfurt-Rhine-Main, Hamburg, Hannover-Braunschweig-Göttingen-Wolfsburg, Munich, Nuremberg, Rhine-Neckar, Rhine-Ruhr (with Cologne/Bonn), and Stuttgart. Italy As of January 1, 2015, there are 14 "metropolitan cities" in Italy. Rome, Milan, Naples and other big cores have taken in urban zones from their surrounding areas and merged them into the new entities, which have been home for one out of three Italians. The provinces remained in the parts of the country not belonging to any Città Metropolitana. Poland The Union of Polish metropolises (Polish: Unia Metropolii Polskich), established in 1990, is an organization of the largest cities in the country. Currently twelve cities are members of the organization, of which 11 have more than a quarter-million inhabitants. The largest metropolitan area in Poland, if ranked solely by the number of inhabitants, is the Katowice metropolitan area with around 3 million inhabitants (5 million inhabitants in the Katowice-Ostrava metropolitan area). The Metropolis GZM is an initiative of recent years attempting to unite the conurbation into one official urban unit. It is followed by Warsaw, with around 1.7 million inhabitants in the city proper and 3.1 million in the Warsaw metropolitan area. Other Polish metropolises are Kraków, Łódź, Wrocław, Poznań, Tricity, Szczecin and Bydgoszcz–Toruń. Turkey In Turkey the metropolitan cities are described as "büyükşehir". There are 30 metropolitan municipalities in Turkey now. The largest by far is Istanbul, followed by Ankara, İzmir and Bursa. Istanbul, the largest city in Europe in terms of population, has a population of over 15 million. The city has surpassed London and Dubai to become the most visited city in the world, with more than 20 million foreign visitors in 2023. This city, which played an important role in the spread of Christianity, is an important heritage for European culture. United Kingdom In the United Kingdom, the term the Metropolis was historically used to exclusively refer to the cities of London and Westminster, at the heart of what later became the Greater London Built-up Area. The term is retained by the Greater London police force, the Metropolitan Police (the "Met"), formed in 1829. The chief officer of the Metropolitan Police is formally known as the Commissioner of Police of the Metropolis. The Metropolis Management Act 1855 created the Metropolitan Board of Works, covering an area now known as Inner London, and in 1889 this became the elected London County Council. Since 1974, six conurbations in England (outside London) have been known as metropolitan counties, each divided into metropolitan districts. These counties are South Yorkshire (centred on the city of Sheffield), the West Midlands (including Birmingham), West Yorkshire (including Leeds), Merseyside (including Liverpool), Greater Manchester and Tyne & Wear (including Newcastle-upon-Tyne). Greater Glasgow, South Hampshire, Greater Nottingham, Greater Bristol, Belfast metropolitan area, and Greater Leicester are also large conurbations with more than half a million citizens. Sweden In Sweden, the term metropolis has been used to exclusively refer to Stockholm or Greater Stockholm. Oceania Australia The Australian Bureau of Statistics defines a metropolitan area as any statistical division or district with a population of more than 100,000. According to this definition, there are currently 19 metropolitan areas in Australia, including every state capital. By population, the largest metropolitan area is Sydney (urban area population at 2020 Census of 5,367,206) and the smallest is Bendigo (urban area population at 2020 Census of 100,632). Rapid urban growth in Victoria has seen the 'Manhattanization' of Melbourne, with high-rise clusters in South Yarra, Box Hill, Moonee Ponds and Footscray. The regional city of Geelong which is approximately 40 miles south west of Melbourne, has seen the emergence of high-rise office and apartment buildings in recent years. Geelong is the fastest growing regional city in Australia, and its growth will transform the Port Phillip region in a similar manner to San Francisco's Bay Area. (urban area population at 2020 Census of 160,991). See also Notes References Further reading Census.gov, U.S. Census Bureau, About Metropolitan and Micropolitan Statistics MetroForum.com, forum dedicated to discussions on metropolis Blog.ar2com.de, a podcast with a worldwide analysis of megacities (focus Latin America) E-geopolis.eu at the Library of Congress Web Archives (archived October 27, 2009): research group, university of Paris-Diderot, France See Ronald Daus´s bibliography, researcher at the Free University of Berlin Knowledge organization (KO), organization of knowledge, organization of information, or information organization is an intellectual discipline concerned with activities such as document description, indexing, and classification that serve to provide systems of representation and order for knowledge and information objects. According to The Organization of Information by Joudrey and Taylor, information organization: examines the activities carried out and tools used by people who work in places that accumulate information resources (e.g., books, maps, documents, datasets, images) for the use of humankind, both immediately and for posterity. It discusses the processes that are in place to make resources findable, whether someone is searching for a single known item or is browsing through hundreds of resources just hoping to discover something useful. Information organization supports a myriad of information-seeking scenarios. Issues related to knowledge sharing can be said to have been an important part of knowledge management for a long time. Knowledge sharing has received a lot of attention in research and business practice both within and outside organizations and its different levels. Sharing knowledge is not only about giving it to others, but it also includes searching, locating, and absorbing knowledge. Unawareness of the employees' work and duties tends to provoke the repetition of mistakes, the waste of resources, and duplication of the same projects. Motivating co-workers to share their knowledge is called knowledge enabling. It leads to trust among individuals and encourages a more open and proactive relationship that grants the exchange of information easily. Knowledge sharing is part of the three-phase knowledge management process which is a continuous process model. The three parts are knowledge creation, knowledge implementation, and knowledge sharing. The process is continuous, which is why the parts cannot be fully separated. Knowledge creation is the consequence of individuals' minds, interactions, and activities. Developing new ideas and arrangements alludes to the process of knowledge creation. Using the knowledge which is present at the company in the most effective manner stands for the implementation of knowledge. Knowledge sharing, the most essential part of the process for our topic, takes place when two or more people benefit by learning from each other. Traditional human-based approaches performed by librarians, archivists, and subject specialists are increasingly challenged by computational (big data) algorithmic techniques. KO as a field of study is concerned with the nature and quality of such knowledge-organizing processes (KOP) (such as taxonomy and ontology) as well as the resulting knowledge organizing systems (KOS). Theoretical approaches Traditional approaches Among the major figures in the history of KO are Melvil Dewey (1851–1931) and Henry Bliss (1870–1955). Dewey's goal was an efficient way to manage library collections; not an optimal system to support users of libraries. His system was meant to be used in many libraries as a standardized way to manage collections. The first version of this system was created in 1876. An important characteristic in Henry Bliss' (and many contemporary thinkers of KO) was that the sciences tend to reflect the order of Nature and that library classification should reflect the order of knowledge as uncovered by science: Natural order → Scientific classification → Library classification (KO) The implication is that librarians, in order to classify books, should know about scientific developments. This should also be reflected in their education: Again from the standpoint of the higher education of librarians, the teaching of systems of classification ... would be perhaps better conducted by including courses in the systematic encyclopedia and methodology of all the sciences, that is to say, outlines which try to summarize the most recent results in the relation to one another in which they are now studied together. ... (Ernest Cushing Richardson, quoted from Bliss, 1935, p. 2) Among the other principles, which may be attributed to the traditional approach to KO are: Principle of controlled vocabulary Cutter's rule about specificity Hulme's principle of literary warrant (1911) Principle of organizing from the general to the specific Today, after more than 100 years of research and development in LIS, the "traditional" approach still has a strong position in KO and in many ways its principles still dominate. Facet analytic approaches The date of the foundation of this approach may be chosen as the publication of S. R. Ranganathan's colon classification in 1933. The approach has been further developed by, in particular, the British Classification Research Group. The best way to explain this approach is probably to explain its analytico-synthetic methodology. The meaning of the term "analysis" is: breaking down each subject into its basic concepts. The meaning of the term synthesis is: combining the relevant units and concepts to describe the subject matter of the information package in hand. Given subjects (as they appear in, for example, book titles) are first analyzed into a few common categories, which are termed "facets". Ranganathan proposed his PMEST formula: Personality, Matter, Energy, Space and Time: Personality is the distinguishing characteristic of a subject. Matter is the physical material of which a subject may be composed. Energy is any action that occurs with respect to the subject. Space is the geographic component of the location of a subject. Time is the period associated with a subject. The information retrieval tradition (IR) Important in the IR-tradition have been, among others, the Cranfield experiments, which were founded in the 1950s, and the TREC experiments (Text Retrieval Conferences) starting in 1992. It was the Cranfield experiments, which introduced the measures "recall" and "precision" as evaluation criteria for systems efficiency. The Cranfield experiments found that classification systems like UDC and facet-analytic systems were less efficient compared to free-text searches or low level indexing systems ("UNITERM"). The Cranfield I test found, according to Ellis (1996, 3–6) the following results: Although these results have been criticized and questioned, the IR-tradition became much more influential while library classification research lost influence. The dominant trend has been to regard only statistical averages. What has largely been neglected is to ask: Are there certain kinds of questions in relation to which other kinds of representation, for example, controlled vocabularies, may improve recall and precision? User-oriented and cognitive views The best way to define this approach is probably by method: Systems based upon user-oriented approaches must specify how the design of a system is made on the basis of empirical studies of users. User studies demonstrated very early that users prefer verbal search systems as opposed to systems based on classification notations. This is one example of a principle derived from empirical studies of users. Adherents of classification notations may, of course, still have an argument: That notations are well-defined and that users may miss important information by not considering them. Folksonomies is a recent kind of KO based on users' rather than on librarians' or subject specialists' indexing. Bibliometric approaches These approaches are primarily based on using bibliographical references to organize networks of papers, mainly by bibliographic coupling (introduced by Kessler 1963) or co-citation analysis ( independently suggested by Marshakova 1973 and Small 1973). In recent years it has become a popular activity to construe bibliometric maps as structures of research fields. Two considerations are important in considering bibliometric approaches to KO: The level of indexing depth is partly determined by the number of terms assigned to each document. In citation indexing this corresponds to the number of references in a given paper. On the average, scientific papers contain 10–15 references, which provide quite a high level of depth. The references, which function as access points, are provided by the highest subject-expertise: The experts writing in the leading journals. This expertise is much higher than that which library catalogs or bibliographical databases typically are able to draw on. The domain analytic approach Domain analysis is a sociological-epistemological standpoint that advocates that the indexing of a given document should reflect the needs of a given group of users or a given ideal purpose. In other words, any description or representation of a given document is more or less suited to the fulfillment of certain tasks. A description is never objective or neutral, and the goal is not to standardize descriptions or make one description once and for all for different target groups. The development of the Danish library "KVINFO" may serve as an example that explains the domain-analytic point of view. KVINFO was founded by the librarian and writer Nynne Koch and its history goes back to 1965. Nynne Koch was employed at the Royal Library in Copenhagen in a position without influence on book selection. She was interested in women's studies and began personally to collect printed catalog cards of books in the Royal Library, which were considered relevant for women's studies. She developed a classification system for this subject. Later she became the head of KVINFO and got a budget for buying books and journals, and still later, KVINFO became an independent library. The important theoretical point of view is that the Royal Library had an official systematic catalog of a high standard. Normally it is assumed that such a catalog is able to identify relevant books for users whatever their theoretical orientation. This example demonstrates, however, that for a specific user group (feminist scholars), an alternative way of organizing catalog cards was important. In other words: Different points of view need different systems of organization. Domain analysis has examined epistemological issues in the field, i.e. comparing the assumptions made in different approaches to KO and examining the questions regarding subjectivity and objectivity in KO. Subjectivity is not just about individual differences. Such differences are of minor interest because they cannot be used as guidelines for KO. What seems important are collective views shared by many users. A kind of subjectivity about many users is related to philosophical positions. In any field of knowledge different views are always at play. In arts, for example, different views of art are always present. Such views determine views on art works, writing on art works, how art works are organized in exhibitions and how writings on art are organized in libraries. In general it can be stated that different philosophical positions on any issue have implications for relevance criteria, information needs and for criteria of organizing knowledge. Other approaches One widely used analysis of information-organizational principles, attributed to Richard Saul Wurman, summarizes them as Location, Alphabet, Time, Category, Hierarchy (LATCH). See also == References == Family therapy (also referred to as family counseling, family systems therapy, marriage and family therapy, couple and family therapy) is a branch of psychotherapy focused on families and couples in intimate relationships to nurture change and development. It tends to view change in terms of the systems of interaction between family members. The different schools of family therapy have in common a belief that, regardless of the origin of the problem, and regardless of whether the clients consider it an "individual" or "family" issue, involving families in solutions often benefits clients. This involvement of families is commonly accomplished by their direct participation in the therapy session. The skills of the family therapist thus include the ability to influence conversations in a way that catalyses the strengths, wisdom, and support of the wider system. In the field's early years, many clinicians defined the family in a narrow, traditional manner usually including parents and children. As the field has evolved, the concept of the family is more commonly defined in terms of strongly supportive, long-term roles and relationships between people who may or may not be related by blood or marriage. The conceptual frameworks developed by family therapists, especially those of family systems theorists, have been applied to a wide range of human behavior, including organisational dynamics and the study of greatness. History and theoretical frameworks Formal interventions with families to help individuals and families experiencing various kinds of problems have been a part of many cultures, probably throughout history. These interventions have sometimes involved formal procedures or rituals, and often included the extended family as well as non-kin members of the community (see for example Ho'oponopono). Following the emergence of specialization in various societies, these interventions were often conducted by particular members of a community – for example, a chief, priest, physician, and so on – usually as an ancillary function. Family therapy as a distinct professional practice within Western cultures can be argued to have had its origins in the social work movements of the 19th century in the United Kingdom and the United States. As a branch of psychotherapy, its roots can be traced somewhat later to the early 20th century with the emergence of the child guidance movement and marriage counseling. The formal development of family therapy dates from the 1940s and early 1950s with the founding in 1942 of the American Association of Marriage Counselors (the precursor of the AAMFT), and through the work of various independent clinicians and groups – in the United Kingdom (John Bowlby at the Tavistock Clinic), the United States (Donald deAvila Jackson, John Elderkin Bell, Nathan Ackerman, Christian Midelfort, Theodore Lidz, Lyman Wynne, Murray Bowen, Carl Whitaker, Virginia Satir, Ivan Boszormenyi-Nagy), and in Hungary, D.L.P. Liebermann – who began seeing family members together for observation or therapy sessions. There was initially a strong influence from psychoanalysis (most of the early founders of the field had psychoanalytic backgrounds) and social psychiatry, and later from learning theory and behavior therapy – and significantly, these clinicians began to articulate various theories about the nature and functioning of the family as an entity that was more than a mere aggregation of individuals. The movement received an important boost starting in the early 1950s through the work of anthropologist Gregory Bateson and colleagues – Jay Haley, Donald D. Jackson, John Weakland, William Fry, and later, Virginia Satir, Ivan Boszormenyi-Nagy, Paul Watzlawick and others – at Palo Alto in the United States, who introduced ideas from cybernetics and general systems theory into social psychology and psychotherapy, focusing in particular on the role of communication (see Bateson Project). This approach eschewed the traditional focus on individual psychology and historical factors – that involve so-called linear causation and content – and emphasized instead feedback and homeostatic mechanisms and "rules" in here-and-now interactions – so-called circular causation and process – that were thought to maintain or exacerbate problems, whatever the original cause(s). (See also systems psychology and systemic therapy.) This group was also influenced significantly by the work of US psychiatrist, hypnotherapist, and brief therapist Milton H. Erickson – especially his innovative use of strategies for change, such as paradoxical directives The members of the Bateson Project (like the founders of a number of other schools of family therapy, including Carl Whitaker, Murray Bowen, and Ivan Boszormenyi-Nagy) had a particular interest in the possible psychosocial causes and treatment of schizophrenia, especially in terms of the putative "meaning" and "function" of signs and symptoms within the family system. The research of psychiatrists and psychoanalysts Lyman Wynne and Theodore Lidz on communication deviance and roles (e.g., pseudo-mutuality, pseudo-hostility, schism and skew) in families of people with schizophrenia also became influential with systems-communications-oriented theorists and therapists. A related theme, applying to dysfunction and psychopathology more generally, was that of the "identified patient" or "presenting problem" as a manifestation of or surrogate for the family's, or even society's, problems. (See also double bind; family nexus.) By the mid-1960s, a number of distinct schools of family therapy had emerged. From those groups that were most strongly influenced by cybernetics and systems theory, there came MRI Brief Therapy, and slightly later, strategic therapy, Salvador Minuchin's structural family therapy and the Milan systems model. Partly in reaction to some aspects of these systemic models, came the experiential approaches of Virginia Satir and Carl Whitaker, which downplayed theoretical constructs, and emphasized subjective experience and unexpressed feelings (including the subconscious), authentic communication, spontaneity, creativity, total therapist engagement, and often included the extended family. Concurrently and somewhat independently, there emerged the various intergenerational therapies of Murray Bowen, Ivan Boszormenyi-Nagy, James Framo, and Norman Paul, which present different theories about the intergenerational transmission of health and dysfunction, but which all deal usually with at least three generations of a family (in person or conceptually), either directly in therapy sessions, or via "homework", "journeys home", etc. Psychodynamic family therapy – which, more than any other school of family therapy, deals directly with individual psychology and the unconscious in the context of current relationships – continued to develop through a number of groups that were influenced by the ideas and methods of Nathan Ackerman, and also by the British School of Object Relations and John Bowlby's work on attachment. Multiple-family group therapy, a precursor of psychoeducational family intervention, emerged, in part, as a pragmatic alternative form of intervention – especially as an adjunct to the treatment of serious mental disorders with a significant biological basis, such as schizophrenia – and represented something of a conceptual challenge to some of the systemic (and thus potentially "family-blaming") paradigms of pathogenesis that were implicit in many of the dominant models of family therapy. The late 1960s and early 1970s saw the development of network therapy (which bears some resemblance to traditional practices such as Ho'oponopono) by Ross Speck and Carolyn Attneave, and the emergence of behavioral marital therapy (renamed behavioral couples therapy in the 1990s) and behavioral family therapy as models in their own right. By the late 1970s, the weight of clinical experience – especially in relation to the treatment of serious mental disorders – had led to some revision of a number of the original models and a moderation of some of the earlier stridency and theoretical purism. There were the beginnings of a general softening of the strict demarcations between schools, with moves toward rapprochement, integration, and eclecticism – although there was, nevertheless, some hardening of positions within some schools. These trends were reflected in and influenced by lively debates within the field and critiques from various sources, including feminism and post-modernism, that reflected in part the cultural and political tenor of the times, and which foreshadowed the emergence (in the 1980s and 1990s) of the various post-systems constructivist and social constructionist approaches. While there was still debate within the field about whether, or to what degree, the systemic-constructivist and medical-biological paradigms were necessarily antithetical to each other (see also Anti-psychiatry; Biopsychosocial model), there was a growing willingness and tendency on the part of family therapists to work in multi-modal clinical partnerships with other members of the helping and medical professions. From the mid-1980s to the present, the field has been marked by a diversity of approaches that partly reflect the original schools, but which also draw on other theories and methods from individual psychotherapy and elsewhere – these approaches and sources include: brief therapy, structural therapy, constructivist approaches (e.g., Milan systems, post-Milan/collaborative/conversational, reflective), Bring forthism approach (e.g. Dr. Karl Tomm's IPscope model and Interventive interviewing), solution-focused therapy, narrative therapy, a range of cognitive and behavioral approaches, psychodynamic and object relations approaches, attachment and emotionally focused therapy, intergenerational approaches, network therapy, and multisystemic therapy (MST). Multicultural, intercultural, and integrative approaches are being developed, with Vincenzo Di Nicola weaving a synthesis of family therapy and transcultural psychiatry in his model of cultural family therapy, A Stranger in the Family: Culture, Families, and Therapy. Many practitioners claim to be eclectic, using techniques from several areas, depending upon their own inclinations and/or the needs of the client(s), and there is a growing movement toward a single "generic" family therapy that seeks to incorporate the best of the accumulated knowledge in the field and which can be adapted to many different contexts; however, there are still a significant number of therapists who adhere more or less strictly to a particular, or limited number of, approach(es). The Liberation Based Healing framework for family therapy offers a complete paradigm shift for working with families while addressing the intersections of race, class, gender identity, sexual orientation and other socio-political identity markers. This theoretical approach and praxis is informed by critical pedagogy, feminism, critical race theory, and decolonizing theory. This framework necessitates an understanding of the ways colonization, cis-heteronormativity, patriarchy, white supremacy and other systems of domination impact individuals, families and communities and centers the need to disrupt the status quo in how power operates. Traditional Western models of family therapy have historically ignored these dimensions and when white, male privilege has been critiqued, largely by feminist theory practitioners, it has often been to the benefit of middle-class, white women's experiences. While an understanding of intersectionality is of particular significance in working with families with violence, a liberatory framework examines how power, privilege and oppression operate within and across all relationships. Liberatory practices are based on the principles of critical consciousness, Accountability and Empowerment. These principles guide not only the content of the therapeutic work with clients but also the supervisory and training process of therapists. Dr. Rhea Almeida developed the cultural context model as a way to operationalize these concepts into practice through the integration of culture circles, sponsors, and a socio-educational process within the therapeutic work. Ideas and methods from family therapy have been influential in psychotherapy generally: a survey of over 2,500 US therapists in 2006 revealed that of the 10 most influential therapists of the previous quarter-century, three were prominent family therapists and that the marital and family systems model was the second most utilized model after cognitive behavioral therapy. Techniques Family therapy uses a range of counseling and other techniques including: Structural therapy – identifies and re-orders the organisation of the family system Strategic therapy – looks at patterns of interactions between family members Systemic/Milan therapy – focuses on belief systems Narrative therapy – restorying of dominant problem-saturated narrative, emphasis on context, separation of the problem from the person Transgenerational therapy – transgenerational transmission of unhelpful patterns of belief and behaviour IPscope model and Interventive Interviewing Communication theory Psychoeducation Psychotherapy Relationship counseling Relationship education Systemic coaching Systems theory Reality therapy the genogram The number of sessions depends on the situation, but the average is 5–20 sessions. A family therapist usually meets several members of the family at the same time. This has the advantage of making differences between the ways family members perceive mutual relations as well as interaction patterns in the session apparent both for the therapist and the family. These patterns frequently mirror habitual interaction patterns at home, even though the therapist is now incorporated into the family system. Therapy interventions usually focus on relationship patterns rather than on analyzing impulses of the unconscious mind or early childhood trauma of individuals as a Freudian therapist would do – although some schools of family therapy, for example psychodynamic and intergenerational, do consider such individual and historical factors (thus embracing both linear and circular causation) and they may use instruments such as the genogram to help to elucidate the patterns of relationship across generations. The distinctive feature of family therapy is its perspective and analytical framework rather than the number of people present at a therapy session. Specifically, family therapists are relational therapists: They are generally more interested in what goes on between individuals rather than within one or more individuals, although some family therapists – in particular those who identify as psychodynamic, object relations, intergenerational, or experiential family therapists (EFTs) – tend to be as interested in individuals as in the systems those individuals and their relationships constitute. Depending on the conflicts at issue and the progress of therapy to date, a therapist may focus on analyzing specific previous instances of conflict, as by reviewing a past incident and suggesting alternative ways family members might have responded to one another during it, or instead proceed directly to addressing the sources of conflict at a more abstract level, as by pointing out patterns of interaction that the family might have not noticed. Family therapists tend to be more interested in the maintenance and/or solving of problems rather than in trying to identify a single cause. Some families may perceive cause-effect analyses as attempts to allocate blame to one or more individuals, with the effect that for many families a focus on causation is of little or no clinical utility. It is important to note that a circular way of problem evaluation is used as opposed to a linear route. Using this method, families can be helped by finding patterns of behaviour, what the causes are, and what can be done to better their situation. Evidence base Family therapy has an evolving evidence base. A summary of current evidence is available via the UK's Association of Family Therapy. Evaluation and outcome studies can also be found on the Family Therapy and Systemic Research Centre website. The website also includes quantitative and qualitative research studies of many aspects of family therapy. According to a 2004 French government study conducted by French Institute of Health and Medical Research, family and couples therapy was the second most effective therapy after Cognitive behavioral therapy. The study used meta-analysis of over a hundred secondary studies to find some level of effectiveness that was either "proven" or "presumed" to exist. Of the treatments studied, family therapy was presumed or proven effective at treating schizophrenia, bipolar disorder, anorexia and alcohol dependency. Concerns and criticism In a 1999 address to the Coalition of Marriage, Family and Couples Education conference in Washington, D.C., University of Minnesota Professor William Doherty said: I take no joy in being a whistle blower, but it's time. I am a committed marriage and family therapist, having practiced this form of therapy since 1977. I train marriage and family therapists. I believe that marriage therapy can be very helpful in the hands of therapists who are committed to the profession and the practice. But there are a lot of problems out there with the practice of therapy – a lot of problems. Doherty suggested questions prospective clients should ask a therapist before beginning treatment: "Can you describe your background and training in marital therapy?" "What is your attitude toward salvaging a troubled marriage versus helping couples break up?" "What is your approach when one partner is seriously considering ending the marriage and the other wants to save it?" "What percentage of your practice is marital therapy?" "Of the couples you treat, what percentage would you say work out enough of their problems to stay married with a reasonable amount of satisfaction with the relationship." "What percentage break up while they are seeing you?" "What percentage do not improve?" "What do you think makes the differences in these results?" Licensing and degrees Family therapy practitioners come from a range of professional backgrounds, and some are specifically qualified or licensed/registered in family therapy (licensing is not required in some jurisdictions and requirements vary from place to place). In the United Kingdom, family therapists will have a prior relevant professional training in one of the helping professions usually psychologists, psychotherapists, or counselors who have done further training in family therapy, either a diploma or an M.Sc. In the United States there is a specific degree and license as a marriage and family therapist; however, psychologists, nurses, psychotherapists, social workers, or counselors, and other licensed mental health professionals may practice family therapy. In the UK, family therapists who have completed a four-year qualifying programme of study (MSc) are eligible to register with the professional body the Association of Family Therapy (AFT), and with the UK Council for Psychotherapy (UKCP). A master's degree is required to work as a Marriage and Family Therapist (MFT) in some American states. Most commonly, MFTs will first earn a M.S. or M.A. degree in marriage and family therapy, counseling, psychology, family studies, or social work. After graduation, prospective MFTs work as interns under the supervision of a licensed professional and are referred to as an MFTi. Prior to 1999 in California, counselors who specialized in this area were called Marriage, Family and Child Counselors. Today, they are known as Marriage and Family Therapists (MFT), and work variously in private practice, in clinical settings such as hospitals, institutions, or counseling organizations. Marriage and family therapists in the United States and Canada often seek degrees from accredited Masters or Doctoral programs recognized by the Commission on Accreditation for Marriage and Family Therapy Education (COAMFTE), a division of the American Association of Marriage and Family Therapy. Requirements vary, but in most states about 3000 hours of supervised work as an intern are needed to sit for a licensing exam. MFTs must be licensed by the state to practice. Only after completing their education and internship and passing the state licensing exam can a person call themselves a Marital and Family Therapist and work unsupervised. License restrictions can vary considerably from state to state. Contact information about licensing boards in the United States are provided by the Association of Marital and Family Regulatory Boards. There have been concerns raised within the profession about the fact that specialist training in couples therapy – as distinct from family therapy in general – is not required to gain a license as an MFT or membership of the main professional body, the AAMFT. Values and ethics Since issues of interpersonal conflict, power, control, values, and ethics are often more pronounced in relationship therapy than in individual therapy, there has been debate within the profession about the different values that are implicit in the various theoretical models of therapy and the role of the therapist's own values in the therapeutic process, and how prospective clients should best go about finding a therapist whose values and objectives are most consistent with their own. An early paper on ethics in family therapy written by Vincenzo Di Nicola in consultation with a bioethicist asked basic questions about whether strategic interventions "mean what they say" and if it is ethical to invent opinions offered to families about the treatment process, such as statements saying that half of the treatment team believes one thing and half believes another. Specific issues that have emerged have included an increasing questioning of the longstanding notion of therapeutic neutrality, a concern with questions of justice and self-determination, connectedness and independence, functioning versus authenticity, and questions about the degree of the therapist's pro-marriage/family versus pro-individual commitment. The American Association for Marriage and Family Therapy requires members to adhere to a code of ethics, including a commitment to "continue therapeutic relationships only so long as it is reasonably clear that clients are benefiting from the relationship." Founders and key influences Some key developers of family therapy are: Summary of theories and techniques (references:) Journals Australian and New Zealand Journal of Family Therapy Contemporary Family Therapy Family Process Family Relations Family Relations, Interdisciplinary Journal of Applied Family Studies ISSN 0197-6664 Journal of Family Therapy Marriage Fitness Murmurations: Journal of Transformative Systemic Practice Sexual and Relationship Therapy Journal of Marital & Family Therapy Families, Systems and Health See also Footnotes Further reading Deborah Weinstein, The Pathological Family: Postwar America and the Rise of Family Therapy. Ithaca, NY: Cornell University Press, 2013. Satir, V., Banmen, J., Gerber, J., & Gomori, M. (1991). The Satir Model: Family Therapy and Beyond. Palo Alto, CA: Science and Behavior Books. The Systemic Thinking and Practice Series. Routledge Gehring, T. M., Debry, M. & Smith, P. K. (Eds.). (2016). The Family System Test FAST. Theory and application. Hove: Brunner-Routledge. The Zoroastrian population decline has garnered considerable attention and discussion within academia, literature, and journalism. A number of studies, conducted by both Indian and Western academics, have offered comprehensive insights into the complex dynamics underlying this demographic shift. Despite some segments of the community hesitating to acknowledge this decline, rigorous examination of census data and demographic studies spanning several decades sheds light on various factors contributing to this trend. History The Sasanian Empire, which was the last Zoroastrian state in Iran, finally succumbed to Arab conquest by 642 CE. Subsequently, a significant migration of Zoroastrians to India occurred, likely in the 8th or 9th century, leading to the establishment of the Parsi community. Scholars, including Jamsheed Choksy, have hypothesized that the transition from Zoroastrianism to Islam among Iranians following the Arab invasion was a gradual process spanning several centuries, as evidenced by Islamic biographical dictionaries. Furthermore, Bulliet’s statistical analysis indicates that a mere 8% of urban Iranians were Muslim a century after the Abbasids’ rise to power in 750 CE, with this figure increasing to approximately 80% by the 10th century, although rural areas retained stronger Zoroastrian adherence. Iran The Zoroastrian population continued to decline due to conversions, persecution, and massacres, dwindling to a few thousand by the 19th century. Maneckji Limji Hataria’s survey in the 1850s found fewer than 7,000 Zoroastrians in Iran. However, the efforts of Hataria and the 'Society for the Amelioration of the Conditions of the Zoroastrians in Persia' led to improved conditions in the 20th century. This resulted in a population rebound, with the 1966 census reporting around 20,000 Zoroastrians, over 9,000 of whom resided in Tehran. Estimates prior to the Islamic Revolution in 1979, ranged between 20,000 and 30,000 Zoroastrians. Post-revolution, the 1981 census reported over 92,000 Zoroastrians, likely inflated due to Baha’is registering as Zoroastrians amidst the Islamic Republic’s non-recognition of Baha’ism. Subsequent censuses have not recorded religious affiliation, necessitating reliance on estimates for current figures. Mobed Firouzgary’s 2004 demographic report for the FEZANA Journal estimated the Zoroastrian population at 24,000, aligning with electoral data. However, migration, particularly of younger individuals, to Western countries due to economic challenges has led to a significant decrease in the Zoroastrian population, leaving an aging community behind. Current estimates suggest no more than 12,000 to 15,000 Zoroastrians remain in Iran, reflecting a serious demographic situation for the religion historically intertwined with the nation. India The majority of Zoroastrians in the world are Parsis, the descendants of refugees who migrated to the Indian subcontinent, fleeing religious persecution after the Islamic invasion and destruction of the Sassanian Empire. It is thought, they were economically prosperous in the centuries subsequent to their migration. However, this period was marred by the invasion of Gujarat by Alauddin Khilji, leading to various conflicts, riots, and massacres endured by both Parsis and Hindus against the invaders. Parsis were a predominantly agricultural community by 1650. Under British rule, Parsis, along with Bhatias, Hindu Banias, Jains, Baghdadi Jews, Bohra, and Khoja Muslims, actively participated in mercantile endeavors, especially in the city of Bombay. Notably, Parsis provided significant support and financial backing to the nascent Indian Independence movement, with leaders such as Dadabhai Naroji, Bhikaji Cama, Phereozeshah Mehta, Khurshed F. Nariman, Ardeshir Godrej and Mithuben Petit. Post-independence, Parsis are known for their patriotism and they have played a leading role in the development of India as a republic, this includes India's first field marshall Sam Manekshaw and architect of India's nuclear program Homi Bhabha. Parsis have been described by various Indian politicians, including Narendra Modi as 'The World's Best Minority'. Key studies addressing Parsi population decline include Paul Axelrod's seminal works, such as "Cultural and Historical Factors in the Population Decline of the Parsis of India" (1990) and "Natality and Family Planning in Three Bombay Communities" (1988), along with other significant contributions like Jayant Kumar Banthia's "Parsi Demography: Past, Recent, and Future" (2003) and Chidambara Chandrasekaran's pioneering research on Parsi demography dating back to 1948. These studies collectively underscore a unanimous observation: the precipitous drop in Parsi fertility rates stands as a central driver of the population decline. Every demographic study conducted on the Indian Parsis, who constitute the largest component of the worldwide Zoroastrian community, indicates a steep and steady decline in population due to low fertility rates. Contrary to biological conjectures, scholarly consensus suggests that low Parsi fertility primarily stems from socio-cultural factors rather than genetic predispositions. Early demographic investigations, such as Chandrasekaran's study, discredited biological explanations, emphasizing instead the influence of social norms and cultural attitudes on fertility rates. Axelrod's subsequent research further corroborated this assertion, in his 1990 publication in the scholarly journal Population Studies, he analyzed statistical findings and determined that “It would appear that Parsi women are not sub-fecund [i.e., more biologically infertile] and, once married, are able to bear children quickly and without difficulty.” When fertility becomes a biological concern within the Parsi community, it often arises from the practice of marrying and attempting to conceive children at an advanced age. The decline in fertility rates among the Parsis is considered one of the most dramatic outside of Europe, leading to an unprecedented fall in population numbers. The impact of late marriage and non-marriage on the Parsi population is evident in the fertility rates, which are well below the replacement level necessary to sustain the population. In 1881, the birthrate for Parsis in Bombay was 34 per 1000 population, which dropped to 25 per 1000 population by 1926 and further slid to 12 per 1000 population by the 1960s. By 1961, due to late marriage or non-marriage, the births to Parsi women in Greater Bombay were only 50 percent of the number that would have occurred if every woman were married throughout her reproductive period. The situation worsened in the following decades. By 1980-82, the total fertility rate (TFR) for Parsis was already 1.12, about half of the replacement level, and it further declined to 0.94 in 2000. Recent data from 2001-06 observed a TFR of 0.88, a figure that is significantly lower than the TFRs of total populations in countries like South Korea and Japan, which are known to have serious demographic decline. The low fertility rate has resulted in an aging Parsi population and a dramatic shift in the age distribution. Despite the decline in fertility rates since the 1880s, the Parsi population in India continued to grow until the 1950s due to lower mortality rates and longer life expectancy. However, as the number of elderly Parsis increased, new generations of Parsi children became smaller. By 2001, one in every eight Parsis was a child under the age of 15, whereas one in every four Parsis was aged 65 and above. As these trends continue, the Parsi community in India will see a significant decline in population numbers in the coming decades. Projections suggest that by 2051, only 32,000 Parsis will remain in India, less than half of the current population. Similarly another prediction is that, the Parsi population of Bombay, which was 46,557 in 2001, is projected to decline to 20,122. Dinshaw Tamboly, the Chairman of The WZO Trust Funds has reckoned by the end of the 21st century there will only be 9,995 Parsis left in India. Contributing factors Late Marriage Late marriage emerges as a salient feature within the Parsi community, with historical records tracing its evolution. While the Parsis, such as Behramji Malbari were pioneers in abandoning the practice of child marriage in the late 19th century, subsequent decades witnessed a progressive delay in the age of marriage. By 1930, the median age of marriage for Parsi women surpassed 24 years, significantly higher than national averages. Current data indicates even higher marriage ages, with the median age standing at 27 for women and 31 for men among Indian Parsis, substantially exceeding the thresholds conducive to population replacement. The trend of late marriages among Parsis has led to a significant increase in the number of individuals who have never married. This phenomenon, as stated by Axelrod, is often a direct result of late marriages, particularly in the case of the Parsis. Non-marriage It has been noted that delaying marriage often increases the likelihood of remaining unmarried. The rates of non-marriage within the Parsi population are striking, attracting the attention of numerous demographers. In 1948, Sapur Faredun Desai, author of “A Community at the Cross-Road”, expressed concern over the increasing rates of unmarried Parsis, which had risen by 40 percent between 1881 and 1931. Chandrasekaran's research shows that by 1931, 16 percent of Parsi women remained unmarried at the end of their reproductive cycle, a significant increase from 6 percent in 1901. This trend was not confined to urban areas like Bombay. Studies conducted in rural Gujarat found that between 13 percent and 55 percent of surveyed women remained unmarried well into middle age. In the 1960s, it was found that the percentage of married Parsis in Karachi was “unusually low” compared to Muslim women of the same age group. Non-marriage of Zoroastrians is prevalent across all educational levels, indicating that it was not linked to any particular socioeconomic class. Current figures suggest that the proportion of single people in the Parsi community might be one of the highest in the world. In 1982, a study commissioned by the Bombay Parsi Punchayet (BPP), found that 45 percent of all adult males and 38 percent of all adult females surveyed were never married. A similar study conducted by the Tata Institute of Social Sciences (TISS) in 1999 found that 40 percent of men and 30 percent of Parsis in Greater Mumbai remained unmarried throughout their lives. According to a paper presented by Sayeed Unisa and others in 2009, one out of every five Parsi Indian males, and one out of every ten females, is still unmarried by the age of 50. Out-migration Due to the sheer magnitude of the departure, out-migration now seems to be significantly contributing to Iran's Zoroastrian population decline. This is the same for Pakistan and since the 1960s, out-migration has been observed as a significant cause in the decline of the relatively tiny community, centered in Karachi which was in the erstwhile Bombay presidency. As of 2009 it has been documented 95% of Pakistani Zoroastrian youth are making the decision to leave their homeland. Zoroastrians who stay in Karachi are noted to face economic discrimination based on their religious indentity. While some academics, acknowledge that migration may have a role in India, they also point out that it is very challenging to quantify and evaluate and it doesn't seem to be all that relevant. Undoubtedly, at least 95% of young people in India are choosing to remain on the country's soil. Unisa, et al. made the decision to search statistics on Parsi age distribution for telltale indicators of significant out-migration in light of the tone of the discussion in India. Between 1961 and 2001, they discovered "no significant distortions in the age pattern"—that is, there were no anomalous declines in the population of young adult age cohorts. Consequently, they came to the conclusion that, while an accurate evaluation is impossible, "migration does not appear to be a reason for the decline in Parsi population in the recent past." Based on the data previously provided in this research, it is also highly probable that out-migration is not enough to explain the demographic reduction in India, given the sharp decline in Zoroastrian populations globally. Thus, out-migration contributes to the fall in the Indian Parsi population, but not significantly. Intermarriage In recent decades, few many neighbourhood concerns have generated as much debate as this one. India has undoubtedly seen a rise in the number of interfaith marriages. When Sapur Faredun Desai wrote his book in 1948, intermarriage was hardly recorded. However, by 1991, it accounted for 19% of all Parsi weddings in Bombay, and by 2005, 31%. The numbers seem to be greater for other parts of India. Our demographers acknowledge once more how difficult it is to pinpoint the exact effects of intermarriage. However, academics like Axelrod are certain that although statistics on out-migration and intermarriage might be "substantial" on their own, they "are not sufficient to account entirely for the population decline." Dr. Zubin Shroff attempted to quantify the proportionate influence of intermarriage on the future population of the Bombay community in his 2010 paper. According to Shroff's demographic predictions for 2051, only the offspring of Parsi males who had married outside of their family would be accepted as Zoroastrians, while the children of both men and women who had married outside of their family would be accepted. His conclusions are rather startling. Regardless of whether offspring from any marriages are accepted, he found that "the Parsi population will decline sharply over the next few decades, given current fertility trends." The birth rate within the community is so low that the inclusion of children from intermarried couples scarcely impacts the situation. Given current fertility and intermarriage rates, Bombay's population in 2051 will be 19,136 if no children of any married pair are accepted, 20,122 if only children of married women are disqualified, and 20,535 if all intermarriage children are accepted. Shroff concludes that both liberals and conservatives have overstated the role of intermarriage in the community's population decline, while the community's "abysmally low fertility" has actually "brought the community to the so-called demographic brink." If rates of intermarriage keep rising and the long-term offspring of these marriages—children and grandkids, for instance—are not brought up as Zoroastrians, intermarriage may very well be a reason for worry to the community, including the population in diaspora within North America. Childless couples A significant proportion of Parsi couples have no children, with a 1999 study by the Tata Institute of Social Sciences (TISS) revealing that 12 percent of married Parsi women are childless. This high rate of childlessness is attributed to both biological fertility issues and deliberate avoidance of pregnancies. The Parsi community, considering itself enlightened, has shown less prejudice and more acceptance towards openly gay members compared to other Indian communities. This acceptance may contribute to a higher visibility of gay Parsis compared to other Indian communities where discrimination and social stigma persist. There is no scientific evidence to support a claim of a high rate of homosexuality within the Parsi community, particularly among men. The issue of religious conversion also plays a role in the Parsi community’s demographic trends. Contrary to the myth of widespread conversion to Zoroastrianism in regions such as Central Asia and Russia, the reality is that conversion remains a rare occurrence. In Iran, conversion is punishable under the Islamic Republic, and there is no significant evidence of Muslim Iranians in the diaspora converting to Zoroastrianism. Conversion Throughout history, there have been notable instances of individuals from the Zoroastrian faith adopting other larger religions. A significant number of Zoroastrians in Iran, for example, embraced Baha’ism in the late 19th century. The initial documented instance of conversion occurred during the 1880s and involved a Yazd merchant by the name of KayKhosro Khodadad. Amighi points out the absence of precise statistics regarding Zoroastrian conversions but suggests that the figure was likely substantial. According to Amighi’s findings, in Yazd, about 30 percent of the Zoroastrian families had a minimum of one Baha’i member within their broader family network. Additionally, Zoroastrians in Iran have been persistently coerced to convert to Islam. In Iran, there have been reports of government-sponsored or sanctioned efforts to convert religious minorities, including Zoroastrians, to Islam. These initiatives may involve financial incentives or other forms of persuasion aimed at encouraging conversion. The Iranian educational system predominantly follows Islamic teachings and practices, which can create an environment where non-Muslim students, including Zoroastrians, feel pressure to conform to Islamic norms. Consequently, Zoroastrians may face employment or advancement barriers in certain fields if they openly practice their faith. Employers may prefer candidates who align more closely with the dominant Islamic culture, leading Zoroastrians to downplay or conceal their religious identity to avoid discrimination. The Parsis faced significant pressure to convert from colonial powers in India. Following the Portuguese consolidation of their occupation of Thana around 1560, they implemented violently intolerant practices towards other religions, similar to those of the preceding Muslim conquerors. The Portuguese authorities issued an ultimatum to the Zoroastrians of Thana: convert to Christianity or face execution. According to tradition, the Zoroastrians requested a delay and deceived the Portuguese by claiming they would convert as a group on the following Sunday. The Portuguese agreed, allowing the Zoroastrians to use the respite to abandon Thana and resettle in Kalyan. Subsequently, the Portuguese destroyed all Zoroastrian places of worship, repurposing their stones to construct churches and other colonial buildings. In the 19th century, the Parsi community experienced a notable increase in conversions to Christianity, driven by the missionary John Wilson. This trend persisted into the 20th century and included prominent individuals such as Sir Ness Wadia, father of Neville Wadia and Nadir Dinshaw. Another significant conversion was that of Rattanbai Jinnah to Islam. These interactions with colonial powers, marked by forced conversions and religious persecution, significantly influenced the Zoroastrian community, shaping its demographic and cultural landscape over the centuries. The Parsi community experienced an increase in conversions to Christianity in the 1830s, initiated by the orientalist and missionary John Wilson, and this trend continued well into the 20th century. Prominent individuals who converted included Sir Ness Wadia (father of Neville Wadia) and Nadir Dinshaw. Additionally, Rattanbai Jinnah's conversion to Islam is another significant instance of religious change from Zoroastrianism. While there are no accurate statistics on the number of individuals formally leaving the religion, anecdotal evidence suggests that the number is extremely limited. However, a more significant concern may be the number of individuals who, due to apathy or irreligiosity, do not pass on Zoroastrianism to their children. This trend could have long-term implications for the community’s religious continuity. Potential solutions The decline in Parsi fertility rates has been a known issue among scholars for over six decades, with numerous studies confirming a drastic population slide. Many potential solutions have been documented and proposed by Zoroastrian organizations, which can be broadly categorized into organizational actions and individual initiatives. Organizational actions Community resources and organizations must be harnessed to arrest the population decline. This primarily involves adults in the community, particularly those in positions of leadership. Some community leaders have already undertaken commendable work to encourage marriage and childbirth and to make housing more accessible to young couples in Bombay. However, more needs to be done, and the rest of the community needs to participate. Community organizations play a crucial role in prioritizing youth issues and promoting marriage and childbearing. In India, the Bombay Parsi Panchayat (BPP) has initiated successful programs such as the Zoroastrian Youth for the Next Generation (ZYNG). Similarly, the Federation of Zoroastrian Associations of North America (FEZANA) has supported youth efforts through organizations like the Zoroastrian Youth of North America (ZYNA). The Jiyo Parsi Scheme is a unique initiative launched by the Ministry of Minority Affairs in 2013 to address the declining population of the Parsi community in India. The scheme aims to reverse this trend by adopting a scientific protocol and structured interventions. It comprises three components: an Advocacy Component that includes workshops and advertisement campaigns to create awareness, a Health of the Community Component which covers childcare, creche support, and assistance to the elderly, and a Medical Component that provides financial assistance for the detection and treatment of infertility, fertility treatment. The scheme offers cash assistance to encourage Parsi couples to have children, regardless of their financial status. Promoting youth interaction Promoting interaction among the youth of the community is a key strategy. Unlike in India, Iran, or Pakistan, Zoroastrians in the diaspora are not concentrated in specific cities or neighborhoods, making regular interaction a challenge. Local Zoroastrian associations can support and fund activities for youth and young adults, organize regional youth meetings, and facilitate participation in congresses and events. Creating Infrastructure to Promote Marriage Addressing the issue of finding suitable matches within the community is another important aspect. The creation of a comprehensive, user-friendly, and regularly updated Zoroastrian matrimonial website could be a potential solution. Such a portal could promote cooperation among different Zoroastrian associations worldwide. Individual responsibility The youth of the community have a significant role to play. The drastic decline in Parsi fertility is not due to biological reasons but cultural and attitudinal ones. If falling numbers are due to cultural norms and attitudes toward marriage and family, then attitudes can change. The process of decline can be stopped and eventually reversed if fertility rates increase. This is achievable, as shown in studies by Zubin Shroff and Sayeeda Unisa, et al., though it will require a substantial increase in current fertility rates. Here are some steps individuals can take, which have been suggested by Dr. Dinyar Patel: Get married: Marriage is an important institution in any community. A significant proportion of potential fertility is lost if a substantial percentage of women never marry. Do not delay marriage until a late age: It becomes increasingly difficult and risky to have children the longer one waits. If one wants to have a family, it is best to start looking for a potential partner at a relatively earlier age. Try to find a Zoroastrian spouse: A union between two Zoroastrians is most likely to result in offspring who are raised as Zoroastrian and in the community’s culture and traditions. Have children: A total fertility rate (TFR) of 2.1 is necessary for the replacement of a population. The current TFR for Parsis is below 1.0, so the Parsis are well below the replacement level. Couples should be encouraged to have at least two children Raise children as Zoroastrians: Individuals should strive to pass on their religion, tradition, and culture to a new generation. Defining Zoroastrian Identity The definition of who is considered a Parsi or a Zoroastrian has been a subject of debate. In 1908, Justice Dinshaw Davar made a significant decision in what is known as the Parsi Punchayet Case (Petit v Jijibhai). This judicial case indirectly dealt with the admission of non-Zoroastrian spouses, including the French wife of R.D. Tata, into the Parsi community. Rather than relying on religious texts or precedent, Justice Davar defined Parsis as a patriarchal community. The question of defining who was a Zoroastrian and a Parsi arose again in the 1915-18 Rangoon Navjote case, which involved Bella, a child of a Parsi mother and a Goan Christian father. Initially, the Burma court sided with Bella’s supporters, affirming her status as a Zoroastrian. However, after a series of appeals, the Privy Court ultimately accepted Justice Davar’s definition. See also Persecution of Zoroastrians Parsis Iranis (India) == References == An accusation is a statement by one person asserting that another person or entity has done something improper. The person who makes the accusation is an accuser, while the subject against whom it is made is the accused. Whether a statement is interpreted as an accusation may rely on the social environment in which it is made: What counts as an accusation is often unclear, and what kind of response is warranted is even less clear. Even a purely surface semantic analysis of accusatory language cannot be performed in the absence of social context, including who is making the accusation and to whom it is being made—often the subject of supposedly accusatory language might well interpret the utterance in question as something that he need not respond to. An accusation can be made in private or in public, to the accused person alone, or to other people with or without the knowledge of the accused person. An accuser can make an accusation with or without evidence; the accusation can be entirely speculative, and can even be a false accusation, made out of malice, for the purpose of harming the reputation of the accused. Perceptions and reactions The perceived strength of an accusation is affected by the trustworthiness of the accuser. For example, in investigative journalism: The claim of wrongdoing relies not on statements attributed to others, as in ordinary news stories, but rather on reportorial fact-finding. An accusation can be made in an authoritative tone because it has originated from research conducted by the journalist, who takes a position by asserting the "true facts" of the story and implicitly urging those in charge to do something about them. Responses to accusations vary, and may include confession to the assertion, but also often manifest as "a state of denial, minimalization, or externalization". Accusations and public relations In journalism, the reporting of an accusation is commonly balanced with an effort to obtain a response to the accusation by the accused person or entity: Investigative stories are balanced only in the sense that they usually allow their targets the courtesy of a response. The "other side" is told, most often through a villain's admission or dodge, because the nature of the accusation— backed with evidence and confirmed well before a decision is made to publish—is such that there is no refuting it. There is therefore usually an opportunity for the subject of an accusation to respond to it. An accusation made against a corporation is often treated as a public relations event, in which a business is accused of wrongdoing in order to influence its behavior. First, the accusation is a small spectacle. It is a small sign that the big, customary social order has broken down, at least for those involved in the market-based relationships. Second, an accusation is a public portrayal of wrongdoing that deploys iconic claims and keywords in its "event-structuring process." These words define and refine an event in crisp, familiar, easily understood, and unambiguously negative terms. As noted, an accusation is an early warning, a danger-ahead signal of trouble. And it involves a redefining of the situation to find out not only what the wrong is, but also who is wronged and by whom. Inevitably, in this event-defining process the accused becomes an archetypal betrayer. Third, the accusation is always highly charged. As opposed to the lengthy legal complaint by a federal or state regulator, or the formal brief filed by a complainant in a legal case, the accusation is short and highly condensed. Unlike the formal complaint or criminal charge, the accusation is shorn of legalistic details. The accusation is sharpened through the use of adjectives, provocative headlines, and dramatic story leads. There is never anything neutral about betrayal, about lying, stealing, and cheating in the market. Fourth, the accusation comes wrapped in a package. It is more than a publicly observable event involving the behavior of market competitors and participants gone wrong. It is an event expressed through catchphrases and keywords. Criminal accusations A criminal accusation is a formal accusation made by the state against an individual or enterprise. In addition to the normal elements of an accusation, a criminal accusation specifies that the wrongdoing on the part of the accused constitutes a violation of the law. A criminal accusation may be informally made through a declaration made to the public at large (generally through news media) or by the filing of a formal accusation in a court of law by a person legally entitled to do so, generally on behalf of the state by a criminal prosecutor. See also Allegation False accusation Indictment Information (formal criminal charge) == References == Public philosophy is a subfield of philosophy that involves engagement with the public. Definition Jack Russell Weinstein defines public philosophy as "doing philosophy with general audiences in a non-academic setting". It must be undertaken in a public venue but might deal with any philosophical issue. Michael J. Sandel describes public philosophy as having two aspects. The first is to "find in the political and legal controversies of our day an occasion for philosophy". The second is "to bring moral and political philosophy to bear on contemporary public discourse". James Tully emphasizes that public philosophy is done through practice, through the contestable concepts of citizenship, civic freedom, and nonviolence. According to Sharon Meagher, one of the founders of the Public Philosophy Network, "'public philosophy' is not simply a matter of doing philosophy in public, but must also engage with the community it finds itself in". Exemplars The Ancient Greek philosopher Socrates has been considered to be the "first public philosopher." He engaged with the general public of Athens, discussing issues of importance to them. In the modern day, some public philosophers are academic professionals, such as Mortimer J. Adler, Jürgen Habermas, Martha Nussbaum, Richard Rorty, James Tully, Jack Russell Weinstein, and Cornel West. Others may work outside of the usual academic contexts of teaching and writing for peer-reviewed journals, such as social activist Jane Addams and novelist Ayn Rand. Jack Russell Weinstein, director of The Institute for Philosophy In Public Life, contends that although it is commonplace to argue that public philosophy promotes democracy, this argument assumes philosophers are better citizens than non-philosophers. See also Public history Public intellectual References External links Essays in Philosophy, special issue on public philosophy Committee on Public Philosophy of the American Philosophical Association Institute for Philosophy in Public Life Public Philosophy Journal Public Philosophy Network In theoretical computer science, the log-rank conjecture states that the deterministic communication complexity of a two-party Boolean function is polynomially related to the logarithm of the rank of its input matrix. Let D ( f ) {\displaystyle D(f)} denote the deterministic communication complexity of a function, and let rank ⁡ ( f ) {\displaystyle \operatorname {rank} (f)} denote the rank of its input matrix M f {\displaystyle M_{f}} (over the reals). Since every protocol using up to c {\displaystyle c} bits partitions M f {\displaystyle M_{f}} into at most 2 c {\displaystyle 2^{c}} monochromatic rectangles, and each of these has rank at most 1, D ( f ) ≥ log 2 ⁡ rank ⁡ ( f ) . {\displaystyle D(f)\geq \log _{2}\operatorname {rank} (f).} The log-rank conjecture states that D ( f ) {\displaystyle D(f)} is also upper-bounded by a polynomial in the log-rank: for some constant C {\displaystyle C} , D ( f ) = O ( ( log ⁡ rank ⁡ ( f ) ) C ) . {\displaystyle D(f)=O((\log \operatorname {rank} (f))^{C}).} Lovett proved the upper bound D ( f ) = O ( rank ⁡ ( f ) log ⁡ rank ⁡ ( f ) ) . {\displaystyle D(f)=O\left({\sqrt {\operatorname {rank} (f)}}\log \operatorname {rank} (f)\right).} This was improved by Sudakov and Tomon, who removed the logarithmic factor, showing that D ( f ) = O ( rank ⁡ ( f ) ) . {\displaystyle D(f)=O\left({\sqrt {\operatorname {rank} (f)}}\right).} This is the best currently known upper bound. The best known lower bound, due to Göös, Pitassi and Watson, states that C ≥ 2 {\displaystyle C\geq 2} . In other words, there exists a sequence of functions f n {\displaystyle f_{n}} , whose log-rank goes to infinity, such that D ( f n ) = Ω ~ ( ( log ⁡ rank ⁡ ( f n ) ) 2 ) . {\displaystyle D(f_{n})={\tilde {\Omega }}((\log \operatorname {rank} (f_{n}))^{2}).} In 2019, an approximate version of the conjecture for randomised communication has been disproved. See also List of unsolved problems in computer science == References == The phrase "surrogate marriage" may refer to marriage as the result of widow inheritance or "woman-to-woman marriage" among certain African communities where a woman is infertile and her family substitutes another woman to bear children for her. In Igbo culture of Nigeria, a practice known as "woman-to-woman marriage" allows a woman to marry another woman and become a "female-husband." This is a recognized social and cultural institution, primarily used to address the cultural significance of male children to carry on lineage continuity when a family lacks an heir. The practice is rooted in the cultural context of inheritance and social dynamics. In that marriage, the emphasis is not on the same-sex romantic relationships between the women involved but rather on the role of the bride as a child-bearer for the female-husband's lineage. Woman-to-woman marriage allows women, who may be childless or have reached the end of their childbearing age, to fulfill their obligation to produce offspring for their family. Historically, woman-to-woman marriage evolved within the Igbo society as a means of addressing various factors, including the importance of children, patterns of inheritance, and economic considerations. The inheritance structure is such that children are primarily recognized through their cultural father, the female-husband who pays the bride price, rather than a biological father. This belief influences the concept of lineage and ownership within the marriage arrangement. Scholars have conducted extensive research on surrogate marriage, exploring its motivations, evolution, and contemporary significance. The practice has been documented among various African communities, and studies estimate that a significant percentage of African married women may be involved in woman-to-woman marriages. However, this practice has sparked debates and confusion, particularly when viewed through the lens of Western cultural norms and perspectives on marriage. Scholars have examined the dynamics of woman-to-woman marriage, its motivations, and the complex power relationships inherent within this form of union. Additionally, it has been noted that the practice persists, in part, due to its relevance to inheritance, wealth, and economic considerations in the societies where it is practiced. In her insightful book, Daniela Bandelli provides a comprehensive exploration of surrogacy by examining it from a multitude of angles, including social, political, cultural, and medical perspectives. Drawing from an extensive review of existing literature and original fieldwork, Bandelli's work aims to equip readers with a nuanced understanding of the complex and evolving landscape of surrogacy. The book delves into various dimensions of surrogacy, presenting an enlightening chapter that outlines key aspects of the surrogacy process, its transnational market, and the medical risks associated with surrogate pregnancies. Moreover, Bandelli's analysis extends to the crucial role of women's movements in shaping public discourse and policy development concerning sexuality, procreation, and bioethics. Unlike conventional marriages that typically center on romantic love and companionship, woman-to-woman marriages prioritize the production of offspring, particularly male children, as a fundamental objective. The surrogate wife plays a critical role in this arrangement, contributing to the lineage by bearing children for the female-husband's family. The practice challenges traditional notions of marriage, family, and parenthood, emphasizing the importance of lineage and inheritance. Woman-to-woman Marriages in the Zulu Culture Woman-to-woman marriages exist in other African cultures, such as the Zulu culture. The root of the arrangement is the belief that marriage is an arrangement for the continuity of life. Where the life of the family or clan cannot be continued due to infertility or death, the family of the wife can substitute a female to bear children for the husband on behalf of the wife. See also Levirate marriage Widow inheritance Sororate marriage The Handmaid's Tale == References == Advocate Nasiruddin (1892–1949) was a lawyer, political and social leader from Bhopal state. He was popularly known as 'Vakil Nasiruddin' as "Vakil' in Urdu stands for 'Advocate'. Biography His father 'Saaduddin' died when he was a toddler. In 1901, when he was merely 8–9 years old, his mother 'Mustaqimun Nisan' took him from Tijara to Bhopal state, where more opportunities for education and job were there. He chose his career as a lawyer and cleared the law examination in 1920. On 1 May 1922 when the 'High Court of the Bhopal State' was started, he took 'First-grade Lawyer' degree from the newly created Bhopal High Court. Because of inborn talent and rational approach in legal decision, he attained not only a very good reputation but also proved himself a competent lawyer in very early age. His name was amongst the uppermost experts of law in the whole Central India Agency. In the State of Bhopal, his advice was sought in almost every legal case. Later of his life during the Undivided India, he was a leading practitioner of law and was amongst the topmost advocates. Few legal cases He was one of the eminent lawyers during his professional career and conducted some very important cases. In one of the famous legal case between the brother and wife of Muhammad Ismail Ali Khan Tonk, he was at the side of Begum Anwar Jahan (daughter of General Abdul Qayyum Khan and wife of Muhammad Ismail Ali Khan Tonk). With him Barrister Abdul Majeed Khwaja while, in the opponent group at the side of the brother of Ismail Ali Khan Tonk, Advocate Muhammad Ali Jinnah was there. It was also the first visit of Muhammad Ali Jinnah to Bhopal as advocate. In the second famous case, when the Government of Bhopal state appealed against the decision of creating the 'Obaidullah Khan Scholarship' by his family members Nawabzadah Rashiduzzafar Khan and Saeeduz Zafar Khan. Advocate Nasiruddin, Muhammad Ali Jinnah, Chaudhry Khaliquzzaman and Barrister Wasim (from Lucknow) took part in the deliberation from the side of the Government of Bhopal while on the other side, Sir Vazir Hasan, Chowdhry Nematullah (Lucknow) and Tej Bahadur Sapru participated. Politics Advocate Nasiruddin is a famous example of a lawyer-turned-politician in Bhopal state. He was elected as member of the Municipal committee and remained its member consecutively for 12 years. He was also elected as Vice-chairman of Baldia Municipality (Municipal administration). He won the election of Legislative Council in 1934 (MLC). He was the member of Constitution Committee, which was formed by Nawab Hamidullah Khan for framing the constitution of Bhopal state. He was member of the managing committee of the 'Juditional Club' and remained as president of the Bar Association of Bhopal for several years. Titles and honour He was awarded the title "Najmul Insha" by the Bhopal state. His active participation and services that he rendered for Civil defense and National Guard during the World War II was greatly acknowledged officially by the Bhopal state Government. Literary and sports activities Advocate Nasiruddin used to take keen interest in literary activities. He was a voracious reader. He along with Advocate Abdur Raqib Alvi also started a weekly newspaper "Rahbar" in 1946 from Bhopal. Sayyad Abdul Raqib Alvi was the General Secretary of Bhopal Muslim League. He made an appeal on 21 June 1947 to make donation to the Pakistan fund because Qaide Azam had appealed for Pakistan fund Advocate Nasiruddin also took interest in sports. His favourite game was chess in which he gained both name and fame in the Bhopal state. Death and legacy He died at the age of 57 on 14 February 1949 and left no issues. Bar Association of Bhopal of which he was the president for several years, in its condolence note entitled him as 'Bhopal ka Sapru' (after the name of Tej Bahadur Sapru). See also Qazi Syed Rafi Mohammad Qazi Syed Mohammad Zaman Qazi Syed Mohammad Rafi Ghulam Mansoor == References == A public disclosure is any non-confidential communication which an inventor or invention owner makes to one or more members of the public, revealing the existence of the invention and enabling an appropriately experienced individual ("person having ordinary skill in the art") to reproduce the invention. A public disclosure may be any form of non-confidential communication. For example an academic poster, presentation to a symposium/conference, website article, book chapter, academic journal article, or even an unguarded conversation in a car park. Patent law In some countries, public disclosure may result in the immediate loss of invention patentability unless a patent application has already been filed, and disclosure may be considered to include oral as well as written communication. Australia The Full Federal Court of Australia held in Fuchs Lubricants v Quaker Chemical that the patent claims lacked novelty due to public disclosures made by the inventor Mr Thomas before any patent applications were filed. The public disclosures arose in the following way. In about September–October 2010, Mr Thompson conceived the idea of using dyes in hydraulic fluid to enable detection of fluid injection injuries. He considered it would be necessary to conduct trials and experiments using hydraulic fluid in real operational mining equipment, so approached the engineering manager of Peabody Energy Australia Pty Ltd, which operated the Metropolitan mine in NSW. Mr Thompson disclosed the invention by explaining his proposed method (i.e., the invention) and discussed the possibility of running trials and experiments at the Metropolitan mine. A further disclosure occurred about a month later when, in the carpark of the Metropolitan mine, Mr Thompson demonstrated his invention to two Peabody managers. The Court found that these disclosures were not confidential and therefore constituted public disclosures that compromised the novelty of the invention. United States In the U.S., public disclosure of an invention results in the loss of patentability of the invention after a period of one year. 35 U.S.C. § 102 establishes various statutory bars to invention patentability with regard to invention novelty; these explicit bars preclude patentability as exceptions to a general underlying entitlement. The public disclosure bar is one of the bars established in section 102 (b): "(b) the invention was patented or described in a printed publication in this or a foreign country or in public use or on sale in this country, more than one year prior to the date of the application for patent in the United States..." "Printed publications" generally include all paper and electronic forms of publication. For example, books, scientific journals, posters, conference slide presentations, and website articles would all qualify as disclosure media. See also On-sale bar References External links 35 U.S.C. 102 Conditions for patentability; novelty and loss of right to patent. Moral statistics most narrowly refers to numerical data generally considered to be indicative of social pathology in groups of people. Examples include statistics on crimes (against persons and property), illiteracy, suicide, illegitimacy, abortion, divorce, prostitution, and the economic situation sometimes called pauperism in the 19th century. The gathering of anything that might be called social statistics is often dated from John Graunt’s (1662) analysis of the London Bills of Mortality, which tabulated birth and death data collected by London parishes. The beginnings of the systematic collection of population statistics (now called demography) occurred in the mid-18th century, often attributed to Johann Peter Süssmilch in 1741. Data on moral variables began to be collected and disseminated by various state agencies (most notably in France and Britain) in the early 19th century, and were widely used in debates about social reform. The first major work on this topic was the Essay on moral statistics of France by André-Michel Guerry in 1833. In this book, Guerry presented thematic maps of the departments of France, shaded according to illiteracy, crimes against persons and against property, illegitimacy, donations to the poor and so forth, and used these to ask questions about how such moral variables were related. In Britain this theme was taken up beginning in 1847 by Joseph Fletcher who published several articles on the topic Moral and educational statistics of England and Wales. References Further reading Friendly M. (2007) "A.-M. Guerry's Moral Statistics of France: Challenges for Multivariable Spatial Analysis", Statistical Science, 22 (3), 368–399. Project Euclid doi:10.1214/07-STS241 "Health 2.0" is a term introduced in the mid-2000s, as the subset of health care technologies mirroring the wider Web 2.0 movement. It has been defined variously as including social media, user-generated content, and cloud-based and mobile technologies. Some Health 2.0 proponents see these technologies as empowering patients to have greater control over their own health care and diminishing medical paternalism. Critics of the technologies have expressed concerns about possible misinformation and violations of patient privacy. History Health 2.0 built on the possibilities for changing health care, which started with the introduction of eHealth in the mid-1990s following the emergence of the World Wide Web. In the mid-2000s, following the widespread adoption both of the Internet and of easy to use tools for communication, social networking, and self-publishing, there was spate of media attention to and increasing interest from patients, clinicians, and medical librarians in using these tools for health care and medical purposes. Early examples of Health 2.0 were the use of a specific set of Web tools (blogs, email list-servs, online communities, podcasts, search, tagging, Twitter, videos, wikis, and more) by actors in health care including doctors, patients, and scientists, using principles of open source and user-generated content, and the power of networks and social networks in order to personalize health care, to collaborate, and to promote health education. Possible explanations why health care has generated its own "2.0" term are the availability and proliferation of Health 2.0 applications across health care in general, and the potential for improving public health in particular. Current use While the "2.0" moniker was originally associated with concepts like collaboration, openness, participation, and social networking, in recent years the term "Health 2.0" has evolved to mean the role of Saas and cloud-based technologies, and their associated applications on multiple devices. Health 2.0 describes the integration of these into much of general clinical and administrative workflow in health care. As of 2014, approximately 3,000 companies were offering products and services matching this definition, with venture capital funding in the sector exceeding $2.3 billion in 2013. Public Health 2.0 Public Health 2.0 is a movement within public health that aims to make the field more accessible to the general public and more user-driven. The term is used in three senses. In the first sense, "Public Health 2.0" is similar to "Health 2.0" and describes the ways in which traditional public health practitioners and institutions are reaching out (or could reach out) to the public through social media and health blogs. In the second sense, "Public Health 2.0" describes public health research that uses data gathered from social networking sites, search engine queries, cell phones, or other technologies. A recent example is the proposal of statistical framework that utilizes online user-generated content (from social media or search engine queries) to estimate the impact of an influenza vaccination campaign in the UK. In the third sense, "Public Health 2.0" is used to describe public health activities that are completely user-driven. An example is the collection and sharing of information about environmental radiation levels after the March 2011 tsunami in Japan. In all cases, Public Health 2.0 draws on ideas from Web 2.0, such as crowdsourcing, information sharing, and user-centered design. While many individual healthcare providers have started making their own personal contributions to "Public Health 2.0" through personal blogs, social profiles, and websites, other larger organizations, such as the American Heart Association (AHA) and United Medical Education (UME), have a larger team of employees centered around online driven health education, research, and training. These private organizations recognize the need for free and easy to access health materials often building libraries of educational articles. Definitions The "traditional" definition of "Health 2.0" focused on technology as an enabler for care collaboration: "The use of social software t-weight tools to promote collaboration between patients, their caregivers, medical professionals and other stakeholders in health." In 2011, Indu Subaiya redefined Health 2.0 as the use in health care of new cloud, Saas, mobile, and device technologies that are: Adaptable technologies which easily allow other tools and applications to link and integrate with them, primarily through use of accessible APIs Focused on the user experience, bringing in the principles of user-centered design Data driven, in that they both create data and present data to the user in order to help improve decision making This wider definition allows recognition of what is or what isn't a Health 2.0 technology. Typically, enterprise-based, customized client-server systems are not, while more open, cloud based systems fit the definition. However, this line was blurring by 2011-2 as more enterprise vendors started to introduce cloud-based systems and native applications for new devices like smartphones and tablets. In addition, Health 2.0 has several competing terms, each with its own followers—if not exact definitions—including Connected Health, Digital Health, Medicine 2.0, and mHealth. All of these support a goal of wider change to the health care system, using technology-enabled system reform—usually changing the relationship between patient and professional.: Personalized search that looks into the long tail but cares about the user experience Communities that capture the accumulated knowledge of patients, caregivers, and clinicians, and explains it to the world Intelligent tools for content delivery—and transactions Better integration of data with content Wider health system definitions In the late 2000s, several commentators used Health 2.0 as a moniker for a wider concept of system reform, seeking a participatory process between patient and clinician: "New concept of health care wherein all the constituents (patients, physicians, providers, and payers) focus on health care value (outcomes/price) and use competition at the medical condition level over the full cycle of care as the catalyst for improving the safety, efficiency, and quality of health care". Health 2.0 defines the combination of health data and health information with (patient) experience, through the use of ICT, enabling the citizen to become an active and responsible partner in his/her own health and care pathway. Health 2.0 is a participatory healthcare. Enabled by information, software, and communities that we collect or create, we the patients can be effective partners in our own healthcare, and we the people can participate in reshaping the health system itself. Definitions of Medicine 2.0 appear to be very similar but typically include more scientific and research aspects—Medicine 2.0: "Medicine 2.0 applications, services and tools are Web-based services for health care consumers, caregivers, patients, health professionals, and biomedical researchers, that use Web 2.0 technologies as well as semantic web and virtual reality tools, to enable and facilitate specifically social networking, participation, apomediation, collaboration, and openness within and between these user groups. Published in JMIR Tom Van de Belt, Lucien Engelen et al. systematic review found 46 (!) unique definitions of health 2.0 Overview Health 2.0 refers to the use of a diverse set of technologies including Connected Health, electronic medical records, mHealth, telemedicine, and the use of the Internet by patients themselves such as through blogs, Internet forums, online communities, patient to physician communication systems, and other more advanced systems. A key concept is that patients themselves should have greater insight and control into information generated about them. Additionally Health 2.0 relies on the use of modern cloud and mobile-based technologies. Much of the potential for change from Health 2.0 is facilitated by combining technology driven trends such as Personal Health Records with social networking —"[which] may lead to a powerful new generation of health applications, where people share parts of their electronic health records with other consumers and 'crowdsource' the collective wisdom of other patients and professionals." Traditional models of medicine had patient records (held on paper or a proprietary computer system) that could only be accessed by a physician or other medical professional. Physicians acted as gatekeepers to this information, telling patients test results when and if they deemed it necessary. Such a model operates relatively well in situations such as acute care, where information about specific blood results would be of little use to a lay person, or in general practice where results were generally benign. However, in the case of complex chronic diseases, psychiatric disorders, or diseases of unknown etiology patients were at risk of being left without well-coordinated care because data about them was stored in a variety of disparate places and in some cases might contain the opinions of healthcare professionals which were not to be shared with the patient. Increasingly, medical ethics deems such actions to be medical paternalism, and they are discouraged in modern medicine. A hypothetical example demonstrates the increased engagement of a patient operating in a Health 2.0 setting: a patient goes to see their primary care physician with a presenting complaint, having first ensured their own medical record was up to date via the Internet. The treating physician might make a diagnosis or send for tests, the results of which could be transmitted directly to the patient's electronic medical record. If a second appointment is needed, the patient will have had time to research what the results might mean for them, what diagnoses may be likely, and may have communicated with other patients who have had a similar set of results in the past. On a second visit a referral might be made to a specialist. The patient might have the opportunity to search for the views of other patients on the best specialist to go to, and in combination with their primary care physician decides whom to see. The specialist gives a diagnosis along with a prognosis and potential options for treatment. The patient has the opportunity to research these treatment options and take a more proactive role in coming to a joint decision with their healthcare provider. They can also choose to submit more data about themselves, such as through a personalized genomics service to identify any risk factors that might improve or worsen their prognosis. As treatment commences, the patient can track their health outcomes through a data-sharing patient community to determine whether the treatment is having an effect for them, and they can stay up to date on research opportunities and clinical trials for their condition. They also have the social support of communicating with other patients diagnosed with the same condition throughout the world. Level of use of Web 2.0 in health care Partly due to weak definitions, the novelty of the endeavor and its nature as an entrepreneurial (rather than academic) movement, little empirical evidence exists to explain how much Web 2.0 is being used in general. While it has been estimated that nearly one-third of the 100 million Americans who have looked for health information online say that they or people they know have been significantly helped by what they found, this study considers only the broader use of the Internet for health management. A study examining physician practices has suggested that a segment of 245,000 physicians in the U.S are using Web 2.0 for their practice, indicating that use is beyond the stage of the early adopter with regard to physicians and Web 2.0. Types of Web 2.0 technology in health care Web 2.0 is commonly associated with technologies such as podcasts, RSS feeds, social bookmarking, weblogs (health blogs), wikis, and other forms of many-to-many publishing; social software; and web application programming interfaces (APIs). The following are examples of uses that have been documented in academic literature. Criticism of the use of Web 2.0 in health care Hughes et al. (2009) argue there are four major tensions represented in the literature on Health/Medicine 2.0. These concern: the lack of clear definitions issues around the loss of control over information that doctors perceive safety and the dangers of inaccurate information issues of ownership and privacy Several criticisms have been raised about the use of Web 2.0 in health care. Firstly, Google has limitations as a diagnostic tool for Medical Doctors (MDs), as it may be effective only for conditions with unique symptoms and signs that can easily be used as search term. Studies of its accuracy have returned varying results, and this remains in dispute. Secondly, long-held concerns exist about the effects of patients obtaining information online, such as the idea that patients may delay seeking medical advice or accidentally reveal private medical data. Finally, concerns exist about the quality of user-generated content leading to misinformation, such as perpetuating the discredited claim that the MMR vaccine may cause autism. In contrast, a 2004 study of a British epilepsy online support group suggested that only 6% of information was factually wrong. In a 2007 Pew Research Center survey of Americans, only 3% reported that online advice had caused them serious harm, while nearly one-third reported that they or their acquaintances had been helped by online health advice. See also e-Patient Health 3.0 Patient opinion leader Digital health References External links "Web Site Harnesses Power of Social Networks", The Washington Post, October 19, 2009 German studies is an academic field that researches, documents and disseminates German language, literature, and culture in its historic and present forms. Academic departments of German studies therefore often focus on German culture, German history, and German politics in addition to the language and literature component. Approaches to the discipline vary by country. Modern German studies is usually seen as a combination of two sub-disciplines: German linguistics alongside Germanophone literature and cultural studies. Common names for "German Studies" for the field within German-speaking countries are Germanistik, Deutsche Philologie, and Deutsche Sprachwissenschaft und Literaturwissenschaft. In English, the terms Germanistics or Germanics are sometimes used (mostly by Germans), but the subject is more often referred to as German studies, German language and literature, or German philology. Academics who specialize in German studies are referred to as Germanists. German linguistics German linguistics is traditionally called philology in Germany, though most German studies departments house linguists whose focus relates to German or Germanic language(s) in both their historic and present forms. The periods of German's philological development are roughly divided as follows: Old High German (Althochdeutsch) 8th–11th centuries Middle High German (Mittelhochdeutsch) 11th–14th centuries Early New High German (Frühneuhochdeutsch) 14th–17th centuries Modern German (Standard German, German dialectology) 18th–21st centuries In addition, the discipline examines German under various aspects: the way it is spoken and written, i.e., spelling; declination; vocabulary; sentence structure; texts; etc. It compares the various manifestations such as social groupings (slang, written texts, etc.) and geographical groupings (dialects, etc.). German literature studies The study of German literature is divided into two parts: Ältere Deutsche Literaturwissenschaft deals with the period from the beginnings of German in the early Middle Ages up to post-Medieval times around AD 1750, while the modern era is covered by Neuere Deutsche Literaturwissenschaft. The field systematically examines German literature in terms of genre, form, content, and motifs as well as looking at it historically by author and epoch. Important areas include edition philology, history of literature, and textual interpretation. The relationships of German literature to the literatures of other languages (e.g. reception and mutual influences) and historical contexts are also important areas of concentration. German literature studies benefits from the particularly rich printing tradition of the German-speaking world. Given Johannes Gutenberg and thus the modern printing press originates from German-speaking Europe, Germanic texts have historically enjoyed a heightened status among scholars. Other prominent historical figures, such as Martin Luther have also marked the history of literature through his dissemination of the Bible and thereby an early and strong German-speaking reading culture compared to other European publics. Much like other literature-centered fields, German literature studies is concerned with ecocriticism, hermeneutics, feminism, narratology, psychoanalytic criticism, postcolonialism, postmodernism, post-structuralism, reader-response, semiotics, sound studies, spatial theory, speech acts, structuralism, symbology, and queer theory. German Cultural Studies The study of German culture encompasses issues related to German politics, German history, Holocaust studies, national identity, German art, art history, migration, film studies, museum studies, memory studies, German Literature, and media. The sub-field is highly interdisciplinary drawing from both the humanities and social sciences to examine issues related to contemporary German-speaking countries. Such approaches are often focalized through the lens of the German-speaking nation at present. Thus, the study of countries such as Austria and Switzerland, as well as other German-speaking groups, are often focalized not only through their shared German linguistic and cultural heritage, but for their distinct national and diasporic contexts. German cultural studies therefore incorporates the historical areas of German influence across Europe and overseas as it relates to both migration and colonization. German cultural studies is most common in departments located outside of a German-speaking country, acting in many forms as a form of area studies related to the German Sprachraum. A heightened focus on German cultural studies became apparent following the fall of the Berlin Wall in 1989 and has increasingly been tied to the field of European Studies. DAAD Centers for German and European Studies In the years following the fall of the wall, the German federal government established Centers for German and European Studies throughout North America. This effort sought to increase transatlantic political, cultural, and academic cooperation between the United States and post-Soviet Europe, with a strong focus on Germany's increased importance within the European Union as a reunified state. Sponsored by the German Academic Exchange Services (DAAD), these Centers for German and European studies mark a distinct departure from traditional German studies programs and are often housed within broader internationally oriented departments dealing with international affairs, area studies, or public policy within North America. The DAAD Centers for German and European Studies in North America are listed as follows: BMW Center for German and European Studies at Georgetown University Center for German and European Studies at Brandeis University Centers for German and European Studies at the University of Wisconsin and University of Minnesota Center for German and European Studies at the University of California, Berkeley Joint Initiative in German and European Studies at the University of Toronto Center for German and European Studies at the University of British Columbia Le Centre Canadien d’Études Allemandes et Européennes at the Université de Montréal The Program for the Study of Germany and Europe at Harvard University DAAD Centers for German and European Studies beyond North America include the following: Institute of German and European Studies, Tongji University, Yangpu District, Shanghai Haifa Center for German and European Studies – University of Haifa European Forum at the Hebrew University - at The Hebrew University of Jerusalem Center of German and European Studies - Pontifical University of Rio Grande do Sul and the Federal University of Rio Grande do Sul Center for German and European Studies at the University of Tokyo Center for German and European Studies at Chung-Ang University in South Korea German teacher education At least in Germany and Austria, German studies in academia play a central role in the education of German school teachers. Their courses usually cover four fields: Linguistics of German (Sprachwissenschaft) German language and literature of up to about 1750 (Ältere Sprache und Literatur) German language and literature since approximately 1750 (Neuere Literaturwissenschaft) Specifics of the didactics of teaching German (Fachdidaktik) Several universities offer specialized curricula for school teachers, usually called "Deutsch (Lehramt)". In Germany, they are leading to a two step exam and certificate by the federated states of Germany cultural authorities, called the Staatsexamen ("state exam"). History As an unsystematic field of interest for individual scholars, German studies can be traced back to Tacitus' Germania. The publication and study of legal and historical source material, such as Medieval Bible translations, were all undertaken during the German Renaissance of the sixteenth century, truly initiating the field of German studies. As an independent university subject, German studies was introduced at the beginning of the nineteenth century by Georg Friedrich Benecke, the Brothers Grimm, and Karl Lachmann. The Nazi period, and immediate predecessor periods before and after World War I, left large parts of the field, which had drifted off more and more into race-biological thinking, greatly compromised and damaged, as major proponents on both the literature (e.g. Prof. Josef Nadler in Vienna) and the linguistics side (e.g. Prof. Eberhard Kranzmayer in Graz) were actively working for the Nazi Party (Kranzmayer, Höfler) and their racist goals (Nadler) While great efforts have been made in the denazification of the field, some biases are suggested by overseas Germanist to have remained. After all, post-war academia, with "Nazi party membership among university professors greatly exceed[ing] that of the population at large," was not a complete restart, least of all, in German philology, where 90% of university teachers were NSDAP members. University departments and research institutions Austria Institute for German Studies (Institut für Germanistik), University of Vienna Institute for German Studies (Institut für Germanistik), University of Graz, the first Germanistik in Austria founded at the request of Anton Emanuel Schönbach in 1873 Bénin Département d'Etudes Germanique (DEG), Université d'Abomey-Calavi Botswana Department of Education and Language Skills, Botho University Brazil Department of German Language and Literature Studies University of São Paulo, Brazil Canada Department of German Language and Literature, Queen's University, Kingston, Ontario Department of Germanic Languages and Literatures, University of Toronto China Department of German, Beijing Foreign Studies University, Haidian District, Beijing Czech Republic Department of German and Austrian Studies, Charles University in Prague Department of German Studies, Palacký University in Olomouc India Jawaharlal Nehru University India Ireland Department of Germanic Studies, Trinity College, The University of Dublin, Ireland Department of German, National University of Ireland – University College Cork, Cork, Ireland Germany "German studies" is taught at many German universities. Some examples are: Germanistisches Seminar der Universität Bonn, Institut für Germanistik, vergleichende Literatur- und Kulturwissenschaft, Rheinische Friedrich-Wilhelms-Universität Bonn Institut für deutsche Sprache und Literatur I & II, Albertus-Magnus-Universität zu Köln Institut für Germanistik I & II, Hamburg University Germanistisches Seminar, Heidelberg University Faculty of Modern Languages Institut für deutsche Philologie, Ludwig Maximilian University of Munich Germanistisches Institut, Westfälische Wilhelms-Universität Münster Deutsches Seminar, Tübingen University Faculty of Modern Languages Greece Faculty of German Language and Literature, National and Kapodistrian University of Athens School of German Language and Literature, Aristotle University of Thessaloniki Russia Department of Area Studies, Moscow State University South Africa School of Languages and Literatures, University of Cape Town Department of Afrikaans and Dutch, German and French, University of the Free State School of Languages, North-West University Ancient Modern Languages Cultures, University of Pretoria School of Languages and Literatures, Rhodes University Department of Modern Foreign Languages, University of Stellenbosch Department of Foreign Languages, University of the Western Cape Department of Literature, Language and Media, University of the Witwatersrand Spain Área de Filología Alemana, University of Salamanca Uganda Department of European and Oriental Languages, German Studies, Makerere University United Kingdom (UK) Department of German, University of Oxford Department of German, University of Cambridge Department of German, University of Manchester Department of German Studies, University of Warwick United States of America (USA) Department of German Studies, University of Arizona, Tucson German Program of the Department of World Languages & Literatures, University of Arkansas, Fayetteville Department of German Studies, Brown University Department of German, University of California, Berkeley Department of Germanic Languages, University of California, Los Angeles Department of German Studies, University of Cincinnati Department of Germanic and Slavic Languages and Literatures, University of Colorado, Boulder, CO Department of Germanic Languages, Columbia University Department of German Studies, Cornell University Department of German Studies, Dartmouth College Department of German, Duke University Department of German, Georgetown University Department of Germanic Languages and Literatures, Harvard University Department of Germanic Studies, University of Illinois at Chicago, Chicago, IL Department of Germanic Languages and Literatures, University of Illinois at Urbana-Champaign Department of Germanic Studies, Indiana University German and Scandinavian Studies, University of Massachusetts Amherst Department of Germanic Languages and Literatures, University of Michigan, Ann Arbor, MI Department of German, Scandinavian, and Dutch, University of Minnesota Department of German, New York University Department of Germanic and Slavic Languages and Literatures, University of North Carolina at Chapel Hill Department of German, Northwestern University Department of Germanic Languages and Literatures, Ohio State University, Columbus, Ohio Department of Germanic Languages and Literatures, University of Pennsylvania Department of Germanic and Slavic Languages, Pennsylvania State University, Pennsylvania Department of Germanic Languages and Literature, University of Pittsburgh, Pennsylvania Department of German, Princeton University Department of Germanic Studies, University of Texas at Austin Department of Classical & Modern Languages, Truman State University, Kirksville, Missouri Department of Germanic and Slavic Languages, Vanderbilt University, Nashville, Tennessee Department of Germanic and Russian, University of Vermont, Burlington, Vermont Department of Germanic Languages and Literatures, University of Virginia Department of Germanics, University of Washington, Seattle, Washington Department of Germanic Languages and Literatures, Washington University in St. Louis, St. Louis, Missouri Department of German, Nordic, and Slavic, University of Wisconsin – Madison Department of German, Yale University Department of Linguistics & Germanic, Slavic, Asian and African Languages, Michigan State University Zimbabwe Department of Languages, Literature and Culture University of Zimbabwe See also Area studies German National Honor Society (Delta Epsilon Phi) in the US German Studies Association Germanic philology Germanisches Nationalmuseum New Objectivity Sturm und Drang Bibliography Books Atlas Deutsche Sprache [CD-ROM]. Berlin: Directmedia Publishing. 2004. Die Deutschen Klassiker (CD-ROM). Berman, Antoine: L'épreuve de l'étranger. Culture et traduction dans l'Allemagne romantique: Herder, Goethe, Schlegel, Novalis, Humboldt, Schleiermacher, Hölderlin. Paris: Gallimard, 1984. ISBN 978-2-07-070076-9. Beutin, Wolfgang. Deutsche Literaturgeschichte. Von den Anfängen bis zur Gegenwart. Stuttgart: J. B. Metzler, 1992. Bogdal, Klaus-Michael, Kai Kauffmann, & Georg Mein. BA-Studium Germanistik. Ein Lehrbuch. In collaboration with Meinolf Schumacher and Johannes Volmert. Reinbek bei Hamburg: Rowohlt, 2008. ISBN 978-3-499-55682-1 Burger, Harald. Sprache der Massenmedien. Berlin: Walter de Gruyter, 1984. Ernst, Peter. Germanistische Sprachwissenschaft. Vienna: WUV, 2004. Fohrmann, Jürgen & Wilhelm Voßkamp, eds. Wissenschaftsgeschichte der Germanistik im 19. Jahrhundert. Stuttgart: J. B. Metzler, 1994. Hartweg, Frédéric G. Frühneuhochdeutsch. Eine Einführung in die deutsche Sprache des Spätmittelalters und der frühen Neuzeit. Tübingen: Niemeyer, 2005. Hermand, Jost. Geschichte der Germanistik. Reinbek bei Hamburg: Rowohlt, 1994. ISBN 978-3-499-55534-3 Hickethier, Knut. Film- und Fernsehanalyse. Stuttgart: J. B. Metzler, 1993. Hickethier, Knut, ed. Aspekte der Fernsehanalyse. Methoden und Modelle. Hamburg: Lit, 1994. Hohendahl, Peter Uwe. German Studies in the United States: A Historical Handbook. New York: Modern Language Association of America, 2003. Kanzog, Klaus. Einführung in die Filmphilologie. Munich: Schaudig, Bauer, Ledig, 1991. Muckenhaupt, Manfred: Text und Bild. Grundfragen der Beschreibung von Text-Bild-Kommunikation aus sprachwissenschaftlicher Sicht. Tübingen: Gunter Narr, 1986. Prokop, Dieter: Medienprodukte. Zugänge – Verfahren – Kritik. Tübingen: Gunter Narr, 1981. Schneider, Jost, ed. Methodengeschichte der Germanistik. Berlin: De Gruyter, 2009. Schumacher, Meinolf. Einführung in die deutsche Literatur des Mittelalters. Darmstadt: Wissenschaftliche Buchgesellschaft, 2010. ISBN 978-3-534-19603-6 Shitanda, So. "Zur Vorgeschichte und Entstehung der deutschen Philologie im 19. Jh.: Karl Lachmann und die Brüder Grimm", in Literarische Problematisierung der Moderne. Medienprodukte : Zugänge-- Verfahren-- Kritik, ed. by Teruaki Takahashi. Munich: Iudicium, 1992. Van Cleve, John W. and A. Leslie Willson. Remarks on the Needed Reform of German Studies in the United States. Columbia, SC: Camden House, 1993. Journals Acta Germanica Arbitrium German Life and Letters German Politics and Society German Studies Review The Germanic Review Germanistik Germanistik in Ireland The German Quarterly Goethe Yearbook Journal of Austrian Studies The Journal of English and Germanic Philology Journal of Germanic Linguistics Lessing Yearbook Modern Language Notes (German Issue) Monatshefte Michigan Germanic Studies New German Critique Oxford German Studies Publications of the English Goethe Society Seminar Teaching German (Unterrichtspraxis) Text+Kritik Transit Zeitschrift für interkulturelle Germanistik Zeitschrift für Germanistik References External links BUBL Link (UK-based) Catalogue of Internet Resources Concerning the German Language (well organized; covers many aspects of the language and the study of it) University of Adelaide's categorized guide to German Area Studies online http://www.dartmouth.edu/~wess/wesslit.html Archived 18 June 2009 at the Wayback Machine (Dartmouth's German-Studies Web links, annotated and arranged by topic) https://web.archive.org/web/20051104142631/http://libadm87.rice.edu/ref/german.cfm (Rice University's guide to German studies, including printed literature and links to German newspapers and magazines) http://www.germanistik.net/ Archived 15 March 2007 at the Wayback Machine germanistik.net (tries to get the user straight to the best sources of help; in German) Germanistik im Netz – Erlanger Liste (The 'Erlanger Liste' is currently the largest collection of links to the various aspects of G***, including such archives, publishers, etc.; in German) Literaturwissenschaft online ("Literaturwissenschaft online" Kiel University's e-learning site with live and archived lectures; free of charge; in German.) Bibliographie der Deutschen Sprach- und Literaturwissenschaft ("BDSL Online" is the electronic version of the largest bibliography in the field of German language and literature studies. Access to report years 1985–1995 is free of charge.) https://web.archive.org/web/20060418211215/http://www.doaj.org/ljbs?cpid=8 (DOAJ Directory of Open Access Journals, Literature and Languages) https://web.archive.org/web/20060411030830/http://www.sign-lang.uni-hamburg.de/Medienprojekt/Literatur/9.med.analy.html (University of Hamburg site with media studies bibliography) Categorical list of German Departments around the world Archived 26 May 2019 at the Wayback Machine Departmental Ratings (USA) Directory of some German resources in libraries and research centers throughout California Archived 5 September 2015 at the Wayback Machine American Library Association German Studies Web Library guides University of Leeds German, Russian and Slavonic Studies University of Wisconsin-Madison German-language Humanities Means of communication or media are used by people to communicate and exchange information with each other as an information sender and a receiver. Diverse arrays of media that reach a large audience via mass communication are called mass media. General information Many different materials are used in communication. Maps, for example, save tedious explanations on how to get to a destination. A means of communication is therefore a means to an end to make communication between people easier, more understandable and, above all, clearer. In everyday language, the term means of communication is often equated with the medium. However, the term "medium" is used in media studies to refer to a large number of concepts, some of which do not correspond to everyday usage. Means of communication are used for communication between sender and recipient and thus for the transmission of information. Elements of communication include a communication-triggering event, sender and recipient, a means of communication, a path of communication and contents of communication. The path of communication is the path that a message travels between sender and recipient; in hierarchies the vertical line of communication is identical to command hierarchies. Paths of communication can be physical (e.g. the road as transportation route) or non-physical (e.g. networks like a computer network). Contents of communication can be for example photography, data, graphics, language, or texts. Means of communication in the narrower sense refer to technical devices that transmit information. They are the manifestations of contents of communication that can be perceived through the senses and replace the communication that originally ran from person to person and make them reproducible. History of the term Up until the 19th century the term "means of communication" was primarily applied to traffic and couriers and to means of transport and transportation routes, such as railways, roads and canals, but also used to include post riders and stagecoachs. In 1861, the national economist Albert Schäffle defined a means of communication as an aid to the circulation of goods and financial services, which included, among other things, newspapers, telegraphy, mail, courier services, remittance advice, invoices, and bills of lading. In the period that followed, the "technical means of communication" increasingly came to the foreground, so that as early as 1895 the German newspaper "Deutsches Wochenblatt" reported that these technical means of communication had been improved to such an extent that "everyone all over the world has become our neighbor". Not until the 20th century was the term medium also a synonym for these technical means of communication. In the 1920s the term mass media started to become more popular. Different types A distinction can be made between oral, written, screen-oriented transfer of information and document transport: In this table means of communication are mentioned that are no longer used today. Furthermore, a distinction can be made between: natural communication: nonverbal communications: applause, gestures, facial expressions (social means of communication); flag signs; language: communication forms such as meetings, discussions; technical communication: writing systems and drawings as data storage of language; Email, fax, teletype, mobile phones, mass media, SMS/MMS, telephone, webcam. Means of communication in the narrower sense are those of technical communication. In companies (businesses, agencies, institutions) typical means of communication include documents, such as analyses, business cases, due diligence reviews, financial analyses, forms, business models, feasibility studies, scientific publications, and contracts. Natural means of communication The means of natural communication or the "primary medias" (see Media studies) include: Speech and other mouth-formed sounds, e.g. screaming; Sign language using hand or body movements, e.g. winking; Other non-verbal means of communication include clothing (see dress code) and other forms of appearance, as well as different accentuations in the living, food and construction culture. Technical means of communication with hands or technical aids written characters on paper or another substrate as a writing medium (letter, message); Printed media produced with the help of printing technology; Playback of sounds or images (in Image Media) by record players such as tape recorders and projectors for slide shows or movies; Transmission of speech by telephone or writing by telegraph, mostly to a single addressee; satellite radio. Communication theory Means of communication are often differentiated in models of communication: in terms of reaching and determining the target audience of a means of communication, whether individual communication, group communication and mass communication; in terms of the technical components in natural and technical means of communication; in terms of the components of speech in verbal and nonverbal communication. Media as a means of communication in the future will be distinguished: by data storage, broadcasting media and processing media, especially to record, reproduce and reduplicate media content. by primary, secondary, tertiary and quaternary media, depending on the technology used by sender and recipient. Mass media Mass media refers to reaching many recipients from one – or less than one – sender simultaneously or nearly simultaneously. Transmission of information via printing products in diverse forms (book, pamphlet, xerography, poster, mail merge, newspaper) Transmission of language, music or other sounds radio waven (radio broadcasting) Transmission of visual image and sound via radio wave (television) The most up-to-date means of communication in a long chain of innovation is the Internet Due to their wide dissemination, mass media are suitable for providing the majority of the population with the same information. Electronic media Developments in telecommunications have provided for media the ability to conduct long-distance communication via analog and digital media: Analog telecommunications include some radio systems, historical telephony systems, and historical television broadcasts. Digital telecommunications allow for computer-mediated communication, telegraphy, computer networks, digital radio, digital telephony and digital television. Modern communication media include long-distance exchanges between larger numbers of people (many-to-many communication via email, Internet forums, and telecommunications ports). Traditional broadcast media and mass media favor one-to-many communication (television, cinema, radio, newspaper, magazines, and social media). Social media Electronic media, specifically social media have become one of the top forms of media that people use in the twenty-first century. The percent of people that use social media and social networking outlets rose dramatically from 5% in 2005 to 79% in 2019. Instagram, Twitter, Pinterest, Tiktok, and Facebook are the most commonly used social media platforms. The average time that an individual spends on social media is 2.5 hours a day. This exponential increase of social media has additionally caused a change in which people communicate with others as well as receive information. About 53% use social media to read/watch the news. Many people use the information specifically from social media influencers to understand more about a topic, business, or organization. Social media has now been made part of everyday news production for journalists around the world. Not only does social media provide more connection between readers and journalists, but it also cultivates the participation and community amongst technical communicators and their audiences, clients, and stakeholders. Gaming Online The gaming community has grown exponentially, and about 65% have taken to playing with others, whether online or in-person. Players online will communicate through the system of microphone applicability either through the game or a third party application such as Discord. The improvements upon connectivity and software allowed for players online to keep in touch and game instantaneously, disregarding location almost entirely. With online gaming platforms it has been noted that they support diverse social gaming communities allowing players to feel a sense of belonging through the screen. Age Gaming is an activity shared amongst others regardless of age, allowing for a diverse group of players to connect and enjoy their favorite games with. This helps with creating or maintaining relationships: friendships, family, or a significant other. Ratings and content As with most interactive media content, games have ratings to assist in choosing appropriate games regarding younger audiences. This is done by ESRB ratings and consists of the following: E for Everyone, E for Everyone 10+, T for Teen, and M for Mature 18+. Whenever a new game is released, it is reviewed by associations to determine a suitable rating so younger audiences do not consume harmful or inappropriate content. With these ratings it helps the risks and effects of gaming on younger audiences because the exposure of media is believed to influence children's attitudes, beliefs, and behaviors. Reach The usage and consumption of gaming has tremendously increased within the last decade with estimates of around 2.3 billion people from around the world playing digital and online video games. The growth rate for the global market for gaming was expected to grow 6.2% towards 2020. Areas like Latin America had a 20.1% increase, Asia-Pacific - 9.2%, North America - 4.0%, and Europe -11.7%. Communication Studies show that digital and online gaming can be used as a communication method to aid in scientific research and create interaction. The narrative, layout, and gaming features all share a relationship that can deliver meaning and value that make games an innovative communication tool. Research-focused games showed a connection towards a greater usage of dialogue within the science community as players had the opportunity to address issues with a game with themselves and scientists. This helped to push the understanding of how gaming and players can help advance scientific research via communication through games. vBook A vBook is an eBook that is digital first media with embedded video, images, graphs, tables, text, and other useful media. E-Book An E-book combines reading and listening media interaction. It is compact and can store a large amount of data which has made them very popular in classrooms. Regulations The role of regulatory authorities (license broadcaster institutions, content providers, platforms) and the resistance to political and commercial interference in the autonomy of the media sector are both considered as significant components of media independence. In order to ensure media independence, regulatory authorities should be placed outside of governments' directives. This can be measured through legislation, agency statutes and rules. Government regulations Licensing In the United States, the Radio Act of 1927 established that the radio frequency spectrum was public property. This prohibited private organizations from owning any portion of the spectrum. A broadcast license is typically given to broadcasters by communications regulators, allowing them to broadcast on a certain frequency and typically in a specific geographical location. Licensing is done by regulators in order to manage a broadcasting medium and as a method to prevent the concentration of media ownership. Licensing has been criticized for an alleged lack of transparency. Regulatory authorities in certain countries have been accused of exhibiting political bias in favor of the government or ruling party, which has resulted in some prospective broadcasters being denied licenses or being threatened with license withdrawal. As a consequence, there has been a decrease in diversity of content and views in certain countries due to actions made against broadcasters by states via their licensing authorities. This can have an impact on competition and may lead to an excessive concentration of power with potential influence on public opinion. Examples include the failure to renew or retain licenses for editorially critical media, reducing the regulator's competences and mandates for action, and a lack of due process in the adoption of regulatory decisions. Internet regulation Governments worldwide have sought to extend regulation to internet companies, whether connectivity providers or application service providers, and whether domestically or foreign-based. The impact on journalistic content can be severe, as internet companies can err too much on the side of caution and take down news reports, including algorithmically, while offering inadequate opportunities for redress to the affected news producers. Self-regulation At the regional level In Western Europe, self-regulation provides an alternative to state regulatory authorities. In such contexts, newspapers have historically been free of licensing and regulation, and there has been repeated pressure for them to self-regulate or at least to have in-house ombudsmen. However, it has often been difficult to establish meaningful self-regulatory entities. In many cases, self-regulations exists in the shadow of state regulation, and is conscious of the possibility of state intervention. In many countries in Central and Eastern Europe, self-regulatory structures seems to be lacking or have not historically been perceived as efficient and effective. The rise of satellite channels that delivered directly to viewers, or through cable or online systems, renders much larger the sphere of unregulated programing. There are, however, varying efforts to regulate the access of programmers to satellite transponders in parts of the Western Europe, North America, the Arab region and in Asia and the Pacific. The Arab Satellite Broadcasting Charter was an example of efforts to bring formal standards and some regulatory authority to bear on what is transmitted, but it appears to not have been implemented. International organizations and NGOs Self-regulation is expressed as a preferential system by journalists but also as a support for media freedom and development organizations by intergovernmental organizations such as UNESCO and non-governmental organizations. There has been a continued trend of establishing self-regulatory bodies, such as press councils, in conflict and post-conflict situations. Major internet companies have responded to pressure by governments and the public by elaborating self-regulatory and complaints systems at the individual company level, using principles they have developed under the framework of the Global Network Initiative. The Global Network Initiative has grown to include several large telecom companies alongside internet companies such as Google, Facebook and others, as well as civil society organizations and academics. The European Commission's 2013 publication, ICT Technology Sector Guide on Implementing the United Nations Guiding Principles on Business and Human Rights, impacts on the presence of independent journalism by defining the limits of what should or should not be carried and prioritized in the most popular digital spaces. Private sector Public pressure on technology giants has motivated the development of new strategies aimed not only at identifying 'fake news', but also at eliminating some of the structural causes of their emergence and proliferation. Facebook has created new buttons for users to report content they believe is false, following previous strategies aimed at countering hate speech and harassment online. These changes reflect broader transformations occurring among tech giants to increase their transparency. As indicated by the Ranking Digital Rights Corporate Accountability Index, most large internet companies have reportedly become relatively more forthcoming in terms of their policies about transparency in regard to third party requests to remove or access content, especially in the case of requests from governments. At the same time, however, the study signaled a number of companies that have become more opaque when it comes to disclosing how they enforce their own terms of service, in restricting certain types of content and account. State governments can also use "Fake news" in order to spread propaganda. Fact-checking and news literacy In addition to responding to pressure for more clearly defined self-regulatory mechanisms, and galvanized by the debates over so-called 'fake news', internet companies such as Facebook have launched campaigns to educate users about how to more easily distinguish between 'fake news' and real news sources. Ahead of the United Kingdom national election in 2017, for example, Facebook published a series of advertisements in newspapers with 'Tips for Spotting False News' which suggested 10 things that might signal whether a story is genuine or not. There have also been broader initiatives bringing together a variety of donors and actors to promote fact-checking and news literacy, such as the News Integrity Initiative at the City University of New York's School of Journalism. This 14 million USD investment by groups including the Ford Foundation and Facebook was launched in 2017 so its full impact remains to be seen. It will, however, complement the offerings of other networks such as the International Fact-Checking Network launched by the Poynter Institute in 2015 which seeks to outline the parameters of the field. Instagram has also created a way to potentially expose "fake news" that is posted on the site. After looking into the site, it seemed as more than a place for political memes, but a weaponized platform, instead of the creative space it used to be. Since that, Instagram has started to put warning labels on certain stories or posts if third-party fact checkers believe that false information is being spread. Instagram works with these fact checkers to ensure that no false information is being spread around the site. Instagram started this work in 2019, following Facebook with the idea as they started fact checking in 2016. See also Media history Bibliography General Lothar Hoffmann, Kommunikationsmittel: Fachsprache: eine Einführung, 1976 Michael Franz, Electric Laokoon: Zeichen und Medien, von der Lochkarte zur Grammatologie, 2007, ISBN 978-3-05-003504-8 Horst Völz, Das ist Information. Shaker Verlag, Aachen 2017, ISBN 978-3-8440-5587-0. Natural communication Jean Werner Sommer, Kommunikationsmittel: Wort und Sprache, 1970 Beat Pfister, Tobias Kaufmann: Sprachverarbeitung: Grundlagen und Methoden der Sprachsynthese und Spracherkennung, 2008, ISBN 978-3-540-75909-6 Renate Rathmayr, Nonverbale Kommunikationsmittel und ihre Versprachlichung, 1987 Mass media Jörg Aufermann, Hans Bohrmann, Massenkommunikationsmittel, 1968 Fritz Eberhard, Optische und akustische Massenkommunikationsmittel, 1967 Theodor Bücher, Pädagogik der Massenkommunikationsmittel, 1967 Hans Kaspar Platte, Soziologie der Massenkommunikationsmittel, 1965 Social means of communication Daniel Michelis, Thomas Schildhauer (2010). Social Media Handbuch – Theorien, Methoden, Modelle. Baden-Baden: Nomos. p. 327. ISBN 978-3-8329-5470-3. Daniel Michelis, Thomas Schildhauer (2012). Social Media Handbuch - Theorien, Methoden, Modelle und Praxis (2. updated and extended ed.). Baden-Baden: Nomos. p. 358. ISBN 978-3-8329-7121-2. == Citations == The Nova Center for Social Innovation is an organization that aims to promote social innovation to help generate alternative socioeconomic models to globalization. The center's objectives also include creating a culture of peace and a more sustainable and participatory society. Nova works with organizations and institutions interested in starting projects that promote social innovation, and provides services that help citizens become actively involved in these processes. The Nova association was founded in 2000. Prior to then, the team behind the center took part in the Centre d'Estudis Joan Bardina (1983–1991) and EcoConcern (1992–1999), where it organized the Catalan Forum for Rethinking Society (1996–2001) within the framework of the Alliance for a responsible, plural and united world (1995–2001). Since 2003, Nova has been a member of Nonviolent Peaceforce, and through its 'NoViolenciaActiva' Team (known as "Forces de Pau Noviolentes" until 2006) it promotes activities on the culture of peace and nonviolence, attempting to implement systems of nonviolent intervention and civilian resistance in conflict areas. Since 2008, Nova has worked on the project Barcelona Consensus: Intercultural Alternatives to Neo-liberal Globalization. External links www.nova.cat The term model minority refers to a minority group, defined by factors such as ethnicity, race, or religion, whose members are perceived to be achieving a higher socioeconomic status in comparison to the overall population average. Consequently, these groups are often regarded as a role model or reference group for comparison to external groups (outgroups). This success is typically assessed through metrics including educational attainment, representation within managerial and professional occupations, household income, and various other socioeconomic indicators such as criminal activity and strong family and marital stability. The prominent association of the model minority concept is with Asian Americans within the United States. Additionally, analogous concepts of classism have been observed in numerous European countries, leading to the stereotyping of specific ethnic groups. The concept of the model minority has generated controversy due to its historical application to suggest that economic intervention by governments is unnecessary to address socioeconomic disparities among particular racial groups. Primarily evident in the American context, this argument has been employed to draw contrasts between Asian Americans (particularly those of East and some South Asian origins) and Jewish Americans in comparison to African Americans and Indigenous peoples. Consequently, this perpetuates the propagation of a 'model minority myth', asserting that Asian and Jewish Americans are exemplary law-abiding and productive citizens or immigrants, while concurrently reinforcing the stereotype that Indigenous and African American communities are predisposed to criminal behavior and dependent on welfare. Issues The concept of a model minority is heavily associated with U.S. culture, due to the term's origins in American sociologist William Petersen's 1966 article. Many European countries have concepts of classism that stereotype ethnic groups in a manner which is similar to the stereotype of the model minority. Generalized statistics, such as higher education attainment rate, high representation in white-collar professional and managerial occupations, and a higher household income than other racial groups in the United States are often cited in support of model-minority status. A common misconception is that the affected communities typically take pride in being labeled as a model minority. However, the model minority stereotype is considered detrimental to relevant minority communities because it is used to justify the exclusion of such groups in the distribution of (public and private) assistance programs, and it is also used to understate or slight the achievements of individuals within that minority. There are a wide variety of theories categorizing types of prejudices, and different types of prejudices are believed to be more at play towards different particular groups, one such model being the stereotype content model. Generally speaking, within the American and European social context, groups such as those with Asian heritage or Jewish heritage are believed to score high on perceived competence but low on perceived warmth and thus are thought to fall into the category of the 'envied outgroup' within the context of this stereotype. Additional studies have shown that when describing a group with the term 'model minority' and associated attributes, responses towards the out-group were significantly more negative than those using other positive attributes. Other scholars have discussed the potential for the stereotype to be the 'positive spin' on the money-mad, stealing and/or greedy Jew or Asian. Recent additional studies have delved into the role of jealous prejudice in instigating certain historical mass casualty events, such as the Holocaust, noting that the theory of the venting of frustrations on an innocent but weak target is a notion that is part of popular "folk psychology" and should be re-examined, arguing instead that envious prejudice plays a relevant role in scapegoating in some social contexts. The concept of the model minority has generated controversy due to its historical application to suggest that economic intervention by governments is unnecessary to address socioeconomic disparities among particular racial groups. Furthermore, the notion of the model minority pits minority groups against one another through the implication that non-model groups are at fault for falling short of the model minority level of achievement. The concept has been criticized by outlets such as NPR and EU Scream for potentially homogenizing the experiences of Asian communities on one side and Hispanics and African Americans on the other, despite the fact that individual groups experience racism in different ways. Critics also argue that the idea perpetuates the belief that any minority has the capability to economically rise without assistance because it ignores the differences between the history of Asian Americans and the history of African Americans, as well as the history of Hispanics, in the United States. It has also been pointed out that the concept, which also has been criticized for over generalizing the success of some community members, has been used to invalidate and render less visible the racism faced by model minorities. Additionally, over generalizing based on a measure success for some members to make the point that racism is over and anyone stating otherwise is "making excuses" is not exclusive to those groups who have been called model minorities, and became a problem for some members of the African American community after Barack Obama's Presidential election. United States One of the earliest uses of the term model minority was in the 9 January 1966 edition of The New York Times Magazine by sociologist William Petersen to describe Asian Americans as ethnic minorities which, despite their marginalization, have achieved success in the United States. In his essay titled "Success Story: Japanese American Style", he wrote that the Japanese cultures have strong work ethics and family values which, consequently, lift them above "problem minorities". Petersen believed that the success of Asian Americans paralleled the success of Jewish Americans. A similar article about Chinese Americans was published in U.S. News & World Report in December 1966. Asian Americans Although the term was first coined to describe the socioeconomic success of Japanese Americans, "model minority" eventually evolved to become associated with American Jews and Asian Americans in general, more specifically with East Asians (Japanese, Chinese, and Korean Americans) as well as Indian Americans and other South Asian Americans. By the 1980s, almost all major U.S. magazines and newspapers printed success stories of Asian Americans.: 222 Racial attacks against Asian Americans were reported since the early 1980s. Some scholars have described the creation of the model minority theory as a partial response to the emergence of the civil rights movement, in which African Americans fought for equal rights and the discontinuation of racial segregation in the United States. In reaction to the success of the movement, white America, citing the accomplishments of Asian Americans, argued that African Americans could raise their communities up by focusing on education and accepting and conforming to the racial segregation, institutional racism and discrimination which were all being practiced at that time. At that time however, Asian Americans were also marginalized and racially segregated, which meant that they also represented lower economic levels and faced the same social issues which other racial and ethnic minorities faced. A few years after The New York Times Magazine article about Asian Americans being the model minority was published, Asian Americans formed their own movement, in which they fought for their own equal rights and the resolution of their own specific social issues. It would be modeled after the Civil Rights Movement, thus, it would effectively challenge White America and the social construct of racial discrimination. Those who resisted the emergent stereotype in the 1960s–1980s could not gain enough support to combat it due to its so-called "positive" connotations. At the time, this led many, even within the Asian American community, to either view it as a welcomed label in contrast to years of negative stereotypes, or view it as a euphemistic stereotype that was no more than a mere annoyance. Many believe that the stereotype comes with more positives than negatives. In contrast, many critics believe that there are just as many negatives as there are positives, or they believe that stereotypes should never be regarded as "good," no matter how "positive" they are intended to be. Scientific studies have revealed that both socially and psychologically, positive stereotypes have many negative and damaging consequences. According to Marita Etcubañez, a director of Asian Americans Advancing Justice - Los Angeles, misconceptions about Asian Americans have an effect on government policy, as "politicians won't talk about our community's needs if they assume people don't require assistance." According to Yanan Wang writing for the Washington Post, since the 1960s, "the idea that Asian Americans are distinct among minority groups and therefore immune to the challenges which are faced by other people of color is a particularly sensitive issue in the community, which has recently fought to reclaim its place in social justice conversations with movements like #ModelMinorityMutiny." In his paper, "Education and the Socialization of Asian Americans: A Revisionist Analysis of the 'Model Minority Thesis'", B. Suzuki, a researcher of multicultural and Asian American studies at University of Massachusetts Amherst, disagrees with how the media has portrayed Asian Americans. Explaining the sociohistorical background of the contemporary social system, Suzuki argues that the model minority stereotype is a myth.: 3 Since the creation of the model minority stereotype, Asian Americans have exceeded White Americans in terms of their level of education, as well as many other racial and ethnic groups in American society. As of 2012, Asian Americans as a whole are considered as having obtained the highest educational attainment level and the highest median household income of any racial and ethnic demographic in the country, a position which African immigrants, and their first generation descendants, have just started to outperform them in. These statistics vary among the Asian American population. Historically, achieving economic and educational success was, and at times still is, seen as a gateway by different groups into greater social acceptance. This notion was shattered at different times for some people within American communities, for example within the Muslim American community dramatically in the wake of 9/11. Statistics There has been a significant change in the perceptions of Asian Americans. In as little as 100 years of American history, stereotypes of East Asian Americans have changed from them being viewed as poor uneducated laborers to being portrayed as a hard-working, well-educated, and upper-middle-class minority. Proponents of the model minority model erroneously assumed that Asian Americans' perseverance, strong work ethic, and general determination to succeed were extensions of their supposedly quiet natures, rather than common characteristics among most immigrants. Among Indian Americans, an example of the model minority stereotype are phenomena such as the high rates of educational attainment and above average household incomes in the Indian American community. Pointing to generalized data, another argument for the model minority stereotype is generalized data such as from the United States Census Bureau, where the median household income of Asian Americans is $68,780, higher than that of the total population ($50,221). Although some Asian American subgroups including East Asians and South Asians are economically successful, other Asian American subgroups such as Southeast Asian Americans which include Hmong, Laotians, Cambodians, and Vietnamese, are less socioeconomically successful. Asian Americans have developed the greatest income inequality gap in comparison to major racial and ethnic groups in the U.S. The economic gap in the standard of living between higher- and lower-income Asians nearly doubled; the ratio of income earned by Asians at the 90th percentile to income earned by Asians at the 10th percentile increased from 6.1 to 10.7 between 1970 and 2016, respectively. The model minority model also points to the percentage of Asian Americans at elite universities. Model minority proponents claim that while Asian Americans are only 5% of the U.S. population, they are over-represented at all these schools. Additionally, Asian Americans go on to win a high proportion of Nobel Prizes. Of the 20 American physicists to win a Nobel Prize in the 21st century, East Asian Americans, who represent less than 4% of the U.S. population, have won 15% of prizes. Additionally, three science Nobel prizes have been won by Indian-Americans. Asian American students are concentrated in a very small percentage of institutions, in only eight states (and half concentrated in California, New York and Texas). Moreover, as more Asian Americans become Americanized and assimilated, more Asian American students are beginning to attend two-year community colleges (363,798 in 2000) than four-year public universities (354,564 in 2000), and this trend of attending community college is accelerating. West Coast academic institutions are amongst those that have the highest concentrations of Asian Americans. The most highly educated group of Asian immigrants are Taiwanese. Education rates of Southeast Asians are low, but these numbers can be considered misleading, as a large percent comes from adult immigrants who came to the United States without any college education due to war. For ages 25 to 34, 45% of Vietnamese Americans have a bachelor's degree or higher compared to 39% of Non-Hispanic Whites. Due to the impacts of the model minority stereotype, unlike other minority-serving institutions, Asian American Pacific Islander-serving institutions (AAPISI) did not receive federal recognition until 2007, with the passage of the College Cost Reduction and Access Act, which federally recognized the existence of AAPISIs, making them eligible for federal funding and designation as minority serving institutions. According to the Federal Bureau of Investigation's 2003 report Crime in the United States, Asian Americans have the lowest total arrest rates despite a younger average age, and high family stability. Indian and South Asian Americans The model minority label also includes Indian Americans, because of their high aggregate socioeconomic success. According to the census report on Asian Americans issued in 2004 by the U.S. Census Bureau, 64% of Indian Americans had a bachelor's degree or higher, the second highest for all national origin groups. In the same census, 60% of Indian Americans had management or professional jobs, compared with a national average of 33%. Indian Americans also earn the highest median household income out of all national origin/ethnic groups and the second highest personal income, after Taiwanese Americans. This has resulted in several stereotypes such as that of the "Indian Doctor". It should however be noted that there are still pockets of poverty within the community, with around 8% classified as living in poverty. Southeast Asian Americans Arguably, the model minority stereotype masks the socioeconomic under performance of other Asian American subgroups and the experiences of Southeast Asian American populations in the U.S. serve to refute the model minority stereotype. For context, Southeast Asian Americans consist of several ethnic groups, including Burmese, Vietnamese, Hmong, Laotian, and Cambodian. Despite high household incomes, many Southeast Asian Americans and other Asian groups such as Filipinos, Vietnamese, Indonesians, Thais, Laotians, Cambodians as well as South Asian groups such as Nepalese and Pakistani have lower per capita incomes then East Asians. An empirical literature review shows that most of the existing data used to justify the model minority image regarding Asian American academic achievement is aggregated. As a result, this data ignores important differences among individual Asian ethnic groups. Although many Asian Americans have succeeded academically and socioeconomically, survey research shows that recent immigrant groups, such as Southeast Asians, have been unable to replicate such success. According to the 2010 U.S. Census, the overall percentage of people 25 years and older with less than a high school education in the U.S. population is 15%, whereas Asian Americans, as an aggregate, are close at 11.1%. However, disparities exist when comparing South Asian Americans and East Asian Americans with Southeast Asian Americans. For example, only 13.6% of Chinese Americans, 4.0% Japanese Americans, and 6.0% of South Asian Americans ages 25 years or older have less than a high school education. In contrast, Southeast Asian Americans more than double the South Asian American and East Asian American percentages with 38.5% of Cambodian Americans, 39.6% of Hmong Americans, 34.3% of Lao Americans, and 51.1% of Vietnamese Americans ages 25 and over holding less than a high school education. According to some studies only 39% of Filipino American men (ages 25–34) had attained a Bachelor's degree, in comparison to 87% of Asian Indian American men, 69% of Chinese American men, 63% of Japanese American men, 62% of Korean American men, and 42 percent of Vietnamese American men. The same study showed that Filipino, Korean and Cambodian men with Bachelor's degrees have lower median wages of $30 an hour compared to Chinese and Indian immigrant men who had median wages of $40 an hour. (Sanchez-Lopez et al ., 2017). The 2010 U.S. Census shows that 52% of Asian Americans ages 25 and over hold a bachelor's degree or higher, which is higher than the national American average of 29.9%. In contrast, the percentage of individuals aged 25 and over holding a bachelor's degree or higher amongst Southeast Asian American groups is much lower with only 44.4% of Filipino Americans and 21.2% of Vietnamese Americans falling within the aforementioned educational bounds. With the exception of Vietnamese Americans, Southeast Asian American representation in higher education is lower than other racial minorities, including African Americans (14.2%) and Latino Americans (10.3%). As cited in an empirical literature review, research that lacks differentiation between the varying Asian ethnic groups may mask under-performing groups as the higher performing groups raise the average. As a result, Southeast Asian American students are often overlooked due to the overwhelming success of their East and South Asian American peers. As cited in a case study, many deficits of Southeast Asian American students' academic achievement can be attributed to the structural barriers of living in an immigrant household. Many Southeast Asian American students are children of refugees from countries at war. While the parents of Southeast Asian American students may have escaped death and persecution from their homelands, they often arrive in the US with fragmented families. As a result, refugees often lack resources, which causes them to not only rely on government assistance, but to also be placed in low-income communities near poorly funded schools. Additionally, families frequently have little to no understanding of the U.S. school system. Thus, Southeast Asian students are at a disadvantage as they have to quickly adjust to the new school system, while also keeping up with native-born students. However, certain Southeast Asian ethnic groups have shown greater progress than others within the regional group and resemble the success of other more established Asian Americans. As cited in a case study, Vietnamese American students are beginning to show comparable rates of academic success to East Asian American students. Furthermore, among Southeast Asian American students, Vietnamese American students are recognized as having the highest academic performance, whereas Cambodian American students have the poorest performance. Although Cambodian and Vietnamese refugees endured similar immigration hardships, the aforementioned differences in academic success is attributed to structural and cultural factors. Another factor which may have an influence on Vietnamese American success is that the majority of 21st century Vietnamese immigrants to the United States are from non-refugee backgrounds, dissimilar from earlier migration patterns. Despite this progress amongst Southeast Asian American students, the subgroup still struggles economically. Similar to data on academic achievement, information regarding Asian American's economic prospects is frequently aggregated and thus hides the diversity of economic struggles amongst subgroups like Southeast Asian Americans. The high poverty rate amongst Hmong Americans places the group in one of the highest poverty brackets within the United States. Hmong Americans, more so men than women, have also been disproportionately racialized and criminalized via gangster stereotyping. Additionally, median income levels differ amongst Asian American subgroups in which Southeast Asian Americans represent a disproportionate amount of low annual median incomes. This is illustrated by research in which Hmong Americans and Cambodian Americans have a per capita income of $10,366 in comparison to Indian Americans who have a per capita income of around $27,514 and Japanese Americans who have a per capita income of $30,075. By analyzing the individual economic data of Asian American subgroups, it becomes evident that the model minority stereotype, which puts forth the notion of Asian Americans achieving higher levels socioeconomic success, may be misleading. It is also written in Racial Wealth Snapshot by NCRC that Asian Americans disproportionately live in metropolitan areas where cost of living is high and that it is important to factor in household size and cost of living when talking about Asian Americans. Media portrayal Media coverage of the increasing success of Asian Americans as a group began in the 1960s, reporting high average test scores and marks in school, winning national spelling bees, and high levels of university attendance. In 1988, the writer Philip K. Chiu identified the prevalence of the model minority stereotype in American media reports on Chinese Americans, and noted the contrast between that stereotype and what he observed as the reality of the Chinese-American population, which was much more varied than the model minority stereotype in the media typically presented. I am fed up with being stereotyped as either a subhuman or superhuman creature. Certainly I am proud of the academic and economic successes of Chinese Americans.… But it's important for people to realize that there is another side.… It is about time for the media to report on Chinese Americans the way they are. Some are superachievers, most are average citizens, and a few are criminals. They are only human—no more and no less. Effects of the stereotype According to Gordon H. Chang, the reference to Asian Americans as model minorities has to do with the work ethic, respect for elders, and high valuation of education, family and elders present in their cultures. The model minority stereotype also comes with an underlying notion of their apoliticality. Such a label one-dimensionalizes Asian Americans as having only traits based around stereotypes and no other human qualities, such as vocal leadership, negative emotions (e.g. anger or sadness), sociopolitical activeness, risk taking, ability to learn from mistakes, desire for creative expression, intolerance towards oppression or being overlooked of their acknowledgements and successes. Asian Americans are labeled as model minorities because they have not been as much of a "threat" to the U.S. political establishment as black people, due to a smaller population and less political advocacy. This label seeks to suppress potential political activism through euphemistic stereotyping. Another effect of the stereotype is that American society may tend to ignore the racism and discrimination Asian Americans still face. Complaints are dismissed with the claim that the racism which occurs to Asian Americans is less important than or not as bad as the racism faced by other minority races, thus establishing a systematic racial hierarchy. Believing that due to their success and that they possess so-called "positive" stereotypes, many assume they face no forms of racial discrimination or social issues in the greater American society, and that their community is fine, having "gained" social and economic equality. The stereotyping of Asian Americans as a model minority and perfidious foreigner influences people's perceptions and attitudes towards Asians and also negatively affects students' academic outcomes, relationships with others, and psychological adjustments. For instance, discrimination and model minority stereotyping are linked to Asian American students' lower valuing of school, lower self-esteem, and higher depressive symptoms. Asian Americans may also be commonly stereotyped by the general public as being studious, intelligent, successful, elitist, brand name conscious, yet paradoxically passive. As a result, Asian Americans have felt as though they have higher and unreasonable expectations due to their race. Also due to the model minority image, Asian American students are viewed as "problem-free" and academically competent students who can succeed with little support and without special services. This emphasis that Asian Americans are being denial by their racial reality because of the assumption that "Asians are the new Whites"; therefore, they are being dismissed by their intelligence and experiences. Thus, educators may overlook the instructional needs and psychological concerns of underachieving Asian American students. The model minority stereotype can also contribute to teachers' having a "blaming the victims" perspective. This means that teachers blame students, their culture, or their families for students' poor performance or misbehavior in school. This is problematic because it shifts responsibility away from schools and teachers and misdirects attention away from finding a solution to improve students' learning experience and alleviate the situation. Furthermore, the model minority stereotype has a negative impact on the home environment. Parents' expectations place high pressure on students to achieve, creating a stressful, school-like home environment. Parents' expressed worry and frustration can also place emotional burdens and psychological stress on students. Another result of Asian American's regarded as a model minority is limiting the amount of accepted applicants to certain colleges. Some educators hold Asian students to a higher standard. This deprives those students with learning disabilities from being given attention that they need. The connotations of being a model minority mean Asian students are often labeled with the unpopular "nerd" or "geek" image.: 223 Asians have been the target of harassment, bullying, and racism from other races due to the racially divisive model minority stereotype.: 165 The higher expectations placed on East Asians as a result of the model minority stereotype carries over from academics to the workplace. The model minority stereotype is emotionally damaging to many Asian Americans, since there are unjustified expectations to live up to stereotypes of high achievement. The pressures from their families to achieve and live up to the model minority image have taken a tremendous mental and psychological toll on young Asian Americans. The model minority stereotype also influences Asian American students' psychological outcomes and academic experience. The model minority image can lead underachieving Asian American students to minimize their own difficulties and experience anxiety or psychological distress about their academic difficulties. Asian American students also have more negative attitudes toward seeking academic or psychological help due to fear of shattering the high expectations of teachers, parents, and classmates. Possible causes of model minority status Selective immigration One possible cause of the higher performance of Asian Americans as a group is that they represent a small population in America so those who are chosen to move to America often come from a selective group of Asians. The relative difficulty of emigrating and immigrating into the United States has created a selective nature of the process with the U.S. often choosing the wealthier and more highly educated out of those with less resources, motivation or ability to immigrate. Asian Americans are the nation's fastest growing ethnic group due to their high rate of immigration. 59% of all Asian Americans are foreign born. The majority of Asian Americans are either 1st or 2nd generation immigrants, with the Asian-American population increasing from only 980,000 in 1960 to 22.4 million in 2019. Due to their high rate of immigration, the Asian American population nearly doubled from 11.9 to 22.4 million in the period between 2000 and 2019 – an 88% increase. For reference, the Black population grew by 20% during this span, while there was virtually no change in the White population. Asia is a much larger pool of skilled workers as the continent has 4.2 billion people, 60% of the world population. This far outnumbers the next two most populous continents of Africa (15% total world population) and Europe (10%). 82% of Asian American workers in STEM fields were foreign born, as well as 81% of the entirety of the Asian workforce. In 2016, Indian and Chinese nationals accounted for 82% of all issued H1-B Visas, a work permit that allows skilled foreign workers to go to the United States and work for American companies. As of May 2016, 77% of the 1.2 million international students enrolled in the United States hailed from Asia. Cultural differences Cultural factors are thought to be part of the reason why East Asian Americans are successful in the United States. East Asian societies often place more resources and emphasis on education. For example, Confucian tenets and Chinese culture places great value on work ethic and the pursuit of knowledge. In traditional Chinese social stratification, scholars were ranked at the top—well above businessmen and landowners. This view of knowledge is evident in the modern lifestyle of many East Asian American families, where the whole family puts emphasis on education and parents will make it their priority to push their children to study and achieve high marks. Similar cultural tendencies and values are found in South Asian American families, whose children similarly face extra pressure by parents to succeed in school and to achieve high-ranked jobs. Although pressure is often perceived as a way to help East Asian American descendants achieve greater success, it can be used as a way to provide better income and living status for families. In other words, much of the East Asian American success in the United States can be due to the stereotypical yet favorable characteristics that their background holds. In most cases, East Asians such as Chinese, Japanese, Korean, and Taiwanese Americans hold a high position in terms of successful educational goals. Others counter this notion of culture as a driving force, as it ignores immigration policies. In the mid-1800s, Asian immigrants were recruited in the United States as laborers for agriculture and to aid in the building of the first transcontinental railroad. Many worked for low wages in the harshest conditions. Confucian values were not seen as a key to success. It was only until the Immigration and Nationality Act of 1965 changed the way Asians were seen, as Asians with higher education backgrounds were selectively chosen from a larger pool of the Asian population. Further, it has also been argued the myth of the Confucian emphasis on education is counterfactual. It also implies Asians are a monolithic group, and ignores the fact that the most educated group of Asian immigrants in the U.S. are Indians, for whom Confucius is virtually non-existent in their upbringing. It has also been argued that self-selecting immigrants do not represent the actual Asian American population as a whole, nor the populations of their home countries. While 50% of Chinese immigrants in the U.S. have a bachelor's degree, only 5% of the population does in their native China. Lastly, if Confucian culture played a vital part of Asian culture, Chinese immigrant children would perform consistently around the world, yet second-generation Chinese immigrants in Spain are the lowest academic achievers among immigrant groups in the country, and less than half are expected to graduate from middle school. Asian American status in affirmative action In the 1980s, one Ivy League school found evidence it had limited admissions of Asian American students. Because of their high degree of success as a group and over-representation in many areas such as college admissions, most Asian Americans are not granted preferential treatment by affirmative action policies as are other minority groups. Some schools choose lower-scoring applicants from other racial groups over Asian Americans in an attempt to promote racial diversity and to maintain some proportion to the society's racial demographics.: 165 In 2014, American business schools began a process to sort candidates based on their country of origin and region of the world they come from. African Americans Often overlooked is the direct contrast of model minorities with African Americans. It is the opinion of some that model minority stereotypes have historically been utilized to discredit African American racial equality movements, such as the civil rights movement, as they highlighted an alternative route to racial reform. African Americans were pushed to follow the lead of the idea of Asian Americans as the model minority, which was used to highlight that success as a minority was possible through hard work and support of the government. Since the success of Asian Americans was frequently attributed to distinctive cultural elements, researchers and policymakers argued that the struggles faced by African Americans was the result of a "culture of poverty". Thus, politicians such as Assistant Secretary of Labor Daniel Patrick Moynihan suggested that fostering cultural change amongst African Americans was essential to address the overall issue of racial inequality. This is illustrated through Moynihan's paper, "The Negro Family: The Case for National Action", which argues for the need to intervene in African American families in order to establish familial values similar to those of Asian Americans. Other examples where people have been concerned about the potential political weaponization of this idea included Florida governor since 2019 Ron DeSantis proposed mandate for Asian American studies which was criticized by many in the Asian American community for proposing the mandate while simultaneously banning courses on institutionalized racism, one of the fears being the use of Asian American history to promote discrimination against other minorities through an "untruthful representation". While scholars of the civil rights era relied on cultural values to describe the varying successes of Asian Americans and African Americans, contemporary scholars have begun to examine the effects of the different types of racism the two ethnic groups experience. Essentially, racism in itself is not monolithic. Instead, it is perpetrated in different ways and different avenues of life in which anti-Black rhetoric often proves to be more harmful to Black personhood than situations involving anti-Asian discrimination. Such generalizations regarding Black peoples' inability to thrive in the United States fail to explain the high levels of success seen by Black African and Caribbean immigrants to America which surpasses the averages of all native-born American ethnic groups. Additionally, Black African immigrant women make up the highest paid group of women in country. African immigrants as the invisible model minority African immigrants and Americans born to African immigrants have been described as an "Invisible Model Minority," primarily as a result of a high degree of success in the United States. Due to misconceptions and stereotypes, their success has not been acknowledged by the greater American society, as well as other Western societies, hence the label of "invisible". The invisibility of the success of Africans was touched upon by Dr. Kefa M. Otiso, an academic professor from Bowling Green State University, who stated that, "because these immigrants come from a continent that is often cast in an unfavorable light in the U.S. media, there is a tendency for many Americans to miss the vital contribution of these immigrants to meeting critical U.S. domestic labor needs, enhancing American global economic and technological competitiveness." Education In the 2000 U.S. census, it was revealed that African immigrants were the most educated immigrant group in the United States even when compared to Asian immigrants. Some 48.9% of all African immigrants hold a college diploma. This is more than double the rate of native-born white Americans, and nearly four times the rate of native-born African Americans. According to the 2000 Census, the rate of college diploma acquisition is highest among Egyptian Americans at 59.7%, followed closely by Nigerian Americans at 58.6%. In 1997, 19.4% of all adult African immigrants in the United States held a graduate degree, compared to 8.1% of adult white Americans and 3.8% of adult Black Americans in the United States. According to the 2000 Census, the percentage of Africans with a graduate degree is highest among Nigerian Americans at 28.3%, followed by Egyptian Americans at 23.8%. Of the African-born population in the United States age 25 and older, 87.9% reported having a high school degree or higher, compared with 78.8% of Asian-born immigrants and 76.8% of European-born immigrants, respectively. This success comes in spite of facts such as that more than 75% of the African foreign-born in the United States have only arrived since the 1990s and that African immigrants make up a disproportionately small percentage of immigrants coming to the United States such as in 2007 alone African immigrants made up only 3.7% of all immigrants in coming to the United States and again in 2009 they made up only 3.9% of all immigrants making this group a fairly recent to the United States diversity. Of the 8% of students at Ivy League schools that are Black, a majority, about 50–66%, was made up of Black African immigrants, Caribbean immigrants, and American born to those immigrants. Many top universities report that a disproportionate of the Black student population consists of recent immigrants, their children, or were mixed race. Socioeconomics The overrepresentation of the highly skilled can be seen in the relatively high share of Black African immigrants with at least a four-year college degree. In 2007, 27 percent of the U.S. population aged 25 and older had a four-year degree or more; 10% had a master's, doctorate, or professional degree. Immigrants from several Anglophone African countries were among the best educated: a majority of Black Immigrants from Nigeria, Cameroon, Uganda, Tanzania, and Zimbabwe had at least a four-year degree. Immigrants from Egypt, where the official language is Arabic, were also among the best educated. The overrepresentation of the highly skilled among U.S. immigrants is particularly striking for several of Africa's largest source countries. The United States was the destination for 59% of Nigeria's highly skilled immigrants along with 47% of those from Ghana and 29% from Kenya. The average annual personal income of African immigrants is about $26,000, nearly $2,000 more than that of workers born in the U.S. This might be because 71% of the Africans 16 years and older are working, compared to 64% of Americans. This is believed to be due larger percentage of African immigrants have higher educational qualifications than Americans, which results in higher per capita incomes for African immigrants and Americans born to African immigrants. Outside of educational success, specific groups have found economic success and have made many contributions to American society. For example, recent statistics indicate that Ugandan Americans have become one of the country's biggest contributors to the economy, their contribution, amounting to US$1 billion in annual remittances which are disproportionately large contributions despite a community and population of less than 13,000. African immigrants like many other immigrant groups are likely to establish and find success in small businesses. Many Africans that have seen the social and economic stability that comes from ethnic enclaves such as Chinatowns have recently been establishing ethnic enclaves of their own at much higher rates to reap the benefits of such communities. Examples of such ethnic enclaves include Little Ethiopia in Los Angeles and Le Petit Senegal in New York City. Demographically, African Immigrants and Americans born of African immigrants tend to typically congregate in urban areas, moving to suburban areas over the next few generations as they try to acquire economic and social stability. They are also one of America's least likely groups to live in racially segregated areas. African Immigrants and Americans born of African immigrants have been reported as having some of the lowest crime rates in the United States and being one of the unlikeliest groups to go into or commit crime. African immigrants have even been reported to have lowered crime rates in neighborhoods in which they have moved into. Black immigrants from Black majority countries are, upon their arrival, revealed to be much healthier than Black people from countries that are not majority Black and where they constitute a minority. Thus, African immigrants are, after arriving, often much healthier than American-born Black people and Black immigrants from Europe, though there is some evidence that as they settle, their health declines to the levels of their native counterparts, suggesting racial discrimination may be a factor in poor health for these communities. Cultural factors Cultural factors have been proposed as an explanation for the success of African immigrants. For example, it is claimed they often integrate into American society more successfully and at higher rates than other immigrants groups due to social factors. One being that many African immigrants have strong English skills even before entering the U.S., many African nations, particularly former Commonwealth nations, use English as a lingua franca. Because of this, many African immigrants to the U.S. are bilingual. Overall, 70% of Black African immigrants either speak English as their primary language or speak another language but are also fluent in English. Compare this to 48% proficiency in English for other immigrant groups. Kefa M. Otiso has proposed another reason for the success of African immigrants, saying that they have a "high work ethic, focus and a drive to succeed that is honed and crafted by the fact that there are limited socioeconomic opportunities in their native African countries," says Otiso. Selective immigration Another possible cause of the higher performance of African immigrants as a group is that they represent a small population in America so those who are chosen to come here often come from a selective group of African people. The relative difficulty of emigrating and immigrating into the United States has created a selective nature of the process with the U.S. often choosing the wealthier and more educated out of those with less resources, motivation or ability to immigrate. Americans born to African immigrants This pushing of second generation African immigrants by their parents has proven to be the key factor in their success, and a combination of family support and the emphasis of family unit has given these citizens social and psychological stability which makes them strive even further for success in many aspects of their daily life and society. Many of these American groups have thus transplanted high cultural emphasis on education and work ethic into their cultures which can be seen in the cultures of Algerian Americans, Kenyan Americans, Sierra Leonean Americans, Ghanaian Americans, Malawian Americans, Congolese Americans, Tanzanian Americans, and especially Nigerian Americans and Egyptian Americans. Though this fails to explain why poverty, corruption, violence, ethnic conflict, and generally poor socioeconomic conditions continue to plague African nations such as Nigeria. Caribbean Americans In 2017, there were approximately 4.4 million Caribbean immigrants in the US. Overall, there are over 8 million people of Caribbean heritage. Cubans, Dominicans, Jamaicans, Haitians, Trinidadians and Tobagonians are the largest groups. Caribbeans are likely to be employed at the same rate as the general immigrant population and at a higher rate than native born Americans. According to a report in the International Business Times, Caribbean immigrants perform better than the general immigrant population in terms of high school graduation rates and some socio-economic indicators. In comparison to other immigrant groups, Caribbeans are far more likely to be naturalised American citizens, display a better standard of English and have higher rates of health insurance cover. Studies by Harvard sociologist Robert Sampson suggest Caribbean immigrants are associated with low crime rates. According to a report drawn from Immigration Studies (CIS), various Caribbean communities are among the top immigrant homeowners in America. The non-Hispanic Caribbean community tend to earn more than the American average. In 2018, their median household income was $57, 339 compared to the American average of $54, 689. In 2019, the figure was $60, 997 compared to the American average of $57, 761 (US Census Bureau 2018 and 2019). Caribbeans make up the majority of America's Black immigrant population (46%). Black immigrants significantly contribute to the U.S. economy, with a spending power of $98 billion in 2018. Black immigrants earned approximately $133.6 billion and paid $36 billion in US taxes. These successes are primarily why some Caribbean Americans have been described as a model minority. Cuban success story The Cuban success story is a popular myth that Cuban Americans are all political exiles who have become wealthy in the United States. This story is often used to prove the accessibility of the American dream. Commonwealth countries Africans African immigrants have experienced success in numerous countries especially Commonwealth countries such as Canada, Australia and the United Kingdom, which have attracted many educated and highly skilled African immigrants with enough resources for them to start a new life in these countries. In the United Kingdom, one report has revealed that African immigrants have high rates of employment and that African immigrants are doing better economically than some other immigrant groups. Africans have obtained much success as entrepreneurs, many owning and starting many successful businesses across the country. Of the African immigrants, certain groups have become and are highly integrated into the country especially groups which have strong English language skills such as Zimbabweans or Nigerians, and they often come from highly educated and highly qualified backgrounds. Many African immigrants have low levels of unemployment, and some groups are known for their high rates of self-employment, as can be seen in the case of Nigerian immigrants. Certain groups outside of having strong English skills have found success mostly because many who immigrated to the UK are already highly educated and highly skilled professionals who come with jobs and positions such as business people, academics, traders, doctors and lawyers as is the case with Sudanese immigrants. As of 2013, Nigerian immigrants were among the nine immigrant populations that were above average academically in the UK. Euromonitor International for the British Council suggests that the high academic achievement by Nigerian students is mainly from most of the pupils already having learned English in their home country. Additionally, many of them hail from the wealthier segments of Nigerian society, which can afford to pursue studies abroad. A notable example of the highly educated nature of British Nigerians is the case of Paula and Peter Imafidon, nine-year-old twins who are the youngest students ever to be admitted to high school in England. Nicknamed the 'Wonder Twins', the twins and other members of their family have accomplished incredible rare feats, passing advanced examinations and being accepted into institutions with students twice their age. Asians In Canada, Asian Canadians are viewed as a model minority, which whether seen as a 'positive' or 'negative' supports the stereotype of Asian Canadians as solely driven by professional and economic success, dehumanizing them in comparison to other groups. The majority of this is aimed toward the East Asian and South Asian communities. In New Zealand, Asian New Zealanders are viewed as a model minority due to attaining above average socioeconomic indicators than the New Zealand average, though the phenomenon remains small, underground, and not as widespread compared with their American counterparts. In a study of a popular New Zealand newspaper, articles "never portrayed the Chinese as a model minority that silently achieves" and this was "not in line with overseas research, suggesting that this stereotype merits further analysis". Egypt Egyptian Copts In Egypt, Copts have relatively higher educational attainment, relatively higher wealth index, and a stronger representation in white collar job types, but limited representation in security agencies. The majority of demographic, socioeconomic and health indicators are similar among Copts and Muslims. Historically; many Copts were accountants, and in 1961 Coptic Christians owned 51% of the Egyptian banks. A Pew Center study about religion and education around the world in 2016, found that around 36% of Egyptian Christians obtain a university degree in institutions of higher education. According to the scholar Andrea Rugh Copts tend to belong to the educated middle and upper-middle class, and according to scholar Lois Farag "The Copts still played the major role in managing Egypt's state finances. They held 20% of total state capital, 45% of government employment, and 45% of government salarie". According to scholar J. D. Pennington 45% of the medical doctors, 60% of the pharmacists of Egypt were Christians. A number of Coptic business and land-owning families became very wealthy and influential such as the Egyptian Coptic Christian Sawiris family that owns the Orascom conglomerate, spanning telecommunications, construction, tourism, industries and technology. In 2008, Forbes estimated the family's net worth at $36 billion. According to scholars Maristella Botticini and Zvi Eckstein argue that Copts have relatively higher educational attainment and relatively higher wealth index, due to Coptic Christianity emphasis on literacy and that Coptic Christianity encouraged the accumulation of human capital. France French Vietnamese The Vietnamese in France are the most well-established overseas Vietnamese community outside eastern Asia, as well as Asian ethnic group in France, with roughly 139,000 Vietnamese immigrants living in France. While the level of integration among immigrants and their place in French society have become prominent issues in France in the past decade, French media and politicians generally view the Vietnamese community as a model minority. This is in part because they are represented as having a high degree of integration within French society, in addition to their economic and academic success. A survey in 1988 asking French citizens which immigrant ethnic group they believe to be the most integrated in French society saw the Vietnamese ranked fourth, behind only the Italian, Spanish and Portuguese communities. The educational attainment rate of the Vietnamese population in France is the highest among overseas Vietnamese populations, a legacy that dates back to the colonial era of Vietnam, when privileged families and those with connections to the colonial government often sent their children to France to study. In addition to high achievement in education, the Vietnamese population in France is also largely successful economically. When the first major wave of Vietnamese migrants arrived in France during World War I, a number of migrants already held professional occupations in their new country shortly after their arrival. More recently, refugees who arrived in France after the Fall of Saigon are often more financially stable than their counterparts who settled in North America, Australia and the rest of Europe, due to better linguistic and cultural knowledge of the host country, which allowed them to enter the education system and/or higher paying professions with little trouble. Within a single generation, the median income for French-born Vietnamese has risen above the French median income. French Laotians Similarly to the Vietnamese, the Laotian community in France is one of the most well-integrated into the country and is the most established overseas Laotian populace. Unlike their counterparts in North America and Australia, Laotians in France have a high rate of educational success and are well-represented in the academic and professional sectors, especially among the generations of French-born Lao. Due to better linguistic and cultural knowledge of the host country, Laotian immigrants to France, who largely came as refugees after the end of the Laotian Civil War, were able to assimilate at a high rate. Criticism According to the French antiracist activist Grace Ly, the model minority myth is associated with the South-East Asian community in France. Ly denounces the positive stereotypes associated with the Asian community in France in her book Model Young Girl (Jeune fille modèle). Notably in France, however, the persistent official policy of "color blindness" makes the concept of minorities, and policies to counter racism, different in modern French culture from America and the UK. This however, is and should be considered as distinct from actual expressions of prejudice in France, regardless of official policy. Germany In Germany the academic success of people of Vietnamese origin has been called "Das vietnamesische Wunder"("The Vietnamese Miracle"). A study revealed that in the Berlin districts of Lichtenberg and Marzahn, both in former East Berlin and possessing a relatively small percentage of immigrants, Vietnamese account for only 2% of the general population, but make up 17% of the prep school population. Another note of Vietnamese Germans' academic success is that even though they can grow up in poverty in places like East Germany, they usually outperform their peers by a wide margin. Another group in Germany that is extremely academically successful and is comparable to that of a model minority are Korean Germans, 70% of whom attended a Gymnasium (which is comparable to a prep school in American society), compared to Vietnamese Germans with only 50% attending a Gymnasium. Also, over 70% of second-generation Korean Germans hold at least an Abitur or higher educational qualification, more than twice the ratio of the rest of Germany. Israel In Israel, Christian Arabs are one of the most educated groups. Maariv has described the Christian Arab sectors as "the most successful in education system," since Christian Arabs fared the best in terms of education in comparison to any other group receiving an education in Israel and they have attained a bachelor's degree and academic degree more than the median Israeli population. Education According to the study "Are Christian Arabs the New Israeli Jews? Reflections on the Educational Level of Arab Christians in Israel" by Hanna David from the University of Tel Aviv, one of the factors why Israeli Arab Christians are the most educated segment of Israel's population is the high level of the Christian educational institutions. Christian schools in Israel are among the best schools in the country, and while those schools represent only 4% of the Arab schooling sector, about 34% of Arab university students come from Christian schools, and about 87% of the Israeli Arabs in the high tech sector have been educated in Christian schools. A 2011 Maariv article described the Christian Arab sector as "the most successful in the education system," an opinion supported by the Israel Central Bureau of Statistics and others who point out that Christian Arabs fared best in terms of education in comparison to any other group receiving an education in Israel. High school and matriculation exams The Israel Central Bureau of Statistics noted that when taking into account the data recorded over the years, Christian Arabs fared the best in terms of education in comparison to any other group receiving an education in Israel. In 2016 Christian Arabs had the highest rates of success at matriculation examinations, namely 73.9%, both in comparison to Muslim and Druze Israelis (41% and 51.9% respectively), and to the students from the different branches of the Hebrew (majority Jewish) education system considered as one group (55.1%). Higher education Arab Christians are one of the most educated groups in Israel. Statistically, Arab Christians in Israel have the highest rates of educational attainment among all religious communities, according to a data by Israel Central Bureau of Statistics in 2010, 63% of Israeli Arab Christians have had college or postgraduate education, the highest of any religious and ethno-religious group. Despite the fact that Arab Christians only represent 2.1% of the total Israeli population, in 2014 they accounted for 17.0% of the country's university students, and for 14.4% of its college students. There are more Christians who have attained a bachelor's degree or higher academic degrees than the median Israeli population. The rate of students studying in the field of medicine was higher among Arab Christian students than that of all other sectors. and the percentage of Arab Christian women who are receiving higher education is also higher than that of other groups. In 2013, Arab Christian students were also the vanguard in terms of eligibility for higher education, as the Christian Arab students had the highest rates of receiving Psychometric Entrance Test scores which eligible them to be accepted into universities, data from the Israel Central Bureau of Statistics show that 61% of Christian Arabs were eligible for university studies, compared to 50% of Jewish, 45% of Druze, and 35% of Muslim students. Socio-economic In terms of their socio-economic situation, Arab Christians are more similar to the Jewish population than to the Muslim Arab population. They have the lowest incidence of poverty and the lowest percentage of unemployment which is 4.9% compared to 6.5% among Jewish men and women. They have also the highest median household income among Arab citizens of Israel and second highest median household income among the Israeli ethno-religious groups. Also Arab Christians have a high presentation in science and in the white-collar professions. In Israel Arab Christians are portrayed as a hard working and upper middle class educated ethno-religious minority. Mexico Due to their business success and cultural assimilation, German Mexicans and Lebanese Mexicans are seen as model minorities in Mexico. More recently, Haitians in Tijuana have been seen favorably by Tijuanenses as model immigrants due to their work ethic and integration into Tijuana society, and have been contrasted with Central American migrants. In the 19th and early 20th century, German immigration was encouraged due to the perceived industriousness of Germans. German Mexicans were instrumental in the development of the cheese and brewing industries in Mexico. Germans in the Soconusco were successful in the coffee industry. Although Lebanese Mexicans made up less than 5% of the total immigrant population in Mexico during the 1930s, they constituted half of the immigrant economic activity. Carlos Slim, one of the richest individuals in the world, is the topmost example of Lebanese Mexican success. Netherlands Background At the end of the colonial era of the Dutch East Indies (now: Indonesia), a community of about 300,000 Indo-Europeans (people of mixed Indonesian and European heritage) was registered as Dutch citizens. Indos formed the vast majority of the European legal class in the colony. When in the second half of the 20th century the independent Republic of Indonesia was established, the majority of Europeans, including the Indo-Europeans, were expelled from the newly established country. Repatriation From 1945 to 1949 the Indonesian National Revolution turned the former Dutch East Indies into an increasingly hostile environment for Indo-Europeans. Violence aimed towards Indo-Europeans during its early Bersiap period (1945–1946) accumulated in almost 20,000 deaths. The Indo diaspora continued up to 1964 and resulted in the emigration of practically all Indo-Europeans from a turbulent young Indonesian nation. Even though most Indos had never set foot in the Netherlands before, this emigration was named repatriation. Notwithstanding the fact that Indos in the former colony of the Dutch East Indies were officially part of the European legal class and were formally considered to be Dutch nationals, the Dutch government practiced an official policy of discouragement with regard to the post-WWII repatriation of Indos to the Netherlands. While Dutch policy was in fact aimed at stimulating Indos to give up Dutch citizenship and opt for Indonesian citizenship, simultaneously the young Indonesian Republic implemented policies increasingly intolerant towards anything remotely reminiscent of Dutch influence. Even though actual aggression against Indos decreased after the extreme violence of the Bersiap period, all Dutch (language) institutions, schools and businesses were gradually eliminated and public discrimination and racism against Indos in the Indonesian job market continued. In the end 98% of the original Indo community repatriated to their distant fatherland in Europe. Integration In the 1990s and early 21st century the Netherlands was confronted with ethnic tension in a now multi-cultural society. Ethnic tensions, rooted in the perceived lack of social integration and rise of crime rates of several ethnic minorities, climaxed with the murders of politician Pim Fortuyn in 2002 and film director Theo van Gogh in 2004. In 2006 statistics show that in Rotterdam, the second largest city in the country, close to 50% of the inhabitants were of foreign descent. The Indo community however is considered the best integrated ethnic and cultural minority in the Netherlands. Statistical data compiled by the CBS shows that Indos belong to the group with the lowest crime rates in the country. A CBS study of 1999 reveals that of all foreign born groups living in the Netherlands, only the Indos have an average income similar to that of citizens born in the Netherlands. Job participation in government, education and health care is similar as well. Another recent CBS study, among foreign born citizens and their children living in the Netherlands in 2005, shows that on average, Indos own the largest number of independent enterprises. A 2007 CBS study shows that already over 50% of first-generation Indos have married a native born Dutch person. A percentage that increased to 80% for the second generation. One of the first and oldest Indo organisations that supported the integration of Indo repatriates into the Netherlands is the Pelita foundation. Although Indo repatriates, being born overseas, are officially registered as Dutch citizens of foreign descent, their Eurasian background puts them in the Western sub-class instead of the Non-Western (Asian) sub-class. Two factors are usually attributed to the essence of their apparently seamless assimilation into Dutch society: Dutch citizenship and the amount of 'Dutch cultural capital', in the form of school attainments and familiarity with the Dutch language and culture, that Indos already possessed before migrating to the Netherlands. New generations Although third- and fourth-generation Indos are part of a fairly large minority community in the Netherlands, the path of assimilation ventured by their parents and grandparents has left them with little knowledge of their actual roots and history, even to the point that they find it hard to recognise their own cultural features. Some Indos find it hard to grasp the concept of their Eurasian identity and either tend to disregard their Indonesian roots or on the contrary attempt to profile themselves as Indonesian. In recent years however the reinvigorated search for roots and identity has also produced several academic studies. See also Bamboo ceiling Benevolent prejudice Barua (Bangladesh) Bengali Christians Dominant minority Global majority Honorary whites Honorary Aryan Intergroup anxiety John Henryism Jewish stereotypes Middleman minority Minority stress Oppression Olympics Overachievement Parsis (India) Race and intelligence Stereotype threat Tiger mother The miller, his son and the donkey World on Fire (book) Victim blaming Notes References Bibliography Ancheta, Angelo N. 2006. Race, Rights, and the Asian American Experience. Rutgers University Press. ISBN 0-8135-3902-1. Bian, Yanjie (August 2002). "Chinese Social Stratification and Social Mobility". Annual Review of Sociology. 28 (1): 91–116. doi:10.1146/annurev.soc.28.110601.140823. Chen, Edith Wen-Chu, and Grace J. Yoo. 2009. Encyclopedia of Asian American Issues Today 1. ABC-CLIO. ISBN 0-313-34749-2. Clark, E. Audrey; Hanisee, Jeanette (July 1982). "Intellectual and adaptive performance of Asian children in adoptive American settings". Developmental Psychology. 18 (4): 595–599. doi:10.1037/0012-1649.18.4.595. Espiritu, Yen Le. 1996. Asian American Women and Men: Labor, Laws, and Love. Frydman, Marcel; Lynn, Richard (January 1989). "The intelligence of Korean children adopted in Belgium". Personality and Individual Differences. 10 (12): 1323–1325. doi:10.1016/0191-8869(89)90246-8. Hartlep, N. 2021. The Model Minority Stereotype: Demystifying Asian American Success (2nd edition). Information Age Publishing. ISBN 978-1648024771. Hartlep, N. 2013. The Model Minority Stereotype: Demystifying Asian American Success. Information Age Publishing. ISBN 978-1-62396-358-3. — 2014. The Model Minority Stereotype Reader: Critical and Challenging Readings for the 21st Century. Cognella Publishing. ISBN 978-1-62131-689-3. Hartlep, N., and B. J. Porfilio, eds. 2015. Killing the Model Minority Stereotype: Asian American Counterstories and Complicity. Information Age Publishing. ISBN 978-1681231105. Hsu, Madeline Y. 2015. The Good Immigrants: How the Yellow Peril Became the Model Minority. Princeton University Press. Li, Guofang, and Lihshing Wang. 2008. Model Minority Myth Revisited: an Interdisciplinary Approach to Demystifying Asian American Educational Experiences. Information Age Publishing. ISBN 978-1-59311-951-5. Marger, Martin N. 2009. Race and Ethnic Relations: American and Global Perspectives (8th ed.). Cengage Brain. ISBN 0-495-50436-X. Min, Zhou and Carl L. Bankston III. 1998. Growing Up American: How Vietnamese Children Adapt to Life in the United States. Russell Sage Foundation. Rothenberg, Paula S. 2006. Race, Class, and Gender in the United States: An Integrated Study (7th ed.). Macmillan. ISBN 0-7167-6148-3. Wu, Helen D. 2014. The Color of Success: Asian Americans and the Origins of the Model Minority. Princeton University Press. Further reading Books Bascara, Victor (2006). Model-Minority Imperialism. University of Minnesota Press. ISBN 9780816645121. Gupta, Prachi (2023). They Called Us Exceptional: And Other Lies That Raised Us. New York, NY, USA: Crown, Penguin Random House. ISBN 9780593442982. Prashad, Vijay (2000). The Karma of Brown Folk. University of Minnesota Press. ISBN 978-0-8166-3439-2. Articles Freeman, Jonathan (Summer 2005). "Transgressions of a Model Minority". Shofar. Special Issue: Race and Jews in America. 23 (4). Purdue University Press: 69–97. doi:10.1353/sho.2005.0147. ISSN 1534-5165. JSTOR 42944291. S2CID 143480665. Yuan (Fall 2022). "Model Minority: The Myth Shaping Society". WOVEN: An Interdisciplinary Journal of Dietrich College (1). Dietrich College General Education Program: Dietrich College of Humanities and Social Sciences:Carnegie Mellon University. Taubenblatt, Emily; Ong, Anna-Leigh (May 20, 2021). "Model Minority Myths: Solidarity and Responsibility between the Asian American and Jewish American Communities". APCO Worldwide. Retrieved September 13, 2023. Nguyen, Viet Thanh (June 26, 2020) [June 25, 2020]. "Asian Americans are Still Caught in the Trap of the 'Model Minority' Stereotype. And it Creates Inequality for All". Time. Chin, Margaret M.; Pan, Yung Yi Diana (April 19, 2021). "The 'model minority' myth hurts Asian Americans – and even leads to violence". Washington Post. Parmar, Simrath. "The Model Minority Myth: How Its Generalizations Have Hurt Asian America and Other Minorities". Fortham University. Wong, Andrew (June 25, 2020). "Why the Model Minority Myth Was Never True". Asian American Christian Collective. External links "Model Minority". The Canadian Encyclopedia. "Model Minority". University of Westminster. "Are Students in the UK also Affected by the Model Minority Myth?". "Deconstructing the Model Minority at the University of Michigan". University of Michigan Department of History. Survey Examines Asian Mobility, Stephen Klineberg's systematic survey of Houston's Asian community Asian-Nation: The Model Minority Image, by C.N. Le, Ph.D. A Brief History of the Model Minority Stereotype, by Andrew Chin Yellow: Race in America Beyond Black and White, by Frank H. Wu Model Minority Stereotype Project Bibliography Will American Science Stay On Top?, by Pratik Chougule Moving Forward Together (MFT) is an Australian charitable organisation based in Sydney that promotes social harmony and the prevention of prejudice. The organisation was founded in 2005 and is led by Holocaust survivor Ernie Friedlander. Harmony Day campaigns Moving Forward Together promotes social harmony, commonly in conjunction with the annual Australian Government's Harmony Day celebrations. The group's Harmony Day activities include community walks, poster competitions, songwriting competitions, and public exhibitions. Moving Forward Together and its activities are recognised by Australian government members as well as by the government of New South Wales. Hamony poster competition Moving Forward Together runs an annual poster competition in New South Wales in conjunction with the annual Harmony Day celebrations with an emphasis on themes of pride and belonging. In 2016, competition posters were displayed at Parliament House, Sydney before being added to a travelling exhibition. In 2021, the poster competition involved submissions from over 5,300 students. Stop Racism Now (campaign) In 2021, the organisation initiated an anti-racism campaign titled "Stop Racism Now AU" which was launched by the Governor of New South Wales, Margaret Beazley , as well as other Australian parliamentarians. The campaign involves promoting messages of mutual respect and understanding as well as the development of educational materials for schools in New South Wales. The Executive Council of Australian Jewry (ECAJ), the peak representative body of the Australian Jewish community, noted the campaign in its annual study of antisemitic activity in Australia following the rejection of the Stop Racism Now campaign by a student union based in the University of Technology Sydney (UTS). The ECAJ determined that the manner in which the student union rejected the camapaign's outreach efforts reflected a prevailing trend of anti-Jewish sentiment expressed by some progressive activists. See also Courage to Care Anti-Defamation Commission References External links Official website A door phone or door bell phone is a set of electrical and electronic elements used to handle communication between a resident in a house, apartment or villa and a guest outside. The device can also lock or unlock the door it has been configured to work with. Door phones have been used across a variety of commercial and residential buildings. For example, offices and apartment blocks both make frequent use of door phones. They are so widely used that, nowadays, they form part of the standard electrical installation of most buildings. The simplest version is an intercom that establishes a communication between an entryway at street level and a resident inside the house. A loudspeaker installed at the street level entrance allows a building resident to speak to their visitor. In buildings where there are more than one door phone plate located outside of a building's entrance, each door phone has a certain number of buttons depending on the number of units in each building. Operation A door phone in its most basic version is a two-way intercom allowing communication from the street to the inside of a building. More complex door phones are connected to electric strikes, and can unlock and open the door to allow access to the interior of the building. The part of the door phone on the exterior of a building is known as the door phone plate. It is located at the outside of the entrance of each building. Door phone plates consist of a matrix of buttons. each of them drive a buzzer located in a unit inside each apartment building. Each apartment building contains a button that can activate the electric strike. There are several installation systems, the most traditional being a system known as 4 + 1, named for the types of wiring that it requires. A 4 + 1 system requires four wires that handle power, communication, and the door system, and another wire for communication between an in-unit apartment resident and the person using the door phone. Video door phones Going a little further in time, we find video door phones featuring a video installation apart from the classical audio. In these cases, the intercom plate has the same structure as the previous version but features a video monitor connected to a surveillance camera that allows inspecting the person who pressed the button and part of the surrounding area. Access control systems. Some manufacturers have also adopted the possibility of opening the door via a keyboard. By introducing a numerical secret code the electric strike is operated. Although there is also the possibility of being used as an access control with either a magnetic card or a smart card. Wiring topology Among other classifications there is one with three different types of door phones that differ mainly in the number of wires required in their installation, having some parallelism between different manufacturers, that use different communication protocols but a similar wiring topology: Conventional: This system is characterized by the use of four wires plus of wires for each home. That is, (4 + "n") wires, where "n" indicates the number of homes. This system is best suited for medium-sized buildings. Simplified: This system is characterized by using a wire pair, plus the number of homes in threads. That is, (1 + "n") wires, where "n" indicates the number of homes. It is best suited for medium-sized buildings with two or three entrance gates. Digital: This system more suitable for use in large buildings or complex installations, as it ensures maximum capacity and ease of installation. It is characterized for using only two wires for all facilities. See also Courtesy telephone Intercom Speaking tube Video door-phone == References == In the social sciences, social groups can be categorized based on the various group dynamics that define social organization. In sociological terms, groups can fundamentally be distinguished from one another by the extent to which their nature influence individuals and how. A primary group, for instance, is a small social group whose members share close, personal, enduring relationships with one another (e.g. family, childhood friend). By contrast, a secondary group is one in which interactions are more impersonal than in a primary group and are typically based on shared interests, activities, and/or achieving a purpose outside the relationship itself (e.g. coworkers, schoolmates). Four basic types of groups have traditionally been recognized: primary groups, secondary groups, collective groups, and categories. Primary and secondary groups The distinction between primary and secondary groups serves to identify between two orders of social organization. Primary groups A primary group is typically a small social group whose members share close, personal, enduring relationships in which one exchanges implicit items, such as love, caring, concern, support, etc. These groups are often long-lasting and marked by members' concern for one another, where the goal is actually the relationship themselves rather than achieving another purpose. In general, they are also psychologically comforting to the individuals involved, providing a source of support. As such, primary groups or lack thereof play an important role in the development of personal identity, and can be understood as tight circles composed of people such as family, long-term romances, crisis-support group, church group, etc. The concept of the primary group was first introduced in 1909 by sociologist Charles Cooley, a member of the famed Chicago school of sociology, through a book titled Social Organization: A Study of the Larger Mind. Although Cooley had initially proposed the term to denote the first intimate group of an individual's childhood, the classification would later extend to include other intimate relations. Additionally, three sub-groups of primary groups can be also identified: Kin (relatives) Close friends Neighbours Secondary groups (social groups) A secondary group is a relatively larger group composed of impersonal and goal-oriented relationships, which are often temporary. These groups are often based on achieving a common purpose outside of the relationship itself and involve much less emotional investment. Since secondary groups are established to perform functions, individual roles are more interchangeable, thus members are able to leave and outgroup are able to join with relative ease. Such groups can be understood to be ones in which individuals exchange explicit commodities (e.g. labour for wage, service for payment, etc.). Examples include study groups, sports teams, schoolmates, attorney-client, doctor-patient, coworkers, etc. Cooley had made the distinction between primary and secondary groups, by noting that the term for the latter refers to relationships that generally develop later in life, likely with much less influence on one's identity than primary groups. Collectives A collective is a large group of individuals whose relationships to one another are loosely formed, spontaneous, and brief. Members are generally connected through performing similar actions or possessing similar outlooks. As they only exist for a very brief period of time, it is very easy for an out-group member to become an in-group member and vice versa. Examples of collectives include audiences at a show, bystanders, people at the park, etc. Collectivities A collectivity comprises "any grouping wider than the individual". Categories Categories are characterized by an aggregate of individuals who share something in common, but only become groups when their similarities have social implications. Categories can appear to be higher in entitativity and essentialism than primary, secondary, and collective groups. This group is generally the largest type of such, where members can be either permanently or temporarily in-group. Categories can include individuals with the same ethnicity, gender, religion, or nationality. For example, Torontonians, women, and gamers can all be characterized as categories. Campbell (1958) famously defines entitativity as the extent to which collections of individuals are perceived to be a group. The degree of entitativity that a group has is influenced by whether a collection of individuals experience the same fate, display similarities, and are close in proximity. If individuals believe that a group is high in entitativity, then they are likely to believe that the group has unchanging characteristics that are essential to the group, known as essentialism. Reference groups A reference group is a group to which an individual or another group is compared, used by sociologists in reference to any group that is used by an individual as a standard for evaluating themselves and their own behavior. More simply, as explained by Thompson and Hickey (2005), such groups are ones "that people refer to when evaluating their [own] qualities, circumstances, attitudes, values and behaviors." Reference groups are used in order to evaluate and determine the nature of a given individual or other group's characteristics and sociological attributes. It is the group to which the individual relates or aspires to relate him or herself psychologically. It becomes the individual's frame of reference and source for ordering his or her experiences, perceptions, cognition, and ideas of self. It is important for determining a person's self-identity, attitudes, and social ties. It becomes the basis of reference in making comparisons or contrasts and in evaluating one's appearance and performance. Reference groups provide the benchmarks and contrast needed for comparison and evaluation of group and personal characteristics. Robert K. Merton hypothesized that individuals compare themselves with reference groups of people who occupy the social role to which the individual aspires.[Merton] developed a theory of the reference group (i.e., the group to which individuals compare themselves, which is not necessarily a group to which those individuals belong), and elaborated on the concepts of in-group and out-group. For any group of people there are always other groups whom they look upon to and aspire to be like them.Such groups act as a frame of reference to which people always refer to evaluate their achievements, their role performance, aspirations and ambitions. A reference group can be either from a membership group or non-membership group. An example of a reference group being used would be the determination of affluence. An individual in the U.S. with an annual income of $80,000, may consider themself affluent if they compare themself to those in the middle of the income strata, who earn roughly $32,000 a year. If, however, the same person considers the relevant reference group to be those in the top 0.1% of households in the US, those making $1.6 million or more, then the individual's income of $80,000 would make them consider themself as rather poor. Examples Basic groups: The smallest possible social group with a defined number of people (i.e. greater than 1)—often associated with family building: Dyad: Will be a group of two people. Social interaction in a dyad is typically more intense than in larger groups as neither member shares the other's attention with anyone else.: 153 (See also couple.) Triad: A group of three people. Triads are generally more stable than dyads because one member can act as a mediator should the relationship between the other two become strained.: 154 Family, Household: Small group of people who live in the same home. Family may or may not form clan, fellowship, larger kinship groups, or a basic unit of community. Various cultures include different models of households, including the nuclear family, blended families, share housing, and group homes. Crew or Band: Small group of skilled people with common interest; a rowing crew; a music band; construction crew; subunit of a tribe as band society. Peer group: A group with members of approximately the same age, social status, and interests. Generally, people are relatively equal in terms of power when they interact with peers. Clique: A group of people that have many of the same interests & commonly found in a high school/college setting; most of the time they have a name & rules for themselves. Club: A group that usually requires one to apply to become a member. Such clubs may be dedicated to particular activities: sporting clubs, for example. Cabal: A group of people united in some close design together, usually to promote their private views or interests in a church, state, or other community, often by intrigue. Community: A group of people with a commonality or sometimes a complex net of overlapping commonalities, often—but not always—in proximity with one another with some degree of continuity over time. Gang: Usually an urban group that gathers in a particular area. It is a group of people that often hang around each other. They can be like some clubs, but much less formal. They are usually known in many countries to cause social unrest and also have negative influence on the members and may be a target for the law enforcers in case of any social vices. Mob: Typically a group of people that has taken the law into their own hands. Mobs are usually groups which gather temporarily for a particular reason. Posse: Originally found in English common law, posses are generally obsolete and survive only in the United States, where they are the law enforcement equivalent of summoning the militia for military purposes. However, posse can also refer to a street group. Squad: Generally a small group, of around 3 to 15 people, who work as a team to accomplish their goals. Team: Similar to a squad, though a team may contain many more members. A team works in a similar way to a squad. See also Fandom Gemeinschaft and Gesellschaft Group dynamics Social complexity Social group Social network References Further reading Appelbaum, R. P., D. Carr, M. Duneir, and A. Giddens. 2009. "Conformity, Deviance, and Crime." Introduction to Sociology, New York: W. W. Norton & Company. p 137. External links McGraw Hill online Sociology Glossary The history of organized firefighting began in ancient Rome while under the rule of the first Roman Emperor Augustus. Prior to that, Ctesibius, a Greek citizen of Alexandria, developed the first fire pump in the third century BC, which was later improved upon in a design by Hero of Alexandria in the first century BC. Ancient Rome Roman Emperor Augustus formed a group of slaves, Vigiles, in AD 6 to combat fires using bucket brigades and pumps, as well as poles, hooks and even ballistae to tear down buildings in advance of the flames. The Corps Vigiles patrolled the streets of Rome to watch for fires and served as a police force. The later brigades consisted of hundreds of volunteers, all ready for action. When there was a fire, the men would line up to the nearest water source and pass buckets hand in hand to the fire. Ancient Rome, known for its architectural marvels and sophisticated infrastructure, was also one of the first civilizations to implement organized firefighting efforts. Under the rule of Emperor Augustus, who reigned from 27 BC to 14 AD, Rome established a rudimentary firefighting force tasked with combating fires that frequently ravaged the city's densely populated neighborhoods. Prior to the institutionalization of firefighting in Rome, Greek ingenuity contributed significantly to the development of early firefighting apparatus. Ctesibius, a Greek inventor hailing from Alexandria, is credited with creating the first known fire pump around the third century BC. This primitive device, employing principles of pneumatics, utilized water pressure to extinguish fires and was a crucial innovation in the fight against conflagrations. Rome suffered a number of serious fires, most notably the fire on 19 July AD 64 which eventually destroyed two thirds of Rome. Early English In Europe, firefighting was quite rudimentary until the 17th century. In 1254, a royal decree of King Saint Louis of France created the so-called guet bourgeois ("burgess watch"), allowing the residents of Paris to establish their own night watches, separate from the king's night watches, to prevent and stop crimes and fires. After the Hundred Years' War, the population of Paris expanded again, and the city, much larger than any other city in Europe at the time, was the scene of several great fires in the 16th century. As a consequence, King Charles IX disbanded the residents' night watches and left the king's watches as the only one responsible for checking crimes and fires.London suffered great fires in 798, 982, 989, 1212 and above all in 1666 (the Great Fire of London). The Great Fire of 1666 started in a baker's shop on Pudding Lane, consumed about two square miles (5 km2) of the city, leaving tens of thousands homeless. Prior to this fire, London had no organized fire protection system. Afterwards, insurance companies formed private fire brigades to protect their clients’ property. These buildings were identified by fire insurance marks. There is an urban legend that insurance brigades would only fight fires at buildings the company insured. This claim has been debunked. The key breakthrough in firefighting arrived in the 17th century with the first fire engines. Manual pumps, rediscovered in Europe after 1500 (allegedly used in Augsburg in 1518 and in Nuremberg in 1657), were only force pumps and had a very short range due to the lack of hoses. German inventor Hans Hautsch improved the manual pump by creating the first suction and force pump and adding some flexible hoses to the pump. In 1672, Dutch artist and inventor Jan Van der Heyden's workshop developed the fire hose. Constructed of flexible leather and coupled every 50 feet (15 m) with brass fittings. The length remains the standard to this day in mainland Europe whilst in the UK the standard length is either 23m or 25m. The fire engine was further developed by the Dutch inventor, merchant and manufacturer, John Lofting (1659–1742) who had worked with Jan Van der Heyden in Amsterdam. Lofting moved to London in or about 1688, became an English citizen and patented (patent number 263/1690) the "Sucking Worm Engine" in 1690. There was a glowing description of the firefighting ability of his device in The London Gazette of 17 March 1691, after the issue of the patent. The British Museum has a print showing Lofting's fire engine at work in London, the engine being pumped by a team of men. In the print three fire plaques of early insurance companies are shown, no doubt indicating that Lofting collaborated with them in firefighting. A later version of what is believed to be one of his fire engines has been lovingly restored by a retired firefighter, and is on show in Marlow Buckinghamshire where John Lofting moved in 1700. Patents only lasted for fourteen years and so the field was open for his competitors after 1704. Richard Newsham of Bray in Berkshire (just 8 miles from Lofting) produced and patented an improved engine in 1721 (Royal Patent Office 1721 patent #439 and 1725 patent #479) and soon dominated the fire engine market in England. Pulled as a cart to the fire, these manual pumps were manned by teams of 4 to 12 men and could deliver up to 160 gallons per minute (12 L/s) at up to 120 feet (36 m). Newsham himself died in 1743 but his company continued making fire engines under other managers and names into the 1770s. The next major development in fire engine design in England was made by Hadley, Simpkin & Lott co. in 1792 with a larger and much improved style of hand pumped engine which could be pulled to a fire by horses. United States In 1631, Boston's governor John Winthrop outlawed wooden chimneys and thatched roofs. In 1648, the New Amsterdam governor Peter Stuyvesant appointed four men to act as fire wardens. They were empowered to inspect all chimneys and to fine any violators of the rules. The city burghers later appointed eight prominent citizens to the "Rattle Watch" - these men volunteered to patrol the streets at night carrying large wooden rattles. If a fire was seen, the men spun the rattles, then directed the responding citizens to form bucket brigades. On January 27, 1678 the first fire engine company went into service with its captain (foreman) Thomas Atkins. In 1736, Benjamin Franklin established the Union Fire Company in Philadelphia. The United States did not have government-run fire departments until around the time of the American Civil War. Prior to this time, private fire brigades competed with one another to be the first to respond to a fire because insurance companies paid brigades to save buildings. Underwriters also employed their own Salvage Corps in some cities. The first known female firefighter, Molly Williams, took her place with the men on the dragropes during the blizzard of 1818 and pulled the pumper to the fire through the deep snow. On 1 April 1853, Cincinnati, Ohio featured the first career fire department made up of 100% full-time employees. In 2015, 70% of firefighters in the United States were volunteers. Only 4% of calls regarded actual fires, while 64% regarded medical aid, and 8% were false alarms. Canada The first organized fire department in Canada was created in Halifax, Nova Scotia, originally named the Union Fire Club. The next companies to become established in the Maritimes in the 1780s, were conceived as a mutual insurance and protection organization, which followed the governors requested rules and regulations. Modern development The world's oldest Firefighter Corporation that is still alive to the present day takes location in Lisbon, Portugal, today called "Regimento de Sapadores Bombeiros de Lisboa" was created in 1395 with the name "Serviço de incendios de Lisboa" The first fire brigades in the modern sense were created in France in the early 18th century. In 1699, a man with bold commercial ideas, François du Mouriez du Périer (grandfather of French Revolution general Charles François Dumouriez), solicited an audience with King Louis XIV. Greatly interested in Jan Van der Heyden's invention, he successfully demonstrated the new pumps and managed to convince the king to grant him the monopoly of making and selling "fire-preventing portable pumps" throughout the kingdom of France. François du Mouriez du Périer offered 12 pumps to the City of Paris, and the first Paris Fire Brigade, known as the Compagnie des gardes-pompes (literally the "Company of Pump Guards"), was created in 1716. François du Mouriez du Périer was appointed directeur des pompes de la Ville de Paris ("director of the City of Paris's pumps"), i.e. chief of the Paris Fire Brigade, and the position stayed in his family until 1760. In the following years, other fire brigades were created in the large French cities. Around that time appeared the current French word pompier ("firefighter"), whose literal meaning is "pumper." On March 11, 1733 the French government decided that the interventions of the fire brigades would be free of charge. This was decided because people always waited until the last moment to call the fire brigades to avoid paying the fee, and it was often too late to stop fires. From 1750 on, the French fire brigades became para-military units and received uniforms. In 1756 the use of a protective helmet for firefighters was recommended by King Louis XV, but it took many more years before the measure was actually enforced on the ground. In North America, Jamestown, Virginia was virtually destroyed in a fire in January, 1608. There were no full-time paid firefighters in America until 1850. Even after the formation of paid fire companies in the United States, there were disagreements and often fights over territory. New York City companies were famous for sending runners out to fires with a large barrel to cover the hydrant closest to the fire in advance of the engines. Often fights would break out between the runners and even the responding fire companies for the right to fight the fire and receive the insurance money that would be paid to the company that fought it. During the 19th century and early 20th century volunteer fire companies served not only as fire protection but as political machines. The most famous volunteer firefighter politician is Boss Tweed, head of the notorious Tammany Hall political machine, who got his start in politics as a member of the Americus Engine Company Number 6 ("The Big Six") in New York City. Napoleon Bonaparte, drawing from the century-old experience of the gardes-pompes, is generally attributed as creating the first "professional" firefighters, known as Sapeurs-Pompiers ("Sappers-Firefighters"), from the French Army. Created under the Commandant of Engineers in 1810, the company was organized after a fire at the ballroom in the Austrian Embassy in Paris which injured several dignitaries. In the UK, the Great Fire of London in 1666 set in motion changes which laid the foundations for organised firefighting in the future. In the wake of the Great Fire, the City Council established the first fire insurance company, "The Fire Office", in 1667, which employed small teams of Thames watermen as firefighters and provided them with uniforms and arm badges showing the company to which they belonged. However, the first organised municipal fire brigade in the world was established in Edinburgh, Scotland, when the Edinburgh Fire Engine Establishment was formed in 1824, led by James Braidwood. London followed in 1832 with the London Fire Engine Establishment. On April 1, 1853, the Cincinnati Fire Department became the first full-time paid career fire department in the United States, and the first in the world to use steam fire engines. The first horse-drawn steam engine for fighting fires was invented in England in 1829, but it was not accepted in structural firefighting until 1860. It continued to be ignored for another two years afterwards. Self-propelled steam-powered fire engines were introduced in 1903, followed by internal combustion engine fire apparatuses which began to be produced as early as 1905, leading to the decline and disappearance of horse-drawn, hand-pumped, and steam-powered fire engines by the mid 1920s. See also Bombay Explosion (1944) Firematic Racing Great Fire of London List of fire departments List of fires History of the Belfast Fire Brigade Fires in Edo Notes References Paulison, David R. (February 1, 2005). Courses of Action. Fire Chief National Incident Management System Further reading External links Fire departments for rural communities : how to organize and operate them hosted by the UNT Government Documents Department Historic firefighting items in the Staten Island Historical Society Online Collections Database "Apparatus for Extinguishing Fires" Popular Science Monthly 1895. Begins on page 477. Series of many short videos about the history of fire fighting in the U.S.A. See article "Fire-Fighting in Bygone Days" starting page 623 Fires, fire engines, and fire brigades: with a history of manual and steam fire engines... 1866 London. "Public enemy" is a term to denounce a notorious criminal whose activities are seen as extremely damaging to society. A notable declared Public Enemy was Al Capone in the 1930s. Origin and usage The expression is a translation of the ancient Roman phrase hostis publicus; for example, the Senate denounced emperor Nero as a hostis publicus in AD 68. While the English word "public" usually refers to the citizenry, government, or State, the Latin word publicus was the adjective form of populus ("people"), so hostis publicus can be translated as "enemy of the people" as well as "public enemy". The phrase is attested in the 17th century in the United Kingdom. The phrase ennemi du peuple was extensively used during the French Revolution. On 25 December 1793, Robespierre stated: "The revolutionary government owes to the good citizen all the protection of the nation; it owes nothing to the Enemies of the People but death." The Law of 22 Prairial in 1794 extended the remit of the Revolutionary Tribunal to punish "enemies of the people", with some political crimes punishable by death, including "spreading false news to divide or trouble the people". US Public Enemy Era The modern use of the term was first popularized in April 1930 by Frank J. Loesch, then chairman of the Chicago Crime Commission, to publicly denounce Al Capone and a list of other organized crime gangsters. In 1933, Loesch recounted the origin and purpose of the list: I had the operating director of the Chicago Crime Commission bring before me a list of the outstanding hoodlums, known murderers, murderers which you and I know but can't prove, and there were about one hundred of them, and out of this list I selected twenty-eight men. I put Al Capone at the head and his brother next, and ran down the twenty-eight, every man being really an outlaw. I called them Public Enemies, and so designated them in my letter, sent to the Chief of Police, the Sheriff [and] every law enforcing officer. The purpose is to keep the publicity light shining on Chicago's most prominent, well known and notorious gangsters to the end that they may be under constant observation by the law enforcing authorities and law-abiding citizens. All of those listed were reputed gangsters or racketeers and most rum-running bootleggers. Although all were known to be consistent law breakers (most prominently violating the widely broken Eighteenth Amendment banning alcohol), none of those named were fugitives or were actively wanted by the law at that time. The list's purpose was clearly to shame those named and to encourage authorities to prosecute them. The phrase was later appropriated by J. Edgar Hoover and the FBI, who used it throughout the 1930s to describe various notorious fugitives. Unlike Loesch's list, the FBI's "Public Enemies" were wanted criminals already charged with crimes, including at various times John Dillinger, Baby Face Nelson, Bonnie and Clyde, Pretty Boy Floyd, Machine Gun Kelly, Ma Barker, and Alvin Karpis. The term was used so extensively during the 1930s that some writers call that period of the FBI's early history the "Public Enemy Era". Dillinger, Floyd, Nelson, and Karpis were successively named "Public Enemy Number 1" from June 1934 to May 1936. The informal designation eventually evolved into the FBI Ten Most Wanted Fugitives list. The FBI's website describes the bureau's use of the term: "The FBI and the U.S. Department of Justice made use of the term, 'Public Enemy', in the 1930s, an era in which the term was synonymous with 'fugitive' or 'notorious gangster'." It was used in speeches, books, press releases, and internal memoranda and remains in usage to this day. See also Enemy of the people References External links The dictionary definition of public enemy at Wiktionary Alphonse Capone Documentary - Public Enemy Number One Livestock Keepers' Rights are a bundle of rights that would support the survival of small-scale livestock keepers such as pastoralists, smallholders and family farms in a general policy environment that favours large-scale industrial modes of livestock production. In the context of the current anti-livestock agenda and the claim by commercial interests that livestock will be eliminated by 2035, they are gaining added traction. The term "Livestock Keepers' Rights" was coined during the World Food Summit in 2002 by civil society attending the Forum for Food Sovereignty to flag the role of livestock keepers in animal genetic resource management. It alluded to "Farmers' Rights" as known from the International Treaty on Plant Genetic Resources for Food and Agriculture that had been recently concluded. Background Between 2003 and 2007, a large number of grassroots consultations were carried out by and with livestock-keeping communities to define the term more closely. These consultations took place in Kenya ("Karen Commitment"), India, Italy ("Bellagio Brief") and Ethiopia ("Addis Résumé") and involved about 500 representatives of livestock keeping communities from Africa, Asia, Latin America and Europe. They identified 7 cornerstones of "Livestock Keepers' Rights" that would enable livestock keepers to continue playing their role as guardians of biological diversity. During this process, Livestock Keepers’ Rights were elaborated into a much more comprehensive concept than Farmers’ Rights. Rather than representing legal rights, they correspond to development principles that would help livestock keepers continue to conserve biodiversity and animal genetic resources. Principles and Rights During a workshop with legal experts held in Kalk Bay, South Africa in December 2008, the rights were further refined and subdivided into principles and rights: Principle 1: Livestock Keepers are creators of breeds and custodians of animal genetic resources for food and agriculture. Principle 2: Livestock Keepers and the sustainable use of traditional breeds are dependent on the conservation of their respective ecosystems. Principle 3: Traditional breeds represent collective property, products of indigenous knowledge and cultural expression of Livestock Keepers. Based on these principles articulated and implicit in existing legal instruments and international agreements, Livestock Keepers from traditional livestock keeping communities and/or adhering to ecological principles of animal production, shall be given the following Livestock Keepers' Rights: Livestock Keepers have the right to make breeding decisions and breed the breeds they maintain. Livestock Keepers shall have the right to participate in policy formulation and implementation processes on animal genetic resources for food and agriculture. Livestock Keepers shall have the right to appropriate training and capacity building and equal access to relevant services enabling and supporting them to raise livestock and to better process and market their products. Livestock Keepers shall have the right to participate in the identification of research needs and research design with respect to their genetic resources, as is mandated by the principle of Prior Informed Consent. Livestock Keepers shall have the right to effectively access information on issues related to their local breeds and livestock diversity. Declaration on Rights The workshop also resulted in a Declaration on Livestock Keepers Rights that references the individual principles and rights to existing international legal frameworks such as the UN Convention on Biological Diversity, the United Nations Convention to Combat Desertification, the Global Plan of Action for Animal Genetic Resources and the Interlaken Declaration on Animal Genetic Resources, as well as the Universal Declaration of Human Rights, the International Covenant on Economic, Social and Cultural Rights, the United Nations Declaration on the Rights of Indigenous Peoples, the Convention on the Protection and Promotion of the Diversity of Cultural Expressions, the convention (No. 169) concerning Indigenous and Tribal Peoples in Independent Countries, the Declaration on the Rights of Persons Belonging to National or Ethnic, Religious and Linguistic Minorities, and other pertinent legal agreements. The Declaration on Livestock Keepers’ Rights was signed by a large number of individuals and organizations. At an International Technical Expert Workshop on Access and Benefit Sharing in Animal Genetic Resources for Food and Agriculture, that was held in Wageningen in the Netherlands from 8–10 December 2010, it was decided that Livestock Keepers’ Rights should be addressed. At another workshop on rights to animal genetic resources that was held in Bern (Switzerland) on 20 June 2011, Livestock Keepers’ Rights were contrasted with Animal Breeders Rights. In July 2011,at a side-event during the 13th session of the Commission on Genetic Resources for Food and Agriculture (CGRFA) held at FAO in Rome, the concept of Livestock Keepers' Rights was introduced and supported by government officials. References Köhler-Rollefson, I., E. Mathias and H.S. Rathore. 2008. Local breeds, livelihoods, and livestock keepers’ rights in South Asia. Tropical Animal Health and Production, published on line 22 November 2008 Köhler-Rollefson, I., Evelyn Mathias, Hanwant Singh, P. Vivekanandan and Jacob Wanyama. 2010. Livestock Keepers’ Rights: The State of Discussion. Animal Genetic Resources: 47, 1–5. Köhler-Rollefson, I., P. Vivekanandan and HS Rathore. 2010. Livestock Keepers Rights and Biocultural Protocols : Tools for Protecting Biodiversity and the Livelihoods of the Poor. LEISA India 12(1):35-36 Köhler-Rollefson, I. and E. Mathias. 2010. Livestock Keepers‘Rights: a Rights-based approach towards invoking justice for pastoralists and biodiversity conserving livestock keepers. Policy Matters. External links Karen Commitment agreed at indigenous livestock breeders workshop in Kenya on 30 October 2003 History of Livestock Keepers’ Rights. Declaration on Livestock Keepers' Rights [1] New guidelines for livestock keepers' rights. Rights to animal genetic resources for food and agriculture. keepers’ rights: Conserving endangered animal genetic resources in Kenya http://www.fao.org/docrep/013/i1823t/i1823t13.pdf Livestock keepers’ rights: the state of discussion] International Technical Expert Workshop on Access and Benefit Sharing in Animal Genetic Resources for Food and Agriculture, Wageningen, 8 –10 December 2010 Workshop on sui-generis-rights to animal genetic resources. University Park is a 1.17 square miles (3.0 km2) neighborhood in the South Los Angeles region of Los Angeles, California. The area includes the University of Southern California (USC), and the residential neighborhoods located immediately north of the campus: North University Park, Chester Place and St. James Park. The area contains two historic districts that are both on the National Register of Historic Places: The North University Park Historic District and the Menlo Avenue–West Twenty-ninth Street Historic District. History Charles Epting, author of the book "University Park", states: "The story of USC is the story of University Park in general, showing how a neighborhood and an educational institution can develop hand-in-hand. After the founding of the USC, the North University Park neighborhood was developed thanks to an influx of wealthy citizens. Within North University Park, there are two historic districts on the National Register of Historic Places: The North University Park Historic District and the Menlo Avenue–West Twenty-ninth Street Historic District. Geography Located in the West Adams district, University Park contains the University of Southern California (USC) on the south and the residential neighborhoods of North University Park, Chester Place and St. James Park on the north. The neighborhood's street boundaries are the Santa Monica Freeway on the north, Washington Boulevard on the northeast, Vermont Avenue on the west, the Harbor Freeway on the east, and Exposition Boulevard on the south. Demographics 2000 A total of 23,596 people lived in University Park's 1.17 square miles, according to the 2000 U.S. census—averaging 20,217 people per square mile, among the highest population densities in the city as a whole. The median age was 23, considered young when compared to the city as a whole. The percentage of residents aged 19 to 34 was among the county's highest. The $16,533 median household income in 2008 dollars was considered low for the city and county. The percentage of households earning $20,000 or less (56.6%) was the second-largest in Los Angeles County, outplaced only by Downtown Los Angeles. The average household size of 2.7 people was average for the city. Renters occupied 92.2% of the housing units, and homeowners occupied the rest. In 2000 there were 590 families headed by single parents, or 20.3%, a rate that was high for the county and the city. The percentages of never-married women (61.5) and never-married men (67.2) were among the county's highest. In the same year there were 198 military veterans, or 1% of the population, considered low when compared to the city and county as a whole. 2008 Population was estimated at 25,181 in 2008. Education Secondary and primary The following public schools are operated by the Los Angeles Unified School District: New Designs Charter School— 2303 South Figueroa Way, 1342 West Adams Blvd Alliance Richard Merkin Middle School - 2023 Union Avenue Downtown Value School - 950 West Washington Boulevard Norwood Street Elementary School - 2020 Oak Street Frank Lanterman School - 2328 Saint James Place Thirty-Second Street USC Performing Arts - 822 West 32nd Street Post-secondary University of Southern California Mount St. Mary's University, Doheny Campus Hebrew Union College Recreation and parks Hoover Recreation Center - 1010 West 25th Street. The parkincludes an auditorium equipped with a studio floor and stage, three meeting rooms, kitchen, private outdoor courtyard with children's play area, basketball courts, outdoor fitness equipment, walking/running paths, picnic tables and barbecue pits. St. James Park - Adams Boulevard and Severance Street Landmarks and attractions The Shrine Auditorium - 665 W. Jefferson Blvd. University Village - 3201 S. Hoover. St. John's Cathedral - 514 W. Adams Blvd. It is the Diocesan Cathedral for the Episcopal Diocese of Los Angeles. Transportation The Metro E Line serves the neighborhood, with stations at 23rd Street, Jefferson Blvd./USC, Exposition Park/USC and Exposition Blvd./Vermont Avenue. Notable residents Newell Mathews (c.1833–1907), 19th Century businessman and member of the Los Angeles Common Council Frank Sabichi (1842–1900), attorney, land developer, member of the Common Council Ygnacio Sepulveda (1842–1916), Superior Court judge, 2639 Monmouth Avenue References == External links == A gang is a group or society of associates, friends, or members of a family with a defined leadership and internal organization that identifies with or claims control over territory in a community and engages, either individually or collectively, in illegal, and possibly violent, behavior, with such behavior often constituting a form of organized crime. Etymology The word gang derives from the past participle of Old English gan, meaning 'to go'. It is cognate with Old Norse gangr, meaning 'journey'. While the term often refers specifically to criminal groups, it also has a broader meaning of any close or organized group of people, and may have neutral, positive or negative connotations depending on usage. History In discussing the banditry in American history, Barrington Moore, Jr. suggests that gangsterism as a "form of self-help which victimizes others" may appear in societies which lack strong "forces of law and order"; he characterizes European feudalism as "mainly gangsterism that had become society itself and acquired respectability through the notions of chivalry". The 17th century saw London "terrorized by a series of organized gangs", some of them known as the Mims, Hectors, Bugles, and Dead Boys. These gangs often came into conflict with each other. Members dressed "with colored ribbons to distinguish the different factions." During the Victorian era, criminals and gangs started to form organizations which would collectively become London's criminal underworld. Criminal societies in the underworld started to develop their own ranks and groups which were sometimes called families, and were often made up of lower-classes and operated on pick-pocketry, prostitution, forgery and counterfeiting, commercial burglary, and money laundering schemes. Unique also were the use of slangs and argots used by Victorian criminal societies to distinguish each other, like those propagated by street gangs like the Peaky Blinders. In the United States, the history of gangs began on the East Coast in 1783 following the American Revolution. Gangs arose further in the United States by the middle of the nineteenth century and were a concern for city leaders from the time they appeared. The emergence of the gangs was largely attributed to the vast rural population immigration to the urban areas. The first street-gang in the United States, the 40 Thieves, began around the late 1820s in New York City. The gangs in Washington D.C. had control of what is now Federal Triangle, in a region then known as Murder Bay. Organized crime in the United States first came to prominence in the Old West and historians such as Brian J. Robb and Erin H. Turner traced the first organized crime syndicates to the Coschise Cowboy Gang and the Wild Bunch. Prohibition would also cause a new boom in the emergence of gangs; Chicago for example had over 1,000 gangs in the 1920s. Outside of the US and the UK, gangs exist in both urban and rural forms, like the French gangs of the Belle Époque like the Apaches and the Bonnot Gang. Many criminal organizations, such as the Italian Cosa Nostra, Japanese yakuza, Russian Bratva, and Chinese triads, have existed for centuries. Types Gangs, syndicates, and other criminal groups, come in many forms, each with their own