{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi0f_3zEEMr4"
      },
      "source": [
        "# pipeline_deberta-v3-small.ipynb\n",
        "This notebook contains some template code to help you with loading/preprocessing the data.\n",
        "\n",
        "We start with some imports and constants.\n",
        "The training data is found in the `data` subfolder.\n",
        "There is also a tokenizer I've trained for you which you can use for the project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCX9Tayrtu2E"
      },
      "source": [
        "Can be executed once — does not depend on the execution environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "P7QGcWQwsygr"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# 1. Clone the Repository (if required)\n",
        "# ========================================\n",
        "import os\n",
        "TOKEN = os.getenv('GITHUB_TOKEN')  # Set this as environment variable: export GITHUB_TOKEN=\"your_token_here\"\n",
        "if not TOKEN:\n",
        "    raise ValueError(\"Please set GITHUB_TOKEN environment variable\")\n",
        "# XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
        "!git clone https://{TOKEN}@github.com/KatsuhitoArasaka/BabyLM-Tiny.git  # your repository link  ⚠️\n",
        "%cd /content/BabyLM-Tiny\n",
        "\n",
        "# ===============\n",
        "# 2. Set Paths\n",
        "# ===============\n",
        "TRAIN_PATH = './datasets/PATH_TO_TRAINING_DATA.train'       # Path to your training data  ⚠️\n",
        "DEV_PATH = './datasets/PATH_TO_VALIDATION_DATA.train'           # Path to your validation data  ⚠️"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n2o7lgau14Y"
      },
      "source": [
        "Execute after restarting the environment -- when changing the device (CPU ↔ GPU) you need to restart the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "23L24AQHId8R"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# 3. Install Dependencies\n",
        "# ==========================\n",
        "%pip install transformers datasets wandb trl bitsandbytes huggingface_hub --quiet\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# 4. Import Libraries and Set Device\n",
        "# =====================================\n",
        "import subprocess\n",
        "import json\n",
        "import torch\n",
        "import datasets\n",
        "from functools import partial\n",
        "from datasets import load_dataset\n",
        "from transformers import DataCollatorForLanguageModeling, set_seed\n",
        "from transformers import AutoConfig, AutoTokenizer, AutoModelForMaskedLM\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "from transformers import set_seed\n",
        "set_seed(42)\n",
        "\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'  # Check for GPU availability\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCYlSdXkGbgw"
      },
      "outputs": [],
      "source": [
        "# ======================================\n",
        "# 5. Authentication with wandb and hf\n",
        "# ======================================\n",
        "import wandb\n",
        "wandb.login()  # Enter your API key when prompted ⚠️\n",
        "\n",
        "from huggingface_hub import login\n",
        "login()  # Paste your HF token from https://huggingface.co/settings/tokens  ⚠️\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "em3xW7Rjf_pT"
      },
      "outputs": [],
      "source": [
        "# ================================================\n",
        "# 6. Start a new wandb run to track this script\n",
        "# ================================================\n",
        "run = wandb.init(\n",
        "    # Set the wandb entity where your project will be logged (generally your team name).\n",
        "    entity=\"Low-Resource_Pretraining\",\n",
        "    # Set the wandb project where this run will be logged.\n",
        "    project=\"NLP_LRP_BabyLM\",\n",
        "    # Track hyperparameters and run metadata.\n",
        "    config={\n",
        "        \"learning_rate\": 0.02,  # the main parameter for configuring the optimizer\n",
        "        \"architecture\": \"DeBERTa\",  # a description of the model architecture, to track which model was used in the project\n",
        "        \"epochs\": 10,  # the number of training epochs, an important parameter for understanding the duration of the experiment and its settings\n",
        "\n",
        "        # \"dataset\": \"CIFAR-100\",\n",
        "        # \"batch_size\": 8,\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glUE2dtEEMr8"
      },
      "source": [
        "Here are we load the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RkhiZZGfEMr9"
      },
      "outputs": [],
      "source": [
        "# ===================\n",
        "# 7. Load Datasets\n",
        "# ===================\n",
        "\n",
        "# Install and load spaCy with the English model\n",
        "!pip install -q spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.max_length = 10_000_000  # Safe limit to handle long texts; can be adjusted if needed\n",
        "\n",
        "# For using the 1M spoken dataset, split by sentences using spaCy\n",
        "# (instead of splitting by '.' to avoid incorrect sentence breaks in abbreviations like 'Dr.', 'U.S.', 'Mr.', etc.)\n",
        "\n",
        "# Function to split long text into smaller chunks for spaCy processing\n",
        "def split_into_chunks(text, chunk_size=200_000):\n",
        "    for i in range(0, len(text), chunk_size):\n",
        "        yield text[i:i+chunk_size]\n",
        "\n",
        "# Function to split text into sentences using spaCy on smaller chunks\n",
        "def spacy_sent_tokenize(text):\n",
        "    sentences = []\n",
        "    for chunk in split_into_chunks(text):\n",
        "        doc = nlp(chunk)\n",
        "        sentences.extend([sent.text.strip() for sent in doc.sents if sent.text.strip()])\n",
        "    return sentences\n",
        "\n",
        "# Function to filter out very short sentences (set min words ⚠️)\n",
        "def filter_short_sentences(sentences, min_words=4):\n",
        "    return [s for s in sentences if len(s.split()) >= min_words]\n",
        "\n",
        "# Load and tokenize training dataset\n",
        "with open(TRAIN_PATH, 'r', encoding='utf-8') as f:\n",
        "    full_text = f.read().strip()\n",
        "    sentences = spacy_sent_tokenize(full_text)\n",
        "    filtered_sentences = filter_short_sentences(sentences)\n",
        "    train_data = [{\"text\": s} for s in filtered_sentences]\n",
        "\n",
        "# Load and tokenize validation dataset\n",
        "with open(DEV_PATH, 'r', encoding='utf-8') as f:\n",
        "    full_text = f.read().strip()\n",
        "    sentences = spacy_sent_tokenize(full_text)\n",
        "    filtered_sentences = filter_short_sentences(sentences)\n",
        "    val_data = [{\"text\": s} for s in filtered_sentences]\n",
        "\n",
        "# Create DatasetDict object for HuggingFace datasets\n",
        "from datasets import Dataset, DatasetDict\n",
        "dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_list(train_data),\n",
        "    \"validation\": Dataset.from_list(val_data)\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sU2WHPCixD5m"
      },
      "outputs": [],
      "source": [
        "# =====================================================================================\n",
        "# 8. Setup Model, Training, and Logging (This is the part that may change per model)\n",
        "# =====================================================================================\n",
        "\n",
        "# Specific Settings for the Model you choose            ⚠️\n",
        "# Replace this block with model-specific settings       ⚠️\n",
        "model_name = \"microsoft/deberta-v3-small\"  # Example: DeBERTa model for MLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "model = AutoModelForMaskedLM.from_config(config)\n",
        "\n",
        "# Enable gradient checkpointing to save GPU memory\n",
        "# model.gradient_checkpointing_enable()  --  causes 'RuntimeError: Trying to backward through the graph a second time'\n",
        "\n",
        "# Data collator to support masked language modeling\n",
        "\n",
        "# data_collator = DataCollatorForLanguageModeling(\n",
        "#     tokenizer=tokenizer,\n",
        "#     seed=0,\n",
        "# )\n",
        "# Passing a custom data collator is not supported when using padding-free.\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# 9. Setup Training Arguments\n",
        "# ===============================\n",
        "# train.py\n",
        "def preprocess_logits_for_metrics(logits, labels):\n",
        "    pred_ids = torch.argmax(logits, dim=-1)\n",
        "    return pred_ids\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.flatten()\n",
        "    labels = labels.flatten()\n",
        "    mask = labels != -100\n",
        "    labels = labels[mask]\n",
        "    predictions = predictions[mask]\n",
        "\n",
        "    correct = labels == predictions\n",
        "    accuracy = correct.sum() / float(len(correct))\n",
        "    return {\"acc\": accuracy}\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 10. Train the Model\n",
        "# ======================\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    train_dataset = dataset['train'],\n",
        "    eval_dataset = dataset['validation'],\n",
        "    # data_collator = data_collator,\n",
        "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
        "    compute_metrics=compute_metrics,\n",
        "    args = SFTConfig(\n",
        "        remove_unused_columns = True,\n",
        "        label_names = [\"labels\"],\n",
        "        dataset_num_proc = 12,\n",
        "        packing = True,\n",
        "        eval_packing = True,\n",
        "        max_seq_length = 64,  # ⚠️\n",
        "        dataset_text_field = \"text\",\n",
        "        eval_strategy = \"steps\",\n",
        "        per_device_train_batch_size = 16,  # 64 / N  ⚠️\n",
        "        gradient_accumulation_steps = 4,  # N  →  16 * 4 = 64 effective batch  ⚠️\n",
        "        warmup_ratio = 0.05,\n",
        "        num_train_epochs = 10,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = True,\n",
        "        bf16 = False,\n",
        "        logging_steps = 10,\n",
        "        eval_steps = 100,\n",
        "        save_steps = 100,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 0,\n",
        "        # output_dir = \"\",\n",
        "        report_to = \"none\",\n",
        "        eval_accumulation_steps=1,\n",
        "        include_for_metrics=[],\n",
        "        max_grad_norm=1,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# torch.cuda.empty_cache()\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qstKQ5d6S692"
      },
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# 11. Finish wandb Logging\n",
        "# ===========================\n",
        "wandb.finish()  # log the final metrics and mark the run as complete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCemWTzJBP1p"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 12. Save the trained model locally (for later evaluation)\n",
        "# ============================================================\n",
        "\n",
        "# This will save the model to a folder inside your Colab environment\n",
        "# The folder will be deleted when the session ends unless uploaded elsewhere (e.g., Hugging Face, Google Drive)\n",
        "\n",
        "# save_path = \"./trained_models/deberta-v3-small-dataset\"  # ⚠️ Change `model_name` if training multiple models in one session\n",
        "# trainer.save_model(save_path)        # Save model weights, config, etc.\n",
        "# tokenizer.save_pretrained(save_path) # Save tokenizer (required for evaluation)\n",
        "\n",
        "# OR use HF repository\n",
        "save_path = \"your_hf_username/model_name\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVSCNJXvSV3i"
      },
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# 13. Upload Trained Model to Hugging Face Hub\n",
        "# ===============================================\n",
        "from huggingface_hub import create_repo, upload_folder\n",
        "\n",
        "hf_repo_name = \"deberta-v3-small-dataset\"  # Change this to a unique name for your model       ⚠️\n",
        "hf_username = \"your_hf_username\"          # Replace with your actual Hugging Face username    ⚠️\n",
        "repo_id = f\"{hf_username}/{hf_repo_name}\"\n",
        "\n",
        "# Create a public repo (use private=True if needed)\n",
        "create_repo(repo_id, exist_ok=True, private=True)\n",
        "\n",
        "# Upload entire trained model folder\n",
        "upload_folder(\n",
        "    repo_id=repo_id,\n",
        "    folder_path=save_path,\n",
        "    path_in_repo=\".\",  # Upload everything from the folder\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "\n",
        "print(f\"Model uploaded to: https://huggingface.co/{repo_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AyEq4ozh44X2"
      },
      "outputs": [],
      "source": [
        "# ===============================================================\n",
        "# 14. Evaluate the model using BabyLM evaluation pipeline 2025\n",
        "# ===============================================================\n",
        "\n",
        "# run chunk \"1. Clone the Repository\" again before starting the evaluation ⚠️\n",
        "\n",
        "import os\n",
        "import pathlib\n",
        "import shutil\n",
        "import subprocess\n",
        "# !pip install transformers --quiet\n",
        "\n",
        "\n",
        "!cp -r BabyLM-Tiny/evaluation_data evaluation-pipeline-2025/\n",
        "!rm -r evaluation-pipeline-2025/evaluation_data/fast_eval/ewok_fast*\n",
        "\n",
        "# --- Configuration ---\n",
        "EVAL_PIPELINE_REPO = \"evaluation-pipeline-2025\"\n",
        "\n",
        "ARCHITECTURE = \"mlm\"  # \"mlm\", \"causal\", or \"mntp\"  # ⚠️\n",
        "EVAL_MODE = \"zero-shot-fast\"  # \"zero-shot\", \"zero-shot-fast\", or \"finetune\"  # ⚠️\n",
        "EVAL_DATA_FOLDER = \"fast_eval\"  # \"full_eval\" or \"fast_eval\" for \"zero-shot-fast\" # ⚠️\n",
        "# EVAL_DATA_FOLDER is ignored when using EVAL_MODE = \"finetune\"\n",
        "# because finetuning tasks load datasets directly from HuggingFace (e.g., GLUE), not from evaluation_data/.\n",
        "\n",
        "DATASET_NAME = \"name-dataset\"  # change 'name' in the \"name-dataset\" to the dataset your model was trained on  # ⚠️\n",
        "ENABLE_GIT_PUSH = True  # set to True to automatically commit and push the result  # ⚠️\n",
        "\n",
        "model_id_path = pathlib.Path(save_path).name.replace(\"/\", \"_\")  # for results directory structure\n",
        "\n",
        "# --- Step 1: Clone evaluation pipeline if not present ---\n",
        "if not os.path.exists(EVAL_PIPELINE_REPO):\n",
        "    subprocess.run([\"git\", \"clone\", \"https://github.com/babylm/evaluation-pipeline-2025.git\"], check=True)\n",
        "\n",
        "# --- Step 2: Run the appropriate evaluation script ---\n",
        "if EVAL_MODE == \"zero-shot\":\n",
        "    eval_script = \"eval_zero_shot.sh\"\n",
        "    eval_dir = os.path.join(\"evaluation_data\", EVAL_DATA_FOLDER)\n",
        "    cmd = [\"bash\", eval_script, save_path, ARCHITECTURE, eval_dir]\n",
        "    working_dir = EVAL_PIPELINE_REPO\n",
        "\n",
        "elif EVAL_MODE == \"zero-shot-fast\":\n",
        "    eval_script = \"eval_zero_shot_fast.sh\"\n",
        "    eval_dir = os.path.join(\"evaluation_data\", EVAL_DATA_FOLDER)\n",
        "    revision_name = \"main\"  # <-- Use \"main\" as default unless you have custom branches/tags\n",
        "\n",
        "    # Optional: specify a revision if using branches or tags in your HF repo\n",
        "    # For example, \"checkpoint_1M\", \"final\", or a commit hash like \"e5a72b3\"\n",
        "    # revision_name = \"checkpoint_1M\"\n",
        "\n",
        "    cmd = [\"bash\", eval_script, save_path, revision_name, ARCHITECTURE, eval_dir]\n",
        "    working_dir = EVAL_PIPELINE_REPO\n",
        "\n",
        "elif EVAL_MODE == \"finetune\":\n",
        "    eval_script = \"eval_finetune.sh\"\n",
        "    cmd = [\"bash\", eval_script, save_path]\n",
        "    working_dir = EVAL_PIPELINE_REPO\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown EVAL_MODE: {EVAL_MODE}\")\n",
        "\n",
        "# --- Run the script and capture output ---\n",
        "result = subprocess.run(cmd, cwd=working_dir, capture_output=True, text=True)\n",
        "\n",
        "# Print outputs for debugging\n",
        "print(\"=== STDOUT ===\")\n",
        "print(result.stdout)\n",
        "print(\"\\n=== STDERR ===\")\n",
        "print(result.stderr)\n",
        "\n",
        "# --- Manually save results if results.txt is missing ---\n",
        "manual_results_path = f\"{EVAL_PIPELINE_REPO}/results/{model_id_path}/manual_results.txt\"\n",
        "results_dir = pathlib.Path(manual_results_path).parent\n",
        "results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(manual_results_path, \"w\") as f:\n",
        "    f.write(result.stdout)\n",
        "\n",
        "print(f\"\\n✅ Saved results to: {manual_results_path}\")\n",
        "\n",
        "# Raise an error if the script failed\n",
        "if result.returncode != 0:\n",
        "    raise RuntimeError(f\"❌ Evaluation script failed with exit code {result.returncode}\")\n",
        "\n",
        "\n",
        "# --- Step 3: Locate the results.txt file ---\n",
        "if EVAL_MODE.startswith(\"zero-shot\"):\n",
        "    eval_subfolder = \"zero_shot\"\n",
        "elif EVAL_MODE == \"finetune\":\n",
        "    eval_subfolder = \"finetune\"\n",
        "else:\n",
        "    raise RuntimeError(\"Unexpected EVAL_MODE value\")\n",
        "\n",
        "results_root = pathlib.Path(EVAL_PIPELINE_REPO) / \"results\" / model_id_path / \"main\" / eval_subfolder\n",
        "\n",
        "# Search recursively for results.txt\n",
        "candidate = None\n",
        "for root, _, files in os.walk(results_root):\n",
        "    for f in files:\n",
        "        if f == \"results.txt\":\n",
        "            candidate = os.path.join(root, f)\n",
        "            break\n",
        "\n",
        "# Fallback to manually saved output if results.txt not found\n",
        "if not candidate:\n",
        "    manual_results_path = f\"{EVAL_PIPELINE_REPO}/results/{model_id_path}/manual_results.txt\"\n",
        "    if os.path.exists(manual_results_path):\n",
        "        candidate = manual_results_path\n",
        "        print(\"ℹ️ results.txt not found — using manually saved output instead.\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\"Evaluation completed, but neither results.txt nor manual_results.txt was found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2ZDh_r7_D-S2"
      },
      "outputs": [],
      "source": [
        "# --- Step 4: Save results using <dataset>_<eval_mode>.txt format ---\n",
        "TARGET_REPO_DIR = \"BabyLM-Tiny\"\n",
        "\n",
        "output_dir = (\n",
        "    pathlib.Path(TARGET_REPO_DIR)\n",
        "    / \"models_evaluation_results\"\n",
        "    / \"deberta-v3-small_results\"  # ⚠️ Change this folder name to match the name of the model being evaluated\n",
        ")\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "result_file_path = output_dir / f\"{DATASET_NAME}_{EVAL_MODE}.txt\"\n",
        "shutil.copy(candidate, result_file_path)\n",
        "\n",
        "print(f\"✅ Evaluation complete. Results saved to {result_file_path}\")\n",
        "\n",
        "# --- Step 5: Optionally commit and push the result to GitHub ---\n",
        "\"# --- Step 5: Optionally commit and push the result to GitHub ---\n",
        "\",\n",
        "        \"!git config --global user.name \"Your Name\"\n",
        "\",\n",
        "        \"\"!git config --global user.email \"your.email@example.com\"\n",
        "\",\n",
        "\",\n",
        "\n",
        "if ENABLE_GIT_PUSH:\n",
        "    rel_path = result_file_path.relative_to(TARGET_REPO_DIR)\n",
        "\n",
        "    subprocess.run([\"git\", \"add\", str(rel_path)], cwd=TARGET_REPO_DIR)\n",
        "    subprocess.run(\n",
        "        [\"git\", \"commit\", \"-m\", f\"Add {EVAL_MODE} evaluation results for {DATASET_NAME}\"],\n",
        "        cwd=TARGET_REPO_DIR,\n",
        "    )\n",
        "\n",
        "    subprocess.run([\"git\", \"pull\", \"origin\", \"main\", \"--rebase\"], cwd=TARGET_REPO_DIR)  # This prevents a push error when there are new commits on GitHub.\n",
        "    result = subprocess.run([\"git\", \"push\"], cwd=TARGET_REPO_DIR, capture_output=True, text=True)\n",
        "    print(result.stdout)\n",
        "    print(result.stderr)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
