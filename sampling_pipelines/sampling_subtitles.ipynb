{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KatsuhitoArasaka/BabyLM-Tiny/blob/main/sampling_pipelines/sampling_subtitles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48bOrrHsKslZ"
      },
      "source": [
        "# Movie Subtitles Sampling Pipeline\n",
        "\n",
        "This notebook creates genre-specific datasets from movie subtitles by fetching movie metadata from TMDb and matching subtitles from OpenSubtitles.\n",
        "\n",
        "## Features\n",
        "- Genre-based movie discovery using TMDb API\n",
        "- Subtitle downloading and processing\n",
        "- 1M-word datasets for different movie genres\n",
        "- Train/dev split generation\n",
        "\n",
        "## Data Sources\n",
        "- **TMDb API**: Movie metadata and genre classification\n",
        "- **OpenSubtitles**: Subtitle content extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_KxNoh2Pv-U"
      },
      "outputs": [],
      "source": [
        "!pip install tmdbsimple\n",
        "\n",
        "import tmdbsimple as tmdb\n",
        "import os\n",
        "# Replace with your own TMDB API key from environment variable\n",
        "tmdb.API_KEY = os.getenv('TMDB_API_KEY')\n",
        "if not tmdb.API_KEY:\n",
        "    raise ValueError(\"Please set TMDB_API_KEY environment variable\")\n",
        "\n",
        "# Example call: get genres\n",
        "genres = tmdb.Genres().movie_list()['genres']\n",
        "print(\"‚úÖ Available TMDb genres:\")\n",
        "for g in genres:\n",
        "    print(f\"{g['id']:>3} ‚Üí {g['name']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. TMDb API Setup and Movie Discovery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IejmxZyV7vT"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def get_movies_by_genre_popularity(genre_id, max_pages):\n",
        "    \"\"\"\n",
        "    Fetches movies by TMDb genre ID using the /discover endpoint.\n",
        "    Returns: list of dicts with title, year, and IMDb ID\n",
        "    \"\"\"\n",
        "    all_movies = []\n",
        "\n",
        "    for page in tqdm(range(1, max_pages + 1)):\n",
        "        try:\n",
        "            response = tmdb.Discover().movie(\n",
        "                with_genres=genre_id,\n",
        "                sort_by='popularity.desc',\n",
        "                page=page,\n",
        "                language='en-US'\n",
        "            )\n",
        "            for movie in response['results']:\n",
        "                tmdb_id = movie['id']\n",
        "                try:\n",
        "                    movie_data = tmdb.Movies(tmdb_id).info()\n",
        "                    imdb_id = movie_data.get(\"imdb_id\")\n",
        "                    title = movie_data.get(\"title\")\n",
        "                    year = movie_data.get(\"release_date\", \"\")[:4]\n",
        "                    if imdb_id and title:\n",
        "                        all_movies.append({\n",
        "                            \"title\": title,\n",
        "                            \"year\": year,\n",
        "                            \"imdb_id\": imdb_id\n",
        "                        })\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è Failed to fetch IMDb ID for TMDb ID {tmdb_id}: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error on page {page}: {e}\")\n",
        "\n",
        "    # Ensure uniqueness by imdb_id\n",
        "    unique_movies = {m['imdb_id']: m for m in all_movies}.values()\n",
        "    return list(unique_movies)\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "def get_movies_by_genre_true_random(genre_id, sample_size=160, pages_to_scan=40):\n",
        "    all_movies = []\n",
        "\n",
        "    for page in tqdm(random.sample(range(1, 1001), pages_to_scan)):\n",
        "        try:\n",
        "            response = tmdb.Discover().movie(\n",
        "                with_genres=genre_id,\n",
        "                sort_by='popularity.desc',\n",
        "                page=page,\n",
        "                language='en-US'\n",
        "            )\n",
        "            for movie in response['results']:\n",
        "                tmdb_id = movie['id']\n",
        "                try:\n",
        "                    movie_data = tmdb.Movies(tmdb_id).info()\n",
        "                    imdb_id = movie_data.get(\"imdb_id\")\n",
        "                    title = movie_data.get(\"title\")\n",
        "                    year = movie_data.get(\"release_date\", \"\")[:4]\n",
        "                    if imdb_id and title:\n",
        "                        all_movies.append({\n",
        "                            \"title\": title,\n",
        "                            \"year\": year,\n",
        "                            \"imdb_id\": imdb_id\n",
        "                        })\n",
        "                except: continue\n",
        "        except: continue\n",
        "\n",
        "    # Ensure uniqueness by imdb_id\n",
        "    unique_movies = {m['imdb_id']: m for m in all_movies}.values()\n",
        "    return random.sample(list(unique_movies), min(sample_size, len(unique_movies)))\n",
        "\n",
        "\n",
        "\n",
        "# Define the TMDb genre IDs for our target genres\n",
        "GENRE_IDS = {\n",
        "    \"Action\": 28,\n",
        "    \"Comedy\": 35,\n",
        "    \"Documentary\": 99,\n",
        "    \"History\": 36,\n",
        "    \"Romance\": 10749,\n",
        "    \"Science Fiction\": 878,\n",
        "}\n",
        "\n",
        "# Collect ~n movies per genre\n",
        "genre_to_movies = {}\n",
        "\n",
        "for genre_name, genre_id in GENRE_IDS.items():\n",
        "    print(f\"\\nüîç Fetching movies for genre: {genre_name}\")\n",
        "\n",
        "    # movies = get_movies_by_genre_popularity(genre_id, max_pages=50)  # ~20 movies per page\n",
        "    # OR\n",
        "    movies = get_movies_by_genre_true_random(genre_id, sample_size=300, pages_to_scan=120)  # adjust\n",
        "\n",
        "    genre_to_movies[genre_name] = movies\n",
        "    print()\n",
        "    print(f\"‚úÖ {len(movies)} movies found for {genre_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aq4px6IpXFsl"
      },
      "outputs": [],
      "source": [
        "for genre, movies in genre_to_movies.items():\n",
        "    print(f\"Genre: {genre} ‚Äî {len(movies)} movies\")\n",
        "    for movie in movies[:5]:\n",
        "        title = movie.get(\"title\", \"N/A\")\n",
        "        imdb_id = movie.get(\"imdb_id\", \"N/A\")\n",
        "        print(f\"   ‚Ä¢ {title} ‚Äî IMDb ID: {imdb_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTVvT9VJWyeF"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "os.makedirs(\"movie_lists\", exist_ok=True)\n",
        "\n",
        "with open(\"movie_lists/genre_to_movies.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(genre_to_movies, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"‚úÖ Saved genre_to_movies.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmXmRJg8XS8Q"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"movie_lists/genre_to_movies.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    genre_to_movies = json.load(f)\n",
        "\n",
        "for genre, movies in genre_to_movies.items():\n",
        "    print(f\"\\nüé¨ Genre: {genre} ‚Äî {len(movies)} movies\")\n",
        "    for movie in movies[:5]:\n",
        "        title = movie.get(\"title\")\n",
        "        imdb_id = movie.get(\"imdb_id\")\n",
        "        print(f\"   ‚Ä¢ {title} ‚Äî IMDb ID: {imdb_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGcp6iCsdNO5"
      },
      "outputs": [],
      "source": [
        "# STEP 1: Install gdown to download from Google Drive\n",
        "!pip install -q gdown\n",
        "\n",
        "# STEP 2: Download the subtitles archive (from Sublens-20M Dataset)\n",
        "# Google Drive file ID from: https://drive.google.com/file/d/1Xmty1wID7RjZLBXIv09hUctq8OMmhOW5/view\n",
        "file_id = \"1Xmty1wID7RjZLBXIv09hUctq8OMmhOW5\"\n",
        "output = \"Sublens_20M_subtitles.zip\"\n",
        "\n",
        "# Download the ZIP file\n",
        "!gdown --id {file_id} -O {output}\n",
        "\n",
        "# STEP 3: Extract the archive\n",
        "!unzip -q Sublens_20M_subtitles.zip -d subtitles_cf\n",
        "\n",
        "# STEP 4: Print full directory tree under 'subtitles_cf'\n",
        "import os\n",
        "for root, dirs, files in os.walk(\"subtitles_cf\"):\n",
        "    print(root)\n",
        "    for fname in files[:5]:  # print up to 5 files per folder\n",
        "        print(\"   ‚îî‚îÄ\", fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Subtitle Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fkkU5BpjV-_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Example IMDb ID\n",
        "\n",
        "imdb_id_raw = \"tt0281176\"\n",
        "imdb_id = imdb_id_raw.replace(\"tt\", \"\")\n",
        "folder_path = f\"subtitles_cf/Sublens_20M/subtitles/{imdb_id}\"\n",
        "\n",
        "# Find the first .srt file in the folder\n",
        "srt_files = [f for f in os.listdir(folder_path) if f.lower().endswith(\".srt\")]\n",
        "srt_path = os.path.join(folder_path, srt_files[0])\n",
        "print(\"‚úÖ File exists:\" if os.path.exists(srt_path) else \"‚ùå File NOT found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBGcR6Tyub9y"
      },
      "outputs": [],
      "source": [
        "!pip install pysrt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Genre-Specific Dataset Generation\n",
        "\n",
        "Creates 1M-word training datasets and 200k-word development sets for each movie genre."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y39CrGUxbnb4"
      },
      "outputs": [],
      "source": [
        "import pysrt\n",
        "from collections import defaultdict\n",
        "import json\n",
        "\n",
        "# Output folders\n",
        "os.makedirs(\"output\", exist_ok=True)\n",
        "os.makedirs(\"logs\", exist_ok=True)\n",
        "\n",
        "# Limits\n",
        "TARGET_WORDS = 1_000_000\n",
        "DEV_TARGET_WORDS = int(0.2 * TARGET_WORDS)\n",
        "\n",
        "# Word counters\n",
        "word_counts = defaultdict(int)\n",
        "dev_word_counts = defaultdict(int)\n",
        "\n",
        "# Log unfound\n",
        "not_found = defaultdict(list)\n",
        "\n",
        "# Track already used imdb_ids to avoid reprocessing\n",
        "used_imdb_ids = defaultdict(set)  # genre ‚Üí set of imdb_ids\n",
        "\n",
        "# Load used_imdb_ids if it exists\n",
        "if os.path.exists(\"logs/used_imdb_ids.json\"):\n",
        "    with open(\"logs/used_imdb_ids.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        raw_used = json.load(f)\n",
        "        for genre, ids in raw_used.items():\n",
        "            used_imdb_ids[genre] = set(ids)\n",
        "\n",
        "# Safe genre name (e.g. Science Fiction ‚Üí Science_Fiction)\n",
        "def safe_genre_name(g):\n",
        "    return g.replace(\" \", \"_\")\n",
        "\n",
        "# Count words already in file\n",
        "def count_words_in_file(path):\n",
        "    if not os.path.exists(path):\n",
        "        return 0\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        return sum(len(line.split()) for line in f)\n",
        "\n",
        "# Initialize output file handles and word counters\n",
        "train_files = {}\n",
        "dev_files = {}\n",
        "\n",
        "for genre in genre_to_movies:\n",
        "    safe_name = safe_genre_name(genre)\n",
        "    train_path = f\"output/{safe_name}.train\"\n",
        "    dev_path = f\"output/{safe_name}_dev.train\"\n",
        "\n",
        "    # Count existing words (if any)\n",
        "    word_counts[genre] = count_words_in_file(train_path)\n",
        "    dev_word_counts[genre] = count_words_in_file(dev_path)\n",
        "\n",
        "    # Open files in append mode\n",
        "    train_files[genre] = open(train_path, \"a\", encoding=\"utf-8\")\n",
        "    dev_files[genre] = open(dev_path, \"a\", encoding=\"utf-8\")\n",
        "\n",
        "# Process\n",
        "for genre, movies in genre_to_movies.items():\n",
        "    print(f\"\\nüìÇ Genre: {genre}\")\n",
        "\n",
        "    for movie in tqdm(movies):\n",
        "        imdb_id_raw = movie[\"imdb_id\"]        # e.g. tt0281176\n",
        "        imdb_id = imdb_id_raw.replace(\"tt\", \"\")  # e.g. 0281176\n",
        "        # Skip if already processed\n",
        "        if imdb_id_raw in used_imdb_ids[genre]:\n",
        "            continue\n",
        "        folder_path = f\"subtitles_cf/Sublens_20M/subtitles/{imdb_id}\"\n",
        "\n",
        "        if not os.path.isdir(folder_path):\n",
        "            not_found[genre].append((movie[\"title\"], imdb_id_raw))\n",
        "            continue\n",
        "\n",
        "        srt_files = [f for f in os.listdir(folder_path) if f.lower().endswith(\".srt\")]\n",
        "        if not srt_files:\n",
        "            not_found[genre].append((movie[\"title\"], imdb_id_raw))\n",
        "            continue\n",
        "\n",
        "        srt_path = os.path.join(folder_path, srt_files[0])\n",
        "\n",
        "        # Try multiple encodings for robustness\n",
        "        encodings_to_try = ['utf-8', 'iso-8859-1', 'cp1252', 'latin1']\n",
        "        subs = None\n",
        "\n",
        "        for enc in encodings_to_try:\n",
        "            try:\n",
        "                subs = pysrt.open(srt_path, encoding=enc)\n",
        "                break  # Success\n",
        "            except UnicodeDecodeError:\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error reading {imdb_id_raw} with encoding {enc}: {e}\")\n",
        "                break  # Skip on other errors\n",
        "\n",
        "        if subs is None:\n",
        "            not_found[genre].append((movie[\"title\"], imdb_id_raw))\n",
        "            continue\n",
        "\n",
        "        # Extract non-empty subtitle lines\n",
        "        lines = [sub.text.strip() for sub in subs if sub.text.strip()]\n",
        "\n",
        "\n",
        "        for line in lines:\n",
        "            num_words = len(line.split())\n",
        "            if num_words == 0:\n",
        "                continue\n",
        "\n",
        "            if word_counts[genre] < TARGET_WORDS:\n",
        "                train_files[genre].write(line + \"\\n\")\n",
        "                word_counts[genre] += num_words\n",
        "            elif dev_word_counts[genre] < DEV_TARGET_WORDS:\n",
        "                dev_files[genre].write(line + \"\\n\")\n",
        "                dev_word_counts[genre] += num_words\n",
        "\n",
        "            if word_counts[genre] >= TARGET_WORDS and dev_word_counts[genre] >= DEV_TARGET_WORDS:\n",
        "                break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Mark movie as used\n",
        "        used_imdb_ids[genre].add(imdb_id_raw)\n",
        "\n",
        "# Close files\n",
        "for f in train_files.values(): f.close()\n",
        "for f in dev_files.values(): f.close()\n",
        "\n",
        "# Save logs\n",
        "for genre, missing in not_found.items():\n",
        "    safe_name = safe_genre_name(genre)\n",
        "    with open(f\"logs/{safe_name}_missing.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for title, imdb in missing:\n",
        "            f.write(f\"{title} ({imdb})\\n\")\n",
        "\n",
        "# Save used_imdb_ids for next runs\n",
        "with open(\"logs/used_imdb_ids.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({g: list(ids) for g, ids in used_imdb_ids.items()}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"\\n‚úÖ Done. Logs saved. Word counts:\")\n",
        "\n",
        "# Final check\n",
        "for genre in genre_to_movies:\n",
        "    print(f\"{genre}: train={word_counts[genre]:,} words, dev={dev_word_counts[genre]:,} words\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vajKdA_pxC-m"
      },
      "outputs": [],
      "source": [
        "# Quick check if logs contain anything\n",
        "import glob\n",
        "\n",
        "for path in sorted(glob.glob(\"logs/*.txt\")):\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "        print(f\"{path}: {len(lines)} missing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "en2BZ1EfmcFu"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "print(\"\\nüìä Summary per genre (existing output files):\")\n",
        "\n",
        "existing_train_files = glob.glob(\"output/*.train\")\n",
        "genres_seen = set()\n",
        "\n",
        "for path in existing_train_files:\n",
        "    filename = os.path.basename(path)\n",
        "    if filename.endswith(\"_dev.train\"):\n",
        "        genre = filename.replace(\"_dev.train\", \"\").replace(\"_\", \" \")\n",
        "    else:\n",
        "        genre = filename.replace(\".train\", \"\").replace(\"_\", \" \")\n",
        "    genres_seen.add(genre)\n",
        "\n",
        "for genre in sorted(genres_seen):\n",
        "    safe_name = safe_genre_name(genre)\n",
        "    train_path = f\"output/{safe_name}.train\"\n",
        "    dev_path = f\"output/{safe_name}_dev.train\"\n",
        "    train_words = count_words_in_file(train_path)\n",
        "    dev_words = count_words_in_file(dev_path)\n",
        "\n",
        "    # Try to load missing subtitles log if exists\n",
        "    missing_path = f\"logs/{safe_name}_missing.txt\"\n",
        "    if os.path.exists(missing_path):\n",
        "        with open(missing_path, encoding=\"utf-8\") as f:\n",
        "            missing_lines = f.readlines()\n",
        "            missing_count = len(missing_lines)\n",
        "    else:\n",
        "        missing_count = \"?\"\n",
        "\n",
        "    print(f\"\\nüé¨ Genre: {genre}\")\n",
        "    print(f\"  ‚Ä¢ Words in train:      {train_words:,}\")\n",
        "    print(f\"  ‚Ä¢ Words in dev:        {dev_words:,}\")\n",
        "    print(f\"  ‚Ä¢ Subtitles missing:   {missing_count}\")\n",
        "\n",
        "    if train_words < TARGET_WORDS or dev_words < DEV_TARGET_WORDS:\n",
        "        print(\"  ‚ö†Ô∏è Dataset incomplete! Consider adding more subtitle sources.\")\n",
        "    else:\n",
        "        print(\"  ‚úÖ Dataset size is correct.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhkUF6LZevdk"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# 1. Clone the Repository (if required)\n",
        "# ========================================\n",
        "\n",
        "import os\n",
        "\n",
        "# Set this as environment variable before running:\n",
        "# export GITHUB_TOKEN=\"your_token_here\"\n",
        "TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
        "\n",
        "if not TOKEN:\n",
        "    raise ValueError(\"Please set GITHUB_TOKEN environment variable\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ha31UwL6H0Ql"
      },
      "outputs": [],
      "source": [
        "# Copy generated dataset files into the correct directory\n",
        "!cp /content/output/*.train datasets/open_subtitles/\n",
        "\n",
        "# List the contents to confirm files are in place\n",
        "!ls -lh datasets/open_subtitles\n",
        "\n",
        "!git config --global user.email \"your.email@example.com\"\n",
        "\n",
        "!git config --global user.name \"Your Name\n",
        "\n",
        "# Stage the new files for commit\n",
        "!git add datasets/open_subtitles/*.train\n",
        "\n",
        "# Commit the changes\n",
        "!git commit -m \"Add OpenSubtitles datasets: 1M train + 200K dev per genre\"\n",
        "\n",
        "# Push to GitHub using the token\n",
        "!git push"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwgdDg0qfRHJ"
      },
      "outputs": [],
      "source": [
        "# Clean and flatten .train files for spaCy sentence tokenization\n",
        "# This script loads TRAIN_PATH and DEV_PATH, removes ellipses, newlines,\n",
        "# and extra spaces, then rewrites the files as single-line texts.\n",
        "# Then it adds a harmless newline to force Git to detect a change and pushes to GitHub. ~~~\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "from datetime import datetime\n",
        "\n",
        "# Paths\n",
        "TRAIN_PATH = './datasets/open_subtitles/1M_science_fiction_subtitles.train'\n",
        "DEV_PATH = './datasets/open_subtitles/1M_science_fiction_subtitles_dev.train'\n",
        "repo_dir = \"/content/BabyLM-Tiny\"\n",
        "os.chdir(repo_dir)\n",
        "\n",
        "# Clean text\n",
        "def clean_text_for_spacy(text):\n",
        "    text = text.replace(\"...\\n...\", \" \")\n",
        "    text = text.replace(\"...\", \".\")\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    while \"  \" in text:\n",
        "        text = text.replace(\"  \", \" \")\n",
        "    return text.strip()\n",
        "\n",
        "# Overwrite files\n",
        "for path in [TRAIN_PATH, DEV_PATH]:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        raw_text = f.read()\n",
        "    cleaned = clean_text_for_spacy(raw_text)\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(cleaned)\n",
        "    print(f\"‚úÖ Cleaned and saved: {path}\")\n",
        "\n",
        "# Setup Git identity (if needed)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", \"your.email@example.com\"])\n",
        "\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", \"NikitaGoryetiy\"])\n",
        "\n",
        "# Show status\n",
        "print(\"\\nüìù Git status:\")\n",
        "subprocess.run([\"git\", \"status\"], text=True)\n",
        "\n",
        "# Show diff for confirmation\n",
        "print(\"\\nüîç Git diff:\")\n",
        "subprocess.run([\"git\", \"diff\", \"--\", TRAIN_PATH, DEV_PATH], text=True)\n",
        "\n",
        "# Check for changes\n",
        "status = subprocess.run([\"git\", \"status\", \"--porcelain\"], capture_output=True, text=True)\n",
        "if status.stdout.strip() == \"\":\n",
        "    print(\"‚ÑπÔ∏è No changes detected ‚Äî nothing to commit.\")\n",
        "else:\n",
        "    try:\n",
        "        subprocess.run([\"git\", \"add\", TRAIN_PATH, DEV_PATH], check=True)\n",
        "        subprocess.run([\"git\", \"commit\", \"-m\", \"Force commit: Cleaned and timestamped dataset files\"], check=True)\n",
        "\n",
        "        # Push with token\n",
        "        remote_url = f\"https://{TOKEN}@github.com/KatsuhitoArasaka/BabyLM-Tiny.git\"\n",
        "        subprocess.run([\"git\", \"push\", remote_url], check=True)\n",
        "        print(\"üöÄ Changes pushed to GitHub.\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Git operation failed:\\n{e.stderr}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNIphgyGbm8ke/nJZcOi9QK",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
