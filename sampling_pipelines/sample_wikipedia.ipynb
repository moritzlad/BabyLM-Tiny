{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Category Sampling Pipeline\n",
    "\n",
    "This notebook samples Wikipedia articles by category to create domain-specific datasets. It extracts text from Wikipedia articles within specified categories and creates balanced training/development splits.\n",
    "\n",
    "## Features\n",
    "- Category-based article collection\n",
    "- Recursive subcategory processing  \n",
    "- Word-limit based sampling\n",
    "- Train/dev split generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import random \n",
    "\n",
    "wikipedia = wikipediaapi.Wikipedia(user_agent='Academic Research Project', language='en')\n",
    "\n",
    "API_URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "wiki_html = wikipediaapi.Wikipedia(\n",
    "    user_agent='Academic Research Project',\n",
    "    language='en',\n",
    "    extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")\n",
    "\n",
    "def get_category_articles(category):\n",
    "    cat = wikipedia.page(f\"Category:{category}\")\n",
    "    return cat.categorymembers.values()\n",
    "\n",
    "def get_children_articles(children):\n",
    "    articles = []\n",
    "    for child in children:\n",
    "        child_articles = get_category_articles(child)  # Missing assignment\n",
    "        articles.extend(child_articles)  # Use extend instead of append\n",
    "    return articles\n",
    "\n",
    "def get_page_text(title):\n",
    "    page = wiki_html.page(title)\n",
    "    if page.exists():\n",
    "        return page.text\n",
    "    else:\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Wikipedia API Setup and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(articles, no_words):\n",
    "    text = \"\"\n",
    "    text_len = 0\n",
    "    children = []\n",
    "    counter = 0\n",
    "    for article in articles:\n",
    "        counter += 1\n",
    "        if counter % 50 == 0:\n",
    "            print(f\"Processed {counter} articles, current text length: {text_len} words.\")\n",
    "        if article.title.startswith(\"Category:\"):\n",
    "            children.append(article.title.replace(\"Category:\", \"\"))\n",
    "            continue\n",
    "        page_text = get_page_text(article.title)\n",
    "        if page_text:\n",
    "            text += page_text + \"\\n\\n \"\n",
    "            text_len += len(page_text.split())\n",
    "        if text_len >= no_words:\n",
    "            break\n",
    "    print(f\"Collected {len(text.split())} words from {len(articles)} articles.\")\n",
    "    return \" \".join(text.split()[:no_words]), get_children_articles(children)\n",
    "\n",
    "def create_dataset_from_category(category, name, no_words):\n",
    "    total_words = int(no_words * 1.2)\n",
    "    articles = list(get_category_articles(category))\n",
    "    random.seed(42)\n",
    "    text = \"\"\n",
    "    text_len = 0\n",
    "    while text_len < total_words and articles: \n",
    "        random.shuffle(articles)    \n",
    "        for i in range(0, len(articles), 20):\n",
    "            print(f\"Processing article: {articles[i].title})\")\n",
    "        print(f\"Current layer has {len(articles)} articles, total words collected: {text_len}\")   \n",
    "        article_texts, children_articles = get_texts(articles, total_words - text_len)\n",
    "        text += article_texts + \"\\n\\n \"\n",
    "        text_len += len(article_texts.split())\n",
    "        articles = children_articles\n",
    "        print(f\" Finished layer, moving further. Total words collected: {text_len}\")\n",
    "\n",
    "\n",
    "    with open(f\"../datasets/wiki_categories/{name}.train\", 'w') as f:\n",
    "        train_text = \" \".join(text.split()[:no_words])\n",
    "        print(f\"Total words: {len(train_text.split())}\")\n",
    "        f.write(train_text)\n",
    "    with open(f\"../datasets/wiki_categories/{name}_dev.train\", 'w') as f:\n",
    "        train_text = \" \".join(text.split()[no_words:total_words])\n",
    "        print(f\"Total dev words: {len(train_text.split())}\")\n",
    "        f.write(train_text)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset_from_category(\"Quantum mechanics\", \"wiki_subfields_of_physics\", 1_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Category-Specific Dataset Generation\n",
    "\n",
    "Examples of creating 1M-word datasets from different Wikipedia categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset_from_category(\"History\", \"wiki_history\", 1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset_from_category(\"Culture\", \"wiki_culture\", 1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset_from_category(\"Society\", \"wiki_society\", 1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset_from_category(\"Linguistics\", \"wiki_linguistics\", 1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
